path,Content,Summary,QnA
Financial_chatbot/Code/0. text_pre_process.py,"def split_into_sentences(text, words_per_sentence=10):
    words = text.split()
    sentences = []

    for i in range(0, len(words), words_per_sentence):
        sentence = ' '.join(words[i:i + words_per_sentence])
        sentences.append(sentence)
        
    return '\n\n'.join(sentences)

with open('path_to_your_input_file', 'r') as file:
    content = file.read()

modified_content = split_into_sentences(content)

with open('path_to_your_output_file', 'w') as file:
    file.write(modified_content)

print(""Text has been split into sentences with 10 words each, and blank lines have been added."")
","This Python code snippet takes a text file as input and splits it into sentences, with each sentence containing a maximum of 10 words. It then writes the modified text to a new output file.

**Here's a breakdown of the code:**

1. **`split_into_sentences(text, words_per_sentence=10)` function:**
   - Takes two arguments: `text` (the input text) and `words_per_sentence` (defaulting to 10).
   - Splits the `text` into individual words using `text.split()`.
   - Iterates through the words in chunks of `words_per_sentence`.
   - For each chunk, it joins the words back into a string (`' '.join(...)`) and appends it to the `sentences` list.
   - Finally, it joins all the sentences in the `sentences` list with blank lines (`'\n\n'.join(...)`) and returns the modified text.

2. **File Handling:**
   - Opens the input file specified by `path_to_your_input_file` in read mode (`'r'`).
   - Reads the entire content of the file into the `content` variable.
   - Calls the `split_into_sentences()` function to split the `content`.

3. **Output:**
   - Opens a new file specified by `path_to_your_output_file` in write mode (`'w'`).
   - Writes the modified text obtained from `split_into_sentences()` to the output file.
   - Prints a confirmation message indicating that the process is complete.

**Key Points:**

- **Sentence Segmentation:** This code uses a simple heuristic for sentence segmentation based on word count. It may not always be accurate, as real sentences can have varying lengths and structures. More sophisticated techniques like using punctuation marks or machine learning models are generally needed for better accuracy.
- **File Paths:** Make sure to replace `path_to_your_input_file` and `path_to_your_output_file` with the actual paths to your input and output files, respectively.



Let me know if you have any other questions!","Here are 3-5 insightful questions with brief answers based on the provided content and summary:

**Question: What is the primary function of the `split_into_sentences` method in this code?**
Answer: The primary function of the `split_into_sentences` method is to split a given text into sentences, with each sentence containing a maximum of 10 words.

**Question: How does the code handle file input and output?**
Answer: The code reads the content from an input file specified by `path_to_your_input_file` and writes the modified text to an output file specified by `path_to_your_output_file`.

**Question: What is the potential limitation of the sentence segmentation heuristic used in this code?**
Answer: The simple heuristic based on word count may not always accurately identify sentence boundaries, as real sentences can have varying lengths and structures.

**Question: Why might more sophisticated techniques be needed for better accuracy in sentence segmentation?**
Answer: More sophisticated techniques like using punctuation marks or machine learning models are generally needed because they can take into account the nuances of language and produce more accurate results."
Financial_chatbot/Code/1. pdf_to_csv.py,"import PyPDF2
import csv

def extract_text_from_pdf(pdf_file_path):
    pdf_reader = PyPDF2.PdfReader(pdf_file_path)
    pages_content = []

    for page_num in range(len(pdf_reader.pages)):
        page = pdf_reader.pages[page_num]
        text = page.extract_text()
        pages_content.append(text)
    
    return pages_content

def save_pages_to_csv(pages_content, output_csv_file):
    with open(output_csv_file, 'w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow([""Page"", ""Content""]) 
        for i, content in enumerate(pages_content):
            writer.writerow([i + 1, content])

if __name__ == ""__main__"":
    input_pdf_file = '.pdf'  # Replace with your PDF file path
    output_csv_file = '.csv'  # Replace with your desired output CSV file path
    
    pages_content = extract_text_from_pdf(input_pdf_file)
    save_pages_to_csv(pages_content, output_csv_file)

    print(f""Content of each page has been extracted and saved to {output_csv_file}"")
","## Explanation of the Python Code:

This code snippet is designed to extract text from a PDF file and save it into a CSV (Comma Separated Values) file. 

**Here's a breakdown:**

1. **Imports:**
   - `PyPDF2`: This library is used for reading and extracting text from PDF files.
   - `csv`: This library is used for working with CSV files, allowing us to write data into them.

2. **`extract_text_from_pdf(pdf_file_path)` function:**
   - Takes the path to your PDF file as input (`pdf_file_path`).
   - Creates a `PdfReader` object from the provided PDF file path using `PyPDF2.PdfReader()`. This object allows you to access information about the PDF.
   - Initializes an empty list `pages_content` to store the extracted text from each page.
   - Iterates through each page of the PDF using a `for` loop (ranging from 0 to the total number of pages).
     - For each page, it extracts the text content using `page.extract_text()` and appends it to the `pages_content` list.
   - Finally, returns the `pages_content` list containing the extracted text from all pages.

3. **`save_pages_to_csv(pages_content, output_csv_file)` function:**
   - Takes two inputs:
     - `pages_content`: The list of page content extracted by `extract_text_from_pdf`.
     - `output_csv_file`: The path to the desired CSV file where you want to save the data.
   - Opens the specified CSV file in write mode (`'w'`) with UTF-8 encoding using `with open(...)`. This ensures proper handling of special characters and automatic file closure.
   - Creates a `csv.writer` object associated with the opened file, which will handle writing data in CSV format.
   - Writes a header row ""Page"", ""Content"" to the CSV file using `writer.writerow()`.
   - Iterates through the `pages_content` list and for each page number (`i`) and its corresponding content, writes a new row to the CSV file with the page number and text content.

4. **`if __name__ == ""__main__"":` block:**
   - This block only executes when the script is run directly (not imported as a module).
   - Sets placeholders for `input_pdf_file` and `output_csv_file`. You need to replace these with the actual paths to your PDF file and desired CSV file.
   - Calls `extract_text_from_pdf()` to get the text content from the PDF.
   - Calls `save_pages_to_csv()` to write the extracted text into the specified CSV file.
   - Prints a confirmation message indicating the completion of the process and the name of the output CSV file.



Let me know if you have any more questions!","Here are 3-5 insightful questions with brief, informative answers based on the provided content and summary:

**Question: What is the purpose of using `PyPDF2` library in this code?**
Answer: The `PyPDF2` library is used to extract text from PDF files. It creates a `PdfReader` object that allows us to access information about the PDF, including the ability to extract text from each page.

**Question: How does the `extract_text_from_pdf()` function handle multiple pages in the PDF?**
Answer: The `extract_text_from_pdf()` function iterates through each page of the PDF using a `for` loop and extracts the text content from each page using `page.extract_text()`. It stores the extracted text in a list, allowing for easy access to the text from all pages.

**Question: What encoding is used when writing data to the CSV file?**
Answer: The code uses UTF-8 encoding when opening the CSV file in write mode (`'w', newline='', encoding='utf-8')`. This ensures proper handling of special characters and automatic file closure."
Financial_chatbot/Code/2.1 csv_summarizer.py,"import pandas as pd
import openai
import logging
import time

# Setup logging
logging.basicConfig(level=logging.INFO)

API_BASE_URL = ""https://llama.us.gaianet.network/v1""
MODEL_NAME = ""llama""
API_KEY = ""GAIA""

client = openai.OpenAI(base_url=API_BASE_URL, api_key=API_KEY)

def summarize_text(text):
    try:
        start_time = time.time()
        response = client.chat.completions.create(
            messages=[
                {
                    ""role"": ""system"",
                    ""content"": ""You are an expert financial analyst. You have been given a 10-Q quarterly report of a public company. Your tasks are: Summarization: Read the context provided from the 10-Q report and create a concise, coherent summary that captures the key points about the company's financial performance, risks, opportunities, and future outlook. The summary should be written in a way that is easy to understand by stakeholders who may not have a financial background. Question Generation: After creating the summary, generate 3-5 insightful questions based on the content of the report. These questions should help stakeholders further explore key issues, risks, or trends that were mentioned in the summary.Answer Generation: Provide brief, informative answers to the questions you generated. These answers should be based on the content of the report, using the context you have summarized."",
                },
                {
                    ""role"": ""user"",
                    ""content"": text,
                }
            ],
            model=MODEL_NAME,
            stream=False,
        )
        logging.info(f""API call took {time.time() - start_time} seconds."")
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f""Error in summarizing text: {e}"")
        return ""Error: Could not summarize""

def summarize_csv_content(input_csv_file, output_csv_file):
    try:
        df = pd.read_csv(input_csv_file)
        if 'Content' not in df.columns:
            raise ValueError(""'Content' column not found in the input CSV file."")

        logging.info(""Starting summarization..."")
        df['summary'] = df['Content'].apply(lambda x: summarize_text(x) if pd.notnull(x) else """")

        df.to_csv(output_csv_file, index=False)
        logging.info(f""Summaries have been generated and saved to {output_csv_file}"")
    except Exception as e:
        logging.error(f""Error processing CSV: {e}"")

if __name__ == ""__main__"":
    input_csv_file = '/home/aru/Desktop/LFX_test/Financial_chatbot/Dataset/CSV/out.csv'  
    output_csv_file = '/home/aru/Desktop/LFX_test/Financial_chatbot/Dataset/CSV/summary_qna.csv' 

    summarize_csv_content(input_csv_file, output_csv_file)
","## Code Breakdown: Financial CSV Summarizer

This Python script utilizes OpenAI's Llama model to summarize financial reports extracted from a CSV file. 

Here's a step-by-step breakdown:

**1. Initialization and Dependencies:**
   - Imports necessary libraries: `pandas`, `openai`, `logging`, and `time`.
   - Sets up logging with basic information level.
   - Defines API base URL, model name (`llama`), and API key (`GAIA`).
   - Initializes an OpenAI client using the specified credentials.

**2. Summarization Function:**
   - `summarize_text(text)`: This function takes a text string (presumably a financial report excerpt) as input.
     - Measures the execution time of the summarization process.
     - Constructs a chat prompt for the Llama model, instructing it to summarize the provided text and generate insightful questions and answers based on the summary.
     - Sends the prompt to the OpenAI API using `client.chat.completion.create()`.
     - Extracts the generated summary from the API response and returns it.
     - Logs the time taken for the API call.
     - Handles exceptions during summarization and returns an error message if encountered.

**3. CSV Processing Function:**
   - `summarize_csv_content(input_csv_file, output_csv_file)`: This function takes input and output CSV file paths as arguments.
     - Reads the input CSV file into a pandas DataFrame (`df`).
     - Checks if the 'Content' column exists in the DataFrame; if not, it raises a ValueError.
     - Applies the `summarize_text` function to each non-null value in the 'Content' column using `apply()`. The generated summaries are stored in a new column named 'summary'.
     - Saves the modified DataFrame (including the added 'summary' column) to the specified output CSV file.
     - Logs messages indicating the start and completion of the summarization process.

**4. Main Execution:**
   - Defines paths for input and output CSV files.
   - Calls `summarize_csv_content` to perform the summarization and save the results to the output CSV file.



This script essentially automates the process of summarizing financial reports extracted from a CSV file, leveraging OpenAI's Llama model for generating concise summaries and insightful questions based on the content. The use of pandas makes it easy to handle tabular data, while logging ensures that the execution process is transparent.","Here are 3-5 insightful questions and brief, informative answers based on the provided content:

Question: What is the primary purpose of the OpenAI client initialization in the script?
Answer: The OpenAI client is initialized to utilize the Llama model for natural language processing tasks such as text summarization.

Question: How does the `summarize_text` function handle exceptions during the summarization process?
Answer: If an exception occurs, it logs the error message and returns a generic error response indicating that the text could not be summarized.

Question: What is the purpose of applying the `apply` method to the 'Content' column in the `summarize_csv_content` function?
Answer: This is used to apply the `summarize_text` function to each non-null value in the 'Content' column, generating a summary for each text snippet.

Question: Why does the script check if the 'Content' column exists in the input CSV file?
Answer: It ensures that the required data (text snippets) is present in the input CSV file before attempting to summarize it."
Financial_chatbot/Code/2.2 summary_qna.py,"import pandas as pd
import openai
import logging
import time

logging.basicConfig(level=logging.INFO)

API_BASE_URL = ""https://llama.us.gaianet.network/v1""
MODEL_NAME = ""llama""
API_KEY = ""GAIA""

client = openai.OpenAI(base_url=API_BASE_URL, api_key=API_KEY)

def generate_qna(text):
    try:
        start_time = time.time()
        response = client.chat.completions.create(
            messages=[
                {
                    ""role"": ""system"",
                    ""content"": ""You are an expert financial analyst. Your task is to generate 3-5 insightful questions and provide brief, informative answers based on the provided text. The questions and answers should help stakeholders further explore key issues, risks, or trends mentioned in the text. Provide the output in the following format: For each question, provide the corresponding answer."",
                },
                {
                    ""role"": ""user"",
                    ""content"": text,
                }
            ],
            model=MODEL_NAME,
            stream=False,
        )
        logging.info(f""API call took {time.time() - start_time} seconds."")
        
        raw_content = response.choices[0].message['content'].strip()
        logging.info(f""Raw response content: {raw_content}"")
        
        qna_pairs = []
        for line in raw_content.split('\n'):
            if 'Answer: ' in line:
                question_part = line.split('Answer: ')[0].strip()
                answer_part = line.split('Answer: ')[1].strip()
                if 'Question: ' in question_part:
                    question = question_part.replace('Question: ', '').strip()
                    qna_pairs.append((question, answer_part))
        
        questions, answers = zip(*qna_pairs) if qna_pairs else ([], [])
        
        return list(questions), list(answers)
    except Exception as e:
        logging.error(f""Error in generating Q&A: {e}"")
        return [], []

def generate_qna_csv(input_csv_file, output_csv_file):
    try:
        df = pd.read_csv(input_csv_file)
        if 'Content' not in df.columns:
            raise ValueError(""'Content' column not found in the input CSV file."")

        questions_list = []
        answers_list = []

        logging.info(""Starting Q&A generation..."")
        for index, row in df.iterrows():
            if pd.notnull(row['Content']):
                questions, answers = generate_qna(row['Content'])
                for q, a in zip(questions, answers):
                    questions_list.append(q)
                    answers_list.append(a)

        qna_df = pd.DataFrame({
            'Question': questions_list,
            'Answer': answers_list
        })

        qna_df.to_csv(output_csv_file, index=False)
        logging.info(f""Q&A have been generated and saved to {output_csv_file}"")
    except Exception as e:
        logging.error(f""Error processing CSV: {e}"")

if __name__ == ""__main__"":
    input_csv_file = '/home/aru/Desktop/LFX_test/Financial_chatbot/Dataset/CSV/test_summary.csv'  
    output_csv_file = '/home/aru/Desktop/LFX_test/Financial_chatbot/Dataset/CSV/qna.csv' 

    generate_qna_csv(input_csv_file, output_csv_file)
","## Summary of the Code

This Python code generates question-and-answer (Q&A) pairs from financial text data provided in a CSV file. It utilizes the openai library to interact with the Llama language model hosted on Gaianet. 

Here's a breakdown:

**1. Initialization:**

* Sets up logging for tracking program execution.
* Defines API base URL, model name (Llama), and API key for interacting with the Gaianet API.
* Creates an OpenAI client object to communicate with the API.

**2. `generate_qna(text)` function:**

* Takes a text string as input.
* Sends a request to the Llama model, instructing it to generate 3-5 Q&A pairs based on the provided text. The prompt guides the model to act as a financial analyst and focus on key issues, risks, or trends.
* Processes the raw response from the API, extracting the generated questions and answers in a structured format.
* Returns a list of questions and a list of corresponding answers.

**3. `generate_qna_csv(input_csv_file, output_csv_file)` function:**

* Reads a CSV file containing financial text data (assuming a column named ""Content"").
* Iterates through each row in the CSV, extracts the content, and calls the `generate_qna` function to get Q&A pairs.
* Appends the generated questions and answers to separate lists.
* Creates a new DataFrame from these lists and saves it as a new CSV file.

**4. Main Execution:**

* Defines the input and output CSV file paths.
* Calls `generate_qna_csv` to process the data and generate the Q&A pairs.



**Overall, this code automates the process of extracting insights from financial text by leveraging a powerful language model. It allows users to obtain concise questions and answers that highlight important aspects of their financial data.**","Here are 3-5 insightful questions and brief, informative answers based on the provided content:

**Question:** What is the primary function of the `generate_qna` function in this code?
**Answer:** The `generate_qna` function takes a text string as input, sends a request to the Llama model, processes the raw response from the API, and returns a list of questions and corresponding answers.

**Question:** How does the code ensure that the generated Q&A pairs are relevant to financial analysis?
**Answer:** The prompt used in the `generate_qna` function instructs the Llama model to act as an expert financial analyst and focus on key issues, risks, or trends mentioned in the text.

**Question:** What is the purpose of the `generate_qna_csv` function?
**Answer:** The `generate_qna_csv` function reads a CSV file containing financial text data, iterates through each row, extracts the content, generates Q&A pairs using the `generate_qna` function, and saves them as a new CSV file.

**Question:** How does the code handle errors during the execution of the `generate_qna` or `generate_qna_csv` functions?
**Answer:** The code uses try-except blocks to catch any exceptions that may occur during the execution of these functions and logs the error messages for further investigation.

**Question:** What is the output format of the generated Q&A pairs in the CSV file?
**Answer:** The output format consists of two columns: ""Question"" and ""Answer"", where each row represents a single question-answer pair."
Financial_chatbot/Code/3.1 qna_generate.py,"import openai
import csv
import sys
import os

API_BASE_URL = ""https://llama.us.gaianet.network/v1""
MODEL_NAME = ""llama""
API_KEY = ""GAIA""

def qgen(source_text):
    client = openai.OpenAI(base_url=API_BASE_URL, api_key=API_KEY)

    chat_completion = client.chat.completions.create(
        messages=[
            {
                ""role"": ""system"",
                ""content"": ""Respond with a list of 5 to 10 questions. The text in the user message must contain specific answers to each question. Each question must be complete without references to unclear context such as \""this team\"" or \""that lab\"". Each question must be on its own line. Just list the questions without any introductory text or numbers."",
            },
            {
                ""role"": ""user"",
                ""content"": source_text,
            }
        ],
        model=MODEL_NAME,
        stream=False,
    )
    return chat_completion.choices[0].message.content

def agen(source_text, question):
    client = openai.OpenAI(base_url=API_BASE_URL, api_key=API_KEY)

    chat_completion = client.chat.completions.create(
        messages=[
            {
                ""role"": ""system"",
                ""content"": ""Give a comprehensive and well-reasoned answer to the user question strictly based on the context below.\n"" + source_text,
            },
            {
                ""role"": ""user"",
                ""content"": question,
            }
        ],
        model=MODEL_NAME,
        stream=False,
    )
    return chat_completion.choices[0].message.content

def main():
    # Input and output file paths
    input_file_path = '/home/aru/Desktop/LFX_test/Financial_chatbot/Dataset/CSV/test.csv'
    output_file_path = '/home/aru/Desktop/LFX_test/Financial_chatbot/Dataset/CSV/qna_test.csv'
    
    results = []

    with open(input_file_path, 'r', newline='') as csvfile:
        csv_reader = csv.DictReader(csvfile)
        for row in csv_reader:
            page_content = row['Content']

            qs = qgen(page_content)
            for q in qs.splitlines():
                if len(q.strip()) == 0:
                    continue

                answer = agen(page_content, q)
                result = {
                    'Question': q,
                    'Answer': answer
                }
                results.append(result)

    with open(output_file_path, 'w', newline='') as csvfile:
        fieldnames = ['Question', 'Answer']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()
        for row in results:
            writer.writerow(row)

if __name__ == ""__main__"":
    main()
","## Breakdown of the Python Code: Financial Chatbot Q&A Generation

This Python code builds a simple financial chatbot by generating questions and answers from text input. 

Here's a breakdown:

**1. Libraries:**

- **`openai`**: This library is used to interact with OpenAI's API, specifically the ""llama"" model for generating text.
- **`csv`**: Used for reading and writing CSV files, essential for handling the dataset.
- **`sys` & `os`**: These libraries provide system-specific functionalities, although they are not directly used in the core logic of the code.

**2. API Configuration:**

- **`API_BASE_URL`, `MODEL_NAME`, `API_KEY`**: These variables store the connection details for the OpenAI API, including the base URL, chosen model (""llama""), and your unique API key (replace ""GAIA"" with your actual key).


**3. Functions:**

- **`qgen(source_text)`**:
    - Takes a text input (`source_text`).
    - Uses the OpenAI API to generate 5-10 questions based on the provided text.
    - Each question is designed to have a specific answer within the input text and is formatted as a separate line.

- **`agen(source_text, question)`**:
    - Takes the same `source_text` and a specific `question`.
    - Uses the OpenAI API to generate a comprehensive answer to the question based solely on the provided context.

**4. Main Function (`main`)**:

- Defines input and output file paths for the CSV dataset.
- Opens the input CSV file and iterates through each row (representing a document).
- For each document:
    - Extracts the content (`page_content`).
    - Calls `qgen` to get a list of questions from the content.
    - Iterates through each generated question and calls `agen` to get an answer based on the context.
    - Stores the question and answer as a dictionary in the `results` list.

- Opens the output CSV file and writes the `results` into it, creating a new CSV with Q&A pairs.



**Overall:** This code effectively demonstrates how to leverage OpenAI's API to build a rudimentary chatbot capable of extracting questions and answers from financial text data. It showcases the potential of using AI for automating tasks like question answering and generating question lists from existing text sources.","Here are 3-5 insightful questions and brief answers based on the code analysis:

**Question:** What is the purpose of the `qgen` function, and how does it generate questions from the input text?
**Answer:** The `qgen` function generates a list of questions from the input text by using the OpenAI API to create a chat completion with the input text as the user message. It instructs the model to respond with a list of 5-10 questions, each on its own line, without any introductory text or numbers.

**Question:** What is the role of the `agen` function in the code?
**Answer:** The `agen` function takes a specific question and uses the OpenAI API to generate a comprehensive answer based solely on the provided context. It creates a chat completion with the input text as the system message and the question as the user message, instructing the model to provide an answer.

**Question:** How does the code handle the output of generated questions and answers?
**Answer:** The code stores each generated question and answer pair as a dictionary in the `results` list. It then writes this list into the output CSV file, creating a new CSV with Q&A pairs.

**Question:** What libraries are used in the code to interact with OpenAI's API and handle CSV files?
**Answer:** The code uses the `openai` library to interact with OpenAI's API and generate text, and the `csv` library to read and write CSV files. Additionally, it uses `sys` and `os` libraries for system-specific functionalities.

**Question:** How is the OpenAI API configured in the code?
**Answer:** The code stores the connection details for the OpenAI API in variables: `API_BASE_URL`, `MODEL_NAME`, and `API_KEY`. These variables are used to create an instance of the OpenAI client, which interacts with the API to generate text."
Financial_chatbot/Code/3.2 qna_format.py,"import csv

def convert_csv_to_output(input_file, output_file):
    with open(input_file, 'r') as csvfile:
        reader = csv.reader(csvfile)
        next(reader)

        with open(output_file, 'w') as outfile:
            for row in reader:
                column_1 = row[0]
                column_2 = row[1]
                outfile.write(f""<SFT><s>[INST] <<SYS>>\n You are a financial assistant. Provide detailed, clear, and professional financial insights, and ensure that your responses are easy to understand. \n<</SYS>>\n\n {column_1} [/INST] {column_2} \n"")

# Testing the function
input_file = '/home/aru/Desktop/LFX_test/Financial_chatbot/Dataset/CSV/qna_test.csv'  
output_file = '/home/aru/Desktop/LFX_test/Financial_chatbot/Dataset/Text/test.txt' 
convert_csv_to_output(input_file, output_file)
","This Python code snippet is designed to take a CSV file containing question-answer pairs and convert it into a specific format suitable for training a chatbot, particularly one focused on financial insights. 

Here's a breakdown:

**1. Importing the `csv` Module:**
   -  `import csv`: This line imports the `csv` module, which provides tools for reading and writing CSV (Comma Separated Values) files.

**2. The `convert_csv_to_output` Function:**

   -  `def convert_csv_to_output(input_file, output_file):`: This defines a function named `convert_csv_to_output` that takes two arguments:
      - `input_file`: The path to the CSV file containing the question-answer pairs.
      - `output_file`: The path where the converted output file will be saved.

   -  `with open(input_file, 'r') as csvfile:`: Opens the input CSV file in read mode (`'r'`) and assigns it to the variable `csvfile`. The `with` statement ensures that the file is automatically closed even if an error occurs.
   -  `reader = csv.reader(csvfile):`: Creates a `csv.reader` object to iterate over the rows of the CSV file.

   -  `next(reader)`: Skips the first row of the CSV, assuming it contains headers. 

   -  `with open(output_file, 'w') as outfile:`: Opens the output file in write mode (`'w'`) and assigns it to `outfile`. This will create a new file or overwrite an existing one with the same name.
   -  `for row in reader:`: Loops through each remaining row in the CSV file.
      - `column_1 = row[0]`: Extracts the first column (question) from the current row and stores it in `column_1`.
      - `column_2 = row[1]`: Extracts the second column (answer) and stores it in `column_2`.
      - `outfile.write(f""<SFT><s>[INST] <<SYS>>\n You are a financial assistant. Provide detailed, clear, and professional financial insights, and ensure that your responses are easy to understand. \n<</SYS>>\n\n {column_1} [/INST] {column_2} \n"")`: Writes the formatted question-answer pair to the output file. The formatting includes:
         - `<SFT>` and `</SFT>` tags (possibly for specific chatbot frameworks).
         - `[INST]` and `[/INST]` tags (might indicate instruction or intent).
         -  `<<SYS>>` and `<</SYS>>`: Possibly delimiting system messages.

**3. Testing the Function:**

   - The lines after the function definition set the paths for the input CSV file (`qn a_test.csv`) and the output text file (`test.txt`).
   - `convert_csv_to_output(input_file, output_file)`: Calls the function to perform the conversion.



**In essence, this code prepares data from a CSV file into a structured format suitable for training a financial chatbot.** 

Let me know if you have any more questions or want to explore specific aspects in more detail!","Here are 3-5 insightful questions with brief, informative answers:

**Question:** What is the purpose of skipping the first row in the CSV file using `next(reader)`?
**Answer:** The first row of the CSV file likely contains headers or metadata that do not need to be included in the output. Skipping this row ensures that only the relevant question-answer pairs are processed.

**Question:** What is the significance of the formatting used when writing the question-answer pair to the output file?
**Answer:** The formatting, including `<SFT>` and `</SFT>`, `[INST]` and `[/INST]`, `<<SYS>>` and `<</SYS>>`, likely indicates specific instructions or metadata for the chatbot framework. This allows the chatbot to understand the context and intent behind each question.

**Question:** What is the implication of using a `csv.reader` object to iterate over the rows of the CSV file?
**Answer:** Using a `csv.reader` object makes it easy to work with the CSV data, as it automatically handles tasks such as handling missing or malformed values. This approach also ensures that the code remains flexible and adaptable to different types of CSV files.

**Question:** How might this code be modified to accommodate CSV files with more than two columns?
**Answer:** To modify the code for CSV files with more than two columns, you would need to adjust the indexing used when extracting column values (e.g., `column_1 = row[0]` and `column_2 = row[1]`). You could use a loop or other approach to handle multiple columns dynamically.

**Question:** What benefits might this code provide in terms of data preprocessing for chatbot training?
**Answer:** This code streamlines the process of preparing question-answer pairs from a CSV file, making it easier to integrate with chatbot development workflows. By automating the conversion process, developers can save time and focus on other aspects of building their chatbots."
Financial_chatbot/Code/data_scrap.py,"from pathlib import Path
from typing import List, Optional
import itertools
import pdfkit
from fire import Fire
from sec_edgar_downloader import Downloader

DEFAULT_OUTPUT_DIR = ""data/""
DEFAULT_CIKS = [
    ""0001018724"",  # AMZN
]
DEFAULT_FILING_TYPES = [
    ""10-K"",
    ""10-Q"",
]

COMPANY_NAME = ""Your Company Name""
EMAIL = ""your-email@example.com""

def filing_exists(cik: str, filing_type: str, output_dir: str) -> bool:
    data_dir = Path(output_dir) / ""sec-edgar-filings""
    filing_dir = data_dir / cik / filing_type
    return filing_dir.exists()

def _download_filing(
    cik: str, filing_type: str, output_dir: str, limit=None, before=None, after=None
):
    dl = Downloader(COMPANY_NAME, EMAIL, output_dir)
    dl.get(filing_type, cik, limit=limit, before=before, after=after, download_details=True)

def _convert_to_pdf(output_dir: str):
    data_dir = Path(output_dir) / ""sec-edgar-filings""

    for cik_dir in data_dir.iterdir():
        for filing_type_dir in cik_dir.iterdir():
            for filing_dir in filing_type_dir.iterdir():
                filing_doc = filing_dir / ""primary-document.html""
                filing_pdf = filing_dir / ""primary-document.pdf""
                if filing_doc.exists() and not filing_pdf.exists():
                    print(f""- Converting {filing_doc}"")
                    input_path = str(filing_doc.absolute())
                    output_path = str(filing_pdf.absolute())
                    try:
                        options = {'enable-local-file-access': None}
                        pdfkit.from_file(input_path, output_path, options=options, verbose=True)
                        filing_doc.unlink()
                    except Exception as e:
                        print(f""Error converting {input_path} to {output_path}: {e}"")

def main(
    output_dir: str = DEFAULT_OUTPUT_DIR,
    ciks: List[str] = DEFAULT_CIKS,
    file_types: List[str] = DEFAULT_FILING_TYPES,
    before: Optional[str] = None,
    after: Optional[str] = None,
    limit: Optional[int] = 3,
    convert_to_pdf: bool = True,
):
    print(f'Downloading filings to ""{Path(output_dir).absolute()}""')
    print(f""File Types: {file_types}"")
    
    if convert_to_pdf:
        if pdfkit.configuration().wkhtmltopdf is None:
            raise Exception(
                ""ERROR: wkhtmltopdf (https://wkhtmltopdf.org/) not found, ""
                ""please install it to convert html to pdf ""
                ""`sudo apt-get install wkhtmltopdf`""
            )
    
    for symbol, file_type in itertools.product(ciks, file_types):
        try:
            if filing_exists(symbol, file_type, output_dir):
                print(f""- Filing for {symbol} {file_type} already exists, skipping"")
            else:
                print(f""- Downloading filing for {symbol} {file_type}"")
                _download_filing(symbol, file_type, output_dir, limit, before, after)
        except Exception as e:
            print(f""Error downloading filing for symbol={symbol} & file_type={file_type}: {e}"")

    if convert_to_pdf:
        print(""Converting html files to pdf files and deleting the html files"")
        _convert_to_pdf(output_dir)

if __name__ == ""__main__"":
    Fire(main)
","## Analysis of Code Snippet:

This code snippet is designed to download SEC filings from the Edgar database and optionally convert them from HTML to PDF format. 

**Here's a breakdown:**

1. **Imports:**
    - `pathlib`: Used for working with file paths in a more object-oriented way.
    - `typing`: Provides type hinting for better code readability and maintainability.
    - `itertools`:  Used for efficient iteration over combinations of cik and filing types.
    - `pdfkit`: A library used to convert HTML to PDF.
    - `fire`: A command-line interface library that allows the function `main` to be executed as a CLI tool.
    - `sec_edgar_downloader`: A library for downloading SEC filings from Edgar (not included in the snippet).

2. **Constants:**
    - `DEFALUT_OUTPUT_DIR`, `DEFAULT_CIKS`, and `DEFAULT_FILING_TYPES`: These variables define default values for output directory, CIKs (company identifiers), and filing types to download. 

3. **Functions:**
    - `filing_exists`: Checks if a specific filing already exists in the specified directory.
    - `_download_filing`: Downloads SEC filings using the `sec_edgar_downloader` library.
    - `_convert_to_pdf`: Converts HTML files to PDF using `pdfkit`.

4. **Main Function (`main`)**:
    - Takes command-line arguments for output directory, CIKs, filing types, date ranges, download limit, and whether to convert to PDF.
    - Prints messages about the process.
    - Iterates through combinations of CIKs and filing types using `itertools`. 
    - Downloads filings if they don't already exist.
    - Converts HTML files to PDF if requested.


**Improvements:**

* **Error Handling:** More robust error handling could be added, especially when downloading and converting files.  
* **Logging:** Incorporate a logging system for better tracking of the process and potential issues.
* **Progress Indicators:** Show progress during the download and conversion processes.
* **Configuration File:** Consider using a configuration file to store settings like output directory, CIKs, and filing types instead of hardcoding them. 



**Overall, this code provides a good foundation for downloading and converting SEC filings. By implementing the suggested improvements, you can make it more robust, maintainable, and user-friendly.**","Here are 3-5 insightful questions with brief, informative answers:

**Q1: What is the purpose of the `filing_exists` function in this code?**
Answer: The `filing_exists` function checks if a specific filing already exists in the specified directory before downloading it.

**Q2: How does the code handle errors when converting HTML files to PDF using `pdfkit`?**
Answer: If an error occurs during the conversion process, the code catches the exception and prints an error message indicating which file failed to convert.

**Q3: What library is used for downloading SEC filings from Edgar in this code snippet?**
Answer: The `sec_edgar_downloader` library is used for downloading SEC filings from Edgar."
Financial_chatbot/Code/summarizer_openai.py,"from openai import OpenAI

client = OpenAI(api_key=""your_openai_api_key_here"")

def read_text_file(file_path):
    with open(file_path, 'r') as file:
        return file.read()

def generate_summary(text, model=""gpt-4o-mini"", max_tokens=150):
    completion = client.chat.completions.create(
        model=model,
        messages=[
            {""role"": ""system"", ""content"": ""You are an expert in analysing 10-Q reports and summarizes the content of those reports.""},
            {""role"": ""user"", ""content"": text},
        ]
    )
    
    summary = completion.choices[0].message['content'].strip()
    return summary


def main(file_path):
    text = read_text_file(file_path)
    summary = generate_summary(text)
    print(""Summary:"")
    print(summary)

if __name__ == ""__main__"":
    input_file_path = ""your_text_file.txt"" 
    main(input_file_path)

","This Python code utilizes the OpenAI API to generate a concise summary of text content from a given file. Let's break down its functionality step-by-step:

**1. Initialization:**
   - `from openai import OpenAI`: Imports the necessary library for interacting with the OpenAI API.
   - `client = OpenAI(api_key=""your_openai_api_key_here"")`:  Creates an OpenAI client object, requiring you to replace `""your_openai_api_key_here""` with your actual API key obtained from OpenAI.

**2. File Reading:**
   - `def read_text_file(file_path):`: Defines a function to read the entire contents of a text file specified by its path and return it as a single string.

**3. Summary Generation:**
   - `def generate_summary(text, model=""gpt-4o-mini"", max_tokens=150):`: This is the core function for generating summaries. 
     - It takes the input text, a specified OpenAI model (`gpt-4o-mini` by default), and a maximum number of tokens for the generated summary (`max_tokens=150`).
     - `client.chat.completion.create(...)`: Sends a request to the OpenAI API using the chat completion endpoint. The request includes:
       - A system message instructing the AI to act as an expert in analyzing 10-Q reports and generating summaries.
       - A user message containing the input text.

     - `summary = completion.choices[0].message['content'].strip()`: Extracts the generated summary from the API response.

**4. Main Execution:**
   - `if __name__ == ""__main__"":`: Ensures the following code only runs when the script is executed directly (not imported as a module).
     - `input_file_path = ""your_text_file.txt""`: Sets the path to your text file. Replace `""your_text_file.txt""` with the actual filename.
     - `main(input_file_path)`: Calls the `main` function to read the file content and generate the summary, then prints it to the console.


**Key Points:**

- **API Key:** You'll need to obtain an API key from OpenAI (https://platform.openai.com/account/api-keys) and replace `""your_openai_api_key_here""` in the code.
- **Model Choice:** You can experiment with different OpenAI models by changing the `model` parameter in `generate_summary`.  
- **Token Limit:** The `max_tokens` parameter controls the length of the generated summary. Adjust this as needed.

Let me know if you have any more questions or want to explore specific aspects of the code further!","Here are 3-5 insightful questions with brief, informative answers:

Question: What is the primary purpose of the `read_text_file` function in this code?
Answer: The `read_text_file` function reads the entire contents of a text file specified by its path and returns it as a single string.

Question: How does the `generate_summary` function interact with the OpenAI API to produce a summary of the input text?
Answer: The `generate_summary` function sends a request to the OpenAI API using the chat completion endpoint, including a system message instructing the AI to act as an expert in analyzing 10-Q reports and generating summaries, and a user message containing the input text. It then extracts the generated summary from the API response.

Question: What is the significance of the `max_tokens` parameter in the `generate_summary` function?
Answer: The `max_tokens` parameter controls the length of the generated summary, allowing you to adjust the maximum number of tokens (characters) included in the output. This can help balance the trade-off between summary brevity and content accuracy.

Question: How does the code handle the OpenAI API key and model selection?
Answer: The API key is hardcoded into the script as `""your_openai_api_key_here""`, which should be replaced with your actual API key obtained from OpenAI. Model selection can be customized by modifying the `model` parameter in the `generate_summary` function, allowing you to experiment with different OpenAI models."
Github_bot/Issue_summarizer.py,"import csv
import logging
import time
from github import Github
from urllib.parse import urlparse
import openai

logging.basicConfig(level=logging.INFO)

API_BASE_URL = ""https://llama.us.gaianet.network/v1""
MODEL_NAME = ""llama""
API_KEY = ""GAIA""

client = openai.OpenAI(base_url=API_BASE_URL, api_key=API_KEY)

def summarize_text(text):
    try:
        start_time = time.time()
        response = client.chat.completions.create(
            messages=[
                {
                    ""role"": ""system"",
                    ""content"": ""You are an expert in summarizing and understanding various types of code and documentation. Summarize the given text in a concise and coherent manner."",
                },
                {
                    ""role"": ""user"",
                    ""content"": text,
                }
            ],
            model=MODEL_NAME,
            stream=False,
        )
        logging.info(f""API call took {time.time() - start_time} seconds."")
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f""Error in summarizing text: {e}"")
        return ""Error: Could not summarize""

def get_repo_issues(repo_url):
    parsed_url = urlparse(repo_url)
    path_parts = parsed_url.path.strip('/').split('/')
    owner, repo_name = path_parts[0], path_parts[1]

    # Initialize GitHub client
    g = Github() 

    # Get the repository
    repo = g.get_repo(f""{owner}/{repo_name}"")

    # Get all issues
    issues = repo.get_issues(state=""all"")
    issue_list = []
    for issue in issues:
        issue_list.append({
            ""title"": issue.title,
            ""body"": issue.body
        })

    return issue_list

def process_issues(repo_url, output_csv_file):
    try:
        issues = get_repo_issues(repo_url)
        
        with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow([""Issue Title"", ""Summary""])  
            
            for issue in issues:
                title = issue[""title""]
                body = issue[""body""]
                logging.info(f""Processing issue: {title}"")
                
                # Summarize the issue body
                summary = summarize_text(body)
                writer.writerow([title, summary])
                
                logging.info(f""Summary for issue '{title}' added to CSV."")

    except Exception as e:
        logging.error(f""Error processing issues: {e}"")

if __name__ == ""__main__"":
    repo_url = input(""Enter the GitHub repository URL: "")
    output_csv_file = ""repo_issues_summaries.csv""

    process_issues(repo_url, output_csv_file)
    print(f""Issue summaries have been saved to {output_csv_file}"")
","## Code Explanation

This Python script automates the process of summarizing issue descriptions from a GitHub repository and saving them to a CSV file. Here's a breakdown:

**1. Setup & Dependencies:**
   - Imports necessary libraries: `csv`, `logging`, `time`, `github`, `urllib.parse`, and `openai`.
   - Sets up logging for debugging purposes.
   - Configures the OpenAI API connection with the specified base URL, model name (Llama), and API key.

**2. Text Summarization Function (`summarize_text`):**
   - Takes text as input.
   - Uses the OpenAI API's `chat.completion` endpoint to send a prompt instructing Llama to summarize the text concisely. 
   - Logs the time taken for the API call.
   - Returns the generated summary or an error message if something goes wrong.

**3. Issue Retrieval Function (`get_repo_issues`):**
   - Takes a GitHub repository URL as input.
   - Parses the URL to extract the owner and repository name.
   - Initializes a GitHub client using the `github` library.
   - Retrieves all issues (open, closed, etc.) from the specified repository.
   - Creates a list of dictionaries, each containing the issue title and body.

**4. Issue Processing Function (`process_issues`):**
   - Takes the repository URL and output CSV file name as input.
   - Calls `get_repo_issues` to retrieve the issues.
   - Opens the output CSV file in write mode.
   - Writes header row for ""Issue Title"" and ""Summary"".
   - Iterates through each issue:
     - Logs the current issue title.
     - Calls `summarize_text` to get a summary of the issue body.
     - Writes the title and summary to the CSV file.
     - Logs confirmation that the summary was added.

**5. Main Execution (`if __name__ == ""__main__"":`):**
   - Prompts the user for the GitHub repository URL.
   - Sets the output CSV file name.
   - Calls `process_issues` to start the summarization and saving process.
   - Prints a message indicating the completion and location of the generated CSV file.



Let me know if you have any further questions or would like more details on any specific part of the code!","Here are 3-5 insightful questions and brief answers based on the provided content and summary:

**Question:** What is the primary function of the `summarize_text` function, and how does it utilize the OpenAI API to achieve its goal?
**Answer:** The `summarize_text` function takes text as input and uses the OpenAI API's `chat.completion` endpoint to send a prompt instructing Llama to summarize the text concisely. It returns the generated summary or an error message if something goes wrong.

**Question:** How does the script parse the GitHub repository URL provided by the user, and what information is extracted from it?
**Answer:** The script uses the `urlparse` function from the `urllib.parse` library to parse the GitHub repository URL. It extracts the owner and repository name from the parsed URL path.

**Question:** What happens if an error occurs during the execution of the `summarize_text` or `process_issues` functions, and how are these errors handled?
**Answer:** If an error occurs during the execution of either function, the script logs an error message with the relevant details. The error is not propagated further, ensuring that the rest of the script remains unaffected.

**Question:** What is the purpose of logging in the script, and what information is logged at different points?
**Answer:** Logging is used for debugging purposes to track the progress and any issues encountered during the execution of the script. The script logs messages at various points, including when an API call is made, when processing an issue, or when an error occurs.

Let me know if you'd like more questions or have further requests!"
Github_bot/repo_summarizer.py,"import os
import csv
import requests
import logging
import time
from github import Github
from urllib.parse import urlparse
import openai

logging.basicConfig(level=logging.INFO)

API_BASE_URL = ""https://llama.us.gaianet.network/v1""
MODEL_NAME = ""llama""
API_KEY = ""GAIA""

client = openai.OpenAI(base_url=API_BASE_URL, api_key=API_KEY)

def summarize_text(text):
    try:
        start_time = time.time()
        response = client.chat.completions.create(
            messages=[
                {
                    ""role"": ""system"",
                    ""content"": ""You are an expert in summarizing and understanding various types of code and documentation. Summarize the given text in a concise and coherent manner."",
                },
                {
                    ""role"": ""user"",
                    ""content"": text,
                }
            ],
            model=MODEL_NAME,
            stream=False,
        )
        logging.info(f""API call took {time.time() - start_time} seconds."")
        return response.choices[0].message.content
    except Exception as e:
        logging.error(f""Error in summarizing text: {e}"")
        return ""Error: Could not summarize""

def get_repo_files(repo_url):
    parsed_url = urlparse(repo_url)
    path_parts = parsed_url.path.strip('/').split('/')
    owner, repo_name = path_parts[0], path_parts[1]

    # Initialize GitHub client
    g = Github() 

    # Get the repository
    repo = g.get_repo(f""{owner}/{repo_name}"")

    # Get all files in the repository
    files = []
    contents = repo.get_contents("""")
    while contents:
        file_content = contents.pop(0)
        if file_content.type == ""dir"":
            contents.extend(repo.get_contents(file_content.path))
        else:
            files.append(file_content.path)

    return files

def fetch_file_content(repo_url, file_path):
    # Extract owner and repo name from the URL
    parsed_url = urlparse(repo_url)
    path_parts = parsed_url.path.strip('/').split('/')
    owner, repo_name = path_parts[0], path_parts[1]

    # Initialize GitHub client
    g = Github() 

    # Get the repository
    repo = g.get_repo(f""{owner}/{repo_name}"")

    # Get file content
    file_content = repo.get_contents(file_path).decoded_content.decode()
    return file_content

def process_files(repo_url, output_csv_file):
    try:
        file_paths = get_repo_files(repo_url)
        
        with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow([""File Path"", ""Summary""])  
            
            for path in file_paths:
                logging.info(f""Processing file: {path}"")
                file_content = fetch_file_content(repo_url, path)
                
                summary = summarize_text(file_content)
                writer.writerow([path, summary])
                
                logging.info(f""Summary for {path} added to CSV."")

    except Exception as e:
        logging.error(f""Error processing files: {e}"")

if __name__ == ""__main__"":
    repo_url = input(""Enter the GitHub repository URL: "")
    output_csv_file = ""repo_file_summaries.csv""

    process_files(repo_url, output_csv_file)
    print(f""File summaries have been saved to {output_csv_file}"")
","## Code Analysis: Github_bot/repo_summarizer.py

This Python script automates the process of summarizing code and documentation from a given GitHub repository. 

Here's a breakdown:

**1. Dependencies:**

- **github:** For interacting with the GitHub API to fetch repository data.
- **openai:** To leverage OpenAI's GPT models for text summarization.
- **requests, urllib.parse, csv, time, logging:** Standard Python libraries for HTTP requests, URL parsing, CSV file handling, timing, and logging.

**2. Configuration:**

- `API_BASE_URL`, `MODEL_NAME`: Defines the OpenAI API endpoint and the desired model (Llama).
- `API_KEY`: Stores your OpenAI API key (should be kept confidential).

**3. Functions:**

- **`summarize_text(text)`:** Takes text as input, sends it to the OpenAI API for summarization using the specified model, and returns the generated summary. Handles potential errors during the process.
- **`get_repo_files(repo_url)`:** Extracts file paths from a given GitHub repository URL using the Github library. Retrieves all files (including nested directories) within the repository.
- **`fetch_file_content(repo_url, file_path)`:** Fetches the content of a specific file from the specified repository on GitHub. 
- **`process_files(repo_url, output_csv_file)`:** Orchestrates the summarization process:
    - Retrieves all files in the repository.
    - Loops through each file, fetches its content, and generates a summary using `summarize_text()`.
    - Writes the file path and summary to a CSV file specified by `output_csv_file`.

**4. Main Execution:**

- Prompts the user for the GitHub repository URL.
- Defines the output CSV filename.
- Calls `process_files()` to perform summarization and save results to the CSV.



**Key Points:**

- Uses OpenAI's powerful language models for code and text summarization.
- Efficiently fetches repository data from GitHub.
- Provides a structured output in CSV format, making it easy to analyze and process the summaries.
- Includes logging for debugging and tracking progress.

**Potential Improvements:**

- **Error Handling:** More robust error handling could be implemented to gracefully handle potential issues like network connectivity problems or API rate limits.
- **Progress Indicators:** Adding progress indicators during file processing would enhance user experience, especially for large repositories.
- **Customization Options:** Allowing users to specify summarization parameters (e.g., length, style) would provide more control over the output.


Let me know if you have any other questions or need further clarification!","Here are 3-5 insightful questions and brief, informative answers based on the provided content:

**Question:** How does the script handle potential errors during the summarization process?
**Answer:** The `summarize_text()` function includes error handling using a try-except block. If an error occurs (e.g., API request fails), it logs the error message and returns a generic ""Error: Could not summarize"" response.

**Question:** Can the script handle large repositories with many files, or is there a performance bottleneck?
**Answer:** The `process_files()` function retrieves all files in the repository using the GitHub API. However, due to rate limits on the API (not explicitly stated in the code), it's possible that processing very large repositories might exceed these limits.

**Question:** How can users customize the summarization process or output?
**Answer:** While the script doesn't currently allow for customization options like specifying summarization parameters, modifying the `summarize_text()` function to accept additional arguments (e.g., length, style) could provide more flexibility."
