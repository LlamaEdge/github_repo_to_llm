Path,Content
.cargo/config.toml,"[build]
target = ""wasm32-wasip1""
rustflags = [""--cfg"", ""wasmedge"", ""--cfg"", ""tokio_unstable""]
"
.github/ISSUE_TEMPLATE/bug_report.yml,"name: ""Bug Report""
description: Running WasmEdge LLM, but it's not working as you expect?
title: ""bug: ""
labels: [
  ""bug""
]
body:
  - type: textarea
    id: summary
    attributes:
      label: ""Summary""
      description: Please shortly describe what bug you encounter.
      placeholder: Describe in a few lines about the bug
    validations:
      required: true
  - type: textarea
    id: reproducuction_steps
    attributes:
      label: ""Reproduction steps""
      description: Please provide as much information as necessary to reproduce the bug.
      placeholder: |
        1. How to install '...'
        2. Execute with models '....'
        3. Execute with inputs and flags '....'
        4. Get error
    validations:
      required: true
  - type: textarea
    id: screenshot
    attributes:
      label: ""Screenshots""
      description: If applicable, add screenshots to help explain your problem.
      value: |
        ![DESCRIPTION](LINK.png)
    validations:
      required: false
  - type: textarea
    id: logs
    attributes:
      label: ""Any logs you want to share for showing the specific issue""
      description: Please copy and paste any relevant log output. This will be automatically formatted into code, so no need for backticks.
    validations:
      required: false
  - type: input
    id: model
    attributes:
      label: ""Model Information""
      description: Model Information
      placeholder: e.g. llama2-7b-chat_Q5_K_M.gguf, etc.
    validations:
      required: true
  - type: input
    id: os
    attributes:
      label: ""Operating system information""
      description: Operating system information
      placeholder: e.g. Ubuntu 20.04, CentOS 7.6, macOS 13.5.2, or Windows 11, etc.
    validations:
      required: true
  - type: input
    id: arch
    attributes:
      label: ""ARCH""
      description: ARCH
      placeholder: e.g. amd64, x86_64, arm64, aarch64, or RISC-V, etc.
    validations:
      required: true
  - type: input
    id: cpu
    attributes:
      label: ""CPU Information""
      description: CPU Information
      placeholder: e.g. Intel i7-12700, AMD 5900X, or Apple M2 Max, etc.
    validations:
      required: true
  - type: input
    id: memory
    attributes:
      label: ""Memory Size""
      description: Memory Size
      placeholder: e.g. 8GB, 16GB, or 64GB, etc.
    validations:
      required: true
  - type: input
    id: gpu
    attributes:
      label: ""GPU Information""
      description: GPU Information
      placeholder: e.g. GTX 1080, NVIDIA Orin, or Apple M3, etc.
    validations:
      required: true
  - type: input
    id: vram
    attributes:
      label: ""VRAM Size""
      description: VRAM Size
      placeholder: e.g. 4GB, 8GB, or 16GB, etc.
    validations:
      required: true
"
.github/ISSUE_TEMPLATE/config.yml,"blank_issues_enabled: false
"
.github/ISSUE_TEMPLATE/enhancement.yml,"name: ""Feature Request""
description: You want to request a feature to enhance the second-state/llama-utils repo, like supporting a new model.
title: ""Feature Request:""
labels: [
  ""enhancement""
]
body:
  - type: textarea
    id: summary
    attributes:
      label: ""Summary""
      description: Please shortly describe what kind of feature do you need.
      placeholder: Describe in a few lines your request
    validations:
      required: true
  - type: textarea
    id: appendix
    attributes:
      label: ""Appendix""
      description: Provide anything you want to supplement.
      placeholder: Some reference links or any information related to this question.
    validations:
      required: false
"
.github/ISSUE_TEMPLATE/question.yml,"name: ""Question""
description: Don't see your issue kind in other issue templates? Use this one instead.
title: ""question: ""
labels: [
  ""question""
]
body:
  - type: textarea
    id: summary
    attributes:
      label: ""Summary""
      description: Please shortly describe by example what question you have
      placeholder: Describe in a few lines your questions
    validations:
      required: true
  - type: textarea
    id: appendix
    attributes:
      label: ""Appendix""
      description: Provide anything you want to supplement.
      placeholder: Some reference links or any information related to this question.
    validations:
      required: false
"
.github/workflows/build.yml,"name: Build

on:
  push:
    branches:
      - dev
      - main
      - release-*
      - feat-*
      - ci-*
      - refactor-*
      - fix-*
      - test-*
    paths:
      - '.github/workflows/build.yml'
      - '**/Cargo.toml'
      - '**/*.rs'
      - '**/*.sh'
      - '**/.cargo/config.toml'
  pull_request:
    branches:
      - dev
      - main
    types: [opened, synchronize, reopened]
    paths:
      - '.github/workflows/**'
      - '**/Cargo.toml'
      - '**/*.rs'
      - '**/*.sh'

jobs:
  build-wasm:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-22.04, macos-13, macos-14]
    steps:
      - name: Clone project
        id: checkout
        uses: actions/checkout@v3

      - name: Install Rust-nightly
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: nightly
          target: wasm32-wasip1
          components: rustfmt, clippy

      - name: Install Rust-stable
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          target: wasm32-wasip1

      - name: Download wasi-sdk for x86_64-macos
        if: matrix.os == 'macos-13'
        run: |
          curl -LO https://github.com/WebAssembly/wasi-sdk/releases/download/wasi-sdk-24/wasi-sdk-24.0-x86_64-macos.tar.gz
          tar -xzvf wasi-sdk-24.0-x86_64-macos.tar.gz
          mv wasi-sdk-24.0-x86_64-macos wasi-sdk-24.0

      - name: Download wasi-sdk for arm64-macos
        if: matrix.os == 'macos-14'
        run: |
          curl -LO https://github.com/WebAssembly/wasi-sdk/releases/download/wasi-sdk-24/wasi-sdk-24.0-arm64-macos.tar.gz
          tar -xzvf wasi-sdk-24.0-arm64-macos.tar.gz
          mv wasi-sdk-24.0-arm64-macos wasi-sdk-24.0

      - name: Run clippy
        if: startsWith(matrix.os, 'ubuntu')
        env:
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
        run: |
          cargo +nightly clippy --target wasm32-wasip1 -- -D warnings

      - name: Run fmt
        if: startsWith(matrix.os, 'ubuntu')
        env:
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
        run: |
          cargo +nightly fmt --all -- --check

      - name: Run tests on linux
        if: startsWith(matrix.os, 'ubuntu')
        env:
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
        run: |
          cargo test -p endpoints --target x86_64-unknown-linux-gnu

      - name: Run tests on macos-14
        if: matrix.os == 'macos-14'
        env:
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
        run: |
          cargo test -p endpoints --target aarch64-apple-darwin

      - name: Build llama-simple
        env:
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
        run: |
          cargo build -p llama-simple --release

      - name: Build llama-chat on linux
        if: startsWith(matrix.os, 'ubuntu')
        env:
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
        run: |
          cargo build -p llama-chat --release

      - name: Build llama-chat on macos
        if: startsWith(matrix.os, 'macos')
        env:
          WASI_SDK_PATH: /Users/runner/work/LlamaEdge/LlamaEdge/wasi-sdk-24.0
          CC: ""/Users/runner/work/LlamaEdge/LlamaEdge/wasi-sdk-24.0/bin/clang --sysroot=/Users/runner/work/LlamaEdge/LlamaEdge/wasi-sdk-24.0/share/wasi-sysroot""
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
        run: |
          cargo build -p llama-chat --release

      - name: Build llama-api-server on linux
        if: startsWith(matrix.os, 'ubuntu')
        env:
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
        run: |
          cargo build -p llama-api-server --release

      - name: Build api-server on macos
        if: startsWith(matrix.os, 'macos')
        env:
          WASI_SDK_PATH: /Users/runner/work/LlamaEdge/LlamaEdge/wasi-sdk-24.0
          CC: ""/Users/runner/work/LlamaEdge/LlamaEdge/wasi-sdk-24.0/bin/clang --sysroot=/Users/runner/work/LlamaEdge/LlamaEdge/wasi-sdk-24.0/share/wasi-sysroot""
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
        run: |
          cargo build -p llama-api-server --release
"
.github/workflows/publish.yml,"name: Release

on:
  workflow_dispatch: # manual trigger release
    inputs:
      create_release:
        description: 'Create new release'
        required: true
        type: boolean
      release_version:
        description: ""Version (e.g. 1.0.0)""
        required: true
        type: string

jobs:
  build-and-release:
    runs-on: ubuntu-latest
    steps:
      - name: Clone project
        id: checkout
        uses: actions/checkout@v3

      - name: Setup rustup
        id: rustup
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          target: wasm32-wasip1

      - name: Build llama-simple
        env:
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
        run: |
          cargo build -p llama-simple --release

      - name: Build llama-chat
        env:
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
        run: |
          cargo build -p llama-chat --release

      - name: Build llama-api-server
        env:
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
        run: |
          cargo build -p llama-api-server --release

      - name: Calculate checksum
        id: checksum
        run: |
          cp target/wasm32-wasip1/release/*.wasm .
          sha256sum *.wasm > SHA256SUM

          echo ""Debug info(SHA256SUM):""
          cat SHA256SUM

      - name: Tag and release names
        id: tag_and_release_names
        run: |
          echo ""tag_name=${{ github.event.inputs.release_version }}"" >> $GITHUB_OUTPUT
          echo ""release_name=LlamaEdge ${{ github.event.inputs.release_version }}"" >> $GITHUB_OUTPUT

      - name: Create Release and Upload Release Asset
        if: ${{ github.event.inputs.create_release == 'true' && github.ref == 'refs/heads/main'}}
        uses: softprops/action-gh-release@v1
        with:
          name: ${{ steps.tag_and_release_names.outputs.release_name }}
          tag_name: ${{ steps.tag_and_release_names.outputs.tag_name }}
          body: TODO New Release.
          draft: true
          prerelease: true
          files: |
            llama-api-server.wasm
            llama-chat.wasm
            llama-simple.wasm
            SHA256SUM
"
.github/workflows/publish_core_doc.yml,"name: Publish Core API Document

on:
  workflow_dispatch: # manual trigger release
    inputs:
      create_release:
        description: 'Publish Core API Document'
        required: true
        type: boolean

jobs:
  build-and-release:
    runs-on: ubuntu-latest
    steps:
      - name: Clone project
        id: checkout
        uses: actions/checkout@v3

      - name: Install Rust-nightly
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: nightly
          target: wasm32-wasip1

      - name: Build API document
        env:
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
          RUSTDOCFLAGS: ""--cfg docsrs""
        run: |
          cargo +nightly doc -p llama-core --no-deps --target wasm32-wasip1 --features full --target-dir=./target

      - name: Deploy API document
        if: github.ref == 'refs/heads/dev'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_branch: gh-pages
          publish_dir: target/wasm32-wasip1/doc/
          force_orphan: true
"
.github/workflows/test_api_server.yml,"name: Test API Server

on:
  push:
    branches:
      - dev
      - main
      - release-*
      - feat-*
      - ci-*
      - refactor-*
      - fix-*
      - test-*
    paths:
      - '.github/workflows/test_api_server.yml'
      - '**/Cargo.toml'
      - '**/*.rs'
      - '**/*.sh'
      - '**/.cargo/config.toml'
  pull_request:
    branches:
      - dev
      - main
    types: [opened, synchronize, reopened]
    paths:
      - '.github/workflows/**'
      - '**/Cargo.toml'
      - '**/*.rs'
      - '**/*.sh'

jobs:
  test-api-server:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        wasmedge_version: [0.14.0]
    steps:
      - name: Clone project
        id: checkout
        uses: actions/checkout@v3

      - name: Install Rust-nightly
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          toolchain: nightly
          target: wasm32-wasip1
          components: rustfmt, clippy

      - name: Install Rust-stable
        uses: actions-rust-lang/setup-rust-toolchain@v1
        with:
          target: wasm32-wasip1

      - name: Install WasmEdge
        run: |
          curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v ${{ matrix.wasmedge_version }}
          ls -al $HOME/.wasmedge/bin

      - name: Install Hurl
        run: |
          curl --location --remote-name https://github.com/Orange-OpenSource/hurl/releases/download/5.0.1/hurl_5.0.1_amd64.deb
          sudo apt update && sudo apt install ./hurl_5.0.1_amd64.deb

      - name: Build llama-api-server on linux
        env:
          RUSTFLAGS: ""--cfg wasmedge --cfg tokio_unstable""
        run: |
          cargo build -p llama-api-server --release
          cp target/wasm32-wasip1/release/llama-api-server.wasm ./llama-api-server.wasm

      - name: Start llama-api-server for testing chat completions
        run: |
          curl -LO https://huggingface.co/second-state/Qwen2-1.5B-Instruct-GGUF/resolve/main/Qwen2-1.5B-Instruct-Q3_K_M.gguf
          nohup $HOME/.wasmedge/bin/wasmedge --dir .:. --nn-preload default:GGML:AUTO:Qwen2-1.5B-Instruct-Q3_K_M.gguf llama-api-server.wasm --model-name Qwen2-1.5B-Instruct --prompt-template chatml --ctx-size 4096 --socket-addr 0.0.0.0:8080 > ./start-llamaedge.log 2>&1 &
          sleep 5
          cat start-llamaedge.log

      - name: Run test_chat.hurl
        run: |
          hurl --test --jobs 1 ./tests/test_chat.hurl

      - name: Stop llama-api-server for testing chat completions
        run: |
          pkill -f wasmedge

      - name: Start llama-api-server for testing embeddings
        run: |
          curl -LO https://huggingface.co/second-state/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5-f16.gguf
          nohup $HOME/.wasmedge/bin/wasmedge --dir .:. --nn-preload default:GGML:AUTO:nomic-embed-text-v1.5-f16.gguf llama-api-server.wasm --model-name nomic-embed-text-v1.5 --prompt-template embedding --ctx-size 512 --socket-addr 0.0.0.0:8080 > ./start-llamaedge.log 2>&1 &
          sleep 5
          cat start-llamaedge.log

      - name: Run test_embeddings.hurl
        run: |
          hurl --test --jobs 1 ./tests/test_embeddings.hurl

      - name: Stop llama-api-server for testing chat completions
        run: |
          pkill -f wasmedge
"
.github/workflows/warn_close_issues.yml,"name: Close inactive issues
on:
  schedule:
    - cron: ""30 1 * * *""

jobs:
  close-issues:
    runs-on: ubuntu-latest
    permissions:
      issues: write
      pull-requests: write
    steps:
      - uses: actions/stale@v5
        with:
          days-before-issue-stale: 30
          days-before-issue-close: 14
          stale-issue-label: ""stale""
          stale-issue-message: ""This issue is stale because it has been open for 30 days with no activity.""
          close-issue-message: ""This issue was closed because it has been inactive for 14 days since being marked as stale.""
          days-before-pr-stale: -1
          days-before-pr-close: -1
          repo-token: ${{ secrets.GITHUB_TOKEN }}
"
.gitignore,"# Generated by Cargo
# will have compiled files and executables
debug/
target/

# Remove Cargo.lock from gitignore if creating an executable, leave it for libraries
# More information here https://doc.rust-lang.org/cargo/guide/cargo-toml-vs-cargo-lock.html
Cargo.lock

# These are backup files generated by rustfmt
**/*.rs.bk

# MSVC Windows builds of rustc generate these, which store debugging information
*.pdb
*.gguf
"
CONTRIBUTING.md,"# Contributing guidelines

## How to become a contributor and submit your own code

### Git

Please check out a recent version of `dev` branch before starting work, and rebase onto `dev` before creating a pull request.
This helps keep the commit graph clean and easy to follow. In addition, please sign off each of your commits.

### GitHub Issues

If you want to work on a GitHub issue, check to make sure it's not assigned to someone first.
If it's not assigned to anyone, assign yourself once you start writing code.
(Please don't assign yourself just because you'd like to work on the issue, but only when you actually start.)
This helps avoid duplicate work.

If you start working on an issue but find that you won't be able to finish, please un-assign yourself so other people know the issue is available.
If you assign yourself but aren't making progress, we may assign the issue to someone else.

If you're working on issue 123, please put ""Fixes #123"" (without quotes) in the commit message below everything else and separated by a blank line.
For example, if issue 123 is a feature request to add foobar, the commit message might look like:

```text
Add foobar

Some longer description goes here, if you
want to describe your change in detail.

Fixes #123
```

This will [close the bug once your pull request is merged](https://help.github.com/articles/closing-issues-using-keywords/).

If you're a first-time contributor, try looking for an issue with the label ""good first issue"", which should be easier for someone unfamiliar with the codebase to work on.
"
Cargo.toml,"[workspace]
members = [
    ""llama-api-server"",
    ""llama-simple"",
    ""llama-chat"",
    ""crates/endpoints"",
    ""crates/chat-prompts"",
    ""crates/llama-core"",
]
resolver = ""2""

[workspace.dependencies]
serde = { version = ""1.0"", features = [""derive""] }
serde_json = ""1.0""
endpoints = { path = ""crates/endpoints"", version = ""^0.14"" }
chat-prompts = { path = ""crates/chat-prompts"", version = ""^0.14"" }
thiserror = ""1""
uuid = { version = ""1.4"", features = [""v4"", ""fast-rng"", ""macro-diagnostics""] }
clap = { version = ""4.4.6"", features = [""cargo"", ""derive""] }
log = { version = ""0.4.21"", features = [""std"", ""kv"", ""kv_serde""] }
wasi-logger = { version = ""0.1.2"", features = [""kv""] }
either = ""1.12.0""
base64 = ""=0.22.0""
llama-core = { path = ""crates/llama-core"", features = [""logging""], version = ""^0.17"" }
tokio = { version = ""^1.36"", features = [""io-util"", ""fs"", ""net"", ""time"", ""rt"", ""macros""] }
anyhow = ""1.0""
once_cell = ""1.18""
wasmedge-wasi-nn = ""0.8.0""
futures = { version = ""0.3.6"", default-features = false, features = [""async-await"", ""std""] }

[patch.crates-io]
socket2 = { git = ""https://github.com/second-state/socket2.git"", branch = ""v0.5.x"" }
reqwest = { git = ""https://github.com/second-state/wasi_reqwest.git"", branch = ""0.11.x"" }
hyper = { git = ""https://github.com/second-state/wasi_hyper.git"", branch = ""v0.14.x"" }
tokio = { git = ""https://github.com/second-state/wasi_tokio.git"", branch = ""v1.36.x"" }
"
LICENSE,"                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      ""License"" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      ""Licensor"" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      ""Legal Entity"" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      ""control"" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      ""You"" (or ""Your"") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      ""Source"" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      ""Object"" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      ""Work"" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      ""Derivative Works"" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      ""Contribution"" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, ""submitted""
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as ""Not a Contribution.""

      ""Contributor"" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a ""NOTICE"" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an ""AS IS"" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets ""[]""
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same ""printed page"" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the ""License"");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an ""AS IS"" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
"
README.md,"# LlamaEdge

The LlamaEdge project makes it easy for you to run LLM inference apps and create OpenAI-compatible API services for the Llama2 series of LLMs locally.

⭐ Like our work? Give us a star!

Checkout our [official docs](https://llamaedge.com/docs) and a [Manning ebook](https://www.manning.com/liveprojectseries/open-source-llms-on-your-own-computer) on how to customize open source models.

## Quick start

Enhance your onboarding experience and quickly get started with LlamaEdge using the following scripts.

#1: Quick start without any argument

```
bash <(curl -sSfL 'https://raw.githubusercontent.com/LlamaEdge/LlamaEdge/main/run-llm.sh')
```

It will download and start the [Gemma-2-9b-it](https://huggingface.co/second-state/gemma-2-9b-it-GGUF) model automatically. Open http://127.0.0.1:8080 in your browser and start chatting right away!


#2: Specify a model using `--model model_name`

```
bash <(curl -sSfL 'https://raw.githubusercontent.com/LlamaEdge/LlamaEdge/main/run-llm.sh') --model llama-3-8b-instruct
```

The script will start an API server for the Llama3 8b model with a chatbot UI based on your choice. Open http://127.0.0.1:8080 in your browser and start chatting right away!

To explore all the available models, please use the following command line

```
bash <(curl -sSfL 'https://raw.githubusercontent.com/LlamaEdge/LlamaEdge/main/run-llm.sh') --model help
```
#3:  Interactively choose and confirm all steps in the script using using `--interactive` flag

```
bash <(curl -sSfL 'https://raw.githubusercontent.com/LlamaEdge/LlamaEdge/main/run-llm.sh') --interactive
```
Follow the on-screen instructions to install the WasmEdge Runtime and download your favorite open-source LLM. Then, choose whether you want to chat with the model via the CLI or via a web UI.

[See it in action](https://youtu.be/Hqu-PBqkzDk) | [Docs](https://www.secondstate.io/articles/run-llm-sh/)

## How it works?

The Rust source code for the inference applications are all open source and you can modify and use them freely for your own purposes.

* The folder `llama-simple` contains the source code project to generate text from a prompt using run llama2 models.
* The folder `llama-chat` contains the source code project to ""chat"" with a llama2 model on the command line.
* The folder `llama-api-server` contains the source code project for a web server. It provides an OpenAI-compatible API service, as well as an optional web UI, for llama2 models.

## The tech stack

The [Rust+Wasm stack](https://medium.com/stackademic/why-did-elon-musk-say-that-rust-is-the-language-of-agi-eb36303ce341) provides a strong alternative to Python in AI inference.

* Lightweight. The total runtime size is 30MB.
* Fast. Full native speed on GPUs.
* Portable. Single cross-platform binary on different CPUs, GPUs, and OSes.
* Secure. Sandboxed and isolated execution on untrusted devices.
* Container-ready. Supported in Docker, containerd, Podman, and Kubernetes.

For more information, please check out [Fast and Portable Llama2 Inference on the Heterogeneous Edge](https://www.secondstate.io/articles/fast-llm-inference/).

## Models

The LlamaEdge project supports all Large Language Models (LLMs) based on the llama2 framework. The model files must be in the GGUF format. We are committed to continuously testing and validating new open-source models that emerge every day.

[Click here](https://huggingface.co/second-state) to see the supported model list with a download link and startup commands for each model. If you have success with other LLMs, don't hesitate to contribute by creating a Pull Request (PR) to help extend this list.

## Platforms

The compiled Wasm file is cross platfrom. You can use the same Wasm file to run the LLM across OSes (e.g., MacOS, Linux, Windows SL), CPUs (e.g., x86, ARM, Apple, RISC-V), and GPUs (e.g., NVIDIA, Apple).

The installer from WasmEdge 0.13.5 will detect NVIDIA CUDA drivers automatically. If CUDA is detected, the installer will always attempt to install a CUDA-enabled version of the plugin. The CUDA support is tested on the following platforms in our automated CI.

* Nvidia Jetson AGX Orin 64GB developer kit
* Intel i7-10700 + Nvidia GTX 1080 8G GPU
* AWS EC2 `g5.xlarge` + Nvidia A10G 24G GPU + Amazon deep learning base Ubuntu 20.04

> If you're using CPU only machine, the installer will install the OpenBLAS version of the plugin instead. You may need to install `libopenblas-dev` by `apt update && apt install -y libopenblas-dev`.

## Troubleshooting

Q: Why I got the following errors after starting the API server?

```
[2024-03-05 16:09:05.800] [error] instantiation failed: module name conflict, Code: 0x60
[2024-03-05 16:09:05.801] [error]     At AST node: module
```

A: TThe module conflict error is a known issue, and these are false-positive errors. They do not impact your program's functionality.

Q: Even though my machine has a large RAM, after asking several questions, I received an error message returns 'Error: Backend Error: WASI-NN'. What should I do?

A: To enable machines with smaller RAM, like 8 GB, to run a 7b model, we've set the context size limit to 512. If your machine has more capacity, you can increase both the context size and batch size up to 4096 using the CLI options available [here](https://github.com/second-state/llama-utils/tree/main/chat#cli-options). Use these commands to adjust the settings:

```
-c, --ctx-size <CTX_SIZE>
-b, --batch-size <BATCH_SIZE>
```

Q: After running `apt update && apt install -y libopenblas-dev`, you may encounter the following error:

  ```bash
  ...
  E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)
  E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?
  ```

A: This indicates that you are not logged in as `root`. Please try installing again using the `sudo` command:

  ```bash
  sudo apt update && sudo apt install -y libopenblas-dev
  ```

Q: After running the `wasmedge` command, you may receive the following error:

  ```bash
  [2023-10-02 14:30:31.227] [error] loading failed: invalid path, Code: 0x20
  [2023-10-02 14:30:31.227] [error]     load library failed:libblas.so.3: cannot open shared object file: No such file or directory
  [2023-10-02 14:30:31.227] [error] loading failed: invalid path, Code: 0x20
  [2023-10-02 14:30:31.227] [error]     load library failed:libblas.so.3: cannot open shared object file: No such file or directory
  unknown option: nn-preload
  ```

A: This suggests that your plugin installation was not successful. To resolve this issue, please attempt to install your desired plugin again.

Q: After executing the `wasmedge` command, you might encounter the error message: `[WASI-NN] GGML backend: Error: unable to init model.`

A: This error signifies that the model setup was not successful. To resolve this issue, please verify the following:

  1. Check if your model file and the WASM application are located in the same directory. The WasmEdge runtime requires them to be in the same location to locate the model file correctly.
  2. Ensure that the model has been downloaded successfully. You can use the command `shasum -a 256 <gguf-filename>` to verify the model's sha256sum. Compare your result with the correct sha256sum available on [the Hugging Face page](https://huggingface.co/second-state/Dolphin-2.2-Yi-34B-GGUF/blob/main/dolphin-2.2-yi-34b-ggml-model-q4_0.gguf) for the model.

<img width=""1286"" alt=""image"" src=""https://github.com/second-state/llama-utils/assets/45785633/24286d8e-b438-4d1a-a443-62c1466e9992"">

## Credits

The WASI-NN ggml plugin embedded [`llama.cpp`](git://github.com/ggerganov/llama.cpp.git@b1217) as its backend.
"
assets/logo.svg,"<svg id=""LlamaEdge_"" data-name=""LlamaEdge "" xmlns=""http://www.w3.org/2000/svg"" xmlns:xlink=""http://www.w3.org/1999/xlink"" width=""2982"" height=""1024"" viewBox=""0 0 2982 1024"">
  <defs>
    <style>
      .cls-1 {
        fill: url(#linear-gradient);
      }

      .cls-2 {
        fill: #fff;
        fill-rule: evenodd;
      }
    </style>
    <linearGradient id=""linear-gradient"" x1=""1491"" y1=""1024"" x2=""1491"" gradientUnits=""userSpaceOnUse"">
      <stop offset=""0"" stop-color=""#23195f""/>
      <stop offset=""1"" stop-color=""#b55edd""/>
    </linearGradient>
  </defs>
  <rect id=""_shape_1"" data-name="" shape 1"" class=""cls-1"" width=""2982"" height=""1024""/>
  <path id=""shape2_"" data-name=""shape2 "" class=""cls-2"" d=""M785.164,784.959c-9.128,3.508-16.527-1.019-20.279-4.68s-50.957-48.689-137.271-92.034c0,0-18.719-7.2-18.719-28.078V231.2s-0.563-7.8,7.8-7.8,17.314,11.839,31.2,42.117,12.479,65.516,12.479,65.516v304.18s0.475,8.485,7.8,12.479,35.872,20.079,57.717,35.878c0,0,12.479,8.7,12.479-6.24V632.089s-0.685-15.6,14.039-15.6c0,0,5.9-.472,14.039,6.24s20.279,17.159,20.279,17.159,6.239,4.887,6.239,12.479V769.36S794.291,781.451,785.164,784.959ZM572.662,531.99L277.334,660.676a13.073,13.073,0,0,1-2.9,1.264l-0.446.194-0.034-.067a13.819,13.819,0,0,1-3.478.452c-7.966,0-15.974-6.433-15.974-14.4v-23.4c0-6.5,4.821-12.6,10.919-14.627v-0.113L409.8,547.071V351.107c0-19.886-24.538-21.471-24.538-21.471H318.8l-0.731-.731c-6.362-.645-15.883-4.015-15.883-18.439,0-21.656-2.362-43.6,41.919-58.278s46.009-15.336,46.009-15.336-24.438-16.237,5.112-26.583c0,0-19.671-18.957,13.291-26.582,0,0-19.338-15.8,23.515-24.538a92.827,92.827,0,0,1,18.522-1.532c-0.594-15.779-3.569-36.724-12.3-61.256-14.829-41.644,40.074,14.869,51.759,65.605q5.192,1.026,10.44,2.277c1.649-16.219,1.93-40.371-3.468-72.363-9.388-55.647,57.957,21.185,50.735,88.838,23.676,11.546,41,26.857,41,44.632q0,6.114-.006,11.776c0,0.165.006,0.327,0.006,0.493V501.4c0,1.674-.031,2.928,0,5.112C588.713,517.125,582.975,527.5,572.662,531.99ZM569.6,587.442a15.552,15.552,0,0,1,4.68,10.919V620.2s1.67,11.552-10.92,15.6c-11.92,3.832-195.643,67-330.7,160.669,0,0-10.578,5.936-18.718,1.56-6.878-3.7-6.24-12.479-6.24-12.479V757.471a15.138,15.138,0,0,1,6.24-12.479s125.035-85.8,341.617-160.67A15.8,15.8,0,0,1,569.6,587.442ZM332.876,775.272s26.465-16.373,70.2-37.437c0,0,10.586-5.535,29.638,1.56s94.321,38.859,160.669,88.914c0,0,12.176,11.6-1.56,21.838s-17.158,12.479-17.158,12.479-17.155,9.034,0,23.4C590.075,898.933,612.1,921.9,612.1,921.9s8.659,9.915-1.56,20.279S596.5,956.22,596.5,956.22s-10.417,10.956-21.838-1.56S498.115,868.1,334.435,800.231c0,0-8.315-2.652-9.359-10.92C324.086,781.471,328.275,777.164,332.876,775.272Zm155.609-64.546c0.13-4.955,2.577-11.092,10.92-14.039l23.4-7.8s8.651-3.256,20.279,1.56S669.97,748.79,739.629,818.359c0,0,12.674,13.4-4.68,24.959s-20.279,14.039-20.279,14.039-10.353,6.661-20.278-3.12S606.468,773.1,499.405,724.766C499.405,724.766,488.205,721.452,488.485,710.726ZM431,237s12.37,5.979,24,0,17.548,5.81,10,13c-4.277,4.074-11.74,9.46-21,10-7.081.413-15.407-2.012-23-10a10.148,10.148,0,0,1-3-8C418.469,236.437,425.949,233.521,431,237Z""/>
  <path id=""shape_3"" data-name=""shape 3"" class=""cls-2"" d=""M2804.8,535.96q-0.315,3.607-.62,6.739H2676.64a43.447,43.447,0,0,0,4.83,12.38,39.8,39.8,0,0,0,17.24,15.985q11.115,5.489,26.17,5.485a63.388,63.388,0,0,0,23.66-4.074,57.529,57.529,0,0,0,18.65-12.224l26.02,28.209a75.858,75.858,0,0,1-29.15,20.216q-17.565,7.053-40.44,7.052-28.83,0-50.62-11.283t-33.69-30.873q-11.91-19.586-11.91-44.664,0-25.389,11.75-44.978a83.418,83.418,0,0,1,32.13-30.716q20.37-11.123,46.07-11.127,24.765,0,44.67,10.5a78.637,78.637,0,0,1,31.5,30.09q11.595,19.591,11.6,46.858Q2805.12,532.355,2804.8,535.96Zm-65.19-51.4a42.813,42.813,0,0,0-21.94-5.485,43.647,43.647,0,0,0-22.1,5.485,38.335,38.335,0,0,0-14.89,15.358,43.956,43.956,0,0,0-4.65,14.262h83.03a42.774,42.774,0,0,0-4.72-14.418A38.971,38.971,0,0,0,2739.61,484.557ZM2499.83,676.534a180.174,180.174,0,0,1-45.91-5.8q-22.1-5.8-36.83-17.4l19.43-35.1q10.65,8.77,27.11,13.947a107.814,107.814,0,0,0,32.44,5.172q26.01,0,37.77-11.6t11.76-34.477v-7.364a57.225,57.225,0,0,1-15.68,12.379q-16.29,8.778-37.61,8.776-23.505,0-42.47-10.187a79.67,79.67,0,0,1-30.4-28.522Q2408,548.027,2408,523.579q0-24.756,11.44-43.1a78.535,78.535,0,0,1,30.4-28.366q18.96-10.027,42.47-10.03,21.315,0,37.61,8.62a56.527,56.527,0,0,1,18.18,15.246V444.6h46.39V585.012q0,47.015-24.45,69.268T2499.83,676.534ZM2540.58,501.8a38.777,38.777,0,0,0-15.67-14.418,53.195,53.195,0,0,0-45.76,0,39.8,39.8,0,0,0-15.83,14.418q-5.805,9.249-5.8,21.783a40.316,40.316,0,0,0,5.8,21.627,39.408,39.408,0,0,0,15.83,14.575,53.2,53.2,0,0,0,45.76,0,38.412,38.412,0,0,0,15.67-14.575,41.22,41.22,0,0,0,5.64-21.627Q2546.22,511.048,2540.58,501.8ZM2328.71,593.73a55.305,55.305,0,0,1-14.58,12.6q-15.825,9.4-38.08,9.4-23.82,0-42.94-10.813a79.327,79.327,0,0,1-30.25-30.246q-11.13-19.431-11.12-45.761,0-26.637,11.12-46.075a78.248,78.248,0,0,1,30.25-30.089q19.11-10.652,42.94-10.657,21.315,0,37.3,9.4a56.564,56.564,0,0,1,13.16,10.706V380.655h48.9V613.221h-46.7V593.73Zm-7.06-89.9a41.6,41.6,0,0,0-15.51-15.985,42.3,42.3,0,0,0-21.63-5.642,42.891,42.891,0,0,0-21.94,5.642,41.455,41.455,0,0,0-15.51,15.985q-5.8,10.344-5.8,25.075,0,14.419,5.8,24.918a41.153,41.153,0,0,0,15.51,16.141,42.881,42.881,0,0,0,21.94,5.642,42.292,42.292,0,0,0,21.63-5.642,41.291,41.291,0,0,0,15.51-16.141q5.8-10.5,5.8-24.918Q2327.45,514.18,2321.65,503.833ZM1996.78,393.819h165.81v40.746H2047.25v47.641h101.86V521.7H2047.25v50.776h119.41v40.746H1996.78v-219.4Zm-93.39,198.433a42.914,42.914,0,0,1-15.05,15.014q-14.1,8.462-36.36,8.462-19.74,0-33.85-6.738t-21.62-18.336a46.727,46.727,0,0,1-7.53-26.015q0-15.045,7.37-26.328T1819.7,520.6q15.99-6.42,41.69-6.425h38.86q0-15.671-9.56-24.448t-29.3-8.776a85.108,85.108,0,0,0-26.49,4.231,70.15,70.15,0,0,0-22.1,11.44l-17.55-34.164q13.785-9.711,33.22-15.044a148.785,148.785,0,0,1,39.5-5.329q38.55,0,59.86,18.179T1949.15,517v96.224h-45.76V592.252Zm-3.14-49.553h-33.54q-17.235,0-23.66,5.642a17.977,17.977,0,0,0-6.43,14.1,17.584,17.584,0,0,0,7.37,14.888q7.365,5.487,20.22,5.485a41.966,41.966,0,0,0,21.94-5.8,33.265,33.265,0,0,0,14.1-17.082V542.7Zm-193.07-18.493q0-20.37-8.46-30.089-8.475-9.711-23.83-9.716a37.7,37.7,0,0,0-19.11,4.858,32.654,32.654,0,0,0-13.17,14.574q-4.695,9.719-4.7,24.761v84.627h-48.9V524.206q0-20.37-8.3-30.089t-23.67-9.716a37.7,37.7,0,0,0-19.11,4.858,32.654,32.654,0,0,0-13.17,14.574q-4.695,9.719-4.7,24.761v84.627h-48.9V444.6h46.71v19.47a58.845,58.845,0,0,1,16.14-13.045q16.14-8.933,36.83-8.933,23.19,0,40.58,11.754a50.959,50.959,0,0,1,14.99,15.69,69.126,69.126,0,0,1,17.93-15.847q18.63-11.592,42.78-11.6,20.055,0,35.57,7.993t24.45,24.447q8.94,16.455,8.93,42.157v96.537h-48.89V524.206Zm-325.97,68.046a42.8,42.8,0,0,1-15.04,15.014q-14.1,8.462-36.36,8.462-19.74,0-33.85-6.738t-21.63-18.336a46.8,46.8,0,0,1-7.52-26.015q0-15.045,7.37-26.328,7.35-11.285,23.35-17.709,15.975-6.42,41.68-6.425h38.87q0-15.671-9.56-24.448t-29.31-8.776a85.092,85.092,0,0,0-26.48,4.231,70.042,70.042,0,0,0-22.1,11.44l-17.55-34.164q13.785-9.711,33.22-15.044a148.785,148.785,0,0,1,39.5-5.329q38.55,0,59.86,18.179T1426.97,517v96.224h-45.76V592.252Zm-3.13-49.553h-33.54q-17.235,0-23.66,5.642a17.977,17.977,0,0,0-6.43,14.1,17.584,17.584,0,0,0,7.37,14.888q7.35,5.487,20.21,5.485a41.966,41.966,0,0,0,21.94-5.8,33.235,33.235,0,0,0,14.11-17.082V542.7ZM1183.44,380.655h48.89V613.221h-48.89V380.655ZM997.578,393.819h50.772V571.848h110.02v41.373H997.578v-219.4Z""/>
</svg>
"
crates/chat-prompts/.gitignore,"/target
/Cargo.lock
"
crates/chat-prompts/Cargo.toml,"[package]
name = ""chat-prompts""
version = ""0.14.0""
edition = ""2021""
readme = ""README.md""
repository = ""https://github.com/LlamaEdge/LlamaEdge""
license = ""Apache-2.0""
documentation = ""https://docs.rs/chat-prompts/""
categories = [""data-structures""]
description = ""Chat prompt template""

[dependencies]
endpoints.workspace = true
thiserror.workspace = true
enum_dispatch = ""0.3.12""
image = ""=0.25.0""
base64.workspace = true
clap.workspace = true
serde.workspace = true
serde_json.workspace = true
"
crates/chat-prompts/README.md,"# Prompt Templates for LLMs

`chat-prompts` is part of [LlamaEdge API Server](https://github.com/LlamaEdge/LlamaEdge/tree/main/api-server) project. It provides a collection of prompt templates that are used to generate prompts for the LLMs (See models in [huggingface.co/second-state](https://huggingface.co/second-state)).

## Prompt Templates

The available prompt templates are listed below:

- `baichuan-2`
  - Prompt string

    ```text
    以下内容为人类用户与与一位智能助手的对话。

    用户:你好！
    助手:
    ```

  - Example: [second-state/Baichuan2-13B-Chat-GGUF](https://huggingface.co/second-state/Baichuan2-13B-Chat-GGUF)

- `codellama-instruct`
  - Prompt string

    ```text
    <s>[INST] <<SYS>>
    Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```: <</SYS>>

    {prompt} [/INST]
    ```

  - Example: [second-state/CodeLlama-13B-Instruct-GGUF](https://huggingface.co/second-state/CodeLlama-13B-Instruct-GGUF)

- `codellama-super-instruct`
  - Prompt string

    ```text
    <s>Source: system\n\n {system_prompt} <step> Source: user\n\n {user_message_1} <step> Source: assistant\n\n {ai_message_1} <step> Source: user\n\n {user_message_2} <step> Source: assistant\nDestination: user\n\n
    ```

  - Example: [second-state/CodeLlama-70b-Instruct-hf-GGUF](https://huggingface.co/second-state/CodeLlama-70b-Instruct-hf-GGUF)

- `chatml`
  - Prompt string

    ```text
    <|im_start|>system
    {system_message}<|im_end|>
    <|im_start|>user
    {prompt}<|im_end|>
    <|im_start|>assistant
    ```

  - Example: [second-state/Yi-34B-Chat-GGUF](https://huggingface.co/second-state/Yi-34B-Chat-GGUF)

- `chatml-tool`
  - Prompt string

    ```text
    <|im_start|>system\n{system_message} Here are the available tools: <tools> [{tool_1}, {tool_2}] </tools> Use the following pydantic model json schema for each tool call you will make: {""properties"": {""arguments"": {""title"": ""Arguments"", ""type"": ""object""}, ""name"": {""title"": ""Name"", ""type"": ""string""}}, ""required"": [""arguments"", ""name""], ""title"": ""FunctionCall"", ""type"": ""object""} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{""arguments"": <args-dict>, ""name"": <function-name>}\n</tool_call><|im_end|>
    <|im_start|>user
    {user_message}<|im_end|>
    <|im_start|>assistant
    ```

    - Example

      ```text
      <|im_start|>system\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> [{""type"":""function"",""function"":{""name"":""get_current_weather"",""description"":""Get the current weather in a given location"",""parameters"":{""type"":""object"",""properties"":{""location"":{""type"":""string"",""description"":""The city and state, e.g. San Francisco, CA""},""format"":{""type"":""string"",""description"":""The temperature unit to use. Infer this from the users location."",""enum"":[""celsius"",""fahrenheit""]}},""required"":[""location"",""format""]}}},{""type"":""function"",""function"":{""name"":""predict_weather"",""description"":""Predict the weather in 24 hours"",""parameters"":{""type"":""object"",""properties"":{""location"":{""type"":""string"",""description"":""The city and state, e.g. San Francisco, CA""},""format"":{""type"":""string"",""description"":""The temperature unit to use. Infer this from the users location."",""enum"":[""celsius"",""fahrenheit""]}},""required"":[""location"",""format""]}}}] </tools> Use the following pydantic model json schema for each tool call you will make: {""properties"": {""arguments"": {""title"": ""Arguments"", ""type"": ""object""}, ""name"": {""title"": ""Name"", ""type"": ""string""}}, ""required"": [""arguments"", ""name""], ""title"": ""FunctionCall"", ""type"": ""object""} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{""arguments"": <args-dict>, ""name"": <function-name>}\n</tool_call><|im_end|>
      <|im_start|>user
      Hey! What is the weather like in Beijing?<|im_end|>
      <|im_start|>assistant
      ```

  - Example: [second-state/Hermes-2-Pro-Llama-3-8B-GGUF](https://huggingface.co/second-state/Hermes-2-Pro-Llama-3-8B-GGUF)

- `deepseek-chat`
  - Prompt string

    ```text
    User: {user_message_1}

    Assistant: {assistant_message_1}<｜end▁of▁sentence｜>User: {user_message_2}

    Assistant:
    ```

  - Example: [second-state/Deepseek-LLM-7B-Chat-GGUF](https://huggingface.co/second-state/Deepseek-LLM-7B-Chat-GGUF)

- `deepseek-chat-2`
  - Prompt string

    ```text
    <|begin_of_sentence|>{system_message}

    User: {user_message_1}

    Assistant: {assistant_message_1}<|end_of_sentence|>User: {user_message_2}

    Assistant:
    ```

  - Example: [second-state/DeepSeek-Coder-V2-Lite-Instruct-GGUF](https://huggingface.co/second-state/DeepSeek-Coder-V2-Lite-Instruct-GGUF)

- `deepseek-chat-25`
  - Prompt string

    ```text
    <|begin_of_sentence|>{system_message}<|User|>{user_message_1}<|Assistant|>{assistant_message_1}<|end_of_sentence|><|User|>{user_message_2}<|Assistant|>
    ```

- `deepseek-coder`
  - Prompt string

    ```text
    {system}
    ### Instruction:
    {question_1}
    ### Response:
    {answer_1}
    <|EOT|>
    ### Instruction:
    {question_2}
    ### Response:
    ```

  - Example: [second-state/Deepseek-Coder-6.7B-Instruct-GGUF](https://huggingface.co/second-state/Deepseek-Coder-6.7B-Instruct-GGUF)

- `embedding`
  - Prompt string
    This prompt template is only used for embedding models. It works as a placeholder, therefore, it has no concrete prompt string.

  - Example: [second-state/E5-Mistral-7B-Instruct-Embedding-GGUF](https://huggingface.co/second-state/E5-Mistral-7B-Instruct-Embedding-GGUF)

- `gemma-instruct`
  - Prompt string

    ```text
    <bos><start_of_turn>user
    {user_message}<end_of_turn>
    <start_of_turn>model
    {model_message}<end_of_turn>model
    ```

  - Example: [second-state/gemma-2-27b-it-GGUF](https://huggingface.co/second-state/gemma-2-27b-it-GGUF)

- `glm-4-chat`
  - Prompt string

    ```text
    [gMASK]<|system|>
    {system_message}<|user|>
    {user_message_1}<|assistant|>
    {assistant_message_1}
    ```

  - Example: [second-state/glm-4-9b-chat-GGUF](https://huggingface.co/second-state/glm-4-9b-chat-GGUF)

- `human-assistant`
  - Prompt string

    ```text
    Human: {input_1}\n\nAssistant:{output_1}Human: {input_2}\n\nAssistant:
    ```

  - Example: [second-state/OrionStar-Yi-34B-Chat-Llama-GGUF](https://huggingface.co/second-state/OrionStar-Yi-34B-Chat-Llama-GGUF)

- `intel-neural`
  - Prompt string

    ```text
    ### System:
    {system}
    ### User:
    {usr}
    ### Assistant:
    ```

  - Example: [second-state/Neural-Chat-7B-v3-3-GGUF](https://huggingface.co/second-state/Neural-Chat-7B-v3-3-GGUF)

- `llama-2-chat`
  - Prompt string

    ```text
    <s>[INST] <<SYS>>
    {system_message}
    <</SYS>>

    {user_message_1} [/INST] {assistant_message} </s><s>[INST] {user_message_2} [/INST]
    ```

- `llama-3-chat`
  - Prompt string

    ```text
    <|begin_of_text|><|start_header_id|>system<|end_header_id|>

    {{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>

    {{ user_message_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

    {{ model_answer_1 }}<|eot_id|><|start_header_id|>user<|end_header_id|>

    {{ user_message_2 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
    ```

- `mediatek-breeze`
  - Prompt string

    ```text
    <s>{system_message}  [INST] {user_message_1} [/INST] {assistant_message_1} [INST] {user_message_2} [/INST]
    ```

  - Example: [second-state/Breeze-7B-Instruct-v1_0-GGUF](https://huggingface.co/second-state/Breeze-7B-Instruct-v1_0-GGUF)

- `mistral-instruct`
  - Prompt string

    ```text
    <s>[INST] {user_message_1} [/INST]{assistant_message_1}</s>[INST] {user_message_2} [/INST]{assistant_message_2}</s>
    ```

  - Example: [second-state/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/second-state/Mistral-7B-Instruct-v0.3-GGUF)

- `mistrallite`
  - Prompt string

    ```text
    <|prompter|>{user_message}</s><|assistant|>{assistant_message}</s>
    ```

  - Example: [second-state/MistralLite-7B-GGUF](https://huggingface.co/second-state/MistralLite-7B-GGUF)

- `mistral-tool`
  - Prompt string

    ```text
    [INST] {user_message_1} [/INST][TOOL_CALLS] [{tool_call_1}]</s>[TOOL_RESULTS]{tool_result_1}[/TOOL_RESULTS]{assistant_message_1}</s>[AVAILABLE_TOOLS] [{tool_1},{tool_2}][/AVAILABLE_TOOLS][INST] {user_message_2} [/INST]
    ```

    - Example

      ```text
      [INST] Hey! What is the weather like in Beijing and Tokyo? [/INST][TOOL_CALLS] [{""name"":""get_current_weather"",""arguments"":{""location"": ""Beijing, CN"", ""format"": ""celsius""}}]</s>[TOOL_RESULTS]Fine, with a chance of showers.[/TOOL_RESULTS]Today in Auckland, the weather is expected to be partly cloudy with a high chance of showers. Be prepared for possible rain and carry an umbrella if you're venturing outside. Have a great day!</s>[AVAILABLE_TOOLS] [{""type"":""function"",""function"":{""name"":""get_current_weather"",""description"":""Get the current weather in a given location"",""parameters"":{""type"":""object"",""properties"":{""location"":{""type"":""string"",""description"":""The city and state, e.g. San Francisco, CA""},""unit"":{""type"":""string"",""enum"":[""celsius"",""fahrenheit""]}},""required"":[""location""]}}},{""type"":""function"",""function"":{""name"":""predict_weather"",""description"":""Predict the weather in 24 hours"",""parameters"":{""type"":""object"",""properties"":{""location"":{""type"":""string"",""description"":""The city and state, e.g. San Francisco, CA""},""unit"":{""type"":""string"",""enum"":[""celsius"",""fahrenheit""]}},""required"":[""location""]}}}][/AVAILABLE_TOOLS][INST] What is the weather like in Beijing now?[/INST]
      ```

  - Example: [second-state/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/second-state/Mistral-7B-Instruct-v0.3-GGUF)

- `octopus`
  - Prompt string

    ```text
    {system_prompt}\n\nQuery: {input_text} \n\nResponse:
    ```

  - Example: [second-state/Octopus-v2-GGUF](https://huggingface.co/second-state/Octopus-v2-GGUF)

- `openchat`
  - Prompt string

    ```text
    GPT4 User: {prompt}<|end_of_turn|>GPT4 Assistant:
    ```

  - Example: [second-state/OpenChat-3.5-0106-GGUF](https://huggingface.co/second-state/OpenChat-3.5-0106-GGUF)

- `phi-2-instruct`
  - Prompt string

    ```text
    Instruct: <prompt>\nOutput:
    ```

  - Example: [second-state/phi-2-GGUF](https://huggingface.co/second-state/phi-2-GGUF)

- `phi-3-chat`
  - Prompt string

    ```text
    <|system|>
    {system_message}<|end|>
    <|user|>
    {user_message_1}<|end|>
    <|assistant|>
    {assistant_message_1}<|end|>
    <|user|>
    {user_message_2}<|end|>
    <|assistant|>
    ```

  - Example: [second-state/Phi-3-medium-4k-instruct-GGUF](https://huggingface.co/second-state/Phi-3-medium-4k-instruct-GGUF)

- `solar-instruct`
  - Prompt string

    ```text
    <s> ### User:
    {user_message}

    \### Assistant:
    {assistant_message}</s>
    ```

  - Example: [second-state/SOLAR-10.7B-Instruct-v1.0-GGUF](https://huggingface.co/second-state/SOLAR-10.7B-Instruct-v1.0-GGUF)

- `stablelm-zephyr`
  - Prompt string

    ```text
    <|user|>
    {prompt}<|endoftext|>
    <|assistant|>
    ```

  - Example: [second-state/stablelm-2-zephyr-1.6b-GGUF](https://huggingface.co/second-state/stablelm-2-zephyr-1.6b-GGUF)

- `vicuna-1.0-chat`
  - Prompt string

    ```text
    {system} USER: {prompt} ASSISTANT:
    ```

  - Example: [second-state/Wizard-Vicuna-13B-Uncensored-GGUF](https://huggingface.co/second-state/Wizard-Vicuna-13B-Uncensored-GGUF)

- `vicuna-1.1-chat`
  - Prompt string

    ```text
    USER: {prompt}
    ASSISTANT:
    ```

  - Example: [second-state/ChatAllInOne-Yi-34B-200K-V1-GGUF](https://huggingface.co/second-state/ChatAllInOne-Yi-34B-200K-V1-GGUF)

- `vicuna-llava`
  - Prompt string

    ```text
    <system_prompt>\nUSER:<image_embeddings>\n<textual_prompt>\nASSISTANT:
    ```

  - Example: [second-state/Llava-v1.6-Vicuna-7B-GGUF](https://huggingface.co/second-state/Llava-v1.6-Vicuna-7B-GGUF)

- `wizard-coder`
  - Prompt string

    ```text
    {system}

    ### Instruction:
    {instruction}

    ### Response:
    ```

  - Example: [second-state/WizardCoder-Python-7B-v1.0-GGUF](https://huggingface.co/second-state/WizardCoder-Python-7B-v1.0-GGUF)

- `zephyr`
  - Prompt string

    ```text
    <|system|>
    {system_prompt}</s>
    <|user|>
    {prompt}</s>
    <|assistant|>
    ```

  - Example: [second-state/Zephyr-7B-Beta-GGUF](https://huggingface.co/second-state/Zephyr-7B-Beta-GGUF)
"
crates/chat-prompts/src/chat/baichuan.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `Baichuan-2-chat` model.
#[derive(Debug, Default, Clone)]
pub struct Baichuan2ChatPrompt;
impl Baichuan2ChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        message.content().to_string()
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => match system_prompt.as_ref().is_empty() {
                true => {
                    format!(""用户:{user_message}"", user_message = content.trim(),)
                }
                false => {
                    format!(
                        ""{system_prompt}\n\n用户:{user_message}"",
                        system_prompt = system_prompt.as_ref().trim(),
                        user_message = content.trim(),
                    )
                }
            },
            false => format!(
                ""{chat_history}用户:{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n助手:{assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Baichuan2ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""以下内容为人类用户与一位智能助手的对话。""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n助手:"");

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/belle.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionUserMessage,
    ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `BELLE-Llama2-13B-chat` model.
#[derive(Debug, Default, Clone)]
pub struct HumanAssistantChatPrompt;
impl HumanAssistantChatPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(""Human: \n{user_message}"", user_message = content.trim(),),
            false => format!(
                ""{chat_history}\nHuman: \n{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{prompt}\n\nAssistant:{assistant_message}"",
            prompt = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for HumanAssistantChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n\nAssistant:\n"");

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/chatml.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionToolMessage, ChatCompletionUserMessage, ChatCompletionUserMessageContent,
    ContentPart, Tool,
};

/// Generate prompts for the models using ChatML template.
#[derive(Debug, Default, Clone)]
pub struct ChatMLPrompt;
impl ChatMLPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
            false => format!(
                ""<|im_start|>system\n{system_prompt}<|im_end|>"",
                system_prompt = content
            ),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => match system_prompt.as_ref().is_empty() {
                true => {
                    format!(
                        ""<|im_start|>user\n{user_message}<|im_end|>"",
                        user_message = content.trim(),
                    )
                }
                false => {
                    format!(
                        ""{system_prompt}\n<|im_start|>user\n{user_message}<|im_end|>"",
                        system_prompt = system_prompt.as_ref().trim(),
                        user_message = content.trim(),
                    )
                }
            },
            false => format!(
                ""{chat_history}\n<|im_start|>user\n{user_message}<|im_end|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n<|im_start|>assistant\n{assistant_message}<|im_end|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }

    /// create a tool prompt from a chat completion request message.
    fn append_tool_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionToolMessage,
    ) -> Result<String> {
        let content = message.content();

        Ok(format!(
            ""{chat_history}\n<|im_start|>tool\n{tool_message}<|im_end|>"",
            chat_history = chat_history.as_ref().trim(),
            tool_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for ChatMLPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|im_start|>assistant"");

        Ok(prompt)
    }
}

/// Generate prompts for the models using ChatML template.
#[derive(Debug, Default, Clone)]
pub struct ChatMLToolPrompt;
impl ChatMLToolPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
            false => format!(
                ""<|im_start|>system\n{system_prompt}<|im_end|>"",
                system_prompt = content
            ),
        }
    }

    fn create_system_prompt_tool(
        &self,
        message: &ChatCompletionSystemMessage,
        tools: Option<&[Tool]>,
    ) -> String {
        let content = message.content();
        match content.is_empty() {
            true => match tools {
                Some(tools) => {
                    let available_tools = serde_json::to_string(tools).unwrap();
                    let tools = format!(""<tools> {} </tools>"", available_tools);

                    let begin = r#""<|im_start|>system\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:""#;

                    let end = r#""Use the following pydantic model json schema for each tool call you will make: {""properties"": {""arguments"": {""title"": ""Arguments"", ""type"": ""object""}, ""name"": {""title"": ""Name"", ""type"": ""string""}}, ""required"": [""arguments"", ""name""], ""title"": ""FunctionCall"", ""type"": ""object""} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{""arguments"": <args-dict>, ""name"": <function-name>}\n</tool_call><|im_end|>""#;

                    format!(""{} {} {}"", begin, tools, end)
                }
                None => {
                    String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>"")
                }
            },
            false => match tools {
                Some(tools) => {
                    let available_tools = serde_json::to_string(tools).unwrap();
                    let tools = format!(""<tools> {} </tools>"", available_tools);

                    let begin = format!(
                            ""<|im_start|>system\n{system_prompt}\nYou are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:"",
                            system_prompt = content
                        );

                    let end = r#""Use the following pydantic model json schema for each tool call you will make: {""properties"": {""arguments"": {""title"": ""Arguments"", ""type"": ""object""}, ""name"": {""title"": ""Name"", ""type"": ""string""}}, ""required"": [""arguments"", ""name""], ""title"": ""FunctionCall"", ""type"": ""object""} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{""arguments"": <args-dict>, ""name"": <function-name>}\n</tool_call><|im_end|>""#;

                    format!(""{} {} {}"", begin, tools, end)
                }
                None => {
                    format!(
                        ""<|im_start|>system\n{system_prompt}<|im_end|>"",
                        system_prompt = content
                    )
                }
            },
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => match system_prompt.as_ref().is_empty() {
                true => {
                    format!(
                        ""<|im_start|>user\n{user_message}<|im_end|>"",
                        user_message = content.trim(),
                    )
                }
                false => {
                    format!(
                        ""{system_prompt}\n<|im_start|>user\n{user_message}<|im_end|>"",
                        system_prompt = system_prompt.as_ref().trim(),
                        user_message = content.trim(),
                    )
                }
            },
            false => format!(
                ""{chat_history}\n<|im_start|>user\n{user_message}<|im_end|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n<|im_start|>assistant\n{assistant_message}<|im_end|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }

    /// create a tool prompt from a chat completion request message.
    fn append_tool_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionToolMessage,
    ) -> String {
        format!(
            ""{chat_history}\n<|im_start|>tool\n<tool_response>\n{tool_message}\n</tool_response>\n<|im_end|>"",
            chat_history = chat_history.as_ref().trim(),
            tool_message = message.content().trim(),
        )
    }
}
impl BuildChatPrompt for ChatMLToolPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|im_start|>assistant"");

        Ok(prompt)
    }

    fn build_with_tools(
        &self,
        messages: &mut Vec<ChatCompletionRequestMessage>,
        tools: Option<&[Tool]>,
    ) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt_tool(message, tools)
            }
            _ => match tools {
                Some(tools) => {
                    let available_tools = serde_json::to_string(tools).unwrap();
                    let tools = format!(""<tools> {} </tools>"", available_tools);

                    let begin = r#""<|im_start|>system\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:""#;

                    let end = r#""Use the following pydantic model json schema for each tool call you will make: {""properties"": {""arguments"": {""title"": ""Arguments"", ""type"": ""object""}, ""name"": {""title"": ""Name"", ""type"": ""string""}}, ""required"": [""arguments"", ""name""], ""title"": ""FunctionCall"", ""type"": ""object""} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{""arguments"": <args-dict>, ""name"": <function-name>}\n</tool_call><|im_end|>""#;

                    format!(""{} {} {}"", begin, tools, end)
                }
                None => {
                    String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>"")
                }
            },
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|im_start|>assistant"");

        Ok(prompt)
    }
}

/// Generate prompts for InternLM-2.5 models in tool use scenario.
pub struct InternLM2ToolPrompt;
impl InternLM2ToolPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
            false => format!(
                ""<|im_start|>system\n{system_prompt}<|im_end|>"",
                system_prompt = content
            ),
        }
    }

    fn create_system_prompt_tool(
        &self,
        message: &ChatCompletionSystemMessage,
        tools: Option<&[Tool]>,
    ) -> String {
        let content = message.content();
        match content.is_empty() {
            true => match tools {
                Some(tools) => {
                    let begin = ""<|im_start|>system\nYou are InternLM2-Chat, a harmless AI assistant.<|im_end|>"";

                    let available_tools = serde_json::to_string_pretty(tools).unwrap();
                    let tools = format!(""<|im_start|>system name=<|plugin|>\n{}\n<|im_end|>"", available_tools);

                    format!(""{}\n{}"", begin, tools)
                }
                None => {
                    String::from(""<|im_start|>system\nYou are InternLM2-Chat, a harmless AI assistant.<|im_end|>"")
                }
            },
            false => match tools {
                Some(tools) => {
                    let begin = format!(""<|im_start|>system\n{}<|im_end|>"", content);

                    let available_tools = serde_json::to_string_pretty(tools).unwrap();
                    let tools = format!(""<|im_start|>system name=<|plugin|>\n{}\n<|im_end|>"", available_tools);

                    format!(""{}\n{}"", begin, tools)
                }
                None => {
                    format!(""<|im_start|>system\n{}<|im_end|>"", content)
                }
            },
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => match system_prompt.as_ref().is_empty() {
                true => {
                    format!(
                        ""<|im_start|>user\n{user_message}<|im_end|>"",
                        user_message = content.trim(),
                    )
                }
                false => {
                    format!(
                        ""{system_prompt}\n<|im_start|>user\n{user_message}<|im_end|>"",
                        system_prompt = system_prompt.as_ref().trim(),
                        user_message = content.trim(),
                    )
                }
            },
            false => format!(
                ""{chat_history}\n<|im_start|>user\n{user_message}<|im_end|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n<|im_start|>assistant\n{assistant_message}<|im_end|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }

    /// create a tool prompt from a chat completion request message.
    fn append_tool_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionToolMessage,
    ) -> String {
        format!(
            ""{chat_history}\n<|im_start|>environment name=<|plugin|>\n{tool_message}<|im_end|>"",
            chat_history = chat_history.as_ref().trim(),
            tool_message = message.content().trim(),
        )
    }
}
impl BuildChatPrompt for InternLM2ToolPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|im_start|>assistant"");

        Ok(prompt)
    }

    fn build_with_tools(
        &self,
        messages: &mut Vec<ChatCompletionRequestMessage>,
        tools: Option<&[Tool]>,
    ) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt_tool(message, tools)
            }
            _ => match tools {
                Some(tools) => {
                    let begin = ""<|im_start|>system\nYou are InternLM2-Chat, a harmless AI assistant.<|im_end|>"";

                    let available_tools = serde_json::to_string_pretty(tools).unwrap();
                    let tools = format!(""<|im_start|>system name=<|plugin|>\n{}\n<|im_end|>"", available_tools);

                    format!(""{}\n{}"", begin, tools)
                }
                None => {
                    String::from(""<|im_start|>system\nYou are InternLM2-Chat, a harmless AI assistant.<|im_end|>"")
                }
            },
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|im_start|>assistant"");

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/deepseek.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `DeepSeek-LLM-Chat` model.
#[derive(Debug, Default, Clone)]
pub struct DeepseekChatPrompt;
impl DeepseekChatPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(""User: {user_message}"", user_message = content.trim(),),
            false => format!(
                ""{chat_history}User: {user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n\nAssistant: {assistant_message}<|end_of_sentence|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for DeepseekChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n\nAssistant:"");

        Ok(prompt)
    }
}

/// Generate prompts for the `DeepSeek-Coder` model.
#[derive(Debug, Default, Clone)]
pub struct DeepseekCoderPrompt;
impl DeepseekCoderPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.""),
            false => content.to_string(),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n### Instruction:\n{user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n### Instruction:\n{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_histroy}\n### Response:\n{assistant_message}\n<|EOT|>"",
            chat_histroy = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for DeepseekCoderPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.""),
        };

        // append user/assistant messages
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n### Response:"");

        Ok(prompt)
    }
}

/// Generate prompts for the `DeepSeek-V2` models.
#[derive(Debug, Default, Clone)]
pub struct DeepseekChat2Prompt;
impl DeepseekChat2Prompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|begin▁of▁sentence|>You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.""),
            false => format!(""<|begin▁of▁sentence|>{system_message}"", system_message=content),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n\nUser: {user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}User: {user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_histroy}\n\nAssistant: {assistant_message}<|end_of_sentence|>"",
            chat_histroy = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for DeepseekChat2Prompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""<|begin▁of▁sentence|>You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.""),
        };

        // append user/assistant messages
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n\nAssistant:"");

        Ok(prompt)
    }
}

/// Generate prompts for the `DeepSeek-V2.5` models.
#[derive(Debug, Default, Clone)]
pub struct DeepseekChat25Prompt;
impl DeepseekChat25Prompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|begin▁of▁sentence|>""),
            false => format!(
                ""<|begin▁of▁sentence|>{system_message}"",
                system_message = content
            ),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}<|User|>{user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<|User|>{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_histroy}<|Assistant|>{assistant_message}<|end_of_sentence|>"",
            chat_histroy = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for DeepseekChat25Prompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<|begin▁of▁sentence|>""),
        };

        // append user/assistant messages
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|Assistant|>"");

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/gemma.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionUserMessage,
    ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `gemma-7b-it` model.
#[derive(Debug, Default, Clone)]
pub struct GemmaInstructPrompt;
impl GemmaInstructPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<bos><start_of_turn>user\n{user_message}<end_of_turn>\n<start_of_turn>model"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n<start_of_turn>user\n{user_message}<end_of_turn>\n<start_of_turn>model"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n{assistant_message}<end_of_turn>model"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for GemmaInstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/glm.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate chat prompt for the `microsoft/phi-2` model.
#[derive(Debug, Default, Clone)]
pub struct Glm4ChatPrompt;
impl Glm4ChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""[gMASK]<|system|>\nYou are a friendly chatbot.""),
            false => format!(""[gMASK]<|system|>\n{content}""),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}<|user|>\n{user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<|user|>\n{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}<|assistant|>\n{assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Glm4ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""[gMASK]<|system|>\nYou are a friendly chatbot.""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|assistant|>"");

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/groq.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionToolMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart, Tool,
};

/// Generate prompts for the `second-state/Llama-3-Groq-8B-Tool-Use-GGUF` model.
#[derive(Debug, Default, Clone)]
pub struct GroqLlama3ToolPrompt;
impl GroqLlama3ToolPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt_tool(&self, tools: Option<&[Tool]>) -> Result<String> {
        match tools {
            Some(tools) => {
                let mut available_tools = String::new();
                for tool in tools {
                    if available_tools.is_empty() {
                        available_tools
                            .push_str(&serde_json::to_string_pretty(&tool.function).unwrap());
                    } else {
                        available_tools.push('\n');
                        available_tools
                            .push_str(&serde_json::to_string_pretty(&tool.function).unwrap());
                    }
                }

                let tools = format!(
                    ""Here are the available tools:\n<tools> {} </tools>"",
                    available_tools
                );

                let format = r#""{""name"": <function-name>,""arguments"": <args-dict>}""#;
                let begin = format!(""<|start_header_id|>system<|end_header_id|>\n\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{}\n</tool_call>"", format);

                let end = r#""<|eot_id|>""#;

                Ok(format!(""{}\n\n{}{}"", begin, tools, end))
            }
            None => Err(PromptError::NoAvailableTools),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}<|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|>"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}<|start_header_id|>assistant<|end_header_id|>\n\n{assistant_message}<|eot_id|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }

    /// create a tool prompt from a chat completion request message.
    fn append_tool_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionToolMessage,
    ) -> String {
        format!(
            ""{chat_history}<|start_header_id|>tool<|end_header_id|>\n\n<tool_response>\n{tool_message}\n</tool_response><|eot_id|>"",
            chat_history = chat_history.as_ref().trim(),
            tool_message = message.content().trim(),
        )
    }
}
impl BuildChatPrompt for GroqLlama3ToolPrompt {
    fn build(&self, _messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        Err(PromptError::Operation(""The GroqToolPrompt struct is only designed for `Groq/Llama-3-Groq-8B-Tool-Use` model, which is for tool use ONLY instead of general knowledge or open-ended tasks."".to_string()))
    }

    fn build_with_tools(
        &self,
        messages: &mut Vec<ChatCompletionRequestMessage>,
        tools: Option<&[Tool]>,
    ) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = self.create_system_prompt_tool(tools)?;

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|start_header_id|>assistant<|end_header_id|>"");

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/intel.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `wizard-vicuna` model.
#[derive(Debug, Default, Clone)]
pub struct NeuralChatPrompt;
impl NeuralChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""### System:\nYou are a chatbot developed by Intel. Please answer all questions to the best of your ability.""),
            false => format!(""### System:\n{content}""),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n### User:\n{user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n### User:\n{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n### Assistant:\n{assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for NeuralChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""### System:\nYou are a chatbot developed by Intel. Please answer all questions to the best of your ability."")
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n### Assistant:"");

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/llama.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionToolMessage, ChatCompletionUserMessage, ChatCompletionUserMessageContent,
    ContentPart, Tool,
};

/// Generate prompts for the `Llama-2-chat` model.
#[derive(Debug, Default, Clone)]
pub struct Llama2ChatPrompt;
impl Llama2ChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe. <</SYS>>""),
            false =>format!(
                ""<<SYS>>\n{content} <</SYS>>""
            )
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => match system_prompt.as_ref().is_empty() {
                true => {
                    format!(
                        ""<s>[INST] {user_message} [/INST]"",
                        user_message = content.trim(),
                    )
                }
                false => {
                    format!(
                        ""<s>[INST] {system_prompt}\n\n{user_message} [/INST]"",
                        system_prompt = system_prompt.as_ref().trim(),
                        user_message = content.trim(),
                    )
                }
            },
            false => format!(
                ""{chat_history}<s>[INST] {user_message} [/INST]"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{prompt} {assistant_message} </s>"",
            prompt = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Llama2ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""<<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe. <</SYS>>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}

/// Generate prompts for the `Codellama-instruct` model.
#[derive(Debug, Default, Clone)]
pub struct CodeLlamaInstructPrompt;
impl CodeLlamaInstructPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<<SYS>>\nWrite code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```: <</SYS>>""),
            false => format!(
                ""<<SYS>>\n{system_prompt} <</SYS>>"", system_prompt=content
            )
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<s>[INST] {system_prompt}\n\n{user_message} [/INST]"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<s>[INST] {user_message} [/INST]"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{prompt} {assistant_message} </s>"",
            prompt = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for CodeLlamaInstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""<<SYS>>\nWrite code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```: <</SYS>>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}

/// Generate prompts for the `Codellama-70b-instruct-hf` model.
#[derive(Debug, Default, Clone)]
pub struct CodeLlamaSuperInstructPrompt;
impl CodeLlamaSuperInstructPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<s>Source: system\n\n Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```: <step>""),
            false => format!(
                ""<s>Source: system\n\n {content} <step>""
            )
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt} Source: user\n\n {user_message} <step>"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history} Source: user\n\n {user_message} <step>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{prompt} Source: assistant\n\n {assistant_message} <step>"",
            prompt = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for CodeLlamaSuperInstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""<s>Source: system\n\n Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```: <step>"")
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str("" Source: assistant\nDestination: user\n\n "");

        Ok(prompt)
    }
}

/// Generate prompts for the `Llama-3-chat` model.
///
/// Reference: <https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/>
#[derive(Debug, Default, Clone)]
pub struct Llama3ChatPrompt;
impl Llama3ChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.<|eot_id|>""),
            false =>format!(
                ""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>"", system_prompt=content
            )
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}<|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|>"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history} <|start_header_id|>assistant<|end_header_id|>\n\n{assistant_message}<|eot_id|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Llama3ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.<|eot_id|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|start_header_id|>assistant<|end_header_id|>"");

        Ok(prompt)
    }
}

/// Generate prompts for the `Llama-3.1-instruct` model.
///
/// Reference: <https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/#json-based-tool-calling>
#[derive(Debug, Default, Clone)]
pub struct Llama3ToolPrompt;
impl Llama3ToolPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.<|eot_id|>""),
            false =>format!(
                ""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>"", system_prompt=content
            )
        }
    }

    /// Create a system prompt for tool use.
    fn create_system_prompt_tool(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal use question.<|eot_id|>""),
            false =>format!(
                ""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>"", system_prompt=content
            )
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}<|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|>"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create a user prompt for tool use.
    fn append_user_message_tool(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
        tools: impl AsRef<[Tool]>,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => {
                let json = serde_json::to_string(tools.as_ref()).unwrap();

                format!(
                    ""{system_prompt}<|start_header_id|>user<|end_header_id|>\n\nGiven the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n\nRespond in the format {format}. Do not use variables.\n\n{available_tools}\n\nQuestion: {user_message}<|eot_id|>"",
                    system_prompt = system_prompt.as_ref().trim(),
                    format = r#""{""name"": function name, ""parameters"": dictionary of argument name and its value}""#,
                    available_tools = json,
                    user_message = content.trim(),
                )
            }
            false => {
                let json = serde_json::to_string(tools.as_ref()).unwrap();

                format!(
                    ""{chat_history}<|start_header_id|>user<|end_header_id|>\n\nGiven the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n\nRespond in the format {format}. Do not use variables.\n\n{available_tools}\n\nQuestion: {user_message}<|eot_id|>"",
                    chat_history = chat_history.as_ref().trim(),
                    format = r#""{""name"": function name, ""parameters"": dictionary of argument name and its value}""#,
                    available_tools = json,
                    user_message = content.trim(),
                )
            }
        }
    }

    /// Create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history} <|start_header_id|>assistant<|end_header_id|>\n\n{assistant_message}<|eot_id|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }

    /// Create a tool prompt.
    fn append_tool_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionToolMessage,
    ) -> String {
        format!(
            ""{chat_history}<|start_header_id|>ipython<|end_header_id|>\n\n{tool_result}<|eot_id|>"",
            chat_history = chat_history.as_ref().trim(),
            tool_result = message.content().trim()
        )
    }
}
impl BuildChatPrompt for Llama3ToolPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.<|eot_id|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|start_header_id|>assistant<|end_header_id|>"");

        Ok(prompt)
    }

    fn build_with_tools(
        &self,
        messages: &mut Vec<ChatCompletionRequestMessage>,
        tools: Option<&[endpoints::chat::Tool]>,
    ) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                match tools {
                    Some(available_tools) => match available_tools.is_empty() {
                        true => self.create_system_prompt(message),
                        false => self.create_system_prompt_tool(message),
                    },
                    None => self.create_system_prompt(message)
                }
            }
            _ => String::from(""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.<|eot_id|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = match tools {
                        Some(available_tools) => match available_tools.is_empty() {
                            true => self.append_user_message(&prompt, &system_prompt, message),
                            false => self.append_user_message_tool(
                                &prompt,
                                &system_prompt,
                                message,
                                available_tools,
                            ),
                        },
                        None => self.append_user_message(&prompt, &system_prompt, message),
                    };
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|start_header_id|>assistant<|end_header_id|>"");

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/mediatek.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `Breeze-7B-Instruct-v1_0` model
#[derive(Debug, Default, Clone)]
pub struct BreezeInstructPrompt;
impl BreezeInstructPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<s>You are a helpful AI assistant built by MediaTek Research. The user you are helping speaks Traditional Chinese and comes from Taiwan.""),
            false => format!(""<s>{content}""),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}  [INST] {user_message} [/INST]"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history} [INST] {user_message} [/INST]"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history} {assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for BreezeInstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<s>You are a helpful AI assistant built by MediaTek Research. The user you are helping speaks Traditional Chinese and comes from Taiwan.""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/mistral.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionToolMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart, Tool,
};

/// Generate prompts for the `Mistral-instruct` model.
#[derive(Debug, Default, Clone)]
pub struct MistralInstructPrompt;
impl MistralInstructPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<s>[INST] {user_message} [/INST]"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}[INST] {user_message} [/INST]"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}{assistant_message}</s>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for MistralInstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}

/// Generate prompts for the amazon `MistralLite-7B` model.
#[derive(Debug, Default, Clone)]
pub struct MistralLitePrompt;
impl MistralLitePrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<|prompter|>{user_message}</s>"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<|prompter|>{user_message}</s>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}<|assistant|>{assistant_message}</s>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for MistralLitePrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        // append user/assistant messages
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|assistant|>"");

        Ok(prompt)
    }
}

/// Generate prompts for the `Mistral-instruct` model.
#[derive(Debug, Default, Clone)]
pub struct MistralToolPrompt;
impl MistralToolPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<s>[INST] {user_message} [/INST]"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}[INST] {user_message} [/INST]"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message_tool(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
        tools: Option<&[Tool]>,
        last_user_message: bool,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => match last_user_message {
                true => match tools {
                    Some(tools) => {
                        let json = serde_json::to_string(tools).unwrap();

                        format!(
                            ""<s>[AVAILABLE_TOOLS] {available_tools}[/AVAILABLE_TOOLS][INST] {user_message}[/INST]"",
                            available_tools = json,
                            user_message = content.trim(),
                        )
                    }
                    None => format!(
                        ""<s>[INST] {user_message} [/INST]"",
                        user_message = content.trim(),
                    ),
                },
                false => format!(
                    ""{chat_history}[INST] {user_message} [/INST]"",
                    chat_history = chat_history.as_ref().trim(),
                    user_message = content.trim(),
                ),
            },
            false => match last_user_message {
                true => match tools {
                    Some(tools) => {
                        let json = serde_json::to_string(tools).unwrap();

                        format!(
                            ""{chat_history}[AVAILABLE_TOOLS] {available_tools}[/AVAILABLE_TOOLS][INST] {user_message}[/INST]"",
                            chat_history = chat_history.as_ref().trim(),
                            available_tools = json,
                            user_message = content.trim(),
                        )
                    }
                    None => format!(
                        ""{chat_history}[INST] {user_message} [/INST]"",
                        chat_history = chat_history.as_ref().trim(),
                        user_message = content.trim(),
                    ),
                },
                false => format!(
                    ""{chat_history}[INST] {user_message} [/INST]"",
                    chat_history = chat_history.as_ref().trim(),
                    user_message = content.trim(),
                ),
            },
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        let content = content.split(""\n"").next().unwrap_or_default();

        Ok(format!(
            ""{chat_history}{assistant_message}</s>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }

    fn append_tool_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionToolMessage,
    ) -> String {
        format!(
            ""{chat_history}[TOOL_RESULTS]{tool_result}[/TOOL_RESULTS]"",
            chat_history = chat_history.as_ref().trim(),
            tool_result = message.content().trim()
        )
    }
}
impl BuildChatPrompt for MistralToolPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }

    fn build_with_tools(
        &self,
        messages: &mut Vec<ChatCompletionRequestMessage>,
        tools: Option<&[endpoints::chat::Tool]>,
    ) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for (idx, message) in messages.iter().enumerate() {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    let last = idx == messages.len() - 1;
                    prompt = self.append_user_message_tool(&prompt, message, tools, last);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/mod.rs,"pub mod baichuan;
pub mod belle;
pub mod chatml;
pub mod deepseek;
pub mod gemma;
pub mod glm;
pub mod groq;
pub mod intel;
pub mod llama;
pub mod mediatek;
pub mod mistral;
pub mod octopus;
pub mod openchat;
pub mod phi;
pub mod solar;
pub mod vicuna;
pub mod wizard;
pub mod zephyr;

use crate::{error::Result, PromptTemplateType};
use baichuan::*;
use belle::*;
use chatml::*;
use deepseek::*;
use endpoints::chat::{ChatCompletionRequestMessage, Tool};
use gemma::*;
use glm::*;
use groq::*;
use intel::*;
use llama::*;
use mediatek::BreezeInstructPrompt;
use mistral::*;
use octopus::*;
use openchat::*;
use phi::*;
use solar::*;
use vicuna::*;
use wizard::*;
use zephyr::*;

/// Trait for building prompts for chat completions.
#[enum_dispatch::enum_dispatch]
pub trait BuildChatPrompt: Send {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String>;

    fn build_with_tools(
        &self,
        messages: &mut Vec<ChatCompletionRequestMessage>,
        _tools: Option<&[Tool]>,
    ) -> Result<String> {
        self.build(messages)
    }
}

#[enum_dispatch::enum_dispatch(BuildChatPrompt)]
pub enum ChatPrompt {
    Llama2ChatPrompt,
    Llama3ChatPrompt,
    Llama3ToolPrompt,
    MistralInstructPrompt,
    MistralToolPrompt,
    MistralLitePrompt,
    OpenChatPrompt,
    CodeLlamaInstructPrompt,
    CodeLlamaSuperInstructPrompt,
    HumanAssistantChatPrompt,
    /// Vicuna 1.0
    VicunaChatPrompt,
    /// Vicuna 1.1
    Vicuna11ChatPrompt,
    VicunaLlavaPrompt,
    ChatMLPrompt,
    ChatMLToolPrompt,
    InternLM2ToolPrompt,
    Baichuan2ChatPrompt,
    WizardCoderPrompt,
    ZephyrChatPrompt,
    StableLMZephyrChatPrompt,
    NeuralChatPrompt,
    DeepseekChatPrompt,
    DeepseekCoderPrompt,
    DeepseekChat2Prompt,
    DeepseekChat25Prompt,
    SolarInstructPrompt,
    Phi2ChatPrompt,
    Phi2InstructPrompt,
    Phi3ChatPrompt,
    Phi3InstructPrompt,
    GemmaInstructPrompt,
    OctopusPrompt,
    Glm4ChatPrompt,
    GroqLlama3ToolPrompt,
    BreezeInstructPrompt,
}
impl From<PromptTemplateType> for ChatPrompt {
    fn from(ty: PromptTemplateType) -> Self {
        match ty {
            PromptTemplateType::Llama2Chat => ChatPrompt::Llama2ChatPrompt(Llama2ChatPrompt),
            PromptTemplateType::Llama3Chat => ChatPrompt::Llama3ChatPrompt(Llama3ChatPrompt),
            PromptTemplateType::Llama3Tool => ChatPrompt::Llama3ToolPrompt(Llama3ToolPrompt),
            PromptTemplateType::MistralInstruct => {
                ChatPrompt::MistralInstructPrompt(MistralInstructPrompt)
            }
            PromptTemplateType::MistralTool => ChatPrompt::MistralToolPrompt(MistralToolPrompt),
            PromptTemplateType::MistralLite => ChatPrompt::MistralLitePrompt(MistralLitePrompt),
            PromptTemplateType::OpenChat => ChatPrompt::OpenChatPrompt(OpenChatPrompt),
            PromptTemplateType::CodeLlama => {
                ChatPrompt::CodeLlamaInstructPrompt(CodeLlamaInstructPrompt)
            }
            PromptTemplateType::CodeLlamaSuper => {
                ChatPrompt::CodeLlamaSuperInstructPrompt(CodeLlamaSuperInstructPrompt)
            }
            PromptTemplateType::HumanAssistant => {
                ChatPrompt::HumanAssistantChatPrompt(HumanAssistantChatPrompt)
            }
            PromptTemplateType::VicunaChat => ChatPrompt::VicunaChatPrompt(VicunaChatPrompt),
            PromptTemplateType::Vicuna11Chat => ChatPrompt::Vicuna11ChatPrompt(Vicuna11ChatPrompt),
            PromptTemplateType::VicunaLlava => ChatPrompt::VicunaLlavaPrompt(VicunaLlavaPrompt),
            PromptTemplateType::ChatML => ChatPrompt::ChatMLPrompt(ChatMLPrompt),
            PromptTemplateType::ChatMLTool => ChatPrompt::ChatMLToolPrompt(ChatMLToolPrompt),
            PromptTemplateType::InternLM2Tool => {
                ChatPrompt::InternLM2ToolPrompt(InternLM2ToolPrompt)
            }
            PromptTemplateType::Baichuan2 => ChatPrompt::Baichuan2ChatPrompt(Baichuan2ChatPrompt),
            PromptTemplateType::WizardCoder => ChatPrompt::WizardCoderPrompt(WizardCoderPrompt),
            PromptTemplateType::Zephyr => ChatPrompt::ZephyrChatPrompt(ZephyrChatPrompt),
            PromptTemplateType::StableLMZephyr => {
                ChatPrompt::StableLMZephyrChatPrompt(StableLMZephyrChatPrompt)
            }
            PromptTemplateType::IntelNeural => ChatPrompt::NeuralChatPrompt(NeuralChatPrompt),
            PromptTemplateType::DeepseekChat => ChatPrompt::DeepseekChatPrompt(DeepseekChatPrompt),
            PromptTemplateType::DeepseekCoder => {
                ChatPrompt::DeepseekCoderPrompt(DeepseekCoderPrompt)
            }
            PromptTemplateType::DeepseekChat2 => {
                ChatPrompt::DeepseekChat2Prompt(DeepseekChat2Prompt)
            }
            PromptTemplateType::DeepseekChat25 => {
                ChatPrompt::DeepseekChat25Prompt(DeepseekChat25Prompt)
            }
            PromptTemplateType::SolarInstruct => {
                ChatPrompt::SolarInstructPrompt(SolarInstructPrompt)
            }
            PromptTemplateType::Phi2Chat => ChatPrompt::Phi2ChatPrompt(Phi2ChatPrompt),
            PromptTemplateType::Phi2Instruct => ChatPrompt::Phi2InstructPrompt(Phi2InstructPrompt),
            PromptTemplateType::Phi3Chat => ChatPrompt::Phi3ChatPrompt(Phi3ChatPrompt),
            PromptTemplateType::Phi3Instruct => ChatPrompt::Phi3InstructPrompt(Phi3InstructPrompt),
            PromptTemplateType::GemmaInstruct => {
                ChatPrompt::GemmaInstructPrompt(GemmaInstructPrompt)
            }
            PromptTemplateType::Octopus => ChatPrompt::OctopusPrompt(OctopusPrompt),
            PromptTemplateType::Glm4Chat => ChatPrompt::Glm4ChatPrompt(Glm4ChatPrompt),
            PromptTemplateType::GroqLlama3Tool => {
                ChatPrompt::GroqLlama3ToolPrompt(GroqLlama3ToolPrompt)
            }
            PromptTemplateType::BreezeInstruct => {
                ChatPrompt::BreezeInstructPrompt(BreezeInstructPrompt)
            }
            PromptTemplateType::Embedding => {
                panic!(""Embedding prompt template is not used for building chat prompts"")
            }
            PromptTemplateType::Null => {
                panic!(""Null prompt template is not used for building chat prompts"")
            }
        }
    }
}
"
crates/chat-prompts/src/chat/octopus.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `Llama-2-chat` model.
#[derive(Debug, Default, Clone)]
pub struct OctopusPrompt;
impl OctopusPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""Below is the query from the users, please call the correct function and generate the parameters to call the function.""),
            false => content.to_string(),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n\nQuery: {user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n\nQuery: {user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history} \n\nResponse: {assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for OctopusPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""Below is the query from the users, please call the correct function and generate the parameters to call the function.""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str("" \n\nResponse:"");

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/openchat.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionUserMessage,
    ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the amazon `MistralLite-7B` model.
#[derive(Debug, Default, Clone)]
pub struct OpenChatPrompt;
impl OpenChatPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""GPT4 User: {user_message}<|end_of_turn|>"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}GPT4 User: {user_message}<|end_of_turn|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}GPT4 Assistant: {assistant_message}<|end_of_turn|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for OpenChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        // append user/assistant messages
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""GPT4 Assistant:"");

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/phi.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate instruct prompt for the `microsoft/phi-2` model.
#[derive(Debug, Default, Clone)]
pub struct Phi2InstructPrompt;
impl Phi2InstructPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(&self, message: &ChatCompletionUserMessage) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        format!(""Instruct: {user_message}"", user_message = content.trim(),)
    }
}
impl BuildChatPrompt for Phi2InstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        let mut prompt = if let Some(user_message) = messages.last() {
            match user_message {
                ChatCompletionRequestMessage::User(message) => self.append_user_message(message),
                _ => {
                    return Err(crate::error::PromptError::NoUserMessage);
                }
            }
        } else {
            return Err(crate::error::PromptError::NoMessages);
        };

        prompt.push_str(""\nOutput:"");

        Ok(prompt)
    }
}

/// Generate chat prompt for the `microsoft/phi-2` model.
#[derive(Debug, Default, Clone)]
pub struct Phi2ChatPrompt;
impl Phi2ChatPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(""Alice: {user_message}"", user_message = content.trim(),),
            false => format!(
                ""{chat_history}\nAlice: {user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\nBob: {assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Phi2ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\nBob:"");

        Ok(prompt)
    }
}

/// Generate chat prompt for the `microsoft/phi-3` model.
#[derive(Debug, Default, Clone)]
pub struct Phi3InstructPrompt;
impl Phi3InstructPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(&self, message: &ChatCompletionUserMessage) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        format!(
            ""<|user|>\n {user_message} <|end|>"",
            user_message = content.trim(),
        )
    }
}
impl BuildChatPrompt for Phi3InstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        let mut prompt = if let Some(user_message) = messages.last() {
            match user_message {
                ChatCompletionRequestMessage::User(message) => self.append_user_message(message),
                _ => {
                    return Err(crate::error::PromptError::NoUserMessage);
                }
            }
        } else {
            return Err(crate::error::PromptError::NoMessages);
        };

        prompt.push_str(""\n <|assistant|>"");

        Ok(prompt)
    }
}

/// Generate chat prompt for the `microsoft/phi-2` model.
#[derive(Debug, Default, Clone)]
pub struct Phi3ChatPrompt;
impl Phi3ChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|system|>\nYou are a friendly chatbot.<|end|>""),
            false => format!(""<|system|>\n{content}<|end|>""),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n<|user|>\n{user_message}<|end|>"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n<|user|>\n{user_message}<|end|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n<|assistant|>\n{assistant_message}<|end|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Phi3ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<|system|>\nYou are a friendly chatbot.<|end|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|assistant|>"");

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/solar.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionUserMessage,
    ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `Mistral-instruct-v0.1` model.
#[derive(Debug, Default, Clone)]
pub struct SolarInstructPrompt;
impl SolarInstructPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<s> ### User:\n{user_message}"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n\n<s> ### User:\n{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n\n### Assistant:\n{assistant_message}</s>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for SolarInstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/vicuna.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use base64::{engine::general_purpose, Engine as _};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};
use image::io::Reader as ImageReader;
use std::io::Cursor;

/// Vicuna-1.0 Prompt Template
#[derive(Debug, Default, Clone)]
pub struct VicunaChatPrompt;
impl VicunaChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.""),
            false => content.to_string(),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt} USER: {user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history} USER: {user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history} ASSISTANT: {assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for VicunaChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str("" ASSISTANT:"");

        Ok(prompt)
    }
}

/// Vicuna-1.1 Prompt Template
#[derive(Debug, Default, Clone)]
pub struct Vicuna11ChatPrompt;
impl Vicuna11ChatPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(""USER: {user_message}"", user_message = content.trim(),),
            false => format!(
                ""{chat_history}\nUSER: {user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\nASSISTANT: {assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Vicuna11ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }
        prompt.push_str("" ASSISTANT:"");

        Ok(prompt)
    }
}

/// Vicuna-1.0 Prompt Template
#[derive(Debug, Default, Clone)]
pub struct VicunaLlavaPrompt;
impl VicunaLlavaPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""You are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.""),
            false => content.to_string(),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> Result<String> {
        let prompt = match message.content() {
            ChatCompletionUserMessageContent::Text(content) => {
                match chat_history.as_ref().is_empty() {
                    true => format!(
                        ""{system_prompt}\nUSER: {user_message}"",
                        system_prompt = system_prompt.as_ref().trim(),
                        user_message = content.trim(),
                    ),
                    false => format!(
                        ""{chat_history}\nUSER: {user_message}"",
                        chat_history = chat_history.as_ref().trim(),
                        user_message = content.trim(),
                    ),
                }
            }
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                let mut image_content = String::new();
                for part in parts {
                    match part {
                        ContentPart::Text(text_content) => {
                            content.push_str(text_content.text());
                            content.push('\n');
                        }
                        ContentPart::Image(part) => {
                            image_content = match part.image().is_url() {
                                true => String::from(""<image>""),
                                false => {
                                    let base64_str = part.image().url.as_str();
                                    let format = is_image_format(base64_str)?;
                                    format!(
                                        r#""<img src=""data:image/{};base64,{}"">""#,
                                        format, base64_str
                                    )
                                }
                            };
                        }
                    }
                }

                match chat_history.as_ref().is_empty() {
                    true => format!(
                        ""{system_prompt}\nUSER:{image_embeddings}\n{user_message}"",
                        system_prompt = system_prompt.as_ref().trim(),
                        image_embeddings = image_content.trim(),
                        user_message = content.trim(),
                    ),
                    false => format!(
                        ""{chat_history}\nUSER:{image_embeddings}\n{user_message}"",
                        chat_history = chat_history.as_ref().trim(),
                        image_embeddings = image_content.trim(),
                        user_message = content.trim(),
                    ),
                }
            }
        };

        Ok(prompt)
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\nASSISTANT: {assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for VicunaLlavaPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""You are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message)?;
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\nASSISTANT:"");

        Ok(prompt)
    }
}

fn is_image_format(base64_str: &str) -> Result<String> {
    let image_data = match general_purpose::STANDARD.decode(base64_str) {
        Ok(data) => data,
        Err(_) => {
            return Err(PromptError::Operation(
                ""Failed to decode base64 string."".to_string(),
            ))
        }
    };

    let format = ImageReader::new(Cursor::new(image_data))
        .with_guessed_format()
        .unwrap()
        .format();

    let image_format = match format {
        Some(image::ImageFormat::Png) => ""png"".to_string(),
        Some(image::ImageFormat::Jpeg) => ""jpeg"".to_string(),
        Some(image::ImageFormat::Tga) => ""tga"".to_string(),
        Some(image::ImageFormat::Bmp) => ""bmp"".to_string(),
        Some(image::ImageFormat::Gif) => ""gif"".to_string(),
        Some(image::ImageFormat::Hdr) => ""hdr"".to_string(),
        Some(image::ImageFormat::Pnm) => ""pnm"".to_string(),
        _ => {
            return Err(PromptError::Operation(
                ""Unsupported image format."".to_string(),
            ))
        }
    };

    Ok(image_format)
}
"
crates/chat-prompts/src/chat/wizard.rs,"use super::BuildChatPrompt;
use crate::error::Result;
use endpoints::chat::{
    ChatCompletionRequestMessage, ChatCompletionSystemMessage, ChatCompletionUserMessage,
    ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `wizard-vicuna` model.
#[derive(Debug, Default, Clone)]
pub struct WizardCoderPrompt;
impl WizardCoderPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""Below is an instruction that describes a task. Write a response that appropriately completes the request.""),
            false => content.to_string(),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        format!(
            ""{system_prompt}\n\n### Instruction:\n{user_message}"",
            system_prompt = system_prompt.as_ref().trim(),
            user_message = content.trim(),
        )
    }
}
impl BuildChatPrompt for WizardCoderPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""Below is an instruction that describes a task. Write a response that appropriately completes the request.""),
        };

        let message = messages.last().unwrap();
        let mut prompt = match message {
            ChatCompletionRequestMessage::User(ref message) => {
                self.append_user_message(system_prompt, message)
            }
            _ => return Err(crate::error::PromptError::NoUserMessage),
        };

        prompt.push_str(""\n\n### Response:"");

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/chat/zephyr.rs,"use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

#[derive(Debug, Default, Clone)]
pub struct ZephyrChatPrompt;
impl ZephyrChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|system|>\nYou are a friendly chatbot.</s>""),
            false => format!(""<|system|>\n{content}</s>""),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n<|user|>\n{user_message}</s>"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n<|user|>\n{user_message}</s>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n<|assistant|>\n{assistant_message}</s>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for ZephyrChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<|system|>\nYou are a friendly chatbot.</s>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|assistant|>"");

        Ok(prompt)
    }
}

#[derive(Debug, Default, Clone)]
pub struct StableLMZephyrChatPrompt;
impl StableLMZephyrChatPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<|user|>\n{user_message}<|endoftext|>"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n<|user|>\n{user_message}<|endoftext|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n<|assistant|>\n{assistant_message}<|endoftext|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for StableLMZephyrChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|assistant|>"");

        Ok(prompt)
    }
}
"
crates/chat-prompts/src/error.rs,"use endpoints::chat::ChatCompletionRole;
use thiserror::Error;

pub type Result<T> = std::result::Result<T, PromptError>;

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum PromptError {
    #[error(""There must be at least one user message to create a prompt from."")]
    NoMessages,
    #[error(""No user message to create prompt from."")]
    NoUserMessage,
    #[error(""No content in the assistant message when the `tool_calls` is not specified."")]
    NoAssistantMessage,
    #[error(""No available tools to create prompt from."")]
    NoAvailableTools,
    #[error(""Bad messages. {0}"")]
    BadMessages(String),
    #[error(""Unknown chat completion role: {0:?}"")]
    UnknownRole(ChatCompletionRole),
    #[error(""Unknown prompt template type: {0}"")]
    UnknownPromptTemplateType(String),
    #[error(""Failed to build prompt. Reason: {0}"")]
    Operation(String),
}
"
crates/chat-prompts/src/lib.rs,"//! `chat-prompts` is part of [LlamaEdge API Server](https://github.com/LlamaEdge/LlamaEdge/tree/main/api-server) project. It provides a collection of prompt templates that are used to generate prompts for the LLMs (See models in [huggingface.co/second-state](https://huggingface.co/second-state)).
//!
//! For the details of available prompt templates, see [README.md](https://github.com/LlamaEdge/LlamaEdge/tree/main/api-server/chat-prompts).

pub mod chat;
pub mod error;

use clap::ValueEnum;
use endpoints::chat::ChatCompletionRequestMessage;
use serde::{Deserialize, Serialize};
use std::str::FromStr;

/// Define the chat prompt template types.
#[derive(Clone, Debug, Copy, PartialEq, Eq, Serialize, Deserialize, ValueEnum)]
pub enum PromptTemplateType {
    #[value(name = ""llama-2-chat"")]
    Llama2Chat,
    #[value(name = ""llama-3-chat"")]
    Llama3Chat,
    #[value(name = ""llama-3-tool"")]
    Llama3Tool,
    #[value(name = ""mistral-instruct"")]
    MistralInstruct,
    #[value(name = ""mistral-tool"")]
    MistralTool,
    #[value(name = ""mistrallite"")]
    MistralLite,
    #[value(name = ""openchat"")]
    OpenChat,
    #[value(name = ""codellama-instruct"")]
    CodeLlama,
    #[value(name = ""codellama-super-instruct"")]
    CodeLlamaSuper,
    #[value(name = ""human-assistant"")]
    HumanAssistant,
    #[value(name = ""vicuna-1.0-chat"")]
    VicunaChat,
    #[value(name = ""vicuna-1.1-chat"")]
    Vicuna11Chat,
    #[value(name = ""vicuna-llava"")]
    VicunaLlava,
    #[value(name = ""chatml"")]
    ChatML,
    #[value(name = ""chatml-tool"")]
    ChatMLTool,
    #[value(name = ""internlm-2-tool"")]
    InternLM2Tool,
    #[value(name = ""baichuan-2"")]
    Baichuan2,
    #[value(name = ""wizard-coder"")]
    WizardCoder,
    #[value(name = ""zephyr"")]
    Zephyr,
    #[value(name = ""stablelm-zephyr"")]
    StableLMZephyr,
    #[value(name = ""intel-neural"")]
    IntelNeural,
    #[value(name = ""deepseek-chat"")]
    DeepseekChat,
    #[value(name = ""deepseek-coder"")]
    DeepseekCoder,
    #[value(name = ""deepseek-chat-2"")]
    DeepseekChat2,
    #[value(name = ""deepseek-chat-25"")]
    DeepseekChat25,
    #[value(name = ""solar-instruct"")]
    SolarInstruct,
    #[value(name = ""phi-2-chat"")]
    Phi2Chat,
    #[value(name = ""phi-2-instruct"")]
    Phi2Instruct,
    #[value(name = ""phi-3-chat"")]
    Phi3Chat,
    #[value(name = ""phi-3-instruct"")]
    Phi3Instruct,
    #[value(name = ""gemma-instruct"")]
    GemmaInstruct,
    #[value(name = ""octopus"")]
    Octopus,
    #[value(name = ""glm-4-chat"")]
    Glm4Chat,
    #[value(name = ""groq-llama3-tool"")]
    GroqLlama3Tool,
    #[value(name = ""mediatek-breeze"")]
    BreezeInstruct,
    #[value(name = ""embedding"")]
    Embedding,
    #[value(name = ""none"")]
    Null,
}
impl PromptTemplateType {
    pub fn has_system_prompt(&self) -> bool {
        match self {
            PromptTemplateType::Llama2Chat
            | PromptTemplateType::Llama3Chat
            | PromptTemplateType::Llama3Tool
            | PromptTemplateType::CodeLlama
            | PromptTemplateType::CodeLlamaSuper
            | PromptTemplateType::VicunaChat
            | PromptTemplateType::VicunaLlava
            | PromptTemplateType::ChatML
            | PromptTemplateType::ChatMLTool
            | PromptTemplateType::InternLM2Tool
            | PromptTemplateType::Baichuan2
            | PromptTemplateType::WizardCoder
            | PromptTemplateType::Zephyr
            | PromptTemplateType::IntelNeural
            | PromptTemplateType::DeepseekCoder
            | PromptTemplateType::DeepseekChat2
            | PromptTemplateType::Octopus
            | PromptTemplateType::Phi3Chat
            | PromptTemplateType::Glm4Chat
            | PromptTemplateType::GroqLlama3Tool
            | PromptTemplateType::BreezeInstruct
            | PromptTemplateType::DeepseekChat25 => true,
            PromptTemplateType::MistralInstruct
            | PromptTemplateType::MistralTool
            | PromptTemplateType::MistralLite
            | PromptTemplateType::HumanAssistant
            | PromptTemplateType::DeepseekChat
            | PromptTemplateType::GemmaInstruct
            | PromptTemplateType::OpenChat
            | PromptTemplateType::Phi2Chat
            | PromptTemplateType::Phi2Instruct
            | PromptTemplateType::Phi3Instruct
            | PromptTemplateType::SolarInstruct
            | PromptTemplateType::Vicuna11Chat
            | PromptTemplateType::StableLMZephyr
            | PromptTemplateType::Embedding
            | PromptTemplateType::Null => false,
        }
    }
}
impl FromStr for PromptTemplateType {
    type Err = error::PromptError;

    fn from_str(template: &str) -> std::result::Result<Self, Self::Err> {
        match template {
            ""llama-2-chat"" => Ok(PromptTemplateType::Llama2Chat),
            ""llama-3-chat"" => Ok(PromptTemplateType::Llama3Chat),
            ""llama-3-tool"" => Ok(PromptTemplateType::Llama3Tool),
            ""mistral-instruct"" => Ok(PromptTemplateType::MistralInstruct),
            ""mistral-tool"" => Ok(PromptTemplateType::MistralTool),
            ""mistrallite"" => Ok(PromptTemplateType::MistralLite),
            ""codellama-instruct"" => Ok(PromptTemplateType::CodeLlama),
            ""codellama-super-instruct"" => Ok(PromptTemplateType::CodeLlamaSuper),
            ""belle-llama-2-chat"" => Ok(PromptTemplateType::HumanAssistant),
            ""human-assistant"" => Ok(PromptTemplateType::HumanAssistant),
            ""vicuna-1.0-chat"" => Ok(PromptTemplateType::VicunaChat),
            ""vicuna-1.1-chat"" => Ok(PromptTemplateType::Vicuna11Chat),
            ""vicuna-llava"" => Ok(PromptTemplateType::VicunaLlava),
            ""chatml"" => Ok(PromptTemplateType::ChatML),
            ""chatml-tool"" => Ok(PromptTemplateType::ChatMLTool),
            ""internlm-2-tool"" => Ok(PromptTemplateType::InternLM2Tool),
            ""openchat"" => Ok(PromptTemplateType::OpenChat),
            ""baichuan-2"" => Ok(PromptTemplateType::Baichuan2),
            ""wizard-coder"" => Ok(PromptTemplateType::WizardCoder),
            ""zephyr"" => Ok(PromptTemplateType::Zephyr),
            ""stablelm-zephyr"" => Ok(PromptTemplateType::StableLMZephyr),
            ""intel-neural"" => Ok(PromptTemplateType::IntelNeural),
            ""deepseek-chat"" => Ok(PromptTemplateType::DeepseekChat),
            ""deepseek-coder"" => Ok(PromptTemplateType::DeepseekCoder),
            ""deepseek-chat-2"" => Ok(PromptTemplateType::DeepseekChat2),
            ""deepseek-chat-25"" => Ok(PromptTemplateType::DeepseekChat25),
            ""solar-instruct"" => Ok(PromptTemplateType::SolarInstruct),
            ""phi-2-chat"" => Ok(PromptTemplateType::Phi2Chat),
            ""phi-2-instruct"" => Ok(PromptTemplateType::Phi2Instruct),
            ""phi-3-chat"" => Ok(PromptTemplateType::Phi3Chat),
            ""phi-3-instruct"" => Ok(PromptTemplateType::Phi3Instruct),
            ""gemma-instruct"" => Ok(PromptTemplateType::GemmaInstruct),
            ""octopus"" => Ok(PromptTemplateType::Octopus),
            ""glm-4-chat"" => Ok(PromptTemplateType::Glm4Chat),
            ""groq-llama3-tool"" => Ok(PromptTemplateType::GroqLlama3Tool),
            ""mediatek-breeze"" => Ok(PromptTemplateType::BreezeInstruct),
            ""embedding"" => Ok(PromptTemplateType::Embedding),
            ""none"" => Ok(PromptTemplateType::Null),
            _ => Err(error::PromptError::UnknownPromptTemplateType(
                template.to_string(),
            )),
        }
    }
}
impl std::fmt::Display for PromptTemplateType {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            PromptTemplateType::Llama2Chat => write!(f, ""llama-2-chat""),
            PromptTemplateType::Llama3Chat => write!(f, ""llama-3-chat""),
            PromptTemplateType::Llama3Tool => write!(f, ""llama-3-tool""),
            PromptTemplateType::MistralInstruct => write!(f, ""mistral-instruct""),
            PromptTemplateType::MistralTool => write!(f, ""mistral-tool""),
            PromptTemplateType::MistralLite => write!(f, ""mistrallite""),
            PromptTemplateType::OpenChat => write!(f, ""openchat""),
            PromptTemplateType::CodeLlama => write!(f, ""codellama-instruct""),
            PromptTemplateType::HumanAssistant => write!(f, ""human-asistant""),
            PromptTemplateType::VicunaChat => write!(f, ""vicuna-1.0-chat""),
            PromptTemplateType::Vicuna11Chat => write!(f, ""vicuna-1.1-chat""),
            PromptTemplateType::VicunaLlava => write!(f, ""vicuna-llava""),
            PromptTemplateType::ChatML => write!(f, ""chatml""),
            PromptTemplateType::ChatMLTool => write!(f, ""chatml-tool""),
            PromptTemplateType::InternLM2Tool => write!(f, ""internlm-2-tool""),
            PromptTemplateType::Baichuan2 => write!(f, ""baichuan-2""),
            PromptTemplateType::WizardCoder => write!(f, ""wizard-coder""),
            PromptTemplateType::Zephyr => write!(f, ""zephyr""),
            PromptTemplateType::StableLMZephyr => write!(f, ""stablelm-zephyr""),
            PromptTemplateType::IntelNeural => write!(f, ""intel-neural""),
            PromptTemplateType::DeepseekChat => write!(f, ""deepseek-chat""),
            PromptTemplateType::DeepseekCoder => write!(f, ""deepseek-coder""),
            PromptTemplateType::DeepseekChat2 => write!(f, ""deepseek-chat-2""),
            PromptTemplateType::DeepseekChat25 => write!(f, ""deepseek-chat-25""),
            PromptTemplateType::SolarInstruct => write!(f, ""solar-instruct""),
            PromptTemplateType::Phi2Chat => write!(f, ""phi-2-chat""),
            PromptTemplateType::Phi2Instruct => write!(f, ""phi-2-instruct""),
            PromptTemplateType::Phi3Chat => write!(f, ""phi-3-chat""),
            PromptTemplateType::Phi3Instruct => write!(f, ""phi-3-instruct""),
            PromptTemplateType::CodeLlamaSuper => write!(f, ""codellama-super-instruct""),
            PromptTemplateType::GemmaInstruct => write!(f, ""gemma-instruct""),
            PromptTemplateType::Octopus => write!(f, ""octopus""),
            PromptTemplateType::Glm4Chat => write!(f, ""glm-4-chat""),
            PromptTemplateType::GroqLlama3Tool => write!(f, ""groq-llama3-tool""),
            PromptTemplateType::BreezeInstruct => write!(f, ""mediatek-breeze""),
            PromptTemplateType::Embedding => write!(f, ""embedding""),
            PromptTemplateType::Null => write!(f, ""none""),
        }
    }
}

/// Trait for merging RAG context into chat messages
pub trait MergeRagContext: Send {
    /// Merge RAG context into chat messages.
    ///
    /// Note that the default implementation simply merges the RAG context into the system message. That is, to use the default implementation, `has_system_prompt` should be set to `true` and `policy` set to `MergeRagContextPolicy::SystemMessage`.
    ///
    /// # Arguments
    ///
    /// * `messages` - The chat messages to merge the context into.
    ///
    /// * `context` - The RAG context to merge into the chat messages.
    ///
    /// * `has_system_prompt` - Whether the chat template has a system prompt.
    ///
    /// * `policy` - The policy for merging RAG context into chat messages.
    fn build(
        messages: &mut Vec<endpoints::chat::ChatCompletionRequestMessage>,
        context: &[String],
        has_system_prompt: bool,
        policy: MergeRagContextPolicy,
    ) -> error::Result<()> {
        if (policy == MergeRagContextPolicy::SystemMessage) && has_system_prompt {
            if messages.is_empty() {
                return Err(error::PromptError::NoMessages);
            }

            if context.is_empty() {
                return Err(error::PromptError::Operation(
                    ""No context provided."".to_string(),
                ));
            }

            let context = context[0].trim_end();

            // update or insert system message
            match messages[0] {
                ChatCompletionRequestMessage::System(ref message) => {
                    // compose new system message content
                    let content = format!(""{original_system_message}\nUse the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{context}"", original_system_message=message.content().trim(), context=context.trim_end());
                    // create system message
                    let system_message = ChatCompletionRequestMessage::new_system_message(
                        content,
                        messages[0].name().cloned(),
                    );
                    // replace the original system message
                    messages[0] = system_message;
                }
                _ => {
                    // prepare system message
                    let content = format!(""Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{}"", context.trim_end());

                    // create system message
                    let system_message = ChatCompletionRequestMessage::new_system_message(
                        content,
                        messages[0].name().cloned(),
                    );
                    // insert system message
                    messages.insert(0, system_message);
                }
            };
        }

        Ok(())
    }
}

/// Define the strategy for merging RAG context into chat messages.
#[derive(Clone, Debug, Copy, Default, PartialEq, Eq, Serialize, Deserialize, ValueEnum)]
pub enum MergeRagContextPolicy {
    /// Merge RAG context into the system message.
    ///
    /// Note that this policy is only applicable when the chat template has a system message.
    #[default]
    SystemMessage,
    /// Merge RAG context into the last user message.
    LastUserMessage,
}
impl std::fmt::Display for MergeRagContextPolicy {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            MergeRagContextPolicy::SystemMessage => write!(f, ""system-message""),
            MergeRagContextPolicy::LastUserMessage => write!(f, ""last-user-message""),
        }
    }
}
"
crates/endpoints/.gitignore,"/target
"
crates/endpoints/Cargo.toml,"[package]
name = ""endpoints""
version = ""0.14.0""
edition = ""2021""
readme = ""README.md""
repository = ""https://github.com/LlamaEdge/LlamaEdge""
license = ""Apache-2.0""
documentation = ""https://docs.rs/endpoints/""
categories = [""data-structures""]
description = ""A collection of data structures for the OpenAI-compatible endpoints.""

[dependencies]
serde.workspace = true
url = ""2.5""
indexmap = { version = ""^2.2"", features = [""serde""] }

[dev-dependencies]
serde_json.workspace = true
"
crates/endpoints/README.md,"# ENDPOINTS

`endpoints` is part of [LlamaEdge API Server](https://github.com/LlamaEdge/LlamaEdge/tree/main/api-server) project. It defines the data types which are derived from the [OpenAI API Reference](https://platform.openai.com/docs/api-reference).
"
crates/endpoints/src/audio/mod.rs,"//! Define types for turning audio into text or text into audio.

pub mod speech;
pub mod transcription;
pub mod translation;
"
crates/endpoints/src/audio/speech.rs,"//! Define types for audio generation from the input text.

use serde::{
    de::{self, Deserializer, MapAccess, Visitor},
    Deserialize, Serialize,
};
use std::fmt;

/// Represents a request for generating audio from text.
#[derive(Debug, Serialize)]
pub struct SpeechRequest {
    /// Model name.
    pub model: String,
    /// The text to generate audio for.
    pub input: String,
    /// The voice to use when generating the audio. Supported voices are `alloy`, `echo`, `fable`, `onyx`, `nova`, and `shimmer`.
    pub voice: SpeechVoice,
    /// The format to audio in. Supported formats are `mp3`, `opus`, `aac`, `flac`, `wav`, and `pcm`.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub response_format: Option<SpeechFormat>,
    /// The speed of the generated audio. Select a value from `0.25` to `4.0`. `1.0` is the default.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub speed: Option<f64>,
}

impl<'de> Deserialize<'de> for SpeechRequest {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        enum Field {
            Model,
            Input,
            Voice,
            ResponseFormat,
            Speed,
        }

        impl<'de> Deserialize<'de> for Field {
            fn deserialize<D>(deserializer: D) -> Result<Field, D::Error>
            where
                D: Deserializer<'de>,
            {
                struct FieldVisitor;

                impl<'de> Visitor<'de> for FieldVisitor {
                    type Value = Field;

                    fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
                        formatter
                            .write_str(""`model`, `input`, `voice`, `response_format`, or `speed`"")
                    }

                    fn visit_str<E>(self, value: &str) -> Result<Field, E>
                    where
                        E: de::Error,
                    {
                        match value {
                            ""model"" => Ok(Field::Model),
                            ""input"" => Ok(Field::Input),
                            ""voice"" => Ok(Field::Voice),
                            ""response_format"" => Ok(Field::ResponseFormat),
                            ""speed"" => Ok(Field::Speed),
                            _ => Err(de::Error::unknown_field(value, FIELDS)),
                        }
                    }
                }

                deserializer.deserialize_identifier(FieldVisitor)
            }
        }

        struct SpeechRequestVisitor;

        impl<'de> Visitor<'de> for SpeechRequestVisitor {
            type Value = SpeechRequest;

            fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
                formatter.write_str(""struct SpeechRequest"")
            }

            fn visit_map<V>(self, mut map: V) -> Result<SpeechRequest, V::Error>
            where
                V: MapAccess<'de>,
            {
                let mut model = None;
                let mut input = None;
                let mut voice = None;
                let mut response_format = None;
                let mut speed = None;

                while let Some(key) = map.next_key()? {
                    match key {
                        Field::Model => {
                            if model.is_some() {
                                return Err(de::Error::duplicate_field(""model""));
                            }
                            model = Some(map.next_value()?);
                        }
                        Field::Input => {
                            if input.is_some() {
                                return Err(de::Error::duplicate_field(""input""));
                            }
                            input = Some(map.next_value()?);
                        }
                        Field::Voice => {
                            if voice.is_some() {
                                return Err(de::Error::duplicate_field(""voice""));
                            }
                            voice = Some(map.next_value()?);
                        }
                        Field::ResponseFormat => {
                            response_format = map.next_value()?;
                        }
                        Field::Speed => {
                            speed = map.next_value()?;
                        }
                    }
                }

                let model = model.ok_or_else(|| de::Error::missing_field(""model""))?;
                let input = input.ok_or_else(|| de::Error::missing_field(""input""))?;
                let voice = voice.ok_or_else(|| de::Error::missing_field(""voice""))?;
                if response_format.is_none() {
                    response_format = Some(SpeechFormat::Wav);
                }
                if speed.is_none() {
                    speed = Some(1.0);
                }

                Ok(SpeechRequest {
                    model,
                    input,
                    voice,
                    response_format,
                    speed,
                })
            }
        }

        const FIELDS: &[&str] = &[""model"", ""input"", ""voice"", ""response_format"", ""speed""];
        deserializer.deserialize_struct(""SpeechRequest"", FIELDS, SpeechRequestVisitor)
    }
}

#[test]
fn test_audio_deserialize_speech_request() {
    {
        let json = r#""{
            ""model"": ""test_model"",
            ""input"": ""This is an input"",
            ""voice"": ""alloy""
        }""#;
        let speech_request: SpeechRequest = serde_json::from_str(json).unwrap();
        assert_eq!(speech_request.model, ""test_model"");
        assert_eq!(speech_request.input, ""This is an input"");
        assert_eq!(speech_request.voice, SpeechVoice::Alloy);
        assert_eq!(speech_request.response_format, Some(SpeechFormat::Wav));
        assert_eq!(speech_request.speed, Some(1.0));
    }

    {
        let json = r#""{
            ""model"": ""test_model"",
            ""input"": ""This is an input"",
            ""voice"": ""alloy"",
            ""response_format"": ""wav"",
            ""speed"": 1.5
        }""#;
        let speech_request: SpeechRequest = serde_json::from_str(json).unwrap();
        assert_eq!(speech_request.model, ""test_model"");
        assert_eq!(speech_request.input, ""This is an input"");
        assert_eq!(speech_request.voice, SpeechVoice::Alloy);
        assert_eq!(speech_request.response_format, Some(SpeechFormat::Wav));
        assert_eq!(speech_request.speed, Some(1.5));
    }

    {
        let json = r#""{
            ""model"": ""test_model"",
            ""input"": ""This is an input"",
            ""voice"": ""alloy"",
            ""response_format"": ""mp3""
        }""#;
        let res: Result<SpeechRequest, serde_json::Error> = serde_json::from_str(json);
        assert!(res.is_err());
        if let Err(e) = res {
            let actual = e.to_string();
            assert!(actual.starts_with(""unknown variant `mp3`, expected `wav`""));
        }
    }

    {
        let json = r#""{
            ""model"": ""test_model"",
            ""input"": ""This is an input"",
            ""voice"": ""unknown"",
        }""#;
        let res: Result<SpeechRequest, serde_json::Error> = serde_json::from_str(json);
        assert!(res.is_err());
        if let Err(e) = res {
            let actual = e.to_string();
            assert!(actual.starts_with(""unknown variant `unknown`, expected one of `alloy`, `echo`, `fable`, `onyx`, `nova`, `shimmer`""));
        }
    }
}

#[derive(Debug, Deserialize, Serialize, Clone, Copy, PartialEq)]
#[serde(rename_all = ""lowercase"")]
pub enum SpeechVoice {
    Alloy,
    Echo,
    Fable,
    Onyx,
    Nova,
    Shimmer,
}

#[derive(Debug, Deserialize, Serialize, Clone, Copy, PartialEq)]
#[serde(rename_all = ""lowercase"")]
pub enum SpeechFormat {
    Wav,
    // Mp3,
    // Opus,
    // Aac,
    // Flac,
    // Pcm,
}
"
crates/endpoints/src/audio/transcription.rs,"//! Define types for audio transcription.

use crate::files::FileObject;
use serde::{Deserialize, Serialize};

/// Represents a rquest for audio transcription into the input language.
#[derive(Debug, Deserialize, Serialize, Default)]
pub struct TranscriptionRequest {
    /// The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
    pub file: FileObject,
    /// ID of the model to use.
    pub model: String,
    /// The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub language: Option<String>,
    /// An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt: Option<String>,
    /// The format of the transcript output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub response_format: Option<String>,
    /// The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    /// The timestamp granularities to populate for this transcription.
    /// `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub timestamp_granularities: Option<Vec<TimestampGranularity>>,
}

/// The timestamp granularities to populate for the transcription.
#[derive(Debug, Deserialize, Serialize)]
pub enum TimestampGranularity {
    /// The model will return timestamps for each word.
    Word,
    /// The model will return timestamps for each segment.
    Segment,
}

/// Represents a transcription response returned by model, based on the provided input.
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct TranscriptionObject {
    /// The transcribed text.
    pub text: String,
}

#[test]
fn test_serialize_transcription_request() {
    let obj = TranscriptionObject {
        text: String::from(""Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger. This is a place where you can get to do that.""),
    };

    let json = serde_json::to_string(&obj).unwrap();
    assert_eq!(
        json,
        r#""{""text"":""Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger. This is a place where you can get to do that.""}""#
    );
}

/// Represents a verbose json transcription response returned by model, based on the provided input.
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct VerboseTranscriptionObject {
    /// The language of the input audio.
    pub language: String,
    /// The duration of the input audio.
    pub duration: String,
    /// The transcribed text.
    pub text: String,
    /// Extracted words and their corresponding timestamps.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub words: Option<Vec<Word>>,
    /// Segments of the transcribed text and their corresponding details.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub segments: Option<Vec<Segment>>,
}

#[test]
fn test_serialize_verbose_transcription_request() {
    let obj = VerboseTranscriptionObject {
        language: String::from(""english""),
        duration: String::from(""8.470000267028809""),
        text: String::from(""The beach was a popular spot on a hot summer day. People were swimming in the ocean, building sandcastles, and playing beach volleyball.""),
        words: None,
        segments: Some(vec![
            Segment {
                id: 0,
                seek: 0,
                start: 0.0,
                end: 3.319999933242798,
                text: String::from(""The beach was a popular spot on a hot summer day.""),
                tokens: vec![50364, 440, 7534, 390, 257, 3743, 4008, 322, 257, 2368, 4266, 786, 13, 50530],
                temperature: 0.0,
                avg_logprob: -0.2860786020755768,
                compression_ratio: 1.2363636493682861,
                no_speech_prob: 0.00985979475080967,
            }
        ]),
    };

    let json = serde_json::to_string(&obj).unwrap();
    assert_eq!(
        json,
        r#""{""language"":""english"",""duration"":""8.470000267028809"",""text"":""The beach was a popular spot on a hot summer day. People were swimming in the ocean, building sandcastles, and playing beach volleyball."",""segments"":[{""id"":0,""seek"":0,""start"":0.0,""end"":3.319999933242798,""text"":""The beach was a popular spot on a hot summer day."",""tokens"":[50364,440,7534,390,257,3743,4008,322,257,2368,4266,786,13,50530],""temperature"":0.0,""avg_logprob"":-0.2860786020755768,""compression_ratio"":1.2363636493682861,""no_speech_prob"":0.00985979475080967}]}""#
    );
}

/// Represents a word and its corresponding timestamps.
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct Word {
    /// The text content of the word.
    pub text: String,
    /// Start time of the word in seconds.
    pub start: f64,
    /// End time of the word in seconds.
    pub end: f64,
}

/// Represents a segment of the transcribed text and its corresponding details.
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct Segment {
    /// Unique identifier of the segment.
    pub id: u64,
    /// Seek offset of the segment.
    pub seek: u64,
    /// Start time of the segment in seconds.
    pub start: f64,
    /// End time of the segment in seconds.
    pub end: f64,
    /// Text content of the segment.
    pub text: String,
    /// Array of token IDs for the text content.
    pub tokens: Vec<u64>,
    /// Temperature parameter used for generating the segment.
    pub temperature: f64,
    /// Average logprob of the segment. If the value is lower than -1, consider the logprobs failed.
    pub avg_logprob: f64,
    /// Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed.
    pub compression_ratio: f64,
    /// Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent.
    pub no_speech_prob: f64,
}
"
crates/endpoints/src/audio/translation.rs,"//! Define types for translating audio into English.

use crate::files::FileObject;
use serde::{
    de::{self, MapAccess, Visitor},
    Deserialize, Deserializer, Serialize,
};
use std::fmt;

/// Represents a rquest for translating audio into English.
#[derive(Debug, Serialize, Default)]
pub struct TranslationRequest {
    /// The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
    pub file: FileObject,
    /// ID of the model to use.
    pub model: Option<String>,
    /// An optional text to guide the model's style or continue a previous audio segment. The prompt should be in English.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt: Option<String>,
    /// The format of the transcript output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub response_format: Option<String>,
    /// The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. Defaults to 0.0.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,

    /// Spoken language. `auto` for auto-detect. Defaults to `en`. This param is only supported for `whisper.cpp`.
    pub language: Option<String>,
}
impl<'de> Deserialize<'de> for TranslationRequest {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        enum Field {
            File,
            Model,
            Prompt,
            ResponseFormat,
            Temperature,
            Language,
        }

        impl<'de> Deserialize<'de> for Field {
            fn deserialize<D>(deserializer: D) -> Result<Field, D::Error>
            where
                D: Deserializer<'de>,
            {
                struct FieldVisitor;

                impl<'de> Visitor<'de> for FieldVisitor {
                    type Value = Field;

                    fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
                        formatter.write_str(""`file`, `model`, `prompt`, `response_format`, `temperature`, or `language`"")
                    }

                    fn visit_str<E>(self, value: &str) -> Result<Field, E>
                    where
                        E: de::Error,
                    {
                        match value {
                            ""file"" => Ok(Field::File),
                            ""model"" => Ok(Field::Model),
                            ""prompt"" => Ok(Field::Prompt),
                            ""response_format"" => Ok(Field::ResponseFormat),
                            ""temperature"" => Ok(Field::Temperature),
                            ""language"" => Ok(Field::Language),
                            _ => Err(de::Error::unknown_field(value, FIELDS)),
                        }
                    }
                }

                deserializer.deserialize_identifier(FieldVisitor)
            }
        }

        struct TranslationRequestVisitor;

        impl<'de> Visitor<'de> for TranslationRequestVisitor {
            type Value = TranslationRequest;

            fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
                formatter.write_str(""struct TranslationRequest"")
            }

            fn visit_map<V>(self, mut map: V) -> Result<TranslationRequest, V::Error>
            where
                V: MapAccess<'de>,
            {
                let mut file = None;
                let mut model = None;
                let mut prompt = None;
                let mut response_format = None;
                let mut temperature = None;
                let mut language = None;

                while let Some(key) = map.next_key()? {
                    match key {
                        Field::File => {
                            if file.is_some() {
                                return Err(de::Error::duplicate_field(""file""));
                            }
                            file = Some(map.next_value()?);
                        }
                        Field::Model => {
                            if model.is_some() {
                                return Err(de::Error::duplicate_field(""model""));
                            }
                            model = Some(map.next_value()?);
                        }
                        Field::Prompt => {
                            if prompt.is_some() {
                                return Err(de::Error::duplicate_field(""prompt""));
                            }
                            prompt = Some(map.next_value()?);
                        }
                        Field::ResponseFormat => {
                            if response_format.is_some() {
                                return Err(de::Error::duplicate_field(""response_format""));
                            }
                            response_format = Some(map.next_value()?);
                        }
                        Field::Temperature => {
                            if temperature.is_some() {
                                return Err(de::Error::duplicate_field(""temperature""));
                            }
                            temperature = Some(map.next_value()?);
                        }
                        Field::Language => {
                            if language.is_some() {
                                return Err(de::Error::duplicate_field(""language""));
                            }
                            language = Some(map.next_value()?);
                        }
                    }
                }

                let file = file.ok_or_else(|| de::Error::missing_field(""file""))?;

                if temperature.is_none() {
                    temperature = Some(0.0);
                }

                if language.is_none() {
                    language = Some(""en"".to_string());
                }

                Ok(TranslationRequest {
                    file,
                    model,
                    prompt,
                    response_format,
                    temperature,
                    language,
                })
            }
        }

        const FIELDS: &[&str] = &[
            ""file"",
            ""model"",
            ""prompt"",
            ""response_format"",
            ""temperature"",
            ""language"",
        ];
        deserializer.deserialize_struct(""TranslationRequest"", FIELDS, TranslationRequestVisitor)
    }
}

/// Represents a translation object.
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct TranslationObject {
    /// The translated text.
    pub text: String,
}
"
crates/endpoints/src/chat.rs,
crates/endpoints/src/common.rs,"//! Define common types used by other types.
use serde::{Deserialize, Serialize};

#[derive(Debug, Deserialize, Serialize)]
#[allow(non_camel_case_types)]
pub enum LlamaCppLogitBiasType {
    input_ids,
    tokens,
}

/// Token usage
#[derive(Debug, Default, Deserialize, Serialize)]
pub struct Usage {
    /// Number of tokens in the prompt.
    pub prompt_tokens: u64,
    /// Number of tokens in the generated completion.
    pub completion_tokens: u64,
    /// Total number of tokens used in the request (prompt + completion).
    pub total_tokens: u64,
}

/// The reason the model stopped generating tokens.
#[derive(Debug, Serialize, Deserialize, PartialEq, Eq, Clone, Copy)]
#[allow(non_camel_case_types)]
pub enum FinishReason {
    /// `stop` if the model hit a natural stop point or a provided stop sequence.
    stop,
    /// `length` if the maximum number of tokens specified in the request was reached.
    length,
    /// `tool_calls` if the model called a tool.
    tool_calls,
}
"
crates/endpoints/src/completions.rs,"//! Define types for the `completions` endpoint.

use super::common::{FinishReason, Usage};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// Creates a completion for the provided prompt and parameters.
#[derive(Debug, Deserialize, Serialize)]
pub struct CompletionRequest {
    /// ID of the model to use.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub model: Option<String>,
    /// The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.
    pub prompt: CompletionPrompt,
    /// Generates `best_of` completions server-side and returns the ""best"" (the one with the highest log probability per token). Results cannot be streamed.When used with `n_choice`, `best_of` controls the number of candidate completions and `n_choice` specifies how many to return – `best_of` must be greater than `n_choice`.
    /// Defaults to 1.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub best_of: Option<u32>,
    /// Echo back the prompt in addition to the completion.
    /// Defaults to false.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub echo: Option<bool>,
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    /// Defaults to 0.0.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f32>,
    /// Modify the likelihood of specified tokens appearing in the completion.
    /// Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    /// As an example, you can pass {""50256"": -100} to prevent the <|endoftext|> token from being generated.
    /// Defaults to None.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub logit_bias: Option<HashMap<String, f32>>,
    /// Include the log probabilities on the logprobs most likely tokens, as well the chosen tokens. For example, if logprobs is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to logprobs+1 elements in the response.
    ///
    /// The maximum value for logprobs is 5.
    /// Defaults to None.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub logprobs: Option<u32>,
    /// The maximum number of tokens to generate in the completion.
    ///
    /// The token count of your prompt plus max_tokens cannot exceed the model's context length.
    /// Defaults to 16.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub max_tokens: Option<u32>,
    /// How many completions to generate for each prompt.
    /// Defaults to 1.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n: Option<u32>,
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    /// Defaults to 0.0.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f32>,
    /// Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
    /// Defaults to None.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub stop: Option<Vec<String>>,
    /// Whether to stream the results as they are generated. Useful for chatbots.
    /// Defaults to false.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub stream: Option<bool>,
    /// The suffix that comes after a completion of inserted text.
    /// Defaults to None.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub suffix: Option<String>,
    /// Adjust the randomness of the generated text. Between 0.0 and 2.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
    ///
    /// We generally recommend altering this or top_p but not both.
    /// Defaults to 1.0.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f32>,
    /// Limit the next token selection to a subset of tokens with a cumulative probability above a threshold P. The value should be between 0.0 and 1.0.
    ///
    /// Top-p sampling, also known as nucleus sampling, is another text generation method that selects the next token from a subset of tokens that together have a cumulative probability of at least p. This method provides a balance between diversity and quality by considering both the probabilities of tokens and the number of tokens to sample from. A higher value for top_p (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text.
    /// Defaults to 1.0.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f32>,
    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub user: Option<String>,
    // //* llama.cpp specific parameters
    // llama_cpp_top_k: i32,
    // llama_cpp_repeat_penalty: f64,
    // llama_cpp_logit_bias_type: Option<LlamaCppLogitBiasType>,
}

#[test]
fn test_serialize_completion_request() {
    {
        let request = CompletionRequest {
            model: Some(""text-davinci-003"".to_string()),
            prompt: CompletionPrompt::SingleText(""Once upon a time"".to_string()),
            best_of: Some(1),
            echo: Some(false),
            frequency_penalty: Some(0.0),
            logit_bias: Some(HashMap::new()),
            logprobs: Some(5),
            max_tokens: Some(16),
            n: Some(1),
            presence_penalty: Some(0.0),
            stop: Some(vec![""\n"".to_string()]),
            stream: Some(false),
            suffix: Some("""".to_string()),
            temperature: Some(1.0),
            top_p: Some(1.0),
            user: Some(""user-123"".to_string()),
        };

        let actual = serde_json::to_string(&request).unwrap();
        let expected = r#""{""model"":""text-davinci-003"",""prompt"":""Once upon a time"",""best_of"":1,""echo"":false,""frequency_penalty"":0.0,""logit_bias"":{},""logprobs"":5,""max_tokens"":16,""n"":1,""presence_penalty"":0.0,""stop"":[""\n""],""stream"":false,""suffix"":"""",""temperature"":1.0,""top_p"":1.0,""user"":""user-123""}""#;
        assert_eq!(actual, expected);
    }

    {
        let request = CompletionRequest {
            model: None,
            prompt: CompletionPrompt::MultiText(vec![
                ""Once upon a time"".to_string(),
                ""There was a cat"".to_string(),
            ]),
            best_of: None,
            echo: None,
            frequency_penalty: None,
            logit_bias: None,
            logprobs: None,
            max_tokens: None,
            n: None,
            presence_penalty: None,
            stop: None,
            stream: None,
            suffix: None,
            temperature: None,
            top_p: None,
            user: None,
        };

        let actual = serde_json::to_string(&request).unwrap();
        let expected = r#""{""prompt"":[""Once upon a time"",""There was a cat""]}""#;
        assert_eq!(actual, expected);
    }
}

#[test]
fn test_deserialize_completion_request() {
    {
        let json = r#""{""model"":""text-davinci-003"",""prompt"":""Once upon a time"",""best_of"":1,""echo"":false,""frequency_penalty"":0.0,""logit_bias"":{},""logprobs"":5,""max_tokens"":16,""n"":1,""presence_penalty"":0.0,""stop"":[""\n""],""stream"":false,""suffix"":"""",""temperature"":1.0,""top_p"":1.0,""user"":""user-123""}""#;
        let request: CompletionRequest = serde_json::from_str(json).unwrap();
        assert_eq!(request.model, Some(""text-davinci-003"".to_string()));
        assert_eq!(
            request.prompt,
            CompletionPrompt::SingleText(""Once upon a time"".to_string())
        );
        assert_eq!(request.best_of, Some(1));
        assert_eq!(request.echo, Some(false));
        assert_eq!(request.frequency_penalty, Some(0.0));
        assert_eq!(request.logit_bias, Some(HashMap::new()));
        assert_eq!(request.logprobs, Some(5));
        assert_eq!(request.max_tokens, Some(16));
        assert_eq!(request.n, Some(1));
        assert_eq!(request.presence_penalty, Some(0.0));
        assert_eq!(request.stop, Some(vec![""\n"".to_string()]));
        assert_eq!(request.stream, Some(false));
        assert_eq!(request.suffix, Some("""".to_string()));
        assert_eq!(request.temperature, Some(1.0));
        assert_eq!(request.top_p, Some(1.0));
        assert_eq!(request.user, Some(""user-123"".to_string()));
    }

    {
        let json = r#""{""prompt"":[""Once upon a time"",""There was a cat""]}""#;
        let request: CompletionRequest = serde_json::from_str(json).unwrap();
        assert_eq!(request.model, None);
        assert_eq!(
            request.prompt,
            CompletionPrompt::MultiText(vec![
                ""Once upon a time"".to_string(),
                ""There was a cat"".to_string()
            ])
        );
        assert_eq!(request.best_of, None);
        assert_eq!(request.echo, None);
        assert_eq!(request.frequency_penalty, None);
        assert_eq!(request.logit_bias, None);
        assert_eq!(request.logprobs, None);
        assert_eq!(request.max_tokens, None);
        assert_eq!(request.n, None);
        assert_eq!(request.presence_penalty, None);
        assert_eq!(request.stop, None);
        assert_eq!(request.stream, None);
        assert_eq!(request.suffix, None);
        assert_eq!(request.temperature, None);
        assert_eq!(request.top_p, None);
        assert_eq!(request.user, None);
    }
}

/// Defines the types of a user message content.
#[derive(Debug, Clone, Deserialize, Serialize, PartialEq, Eq)]
#[serde(untagged)]
pub enum CompletionPrompt {
    /// A single text prompt.
    SingleText(String),
    /// Multiple text prompts.
    MultiText(Vec<String>),
}

/// Represents a completion response from the API.
///
/// Note: both the streamed and non-streamed response objects share the same shape (unlike the chat endpoint).
#[derive(Debug, Deserialize, Serialize)]
pub struct CompletionObject {
    /// A unique identifier for the completion.
    pub id: String,
    /// The list of completion choices the model generated for the input prompt.
    pub choices: Vec<CompletionChoice>,
    /// The Unix timestamp (in seconds) of when the completion was created.
    pub created: u64,
    /// The model used for completion.
    pub model: String,
    /// The object type, which is always ""text_completion"".
    pub object: String,
    /// Usage statistics for the completion request.
    pub usage: Usage,
}

#[derive(Debug, Deserialize, Serialize)]
pub struct CompletionChoice {
    /// The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, or `function_call` if the model called a function.
    pub finish_reason: FinishReason,
    /// The index of the choice in the list of choices.
    pub index: u32,
    /// A chat completion delta generated by streamed model responses.
    pub logprobs: Option<LogprobResult>,
    pub text: String,
}

#[derive(Debug, Deserialize, Serialize)]
pub struct LogprobResult {
    pub tokens: Vec<String>,
    pub token_logprobs: Vec<f32>,
    pub top_logprobs: Vec<HashMap<String, f32>>,
    pub text_offset: Vec<i32>,
}
"
crates/endpoints/src/embeddings.rs,"//! Define types for the `embeddings` endpoint.

use crate::common::Usage;
use serde::{Deserialize, Serialize};

/// Creates an embedding vector representing the input text.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmbeddingRequest {
    /// ID of the model to use.
    pub model: String,
    /// Input text to embed,encoded as a string or array of tokens.
    ///
    /// To embed multiple inputs in a single request, pass an array of strings or array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for text-embedding-ada-002), cannot be an empty string, and any array must be 2048 dimensions or less.
    pub input: InputText,
    /// The format to return the embeddings in. Can be either float or base64.
    /// Defaults to float.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub encoding_format: Option<String>,
    /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub user: Option<String>,
}

#[test]
fn test_embedding_serialize_embedding_request() {
    let embedding_request = EmbeddingRequest {
        model: ""text-embedding-ada-002"".to_string(),
        input: ""Hello, world!"".into(),
        encoding_format: None,
        user: None,
    };
    let serialized = serde_json::to_string(&embedding_request).unwrap();
    assert_eq!(
        serialized,
        r#""{""model"":""text-embedding-ada-002"",""input"":""Hello, world!""}""#
    );

    let embedding_request = EmbeddingRequest {
        model: ""text-embedding-ada-002"".to_string(),
        input: vec![""Hello, world!"", ""This is a test string""].into(),
        encoding_format: None,
        user: None,
    };
    let serialized = serde_json::to_string(&embedding_request).unwrap();
    assert_eq!(
        serialized,
        r#""{""model"":""text-embedding-ada-002"",""input"":[""Hello, world!"",""This is a test string""]}""#
    );
}

#[test]
fn test_embedding_deserialize_embedding_request() {
    let serialized = r#""{""model"":""text-embedding-ada-002"",""input"":""Hello, world!""}""#;
    let embedding_request: EmbeddingRequest = serde_json::from_str(serialized).unwrap();
    assert_eq!(embedding_request.model, ""text-embedding-ada-002"");
    assert_eq!(embedding_request.input, InputText::from(""Hello, world!""));
    assert_eq!(embedding_request.encoding_format, None);
    assert_eq!(embedding_request.user, None);

    let serialized =
        r#""{""model"":""text-embedding-ada-002"",""input"":[""Hello, world!"",""This is a test string""]}""#;
    let embedding_request: EmbeddingRequest = serde_json::from_str(serialized).unwrap();
    assert_eq!(embedding_request.model, ""text-embedding-ada-002"");
    assert_eq!(
        embedding_request.input,
        InputText::from(vec![""Hello, world!"", ""This is a test string""])
    );
    assert_eq!(embedding_request.encoding_format, None);
    assert_eq!(embedding_request.user, None);
}

/// Defines the input text for the embedding request.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq)]
#[serde(untagged)]
pub enum InputText {
    /// The string that will be turned into an embedding.
    String(String),
    /// The array of strings that will be turned into an embedding.
    ArrayOfStrings(Vec<String>),
    /// The array of integers that will be turned into an embedding.
    ArrayOfTokens(Vec<i64>),
    /// The array of arrays containing integers that will be turned into an embedding.
    ArrayOfTokenArrays(Vec<Vec<i64>>),
}
impl From<&str> for InputText {
    fn from(s: &str) -> Self {
        InputText::String(s.to_string())
    }
}
impl From<&String> for InputText {
    fn from(s: &String) -> Self {
        InputText::String(s.to_string())
    }
}
impl From<&[String]> for InputText {
    fn from(s: &[String]) -> Self {
        InputText::ArrayOfStrings(s.to_vec())
    }
}
impl From<Vec<&str>> for InputText {
    fn from(s: Vec<&str>) -> Self {
        InputText::ArrayOfStrings(s.iter().map(|s| s.to_string()).collect())
    }
}
impl From<Vec<String>> for InputText {
    fn from(s: Vec<String>) -> Self {
        InputText::ArrayOfStrings(s)
    }
}
impl From<&[i64]> for InputText {
    fn from(s: &[i64]) -> Self {
        InputText::ArrayOfTokens(s.to_vec())
    }
}
impl From<Vec<i64>> for InputText {
    fn from(s: Vec<i64>) -> Self {
        InputText::ArrayOfTokens(s)
    }
}
impl From<Vec<Vec<i64>>> for InputText {
    fn from(s: Vec<Vec<i64>>) -> Self {
        InputText::ArrayOfTokenArrays(s)
    }
}

/// Defines the embedding response.
#[derive(Debug, Serialize, Deserialize)]
pub struct EmbeddingsResponse {
    pub object: String,
    pub data: Vec<EmbeddingObject>,
    pub model: String,
    pub usage: Usage,
}

/// Represents an embedding vector returned by embedding endpoint.
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct EmbeddingObject {
    /// The index of the embedding in the list of embeddings.
    pub index: u64,
    /// The object type, which is always ""embedding"".
    pub object: String,
    /// The embedding vector, which is a list of floats.
    pub embedding: Vec<f64>,
}
"
crates/endpoints/src/files.rs,"//! Define types for the `files` endpoint.

use serde::{Deserialize, Serialize};

#[derive(Debug, Deserialize, Serialize, Default)]
pub struct FilesRequest {
    /// The File object (not file name) to be uploaded.
    file: FileObject,
    /// The intended purpose of the uploaded file.
    /// Use ""fine-tune"" for Fine-tuning and ""assistants"" for `Assistants` and `Messages`.
    purpose: String,
}

/// The File object represents a document that has been uploaded to the server.
#[derive(Debug, Deserialize, Serialize, Default)]
pub struct FileObject {
    /// The file identifier, which can be referenced in the API endpoints.
    pub id: String,
    /// The size of the file, in bytes.
    pub bytes: u64,
    /// The Unix timestamp (in seconds) for when the file was created.
    pub created_at: u64,
    /// The name of the file.
    pub filename: String,
    /// The object type, which is always `file`.
    pub object: String,
    /// The intended purpose of the file. Supported values are `fine-tune`, `fine-tune-results`, `assistants`, and `assistants_output`.
    pub purpose: String,
}

/// Represent the response from the `files` endpoint.
#[derive(Debug, Deserialize, Serialize)]
pub struct ListFilesResponse {
    /// The object type, which is always `list`.
    pub object: String,
    /// The list of file objects.
    pub data: Vec<FileObject>,
}

/// Represents the status of a file deletion operation.
#[derive(Debug, Deserialize, Serialize)]
pub struct DeleteFileStatus {
    /// The file identifier, which can be referenced in the API endpoints.
    pub id: String,
    /// The object type, which is always `file`.
    pub object: String,
    /// The status of the deletion operation.
    pub deleted: bool,
}
"
crates/endpoints/src/images.rs,"//! Define types for image generation.

use crate::files::FileObject;
use serde::{
    de::{self, MapAccess, SeqAccess, Visitor},
    Deserialize, Deserializer, Serialize,
};
use std::{fmt, str::FromStr};

/// Builder for creating a `ImageCreateRequest` instance.
pub struct ImageCreateRequestBuilder {
    req: ImageCreateRequest,
}
impl ImageCreateRequestBuilder {
    /// Create a new builder with the given model and prompt.
    pub fn new(model: impl Into<String>, prompt: impl Into<String>) -> Self {
        Self {
            req: ImageCreateRequest {
                model: model.into(),
                prompt: prompt.into(),
                n: Some(1),
                response_format: Some(ResponseFormat::Url),
                cfg_scale: Some(7.0),
                sample_method: Some(SamplingMethod::EulerA),
                steps: Some(20),
                height: Some(512),
                width: Some(512),
                ..Default::default()
            },
        }
    }

    /// Set negative prompt
    pub fn with_negative_prompt(mut self, negative_prompt: impl Into<String>) -> Self {
        self.req.negative_prompt = Some(negative_prompt.into());
        self
    }

    /// Set the number of images to generate.
    pub fn with_number_of_images(mut self, n: u64) -> Self {
        self.req.n = Some(n);
        self
    }

    /// This param is only supported for OpenAI `dall-e-3`.
    pub fn with_quality(mut self, quality: impl Into<String>) -> Self {
        self.req.quality = Some(quality.into());
        self
    }

    /// Set the format in which the generated images are returned.
    pub fn with_response_format(mut self, response_format: ResponseFormat) -> Self {
        self.req.response_format = Some(response_format);
        self
    }

    /// This param is only supported for `dall-e-3`.
    pub fn with_style(mut self, style: impl Into<String>) -> Self {
        self.req.style = Some(style.into());
        self
    }

    /// Set the user id
    pub fn with_user(mut self, user: impl Into<String>) -> Self {
        self.req.user = Some(user.into());
        self
    }

    /// Set the unconditional guidance scale. This param is only supported for `stable-diffusion.cpp`.
    pub fn with_cfg_scale(mut self, cfg_scale: f32) -> Self {
        self.req.cfg_scale = Some(cfg_scale);
        self
    }

    /// Set the sampling method. This param is only supported for `stable-diffusion.cpp`.
    pub fn with_sample_method(mut self, sample_method: SamplingMethod) -> Self {
        self.req.sample_method = Some(sample_method);
        self
    }

    /// Set the number of sample steps. This param is only supported for `stable-diffusion.cpp`.
    pub fn with_steps(mut self, steps: usize) -> Self {
        self.req.steps = Some(steps);
        self
    }

    /// Set the image size.
    pub fn with_image_size(mut self, height: usize, width: usize) -> Self {
        self.req.height = Some(height);
        self.req.width = Some(width);
        self
    }

    /// Build the request.
    pub fn build(self) -> ImageCreateRequest {
        self.req
    }
}

/// Request to create an image by a given prompt.
#[derive(Debug, Serialize, Default)]
pub struct ImageCreateRequest {
    /// A text description of the desired image.
    pub prompt: String,
    /// Negative prompt for the image generation.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub negative_prompt: Option<String>,
    /// Name of the model to use for image generation.
    pub model: String,
    /// Number of images to generate. Defaults to 1.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n: Option<u64>,
    /// The quality of the image that will be generated. hd creates images with finer details and greater consistency across the image. Defaults to ""standard"". This param is only supported for OpenAI `dall-e-3`.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub quality: Option<String>,
    /// The format in which the generated images are returned. Must be one of `url` or `b64_json`. Defaults to `Url`.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub response_format: Option<ResponseFormat>,
    /// The size of the generated images. Defaults to use the values of `height` and `width` fields.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub size: Option<String>,
    /// The style of the generated images. Must be one of `vivid` or `natural`. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This param is only supported for `dall-e-3`.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub style: Option<String>,
    /// A unique identifier representing your end-user, which can help monitor and detect abuse.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub user: Option<String>,

    /// Unconditional guidance scale. Defaults to 7.0. This param is only supported for `stable-diffusion.cpp`.
    pub cfg_scale: Option<f32>,
    /// Sampling method. Defaults to ""euler_a"". This param is only supported for `stable-diffusion.cpp`.
    pub sample_method: Option<SamplingMethod>,
    /// Number of sample steps. Defaults to 20. This param is only supported for `stable-diffusion.cpp`.
    pub steps: Option<usize>,
    /// Image height, in pixel space. Defaults to 512. If `size` is provided, this field will be ignored.
    pub height: Option<usize>,
    /// Image width, in pixel space. Defaults to 512. If `size` is provided, this field will be ignored.
    pub width: Option<usize>,
}
impl<'de> Deserialize<'de> for ImageCreateRequest {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        enum Field {
            Prompt,
            NegativePrompt,
            Model,
            N,
            Quality,
            ResponseFormat,
            Size,
            Style,
            User,
            CfgScale,
            SampleMethod,
            Steps,
            Height,
            Width,
        }

        impl<'de> Deserialize<'de> for Field {
            fn deserialize<D>(deserializer: D) -> Result<Field, D::Error>
            where
                D: Deserializer<'de>,
            {
                struct FieldVisitor;

                impl<'de> Visitor<'de> for FieldVisitor {
                    type Value = Field;

                    fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
                        formatter.write_str(""field identifier"")
                    }

                    fn visit_str<E>(self, value: &str) -> Result<Field, E>
                    where
                        E: de::Error,
                    {
                        match value {
                            ""prompt"" => Ok(Field::Prompt),
                            ""negative_prompt"" => Ok(Field::NegativePrompt),
                            ""model"" => Ok(Field::Model),
                            ""n"" => Ok(Field::N),
                            ""quality"" => Ok(Field::Quality),
                            ""response_format"" => Ok(Field::ResponseFormat),
                            ""size"" => Ok(Field::Size),
                            ""style"" => Ok(Field::Style),
                            ""user"" => Ok(Field::User),
                            ""cfg_scale"" => Ok(Field::CfgScale),
                            ""sample_method"" => Ok(Field::SampleMethod),
                            ""steps"" => Ok(Field::Steps),
                            ""height"" => Ok(Field::Height),
                            ""width"" => Ok(Field::Width),
                            _ => Err(de::Error::unknown_field(value, FIELDS)),
                        }
                    }
                }

                deserializer.deserialize_identifier(FieldVisitor)
            }
        }

        struct CreateImageRequestVisitor;

        impl<'de> Visitor<'de> for CreateImageRequestVisitor {
            type Value = ImageCreateRequest;

            fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
                formatter.write_str(""struct CreateImageRequest"")
            }

            fn visit_seq<V>(self, mut seq: V) -> Result<ImageCreateRequest, V::Error>
            where
                V: SeqAccess<'de>,
            {
                let prompt = seq
                    .next_element()?
                    .ok_or_else(|| de::Error::invalid_length(0, &self))?;
                let negative_prompt = seq.next_element()?;
                let model = seq
                    .next_element()?
                    .ok_or_else(|| de::Error::invalid_length(1, &self))?;
                let n = seq.next_element()?.unwrap_or(Some(1));
                let quality = seq.next_element()?;
                let response_format = seq.next_element()?.unwrap_or(Some(ResponseFormat::Url));
                let size = seq.next_element()?;
                let style = seq.next_element()?;
                let user = seq.next_element()?;
                let cfg_scale = seq.next_element()?;
                let sample_method = seq.next_element()?;
                let steps = seq.next_element()?;
                let height = seq.next_element()?;
                let width = seq.next_element()?;

                Ok(ImageCreateRequest {
                    prompt,
                    negative_prompt,
                    model,
                    n,
                    quality,
                    response_format,
                    size,
                    style,
                    user,
                    cfg_scale,
                    sample_method,
                    steps,
                    height,
                    width,
                })
            }

            fn visit_map<V>(self, mut map: V) -> Result<ImageCreateRequest, V::Error>
            where
                V: MapAccess<'de>,
            {
                let mut prompt = None;
                let mut negative_prompt = None;
                let mut model = None;
                let mut n = None;
                let mut quality = None;
                let mut response_format = None;
                let mut size: Option<String> = None;
                let mut style = None;
                let mut user = None;
                let mut cfg_scale = None;
                let mut sample_method = None;
                let mut steps = None;
                let mut height = None;
                let mut width = None;

                while let Some(key) = map.next_key()? {
                    match key {
                        Field::Prompt => {
                            if prompt.is_some() {
                                return Err(de::Error::duplicate_field(""prompt""));
                            }
                            prompt = Some(map.next_value()?);
                        }
                        Field::NegativePrompt => {
                            if negative_prompt.is_some() {
                                return Err(de::Error::duplicate_field(""negative_prompt""));
                            }
                            negative_prompt = Some(map.next_value()?);
                        }
                        Field::Model => {
                            if model.is_some() {
                                return Err(de::Error::duplicate_field(""model""));
                            }
                            model = Some(map.next_value()?);
                        }
                        Field::N => {
                            if n.is_some() {
                                return Err(de::Error::duplicate_field(""n""));
                            }
                            n = Some(map.next_value()?);
                        }
                        Field::Quality => {
                            if quality.is_some() {
                                return Err(de::Error::duplicate_field(""quality""));
                            }
                            quality = Some(map.next_value()?);
                        }
                        Field::ResponseFormat => {
                            if response_format.is_some() {
                                return Err(de::Error::duplicate_field(""response_format""));
                            }
                            response_format = Some(map.next_value()?);
                        }
                        Field::Size => {
                            if size.is_some() {
                                return Err(de::Error::duplicate_field(""size""));
                            }
                            size = Some(map.next_value()?);
                        }
                        Field::Style => {
                            if style.is_some() {
                                return Err(de::Error::duplicate_field(""style""));
                            }
                            style = Some(map.next_value()?);
                        }
                        Field::User => {
                            if user.is_some() {
                                return Err(de::Error::duplicate_field(""user""));
                            }
                            user = Some(map.next_value()?);
                        }
                        Field::CfgScale => {
                            if cfg_scale.is_some() {
                                return Err(de::Error::duplicate_field(""cfg_scale""));
                            }
                            cfg_scale = Some(map.next_value()?);
                        }
                        Field::SampleMethod => {
                            if sample_method.is_some() {
                                return Err(de::Error::duplicate_field(""sample_method""));
                            }
                            sample_method = Some(map.next_value()?);
                        }
                        Field::Steps => {
                            if steps.is_some() {
                                return Err(de::Error::duplicate_field(""steps""));
                            }
                            steps = Some(map.next_value()?);
                        }
                        Field::Height => {
                            if height.is_some() {
                                return Err(de::Error::duplicate_field(""height""));
                            }
                            height = Some(map.next_value()?);
                        }
                        Field::Width => {
                            if width.is_some() {
                                return Err(de::Error::duplicate_field(""width""));
                            }
                            width = Some(map.next_value()?);
                        }
                    }
                }

                if n.is_none() {
                    n = Some(1);
                }

                if response_format.is_none() {
                    response_format = Some(ResponseFormat::Url);
                }

                if cfg_scale.is_none() {
                    cfg_scale = Some(7.0);
                }

                if sample_method.is_none() {
                    sample_method = Some(SamplingMethod::EulerA);
                }

                if steps.is_none() {
                    steps = Some(20);
                }

                match &size {
                    Some(size) => {
                        let parts: Vec<&str> = size.split('x').collect();
                        if parts.len() != 2 {
                            return Err(de::Error::custom(""invalid size format""));
                        }
                        height = Some(parts[0].parse().unwrap());
                        width = Some(parts[1].parse().unwrap());
                    }
                    None => {
                        if height.is_none() {
                            height = Some(512);
                        }
                        if width.is_none() {
                            width = Some(512);
                        }
                    }
                }

                Ok(ImageCreateRequest {
                    prompt: prompt.ok_or_else(|| de::Error::missing_field(""prompt""))?,
                    negative_prompt,
                    model: model.ok_or_else(|| de::Error::missing_field(""model""))?,
                    n,
                    quality,
                    response_format,
                    size,
                    style,
                    user,
                    cfg_scale,
                    sample_method,
                    steps,
                    height,
                    width,
                })
            }
        }

        const FIELDS: &[&str] = &[
            ""prompt"",
            ""negative_prompt"",
            ""model"",
            ""n"",
            ""quality"",
            ""response_format"",
            ""size"",
            ""style"",
            ""user"",
            ""cfg_scale"",
            ""sample_method"",
            ""steps"",
            ""height"",
            ""width"",
        ];
        deserializer.deserialize_struct(""CreateImageRequest"", FIELDS, CreateImageRequestVisitor)
    }
}

/// Sampling method
#[derive(Debug, Clone, Copy, Deserialize, Serialize, PartialEq, Eq)]
pub enum SamplingMethod {
    #[serde(rename = ""euler"")]
    Euler,
    #[serde(rename = ""euler_a"")]
    EulerA,
    #[serde(rename = ""heun"")]
    Heun,
    #[serde(rename = ""dpm2"")]
    Dpm2,
    #[serde(rename = ""dpm++2s_a"")]
    DpmPlusPlus2sA,
    #[serde(rename = ""dpm++2m"")]
    DpmPlusPlus2m,
    #[serde(rename = ""dpm++2mv2"")]
    DpmPlusPlus2mv2,
    #[serde(rename = ""ipndm"")]
    Ipndm,
    #[serde(rename = ""ipndm_v"")]
    IpndmV,
    #[serde(rename = ""lcm"")]
    Lcm,
}
impl fmt::Display for SamplingMethod {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        match self {
            SamplingMethod::Euler => write!(f, ""euler""),
            SamplingMethod::EulerA => write!(f, ""euler_a""),
            SamplingMethod::Heun => write!(f, ""heun""),
            SamplingMethod::Dpm2 => write!(f, ""dpm2""),
            SamplingMethod::DpmPlusPlus2sA => write!(f, ""dpm++2s_a""),
            SamplingMethod::DpmPlusPlus2m => write!(f, ""dpm++2m""),
            SamplingMethod::DpmPlusPlus2mv2 => write!(f, ""dpm++2mv2""),
            SamplingMethod::Ipndm => write!(f, ""ipndm""),
            SamplingMethod::IpndmV => write!(f, ""ipndm_v""),
            SamplingMethod::Lcm => write!(f, ""lcm""),
        }
    }
}

#[test]
fn test_serialize_image_create_request() {
    {
        let req = ImageCreateRequestBuilder::new(""test-model-name"", ""This is a prompt"")
            .with_negative_prompt(""This is the negative prompt."")
            .build();
        let json = serde_json::to_string(&req).unwrap();
        assert_eq!(
            json,
            r#""{""prompt"":""This is a prompt"",""negative_prompt"":""This is the negative prompt."",""model"":""test-model-name"",""n"":1,""response_format"":""url"",""cfg_scale"":7.0,""sample_method"":""euler_a"",""steps"":20,""height"":512,""width"":512}""#
        );
    }

    {
        let req = ImageCreateRequestBuilder::new(""test-model-name"", ""This is a prompt"")
            .with_number_of_images(2)
            .with_response_format(ResponseFormat::B64Json)
            .with_style(""vivid"")
            .with_user(""user"")
            .with_cfg_scale(1.0)
            .with_sample_method(SamplingMethod::Euler)
            .with_steps(4)
            .build();
        let json = serde_json::to_string(&req).unwrap();
        assert_eq!(
            json,
            r#""{""prompt"":""This is a prompt"",""model"":""test-model-name"",""n"":2,""response_format"":""b64_json"",""style"":""vivid"",""user"":""user"",""cfg_scale"":1.0,""sample_method"":""euler"",""steps"":4,""height"":512,""width"":512}""#
        );
    }
}

#[test]
fn test_deserialize_image_create_request() {
    {
        let json = r#""{""prompt"":""This is a prompt"",""negative_prompt"":""This is the negative prompt."",""model"":""test-model-name""}""#;
        let req: ImageCreateRequest = serde_json::from_str(json).unwrap();
        assert_eq!(req.prompt, ""This is a prompt"");
        assert!(req.negative_prompt.is_some());
        assert_eq!(
            req.negative_prompt,
            Some(""This is the negative prompt."".to_string())
        );
        assert_eq!(req.model, ""test-model-name"");
        assert_eq!(req.n, Some(1));
        assert_eq!(req.response_format, Some(ResponseFormat::Url));
        assert_eq!(req.cfg_scale, Some(7.0));
        assert_eq!(req.sample_method, Some(SamplingMethod::EulerA));
        assert_eq!(req.steps, Some(20));
        assert_eq!(req.height, Some(512));
        assert_eq!(req.width, Some(512));
    }

    {
        let json = r#""{""prompt"":""This is a prompt"",""model"":""test-model-name"",""n"":2,""response_format"":""url"",""size"":""1024x1024"",""style"":""vivid"",""user"":""user"",""cfg_scale"":1.0,""sample_method"":""euler"",""steps"":4}""#;
        let req: ImageCreateRequest = serde_json::from_str(json).unwrap();
        assert_eq!(req.prompt, ""This is a prompt"");
        assert_eq!(req.model, ""test-model-name"");
        assert_eq!(req.n, Some(2));
        assert_eq!(req.response_format, Some(ResponseFormat::Url));
        assert_eq!(req.size, Some(""1024x1024"".to_string()));
        assert_eq!(req.style, Some(""vivid"".to_string()));
        assert_eq!(req.user, Some(""user"".to_string()));
        assert_eq!(req.cfg_scale, Some(1.0));
        assert_eq!(req.sample_method, Some(SamplingMethod::Euler));
        assert_eq!(req.steps, Some(4));
        assert_eq!(req.height, Some(1024));
        assert_eq!(req.width, Some(1024));
    }

    {
        let json = r#""{""prompt"":""This is a prompt"",""model"":""test-model-name"",""n"":2,""response_format"":""url"",""size"":""1024x1024"",""style"":""vivid"",""user"":""user"",""cfg_scale"":1.0,""sample_method"":""euler"",""steps"":4,""height"":512,""width"":512}""#;
        let req: ImageCreateRequest = serde_json::from_str(json).unwrap();
        assert_eq!(req.prompt, ""This is a prompt"");
        assert_eq!(req.model, ""test-model-name"");
        assert_eq!(req.n, Some(2));
        assert_eq!(req.response_format, Some(ResponseFormat::Url));
        assert_eq!(req.size, Some(""1024x1024"".to_string()));
        assert_eq!(req.style, Some(""vivid"".to_string()));
        assert_eq!(req.user, Some(""user"".to_string()));
        assert_eq!(req.cfg_scale, Some(1.0));
        assert_eq!(req.sample_method, Some(SamplingMethod::Euler));
        assert_eq!(req.steps, Some(4));
        assert_eq!(req.height, Some(1024));
        assert_eq!(req.width, Some(1024));
    }
}

/// Builder for creating a `ImageEditRequest` instance.
pub struct ImageEditRequestBuilder {
    req: ImageEditRequest,
}
impl ImageEditRequestBuilder {
    /// Create a new builder with the given image, prompt, and mask.
    pub fn new(model: impl Into<String>, image: FileObject, prompt: impl Into<String>) -> Self {
        Self {
            req: ImageEditRequest {
                image,
                prompt: prompt.into(),
                mask: None,
                model: model.into(),
                n: Some(1),
                response_format: Some(ResponseFormat::Url),
                ..Default::default()
            },
        }
    }

    /// Set an additional image whose fully transparent areas (e.g. where alpha is zero) indicate where `image` should be edited. Must have the same dimensions as `image`.
    pub fn with_mask(mut self, mask: FileObject) -> Self {
        self.req.mask = Some(mask);
        self
    }

    /// Set the number of images to generate.
    pub fn with_number_of_images(mut self, n: u64) -> Self {
        self.req.n = Some(n);
        self
    }

    /// Set the size of the generated images.
    pub fn with_size(mut self, size: impl Into<String>) -> Self {
        self.req.size = Some(size.into());
        self
    }

    /// Set the format in which the generated images are returned.
    pub fn with_response_format(mut self, response_format: ResponseFormat) -> Self {
        self.req.response_format = Some(response_format);
        self
    }

    /// Set the user id
    pub fn with_user(mut self, user: impl Into<String>) -> Self {
        self.req.user = Some(user.into());
        self
    }

    /// Build the request.
    pub fn build(self) -> ImageEditRequest {
        self.req
    }
}

/// Request to create an edited or extended image given an original image and a prompt.
#[derive(Debug, Serialize, Default)]
pub struct ImageEditRequest {
    /// The image to edit. If mask is not provided, image must have transparency, which will be used as the mask.
    pub image: FileObject,
    /// A text description of the desired image(s).
    pub prompt: String,
    /// An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where `image` should be edited. Must have the same dimensions as `image`.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub mask: Option<FileObject>,
    /// The model to use for image generation.
    pub model: String,
    /// The number of images to generate. Defaults to 1.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n: Option<u64>,
    /// The size of the generated images. Defaults to 1024x1024.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub size: Option<String>,
    /// The format in which the generated images are returned. Must be one of `url` or `b64_json`. Defaults to `url`.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub response_format: Option<ResponseFormat>,
    /// A unique identifier representing your end-user, which can help monitor and detect abuse.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub user: Option<String>,
}
impl<'de> Deserialize<'de> for ImageEditRequest {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        enum Field {
            Image,
            Prompt,
            Mask,
            Model,
            N,
            Size,
            ResponseFormat,
            User,
        }

        impl<'de> Deserialize<'de> for Field {
            fn deserialize<D>(deserializer: D) -> Result<Field, D::Error>
            where
                D: Deserializer<'de>,
            {
                struct FieldVisitor;

                impl<'de> Visitor<'de> for FieldVisitor {
                    type Value = Field;

                    fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
                        formatter.write_str(""field identifier"")
                    }

                    fn visit_str<E>(self, value: &str) -> Result<Field, E>
                    where
                        E: de::Error,
                    {
                        match value {
                            ""image"" => Ok(Field::Image),
                            ""prompt"" => Ok(Field::Prompt),
                            ""mask"" => Ok(Field::Mask),
                            ""model"" => Ok(Field::Model),
                            ""n"" => Ok(Field::N),
                            ""size"" => Ok(Field::Size),
                            ""response_format"" => Ok(Field::ResponseFormat),
                            ""user"" => Ok(Field::User),
                            _ => Err(de::Error::unknown_field(value, FIELDS)),
                        }
                    }
                }

                deserializer.deserialize_identifier(FieldVisitor)
            }
        }

        struct ImageEditRequestVisitor;

        impl<'de> Visitor<'de> for ImageEditRequestVisitor {
            type Value = ImageEditRequest;

            fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
                formatter.write_str(""struct ImageEditRequest"")
            }

            fn visit_map<V>(self, mut map: V) -> Result<ImageEditRequest, V::Error>
            where
                V: MapAccess<'de>,
            {
                let mut image = None;
                let mut prompt = None;
                let mut mask = None;
                let mut model = None;
                let mut n = None;
                let mut size = None;
                let mut response_format = None;
                let mut user = None;
                while let Some(key) = map.next_key()? {
                    match key {
                        Field::Image => {
                            if image.is_some() {
                                return Err(de::Error::duplicate_field(""image""));
                            }
                            image = Some(map.next_value()?);
                        }
                        Field::Prompt => {
                            if prompt.is_some() {
                                return Err(de::Error::duplicate_field(""prompt""));
                            }
                            prompt = Some(map.next_value()?);
                        }
                        Field::Mask => {
                            if mask.is_some() {
                                return Err(de::Error::duplicate_field(""mask""));
                            }
                            mask = Some(map.next_value()?);
                        }
                        Field::Model => {
                            if model.is_some() {
                                return Err(de::Error::duplicate_field(""model""));
                            }
                            model = Some(map.next_value()?);
                        }
                        Field::N => {
                            if n.is_some() {
                                return Err(de::Error::duplicate_field(""n""));
                            }
                            n = Some(map.next_value()?);
                        }
                        Field::Size => {
                            if size.is_some() {
                                return Err(de::Error::duplicate_field(""size""));
                            }
                            size = Some(map.next_value()?);
                        }
                        Field::ResponseFormat => {
                            if response_format.is_some() {
                                return Err(de::Error::duplicate_field(""response_format""));
                            }
                            response_format = Some(map.next_value()?);
                        }
                        Field::User => {
                            if user.is_some() {
                                return Err(de::Error::duplicate_field(""user""));
                            }
                            user = Some(map.next_value()?);
                        }
                    }
                }
                Ok(ImageEditRequest {
                    image: image.ok_or_else(|| de::Error::missing_field(""image""))?,
                    prompt: prompt.ok_or_else(|| de::Error::missing_field(""prompt""))?,
                    mask,
                    model: model.ok_or_else(|| de::Error::missing_field(""model""))?,
                    n: n.unwrap_or(Some(1)),
                    size,
                    response_format: response_format.unwrap_or(Some(ResponseFormat::Url)),
                    user,
                })
            }
        }

        const FIELDS: &[&str] = &[
            ""image"",
            ""prompt"",
            ""mask"",
            ""model"",
            ""n"",
            ""size"",
            ""response_format"",
            ""user"",
        ];
        deserializer.deserialize_struct(""ImageEditRequest"", FIELDS, ImageEditRequestVisitor)
    }
}

#[test]
fn test_serialize_image_edit_request() {
    {
        let req = ImageEditRequestBuilder::new(
            ""test-model-name"",
            FileObject {
                id: ""test-image-id"".to_string(),
                bytes: 1024,
                created_at: 1234567890,
                filename: ""test-image.png"".to_string(),
                object: ""file"".to_string(),
                purpose: ""fine-tune"".to_string(),
            },
            ""This is a prompt"",
        )
        .build();
        let json = serde_json::to_string(&req).unwrap();
        assert_eq!(
            json,
            r#""{""image"":{""id"":""test-image-id"",""bytes"":1024,""created_at"":1234567890,""filename"":""test-image.png"",""object"":""file"",""purpose"":""fine-tune""},""prompt"":""This is a prompt"",""model"":""test-model-name"",""n"":1,""response_format"":""url""}""#
        );
    }

    {
        let req = ImageEditRequestBuilder::new(
            ""test-model-name"",
            FileObject {
                id: ""test-image-id"".to_string(),
                bytes: 1024,
                created_at: 1234567890,
                filename: ""test-image.png"".to_string(),
                object: ""file"".to_string(),
                purpose: ""fine-tune"".to_string(),
            },
            ""This is a prompt"",
        )
        .with_number_of_images(2)
        .with_response_format(ResponseFormat::B64Json)
        .with_size(""256x256"")
        .with_user(""user"")
        .build();
        let json = serde_json::to_string(&req).unwrap();
        assert_eq!(
            json,
            r#""{""image"":{""id"":""test-image-id"",""bytes"":1024,""created_at"":1234567890,""filename"":""test-image.png"",""object"":""file"",""purpose"":""fine-tune""},""prompt"":""This is a prompt"",""model"":""test-model-name"",""n"":2,""size"":""256x256"",""response_format"":""b64_json"",""user"":""user""}""#
        );
    }
}

#[test]
fn test_deserialize_image_edit_request() {
    {
        let json = r#""{""image"":{""id"":""test-image-id"",""bytes"":1024,""created_at"":1234567890,""filename"":""test-image.png"",""object"":""file"",""purpose"":""fine-tune""},""prompt"":""This is a prompt"",""model"":""test-model-name""}""#;
        let req: ImageEditRequest = serde_json::from_str(json).unwrap();
        assert_eq!(req.image.id, ""test-image-id"");
        assert_eq!(req.image.bytes, 1024);
        assert_eq!(req.image.created_at, 1234567890);
        assert_eq!(req.image.filename, ""test-image.png"");
        assert_eq!(req.image.object, ""file"");
        assert_eq!(req.image.purpose, ""fine-tune"");
        assert_eq!(req.prompt, ""This is a prompt"");
        assert!(req.mask.is_none());
        assert_eq!(req.model, ""test-model-name"");
        assert_eq!(req.n, Some(1));
        assert_eq!(req.response_format, Some(ResponseFormat::Url));
    }

    {
        let json = r#""{""image"":{""id"":""test-image-id"",""bytes"":1024,""created_at"":1234567890,""filename"":""test-image.png"",""object"":""file"",""purpose"":""fine-tune""},""prompt"":""This is a prompt"",""model"":""test-model-name"",""n"":2,""size"":""256x256"",""response_format"":""b64_json"",""user"":""user""}""#;
        let req: ImageEditRequest = serde_json::from_str(json).unwrap();
        assert_eq!(req.image.id, ""test-image-id"");
        assert_eq!(req.image.bytes, 1024);
        assert_eq!(req.image.created_at, 1234567890);
        assert_eq!(req.image.filename, ""test-image.png"");
        assert_eq!(req.image.object, ""file"");
        assert_eq!(req.image.purpose, ""fine-tune"");
        assert_eq!(req.prompt, ""This is a prompt"");
        assert!(req.mask.is_none());
        assert_eq!(req.model, ""test-model-name"");
        assert_eq!(req.n, Some(2));
        assert_eq!(req.size, Some(""256x256"".to_string()));
        assert_eq!(req.response_format, Some(ResponseFormat::B64Json));
        assert_eq!(req.user, Some(""user"".to_string()));
    }
}

/// Request to generate an image variation.
#[derive(Debug, Serialize, Default)]
pub struct ImageVariationRequest {
    /// The image to use as the basis for the variation(s).
    pub image: FileObject,
    /// Name of the model to use for image generation.
    pub model: String,
    /// The number of images to generate. Defaults to 1.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n: Option<u64>,
    /// The format in which the generated images are returned. Must be one of `url` or `b64_json`. Defaults to `b64_json`.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub response_format: Option<ResponseFormat>,
    /// The size of the generated images. Defaults to 1024x1024.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub size: Option<String>,
    /// A unique identifier representing your end-user, which can help monitor and detect abuse.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub user: Option<String>,
}
impl<'de> Deserialize<'de> for ImageVariationRequest {
    fn deserialize<D>(deserializer: D) -> Result<ImageVariationRequest, D::Error>
    where
        D: Deserializer<'de>,
    {
        enum Field {
            Image,
            Model,
            N,
            ResponseFormat,
            Size,
            User,
        }

        impl<'de> Deserialize<'de> for Field {
            fn deserialize<D>(deserializer: D) -> Result<Field, D::Error>
            where
                D: Deserializer<'de>,
            {
                struct FieldVisitor;

                impl<'de> Visitor<'de> for FieldVisitor {
                    type Value = Field;

                    fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
                        formatter.write_str(""field identifier"")
                    }

                    fn visit_str<E>(self, value: &str) -> Result<Field, E>
                    where
                        E: de::Error,
                    {
                        match value {
                            ""image"" => Ok(Field::Image),
                            ""model"" => Ok(Field::Model),
                            ""n"" => Ok(Field::N),
                            ""response_format"" => Ok(Field::ResponseFormat),
                            ""size"" => Ok(Field::Size),
                            ""user"" => Ok(Field::User),
                            _ => Err(de::Error::unknown_field(value, FIELDS)),
                        }
                    }
                }

                deserializer.deserialize_identifier(FieldVisitor)
            }
        }

        struct ImageVariationRequestVisitor;

        impl<'de> Visitor<'de> for ImageVariationRequestVisitor {
            type Value = ImageVariationRequest;

            fn expecting(&self, formatter: &mut fmt::Formatter) -> fmt::Result {
                formatter.write_str(""struct ImageVariationRequest"")
            }

            fn visit_map<V>(self, mut map: V) -> Result<ImageVariationRequest, V::Error>
            where
                V: MapAccess<'de>,
            {
                let mut image = None;
                let mut model = None;
                let mut n = None;
                let mut response_format = None;
                let mut size = None;
                let mut user = None;
                while let Some(key) = map.next_key()? {
                    match key {
                        Field::Image => {
                            if image.is_some() {
                                return Err(de::Error::duplicate_field(""image""));
                            }
                            image = Some(map.next_value()?);
                        }
                        Field::Model => {
                            if model.is_some() {
                                return Err(de::Error::duplicate_field(""model""));
                            }
                            model = Some(map.next_value()?);
                        }
                        Field::N => {
                            if n.is_some() {
                                return Err(de::Error::duplicate_field(""n""));
                            }
                            n = Some(map.next_value()?);
                        }
                        Field::ResponseFormat => {
                            if response_format.is_some() {
                                return Err(de::Error::duplicate_field(""response_format""));
                            }
                            response_format = Some(map.next_value()?);
                        }
                        Field::Size => {
                            if size.is_some() {
                                return Err(de::Error::duplicate_field(""size""));
                            }
                            size = Some(map.next_value()?);
                        }
                        Field::User => {
                            if user.is_some() {
                                return Err(de::Error::duplicate_field(""user""));
                            }
                            user = Some(map.next_value()?);
                        }
                    }
                }
                Ok(ImageVariationRequest {
                    image: image.ok_or_else(|| de::Error::missing_field(""image""))?,
                    model: model.ok_or_else(|| de::Error::missing_field(""model""))?,
                    n: n.unwrap_or(Some(1)),
                    response_format: response_format.unwrap_or(Some(ResponseFormat::B64Json)),
                    size,
                    user,
                })
            }
        }

        const FIELDS: &[&str] = &[""image"", ""model"", ""n"", ""response_format"", ""size"", ""user""];
        deserializer.deserialize_struct(
            ""ImageVariationRequest"",
            FIELDS,
            ImageVariationRequestVisitor,
        )
    }
}

/// The format in which the generated images are returned.
#[derive(Debug, Deserialize, Serialize, Clone, Copy, PartialEq, Eq)]
pub enum ResponseFormat {
    #[serde(rename = ""url"")]
    Url,
    #[serde(rename = ""b64_json"")]
    B64Json,
}
impl FromStr for ResponseFormat {
    type Err = ParseError;

    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""url"" => Ok(ResponseFormat::Url),
            ""b64_json"" => Ok(ResponseFormat::B64Json),
            _ => Err(ParseError),
        }
    }
}

// Custom error type for conversion errors
#[derive(Debug, Clone, PartialEq)]
pub struct ParseError;
impl fmt::Display for ParseError {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        write!(
            f,
            ""provided string did not match any ResponseFormat variants""
        )
    }
}

/// Represents the url or the content of an image generated.
#[derive(Debug, Deserialize, Serialize, Default)]
pub struct ImageObject {
    /// The base64-encoded JSON of the generated image, if response_format is `b64_json`.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub b64_json: Option<String>,
    /// The URL of the generated image, if response_format is `url`.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub url: Option<String>,
    /// The prompt that was used to generate the image, if there was any revision to the prompt.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt: Option<String>,
}

/// Represent the response from the `images` endpoint.
#[derive(Debug, Deserialize, Serialize)]
pub struct ListImagesResponse {
    /// The Unix timestamp (in seconds) for when the response was created.
    pub created: u64,
    /// The list of file objects.
    pub data: Vec<ImageObject>,
}
"
crates/endpoints/src/lib.rs,"//! `endpoints` is part of [LlamaEdge API Server](https://github.com/LlamaEdge/LlamaEdge/tree/main/api-server) project. It defines the data types which are derived from the [OpenAI API Reference](https://platform.openai.com/docs/api-reference).

pub mod audio;
pub mod chat;
pub mod common;
pub mod completions;
pub mod embeddings;
pub mod files;
pub mod images;
pub mod models;
pub mod rag;
"
crates/endpoints/src/models.rs,"//! Define types for the `models` endpoint.

use serde::{Deserialize, Serialize};

/// Lists the currently available models, and provides basic information about each one such as the owner and availability.
#[derive(Debug, Deserialize, Serialize)]
pub struct ListModelsResponse {
    pub object: String,
    pub data: Vec<Model>,
}

/// Describes a model offering that can be used with the API.
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct Model {
    /// The model identifier, which can be referenced in the API endpoints.
    pub id: String,
    /// The Unix timestamp (in seconds) of when the chat completion was created.
    pub created: u64,
    /// The object type, which is always ""model"".
    pub object: String,
    /// The organization that owns the model.
    pub owned_by: String,
}
"
crates/endpoints/src/rag.rs,"//! Define types for the `rag` endpoint.

use crate::{
    chat::{
        ChatCompletionRequest, ChatCompletionRequestMessage, ChatCompletionRequestSampling,
        ChatResponseFormat, StreamOptions, Tool, ToolChoice,
    },
    embeddings::EmbeddingRequest,
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RagEmbeddingRequest {
    #[serde(rename = ""embeddings"")]
    pub embedding_request: EmbeddingRequest,
    #[serde(rename = ""url"")]
    pub qdrant_url: String,
    #[serde(rename = ""collection_name"")]
    pub qdrant_collection_name: String,
}
impl RagEmbeddingRequest {
    pub fn new(
        input: &[String],
        qdrant_url: impl AsRef<str>,
        qdrant_collection_name: impl AsRef<str>,
    ) -> Self {
        RagEmbeddingRequest {
            embedding_request: EmbeddingRequest {
                model: ""dummy-embedding-model"".to_string(),
                input: input.into(),
                encoding_format: None,
                user: None,
            },
            qdrant_url: qdrant_url.as_ref().to_string(),
            qdrant_collection_name: qdrant_collection_name.as_ref().to_string(),
        }
    }

    pub fn from_embedding_request(
        embedding_request: EmbeddingRequest,
        qdrant_url: impl AsRef<str>,
        qdrant_collection_name: impl AsRef<str>,
    ) -> Self {
        RagEmbeddingRequest {
            embedding_request,
            qdrant_url: qdrant_url.as_ref().to_string(),
            qdrant_collection_name: qdrant_collection_name.as_ref().to_string(),
        }
    }
}

#[test]
fn test_rag_serialize_embedding_request() {
    let embedding_request = EmbeddingRequest {
        model: ""model"".to_string(),
        input: ""Hello, world!"".into(),
        encoding_format: None,
        user: None,
    };
    let qdrant_url = ""http://localhost:6333"".to_string();
    let qdrant_collection_name = ""qdrant_collection_name"".to_string();
    let rag_embedding_request = RagEmbeddingRequest {
        embedding_request,
        qdrant_url,
        qdrant_collection_name,
    };
    let json = serde_json::to_string(&rag_embedding_request).unwrap();
    assert_eq!(
        json,
        r#""{""embeddings"":{""model"":""model"",""input"":""Hello, world!""},""url"":""http://localhost:6333"",""collection_name"":""qdrant_collection_name""}""#
    );
}

#[test]
fn test_rag_deserialize_embedding_request() {
    let json = r#""{""embeddings"":{""model"":""model"",""input"":[""Hello, world!""]},""url"":""http://localhost:6333"",""collection_name"":""qdrant_collection_name""}""#;
    let rag_embedding_request: RagEmbeddingRequest = serde_json::from_str(json).unwrap();
    assert_eq!(rag_embedding_request.qdrant_url, ""http://localhost:6333"");
    assert_eq!(
        rag_embedding_request.qdrant_collection_name,
        ""qdrant_collection_name""
    );
    assert_eq!(rag_embedding_request.embedding_request.model, ""model"");
    assert_eq!(
        rag_embedding_request.embedding_request.input,
        vec![""Hello, world!""].into()
    );
}

#[derive(Debug, Deserialize, Serialize, Default)]
pub struct RagChatCompletionsRequest {
    /// The model to use for generating completions.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub chat_model: Option<String>,
    /// A list of messages comprising the conversation so far.
    pub messages: Vec<ChatCompletionRequestMessage>,
    /// ID of the embedding model to use.
    pub embedding_model: String,
    /// The format to return the embeddings in. Can be either float or base64.
    /// Defaults to float.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub encoding_format: Option<String>,
    /// The URL of the Qdrant server.
    pub qdrant_url: String,
    /// The name of the collection in Qdrant.
    pub qdrant_collection_name: String,
    /// Max number of retrieved results.
    pub limit: u64,
    /// Adjust the randomness of the generated text. Between 0.0 and 2.0. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
    ///
    /// We generally recommend altering this or top_p but not both.
    /// Defaults to 1.0.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    /// Limit the next token selection to a subset of tokens with a cumulative probability above a threshold P. The value should be between 0.0 and 1.0.
    ///
    /// Top-p sampling, also known as nucleus sampling, is another text generation method that selects the next token from a subset of tokens that together have a cumulative probability of at least p. This method provides a balance between diversity and quality by considering both the probabilities of tokens and the number of tokens to sample from. A higher value for top_p (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text.
    ///
    /// We generally recommend altering this or temperature but not both.
    /// Defaults to 1.0.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    /// How many chat completion choices to generate for each input message.
    /// Defaults to 1.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_choice: Option<u64>,
    /// Whether to stream the results as they are generated. Useful for chatbots.
    /// Defaults to false.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub stream: Option<bool>,
    /// Options for streaming response. Only set this when you set `stream: true`.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub stream_options: Option<StreamOptions>,
    /// A list of tokens at which to stop generation. If None, no stop tokens are used. Up to 4 sequences where the API will stop generating further tokens.
    /// Defaults to None
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub stop: Option<Vec<String>>,
    /// The maximum number of tokens to generate. The value should be no less than 1.
    /// Defaults to 16.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub max_tokens: Option<u64>,
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    /// Defaults to 0.0.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    /// Defaults to 0.0.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
    /// Modify the likelihood of specified tokens appearing in the completion.
    ///
    /// Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
    /// Defaults to None.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub logit_bias: Option<HashMap<String, f64>>,
    /// A unique identifier representing your end-user.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub user: Option<String>,
    /// Format that the model must output
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub response_format: Option<ChatResponseFormat>,
    /// A list of tools the model may call.
    ///
    /// Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.
    pub tools: Option<Vec<Tool>>,
    /// Controls which (if any) function is called by the model.
    pub tool_choice: Option<ToolChoice>,
}
impl RagChatCompletionsRequest {
    pub fn as_chat_completions_request(&self) -> ChatCompletionRequest {
        ChatCompletionRequest {
            model: self.chat_model.clone(),
            messages: self.messages.clone(),
            temperature: self.temperature,
            top_p: self.top_p,
            n_choice: self.n_choice,
            stream: self.stream,
            stream_options: self.stream_options.clone(),
            stop: self.stop.clone(),
            max_tokens: self.max_tokens,
            presence_penalty: self.presence_penalty,
            frequency_penalty: self.frequency_penalty,
            logit_bias: self.logit_bias.clone(),
            user: self.user.clone(),
            functions: None,
            function_call: None,
            response_format: self.response_format.clone(),
            tool_choice: self.tool_choice.clone(),
            tools: self.tools.clone(),
        }
    }

    pub fn from_chat_completions_request(
        chat_completions_request: ChatCompletionRequest,
        qdrant_url: impl Into<String>,
        qdrant_collection_name: impl Into<String>,
        limit: u64,
    ) -> Self {
        RagChatCompletionsRequest {
            chat_model: chat_completions_request.model,
            messages: chat_completions_request.messages,
            embedding_model: ""dummy-embedding-model"".to_string(),
            encoding_format: None,
            qdrant_url: qdrant_url.into(),
            qdrant_collection_name: qdrant_collection_name.into(),
            limit,
            temperature: chat_completions_request.temperature,
            top_p: chat_completions_request.top_p,
            n_choice: chat_completions_request.n_choice,
            stream: chat_completions_request.stream,
            stream_options: chat_completions_request.stream_options,
            stop: chat_completions_request.stop,
            max_tokens: chat_completions_request.max_tokens,
            presence_penalty: chat_completions_request.presence_penalty,
            frequency_penalty: chat_completions_request.frequency_penalty,
            logit_bias: chat_completions_request.logit_bias,
            user: chat_completions_request.user,
            response_format: chat_completions_request.response_format,
            tool_choice: chat_completions_request.tool_choice,
            tools: chat_completions_request.tools,
        }
    }
}

/// Request builder for creating a new RAG chat completion request.
pub struct RagChatCompletionRequestBuilder {
    req: RagChatCompletionsRequest,
}
impl RagChatCompletionRequestBuilder {
    /// Creates a new builder with the given model.
    ///
    /// # Arguments
    ///
    /// * `model` - ID of the model to use.
    ///
    /// * `messages` - A list of messages comprising the conversation so far.
    ///
    /// * `sampling` - The sampling method to use.
    pub fn new(
        messages: Vec<ChatCompletionRequestMessage>,
        qdrant_url: impl Into<String>,
        qdrant_collection_name: impl Into<String>,
        limit: u64,
    ) -> Self {
        Self {
            req: RagChatCompletionsRequest {
                chat_model: Some(""dummy-chat-model"".to_string()),
                messages,
                embedding_model: ""dummy-embedding-model"".to_string(),
                encoding_format: None,
                qdrant_url: qdrant_url.into(),
                qdrant_collection_name: qdrant_collection_name.into(),
                limit,
                temperature: None,
                top_p: None,
                n_choice: None,
                stream: None,
                stream_options: None,
                stop: None,
                max_tokens: None,
                presence_penalty: None,
                frequency_penalty: None,
                logit_bias: None,
                user: None,
                response_format: None,
                tool_choice: None,
                tools: None,
            },
        }
    }

    pub fn with_sampling(mut self, sampling: ChatCompletionRequestSampling) -> Self {
        let (temperature, top_p) = match sampling {
            ChatCompletionRequestSampling::Temperature(t) => (t, 1.0),
            ChatCompletionRequestSampling::TopP(p) => (1.0, p),
        };
        self.req.temperature = Some(temperature);
        self.req.top_p = Some(top_p);
        self
    }

    /// Sets the number of chat completion choices to generate for each input message.
    ///
    /// # Arguments
    ///
    /// * `n` - How many chat completion choices to generate for each input message. If `n` is less than 1, then sets to `1`.
    pub fn with_n_choices(mut self, n: u64) -> Self {
        let n_choice = if n < 1 { 1 } else { n };
        self.req.n_choice = Some(n_choice);
        self
    }

    pub fn with_stream(mut self, flag: bool) -> Self {
        self.req.stream = Some(flag);
        self
    }

    pub fn with_stop(mut self, stop: Vec<String>) -> Self {
        self.req.stop = Some(stop);
        self
    }

    /// Sets the maximum number of tokens to generate in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.
    ///
    /// # Argument
    ///
    /// * `max_tokens` - The maximum number of tokens to generate in the chat completion. If `max_tokens` is less than 1, then sets to `16`.
    pub fn with_max_tokens(mut self, max_tokens: u64) -> Self {
        let max_tokens = if max_tokens < 1 { 16 } else { max_tokens };
        self.req.max_tokens = Some(max_tokens);
        self
    }

    /// Sets the presence penalty. Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    pub fn with_presence_penalty(mut self, penalty: f64) -> Self {
        self.req.presence_penalty = Some(penalty);
        self
    }

    /// Sets the frequency penalty. Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    pub fn with_frequency_penalty(mut self, penalty: f64) -> Self {
        self.req.frequency_penalty = Some(penalty);
        self
    }

    pub fn with_logits_bias(mut self, map: HashMap<String, f64>) -> Self {
        self.req.logit_bias = Some(map);
        self
    }

    pub fn with_user(mut self, user: impl Into<String>) -> Self {
        self.req.user = Some(user.into());
        self
    }

    pub fn build(self) -> RagChatCompletionsRequest {
        self.req
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChunksRequest {
    pub id: String,
    pub filename: String,
    pub chunk_capacity: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChunksResponse {
    pub id: String,
    pub filename: String,
    pub chunks: Vec<String>,
}

#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct RetrieveObject {
    /// The retrieved sources.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub points: Option<Vec<RagScoredPoint>>,

    /// The number of similar points to retrieve
    pub limit: usize,

    /// The score threshold
    pub score_threshold: f32,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RagScoredPoint {
    /// Source of the context
    pub source: String,

    /// Points vector distance to the query vector
    pub score: f32,
}

#[test]
fn test_rag_serialize_retrieve_object() {
    {
        let ro = RetrieveObject {
            points: Some(vec![RagScoredPoint {
                source: ""source"".to_string(),
                score: 0.5,
            }]),
            limit: 1,
            score_threshold: 0.5,
        };
        let json = serde_json::to_string(&ro).unwrap();
        assert_eq!(
            json,
            r#""{""points"":[{""source"":""source"",""score"":0.5}],""limit"":1,""score_threshold"":0.5}""#
        );
    }

    {
        let ro = RetrieveObject {
            points: None,
            limit: 1,
            score_threshold: 0.5,
        };
        let json = serde_json::to_string(&ro).unwrap();
        assert_eq!(json, r#""{""limit"":1,""score_threshold"":0.5}""#);
    }
}

#[test]
fn test_rag_deserialize_retrieve_object() {
    {
        let json =
            r#""{""points"":[{""source"":""source"",""score"":0.5}],""limit"":1,""score_threshold"":0.5}""#;
        let ro: RetrieveObject = serde_json::from_str(json).unwrap();
        assert_eq!(ro.limit, 1);
        assert_eq!(ro.score_threshold, 0.5);
        assert!(ro.points.is_some());
        let points = ro.points.unwrap();
        assert_eq!(points.len(), 1);
        assert_eq!(points[0].source, ""source"");
        assert_eq!(points[0].score, 0.5);
    }

    {
        let json = r#""{""limit"":1,""score_threshold"":0.5}""#;
        let ro: RetrieveObject = serde_json::from_str(json).unwrap();
        assert_eq!(ro.limit, 1);
        assert_eq!(ro.score_threshold, 0.5);
        assert!(ro.points.is_none());
    }
}
"
crates/llama-core/Cargo.toml,"[package]
name = ""llama-core""
version = ""0.17.0""
edition = ""2021""
readme = ""README.md""
repository = ""https://github.com/LlamaEdge/LlamaEdge""
license = ""Apache-2.0""
documentation = ""https://llamaedge.github.io/LlamaEdge/llama_core/index.html""
categories = [""wasm"", ""science""]
description = ""The core library of LlamaEdge""

[dependencies]
endpoints.workspace = true
chat-prompts.workspace = true
wasmedge-wasi-nn.workspace = true
thiserror.workspace = true
serde.workspace = true
serde_json.workspace = true
uuid.workspace = true
once_cell.workspace = true
futures.workspace = true
reqwest = { version = ""0.11"", default-features = false, features = [""json"", ""stream"", ""rustls-tls""] }
qdrant = { package = ""qdrant_rest_client"", version = ""0.1.1"" }
text-splitter = { version = ""^0.7"", features = [""tiktoken-rs"", ""markdown""] }
tiktoken-rs = ""^0.5""
wasi-logger = { workspace = true, optional = true }
log = { workspace = true, optional = true }
regex = ""1""
either.workspace = true
wasmedge_stable_diffusion = { version = ""=0.2.1"" }
base64.workspace = true

[package.metadata.cargo-machete]
ignored = [""wasi-logger""]

[features]
default = []
full = [""logging"", ""search""]
logging = [""wasi-logger"", ""log""]
search = []
"
crates/llama-core/README.md,"# LlamaEdge Core

`llama-core` is part of [LlamaEdge API Server](https://github.com/LlamaEdge/LlamaEdge/tree/main/api-server) project. It defines a set of APIs. Developers can utilize these APIs to build applications based on large models, such as chatbots, RAG, and more.
"
crates/llama-core/src/audio.rs,"//! Define APIs for audio generation, transcription, and translation.

use crate::{
    error::LlamaCoreError, utils::set_tensor_data, AUDIO_GRAPH, MAX_BUFFER_SIZE, PIPER_GRAPH,
};
use endpoints::{
    audio::{
        speech::SpeechRequest,
        transcription::{TranscriptionObject, TranscriptionRequest},
        translation::{TranslationObject, TranslationRequest},
    },
    files::FileObject,
};
use std::{fs, io::Write, path::Path, time::SystemTime};

/// Transcribe audio into the input language.
pub async fn audio_transcriptions(
    request: TranscriptionRequest,
) -> Result<TranscriptionObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""processing audio transcription request"");

    let graph = match AUDIO_GRAPH.get() {
        Some(graph) => graph,
        None => {
            let err_msg = ""The AUDIO_GRAPH is not initialized."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.to_owned()));
        }
    };

    let mut graph = match graph.lock() {
        Ok(graph) => graph,
        Err(e) => {
            let err_msg = format!(""Failed to lock the graph. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg));
        }
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""translation enabled: {}"", graph.metadata.translate);

    // check if translation is disabled so that transcription tasks can be done
    if graph.metadata.translate {
        // enable translation
        graph.metadata.translate = false;

        // set the metadata to the model
        let metadata = graph.metadata.clone();

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""Update the model metadata to disable translation."");

        match serde_json::to_string(&metadata) {
            Ok(config) => {
                // update metadata
                set_tensor_data(&mut graph, 1, config.as_bytes(), [1])?;
            }
            Err(e) => {
                let err_msg = format!(""Fail to serialize metadata to a JSON string. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                return Err(LlamaCoreError::Operation(err_msg));
            }
        };

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""Disabled translation"");

        // update the metadata
        graph.metadata.translate = false;
    }

    let path = Path::new(""archives"")
        .join(&request.file.id)
        .join(&request.file.filename);

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""audio file path: {:?}"", &path);

    // load the audio waveform
    let wav_buf = load_audio_waveform(path)?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""read input tensor, size in bytes: {}"", wav_buf.len());

    // set the input tensor
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Feed the audio data to the model."");
    set_tensor_data(&mut graph, 0, &wav_buf, [1, wav_buf.len()])?;

    // compute the graph
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Transcribe audio to text."");
    if let Err(e) = graph.compute() {
        let err_msg = format!(""Failed to compute the graph. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    // get the output tensor
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""[INFO] Retrieve the transcription data."");

    // Retrieve the output.
    let mut output_buffer = vec![0u8; MAX_BUFFER_SIZE];
    let output_size = graph.get_output(0, &mut output_buffer).map_err(|e| {
        let err_msg = format!(""Failed to get the output tensor. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Output buffer size: {}"", output_size);

    // decode the output buffer
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Decode the transcription data to plain text."");

    let text = std::str::from_utf8(&output_buffer[..output_size]).map_err(|e| {
        let err_msg = format!(
            ""Failed to decode the gerated buffer to a utf-8 string. {}"",
            e
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    let obj = TranscriptionObject {
        text: text.trim().to_owned(),
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""End of the audio transcription."");

    Ok(obj)
}

fn load_audio_waveform(filename: impl AsRef<std::path::Path>) -> Result<Vec<u8>, LlamaCoreError> {
    std::fs::read(filename)
        .map_err(|e| {
            let err_msg = format!(""Failed to read the input tensor. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })
        .map_err(|e| LlamaCoreError::Operation(e.to_string()))
}

/// Generate audio from the input text.
pub async fn create_speech(request: SpeechRequest) -> Result<FileObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""processing audio speech request"");

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get the model instance."");
    let graph = match PIPER_GRAPH.get() {
        Some(graph) => graph,
        None => {
            let err_msg = ""The PIPER_GRAPH is not initialized."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.to_owned()));
        }
    };

    let mut graph = match graph.lock() {
        Ok(graph) => graph,
        Err(e) => {
            let err_msg = format!(""Failed to lock the graph. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg));
        }
    };

    // set the input tensor
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Feed the text to the model."");
    set_tensor_data(&mut graph, 0, request.input.as_bytes(), [1])?;

    // compute the graph
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""create audio."");
    if let Err(e) = graph.compute() {
        let err_msg = format!(""Failed to compute the graph. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    // get the output tensor
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""[INFO] Retrieve the audio."");

    let mut output_buffer = vec![0u8; MAX_BUFFER_SIZE];
    let output_size = graph.get_output(0, &mut output_buffer).map_err(|e| {
        let err_msg = format!(""Failed to get the output tensor. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Output buffer size: {}"", output_size);

    // * save the audio data to a file

    // create a unique file id
    let id = format!(""file_{}"", uuid::Uuid::new_v4());

    // save the file
    let path = Path::new(""archives"");
    if !path.exists() {
        fs::create_dir(path).unwrap();
    }
    let file_path = path.join(&id);
    if !file_path.exists() {
        fs::create_dir(&file_path).unwrap();
    }
    let filename = ""output.wav"";
    let mut audio_file = match fs::File::create(file_path.join(filename)) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to create the output file. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg));
        }
    };
    audio_file.write_all(&output_buffer[..output_size]).unwrap();

    // log
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

    let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
        Ok(n) => n.as_secs(),
        Err(_) => {
            let err_msg = ""Failed to get the current time."";

            // log
            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.to_owned()));
        }
    };

    Ok(FileObject {
        id,
        bytes: output_size as u64,
        created_at,
        filename: filename.to_owned(),
        object: ""file"".to_owned(),
        purpose: ""assistants_output"".to_owned(),
    })
}

/// Translate audio into the target language
pub async fn audio_translations(
    request: TranslationRequest,
) -> Result<TranslationObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""processing audio translation request"");

    let graph = match AUDIO_GRAPH.get() {
        Some(graph) => graph,
        None => {
            let err_msg = ""The AUDIO_GRAPH is not initialized."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.to_owned()));
        }
    };

    let mut graph = match graph.lock() {
        Ok(graph) => graph,
        Err(e) => {
            let err_msg = format!(""Failed to lock the graph. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg));
        }
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""translation enabled: {}"", graph.metadata.translate);

    // update metadata
    if !graph.metadata.translate {
        // update the metadata
        graph.metadata.translate = true;

        // set the metadata to the model
        let metadata = graph.metadata.clone();

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""Update the model metadata to enable translation."");

        match serde_json::to_string(&metadata) {
            Ok(config) => {
                // update metadata
                set_tensor_data(&mut graph, 1, config.as_bytes(), [1])?;
            }
            Err(e) => {
                let err_msg = format!(""Fail to serialize metadata to a JSON string. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                return Err(LlamaCoreError::Operation(err_msg));
            }
        };

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""enabled translation"");
    }

    let path = Path::new(""archives"")
        .join(&request.file.id)
        .join(&request.file.filename);

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""audio file path: {:?}"", &path);

    // load the audio waveform
    let wav_buf = load_audio_waveform(path)?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""read input tensor, size in bytes: {}"", wav_buf.len());

    // set the input tensor
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""feed the audio data to the model."");
    set_tensor_data(&mut graph, 0, &wav_buf, [1, wav_buf.len()])?;

    // compute the graph
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""translate audio to text."");
    if let Err(e) = graph.compute() {
        let err_msg = format!(""Failed to compute the graph. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    // get the output tensor
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""[INFO] retrieve the translation data."");

    // Retrieve the output.
    let mut output_buffer = vec![0u8; MAX_BUFFER_SIZE];
    let output_size = graph.get_output(0, &mut output_buffer).map_err(|e| {
        let err_msg = format!(""Failed to get the output tensor. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""output buffer size: {}"", output_size);

    // decode the output buffer
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""decode the translation data to plain text."");

    let text = std::str::from_utf8(&output_buffer[..output_size]).map_err(|e| {
        let err_msg = format!(
            ""Failed to decode the gerated buffer to a utf-8 string. {}"",
            e
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    let obj = TranslationObject {
        text: text.trim().to_owned(),
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""End of the audio translation."");

    Ok(obj)
}
"
crates/llama-core/src/completions.rs,"//! Define APIs for completions.

use crate::{
    error::{BackendError, LlamaCoreError},
    running_mode,
    utils::{get_output_buffer, get_token_info_by_graph},
    Graph, RunningMode, CHAT_GRAPHS, OUTPUT_TENSOR,
};
use endpoints::{
    common::{FinishReason, Usage},
    completions::{CompletionChoice, CompletionObject, CompletionPrompt, CompletionRequest},
};
use std::time::SystemTime;

/// Given a prompt, the model will return one or more predicted completions along with the probabilities of alternative tokens at each position.
pub async fn completions(request: &CompletionRequest) -> Result<CompletionObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Generate completions"");

    let running_mode = running_mode()?;
    if running_mode == RunningMode::Embeddings || running_mode == RunningMode::Rag {
        let err_msg = format!(
            ""The completion is not supported in the {} mode."",
            running_mode
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    let prompt = match &request.prompt {
        CompletionPrompt::SingleText(prompt) => prompt.to_owned(),
        CompletionPrompt::MultiText(prompts) => prompts.join("" ""),
    };

    compute(prompt.trim(), request.model.as_ref())
}

fn compute(
    prompt: impl AsRef<str>,
    model_name: Option<&String>,
) -> std::result::Result<CompletionObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Compute completions"");

    let chat_graphs = match CHAT_GRAPHS.get() {
        Some(chat_graphs) => chat_graphs,
        None => {
            let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let mut chat_graphs = chat_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    match model_name {
        Some(model_name) => match chat_graphs.contains_key(model_name) {
            true => {
                let graph = chat_graphs.get_mut(model_name).unwrap();
                compute_by_graph(graph, prompt)
            }
            false => match chat_graphs.iter_mut().next() {
                Some((_, graph)) => compute_by_graph(graph, prompt),
                None => {
                    let err_msg = ""There is no model available in the chat graphs."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    Err(LlamaCoreError::Operation(err_msg.into()))
                }
            },
        },
        None => match chat_graphs.iter_mut().next() {
            Some((_, graph)) => compute_by_graph(graph, prompt),
            None => {
                let err_msg = ""There is no model available in the chat graphs."";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                Err(LlamaCoreError::Operation(err_msg.into()))
            }
        },
    }
}

/// Runs inference on the model with the given name and returns the output.
fn compute_by_graph(
    graph: &mut Graph,
    prompt: impl AsRef<str>,
) -> std::result::Result<CompletionObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Compute completions by graph"");

    // check if the `embedding` model is disabled or not
    if graph.metadata.embeddings {
        graph.metadata.embeddings = false;

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""The `embedding` field of metadata sets to false."");

        graph.update_metadata()?;
    }

    // set input
    let tensor_data = prompt.as_ref().as_bytes().to_vec();
    graph
        .set_input(0, wasmedge_wasi_nn::TensorType::U8, &[1], &tensor_data)
        .map_err(|e| {
            let err_msg = format!(""Failed to set the input tensor. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Backend(BackendError::SetInput(err_msg))
        })?;

    // execute the inference
    graph.compute().map_err(|e| {
        let err_msg = format!(""Failed to execute the inference. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Backend(BackendError::Compute(err_msg))
    })?;

    // Retrieve the output
    let buffer = get_output_buffer(graph, OUTPUT_TENSOR)?;

    // convert inference result to string
    let model_answer = String::from_utf8(buffer).map_err(|e| {
        let err_msg = format!(
            ""Failed to decode the buffer of the inference result to a utf-8 string. {}"",
            e
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;
    let answer = model_answer.trim();

    // retrieve the number of prompt and completion tokens
    let token_info = get_token_info_by_graph(graph)?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Prompt tokens: {}, Completion tokens: {}"", token_info.prompt_tokens, token_info.completion_tokens);

    let created = SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .map_err(|e| {
            let err_msg = format!(""Failed to get the current time. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Completions generated successfully."");

    Ok(CompletionObject {
        id: uuid::Uuid::new_v4().to_string(),
        object: String::from(""text_completion""),
        created: created.as_secs(),
        model: graph.name().to_string(),
        choices: vec![CompletionChoice {
            index: 0,
            text: String::from(answer),
            finish_reason: FinishReason::stop,
            logprobs: None,
        }],
        usage: Usage {
            prompt_tokens: token_info.prompt_tokens,
            completion_tokens: token_info.completion_tokens,
            total_tokens: token_info.prompt_tokens + token_info.completion_tokens,
        },
    })
}
"
crates/llama-core/src/embeddings.rs,"//! Define APIs for computing embeddings.

use crate::{
    error::{BackendError, LlamaCoreError},
    running_mode,
    utils::{get_output_buffer, get_token_info_by_graph},
    Graph, RunningMode, CHAT_GRAPHS, EMBEDDING_GRAPHS, OUTPUT_TENSOR,
};
use endpoints::{
    common::Usage,
    embeddings::{EmbeddingObject, EmbeddingRequest, EmbeddingsResponse, InputText},
};
use serde::{Deserialize, Serialize};

/// Compute embeddings for the given input.
///
/// # Argument
///
/// * `embedding_request` - The embedding request.
///
/// # Returns
///
/// The embeddings response.
pub async fn embeddings(
    embedding_request: &EmbeddingRequest,
) -> Result<EmbeddingsResponse, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Computing embeddings"");

    let running_mode = running_mode()?;
    if running_mode == RunningMode::Chat {
        let err_msg = format!(
            ""Computing embeddings is not supported in the {} mode."",
            running_mode
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    let model_name = &embedding_request.model;

    // For general embedding scenario, the embedding model is the same as the chat model.
    // For RAG scenario, the embedding model is different from the chat model.
    let embedding_graphs = match EMBEDDING_GRAPHS.get() {
        Some(embedding_graphs) => embedding_graphs,
        None => match CHAT_GRAPHS.get() {
            Some(chat_graphs) => chat_graphs,
            None => {
                let err_msg = ""No embedding model is available."";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                return Err(LlamaCoreError::Operation(err_msg.into()));
            }
        },
    };

    let mut embedding_graphs = embedding_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `EMBEDDING_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    let graph = match embedding_graphs.contains_key(model_name) {
        true => embedding_graphs.get_mut(model_name).unwrap(),
        false => match embedding_graphs.iter_mut().next() {
            Some((_, graph)) => graph,
            None => {
                let err_msg = ""There is no model available in the chat graphs."";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                return Err(LlamaCoreError::Operation(err_msg.into()));
            }
        },
    };

    // check if the `embedding` option of metadata is enabled
    if !graph.metadata.embeddings {
        graph.metadata.embeddings = true;
        graph.update_metadata()?;
    }

    // compute embeddings
    let (data, usage) = match &embedding_request.input {
        InputText::String(text) => compute_embeddings(graph, &[text.to_owned()])?,
        InputText::ArrayOfStrings(texts) => compute_embeddings(graph, texts.as_slice())?,
        InputText::ArrayOfTokens(tokens) => {
            let texts: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();
            compute_embeddings(graph, texts.as_slice())?
        }
        InputText::ArrayOfTokenArrays(token_arrays) => {
            let texts: Vec<String> = token_arrays
                .iter()
                .map(|tokens| {
                    tokens
                        .iter()
                        .map(|t| t.to_string())
                        .collect::<Vec<String>>()
                        .join("" "")
                })
                .collect();
            compute_embeddings(graph, texts.as_slice())?
        }
    };

    let embedding_reponse = EmbeddingsResponse {
        object: String::from(""list""),
        data,
        model: graph.name().to_owned(),
        usage,
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Embeddings computed successfully."");

    Ok(embedding_reponse)
}

fn compute_embeddings(
    graph: &mut Graph,
    input: &[String],
) -> Result<(Vec<EmbeddingObject>, Usage), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Compute embeddings for {} chunks"", input.len());

    // compute embeddings
    let mut embeddings: Vec<EmbeddingObject> = Vec::new();
    let mut usage = Usage::default();
    for (idx, input) in input.iter().enumerate() {
        // set input
        let tensor_data = input.as_bytes().to_vec();
        graph
            .set_input(0, wasmedge_wasi_nn::TensorType::U8, &[1], &tensor_data)
            .map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Backend(BackendError::SetInput(err_msg))
            })?;

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""compute embeddings for chunk {}"", idx + 1);

        match graph.compute() {
            Ok(_) => {
                // Retrieve the output.
                let output_buffer = get_output_buffer(graph, OUTPUT_TENSOR)?;

                // convert inference result to string
                let output = std::str::from_utf8(&output_buffer[..]).map_err(|e| {
                    let err_msg = format!(
                        ""Failed to decode the buffer of the inference result to a utf-8 string. Reason: {}"",
                        e
                    );

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    LlamaCoreError::Operation(err_msg)
                })?;

                // deserialize the embedding data
                let embedding = serde_json::from_str::<Embedding>(output).map_err(|e| {
                    let err_msg =
                        format!(""Failed to deserialize the embedding data. Reason: {}"", e);

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    LlamaCoreError::Operation(err_msg)
                })?;

                let embedding_object = EmbeddingObject {
                    index: idx as u64,
                    object: String::from(""embedding""),
                    embedding: embedding.data,
                };

                embeddings.push(embedding_object);

                // retrieve the number of prompt and completion tokens
                let token_info = get_token_info_by_graph(graph)?;

                usage.prompt_tokens += token_info.prompt_tokens;
                usage.completion_tokens += token_info.completion_tokens;
                usage.total_tokens = usage.prompt_tokens + usage.completion_tokens;
            }
            Err(e) => {
                let err_msg = format!(""Failed to compute embeddings. Reason: {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                return Err(LlamaCoreError::Backend(BackendError::Compute(err_msg)));
            }
        }
    }

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""token usage of embeddings: {} prompt tokens, {} comletion tokens"", usage.prompt_tokens, usage.completion_tokens);

    Ok((embeddings, usage))
}

/// Get the dimension of the embedding model.
///
/// # Arguments
///
/// * `name` - The name of the embedding model. If `None`, the dimension of the first model will be returned.
///
/// # Returns
///
/// The dimension of the embedding model.
///
/// # Errors
///
/// * The model does not exist in the embedding graphs.
/// * No embedding model is available.
pub fn dimension(name: Option<&str>) -> Result<u64, LlamaCoreError> {
    let embedding_graphs = match EMBEDDING_GRAPHS.get() {
        Some(embedding_graphs) => embedding_graphs,
        None => {
            let err_msg = ""Fail to get the underlying value of `EMBEDDING_GRAPHS`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let embedding_graphs = embedding_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `EMBEDDING_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    match name {
        Some(model_name) => match embedding_graphs.get(model_name) {
            Some(graph) => Ok(graph.metadata.ctx_size),
            None => {
                let err_msg = format!(
                    ""The model `{}` does not exist in the embedding graphs."",
                    model_name
                );

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                Err(LlamaCoreError::Operation(err_msg))
            }
        },
        None => {
            if !embedding_graphs.is_empty() {
                let graph = match embedding_graphs.values().next() {
                    Some(graph) => graph,
                    None => {
                        let err_msg = ""Fail to get the underlying value of `EMBEDDING_GRAPHS`."";

                        #[cfg(feature = ""logging"")]
                        error!(target: ""stdout"", ""{}"", err_msg);

                        return Err(LlamaCoreError::Operation(err_msg.into()));
                    }
                };

                Ok(graph.metadata.ctx_size)
            } else {
                let err_msg = ""There is no model available in the embedding graphs."";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                Err(LlamaCoreError::Operation(err_msg.into()))
            }
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
struct Embedding {
    #[serde(rename = ""n_embedding"")]
    len: u64,
    #[serde(rename = ""embedding"")]
    data: Vec<f64>,
}
"
crates/llama-core/src/error.rs,"//! Error types for the Llama Core library.

use thiserror::Error;

/// Error types for the Llama Core library.
#[derive(Error, Debug)]
pub enum LlamaCoreError {
    /// Errors in General operation.
    #[error(""{0}"")]
    Operation(String),
    /// Errors in Context initialization.
    #[error(""Failed to initialize computation context. Reason: {0}"")]
    InitContext(String),
    /// Errors thrown by the wasi-nn-ggml plugin and runtime.
    #[error(""{0}"")]
    Backend(#[from] BackendError),
    /// Errors thrown by the Search Backend
    #[cfg(feature = ""search"")]
    #[error(""{0}"")]
    Search(String),
}

/// Error types for wasi-nn errors.
#[derive(Error, Debug)]
pub enum BackendError {
    /// Errors in setting the input tensor.
    #[error(""{0}"")]
    SetInput(String),
    /// Errors in the model inference.
    #[error(""{0}"")]
    Compute(String),
    /// Errors in the model inference in the stream mode.
    #[error(""{0}"")]
    ComputeSingle(String),
    /// Errors in getting the output tensor.
    #[error(""{0}"")]
    GetOutput(String),
    /// Errors in getting the output tensor in the stream mode.
    #[error(""{0}"")]
    GetOutputSingle(String),
    /// Errors in cleaning up the computation context in the stream mode.
    #[error(""{0}"")]
    FinishSingle(String),
}
"
crates/llama-core/src/graph.rs,"//! Define Graph and GraphBuilder APIs for creating a new computation graph.

use crate::{error::LlamaCoreError, utils::set_tensor_data_u8, Metadata};
use chat_prompts::PromptTemplateType;
use wasmedge_wasi_nn::{
    Error as WasiNnError, Graph as WasiNnGraph, GraphExecutionContext, TensorType,
};

/// Builder for creating a new computation graph.
#[derive(Debug)]
pub struct GraphBuilder {
    metadata: Option<Metadata>,
    wasi_nn_graph_builder: wasmedge_wasi_nn::GraphBuilder,
}
impl GraphBuilder {
    /// Create a new computation graph builder.
    pub fn new(ty: EngineType) -> Result<Self, LlamaCoreError> {
        let encoding = match ty {
            EngineType::Ggml => wasmedge_wasi_nn::GraphEncoding::Ggml,
            EngineType::Whisper => wasmedge_wasi_nn::GraphEncoding::Whisper,
            EngineType::Piper => wasmedge_wasi_nn::GraphEncoding::Piper,
        };

        let wasi_nn_graph_builder =
            wasmedge_wasi_nn::GraphBuilder::new(encoding, wasmedge_wasi_nn::ExecutionTarget::AUTO);

        Ok(Self {
            metadata: None,
            wasi_nn_graph_builder,
        })
    }

    pub fn with_config(mut self, metadata: &Metadata) -> Result<Self, LlamaCoreError> {
        let config = serde_json::to_string(&metadata).map_err(|e| {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;
        self.wasi_nn_graph_builder = self.wasi_nn_graph_builder.config(config);
        self.metadata = Some(metadata.clone());

        Ok(self)
    }

    pub fn use_cpu(mut self) -> Self {
        self.wasi_nn_graph_builder = self.wasi_nn_graph_builder.cpu();
        self
    }

    pub fn use_gpu(mut self) -> Self {
        self.wasi_nn_graph_builder = self.wasi_nn_graph_builder.gpu();
        self
    }

    pub fn use_tpu(mut self) -> Self {
        self.wasi_nn_graph_builder = self.wasi_nn_graph_builder.tpu();
        self
    }

    pub fn build_from_buffer<B>(self, bytes_array: impl AsRef<[B]>) -> Result<Graph, LlamaCoreError>
    where
        B: AsRef<[u8]>,
    {
        // load the model
        let graph = self
            .wasi_nn_graph_builder
            .build_from_bytes(bytes_array)
            .map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

        // initialize the execution context
        let context = graph.init_execution_context().map_err(|e| {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

        let created = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

        Ok(Graph {
            created,
            metadata: self.metadata.clone().unwrap_or_default(),
            _graph: graph,
            context,
        })
    }

    pub fn build_from_files<P>(self, files: impl AsRef<[P]>) -> Result<Graph, LlamaCoreError>
    where
        P: AsRef<std::path::Path>,
    {
        // load the model
        let graph = self
            .wasi_nn_graph_builder
            .build_from_files(files)
            .map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

        // initialize the execution context
        let context = graph.init_execution_context().map_err(|e| {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

        let created = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

        Ok(Graph {
            created,
            metadata: self.metadata.clone().unwrap_or_default(),
            _graph: graph,
            context,
        })
    }

    pub fn build_from_cache(self) -> Result<Graph, LlamaCoreError> {
        match &self.metadata {
            Some(metadata) => {
                // load the model
                let graph = self
                    .wasi_nn_graph_builder
                    .build_from_cache(&metadata.model_alias)
                    .map_err(|e| {
                        let err_msg = e.to_string();

                        #[cfg(feature = ""logging"")]
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        LlamaCoreError::Operation(err_msg)
                    })?;

                // initialize the execution context
                let context = graph.init_execution_context().map_err(|e| {
                    let err_msg = e.to_string();

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    LlamaCoreError::Operation(err_msg)
                })?;

                let created = std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .map_err(|e| {
                        let err_msg = e.to_string();

                        #[cfg(feature = ""logging"")]
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        LlamaCoreError::Operation(err_msg)
                    })?;

                Ok(Graph {
                    created,
                    metadata: metadata.clone(),
                    _graph: graph,
                    context,
                })
            }
            None => {
                let err_msg =
                    ""Failed to create a Graph from cache. Reason: Metadata is not provided.""
                        .to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                Err(LlamaCoreError::Operation(err_msg))
            }
        }
    }
}

/// Wrapper of the `wasmedge_wasi_nn::Graph` struct
#[derive(Debug)]
pub struct Graph {
    pub created: std::time::Duration,
    pub metadata: Metadata,
    _graph: WasiNnGraph,
    context: GraphExecutionContext,
}
impl Graph {
    /// Create a new computation graph from the given metadata.
    pub fn new(metadata: &Metadata) -> Result<Self, LlamaCoreError> {
        let config = serde_json::to_string(&metadata).map_err(|e| {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

        // load the model
        let graph = wasmedge_wasi_nn::GraphBuilder::new(
            wasmedge_wasi_nn::GraphEncoding::Ggml,
            wasmedge_wasi_nn::ExecutionTarget::AUTO,
        )
        .config(config)
        .build_from_cache(&metadata.model_alias)
        .map_err(|e| {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

        // initialize the execution context
        let context = graph.init_execution_context().map_err(|e| {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

        let created = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

        Ok(Self {
            created,
            metadata: metadata.clone(),
            _graph: graph,
            context,
        })
    }

    /// Get the name of the model
    pub fn name(&self) -> &str {
        &self.metadata.model_name
    }

    /// Get the alias of the model
    pub fn alias(&self) -> &str {
        &self.metadata.model_alias
    }

    /// Get the prompt template type
    pub fn prompt_template(&self) -> PromptTemplateType {
        self.metadata.prompt_template
    }

    /// Update metadata
    pub fn update_metadata(&mut self) -> Result<(), LlamaCoreError> {
        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""Update metadata for the model named {}"", self.name());

        // update metadata
        let config = match serde_json::to_string(&self.metadata) {
            Ok(config) => config,
            Err(e) => {
                let err_msg = format!(""Failed to update metadta. Reason: Fail to serialize metadata to a JSON string. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                return Err(LlamaCoreError::Operation(err_msg));
            }
        };

        let res = set_tensor_data_u8(self, 1, config.as_bytes());

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""Metadata updated successfully."");

        res
    }

    /// Set input uses the data, not only [u8](https://doc.rust-lang.org/nightly/std/primitive.u8.html), but also [f32](https://doc.rust-lang.org/nightly/std/primitive.f32.html), [i32](https://doc.rust-lang.org/nightly/std/primitive.i32.html), etc.
    pub fn set_input<T: Sized>(
        &mut self,
        index: usize,
        tensor_type: TensorType,
        dimensions: &[usize],
        data: impl AsRef<[T]>,
    ) -> Result<(), WasiNnError> {
        self.context.set_input(index, tensor_type, dimensions, data)
    }

    /// Compute the inference on the given inputs.
    pub fn compute(&mut self) -> Result<(), WasiNnError> {
        self.context.compute()
    }

    /// Compute the inference on the given inputs.
    ///
    /// Note that this method is used for the stream mode. It generates one token at a time.
    pub fn compute_single(&mut self) -> Result<(), WasiNnError> {
        self.context.compute_single()
    }

    /// Copy output tensor to out_buffer, return the output’s **size in bytes**.
    pub fn get_output<T: Sized>(
        &self,
        index: usize,
        out_buffer: &mut [T],
    ) -> Result<usize, WasiNnError> {
        self.context.get_output(index, out_buffer)
    }

    /// Copy output tensor to out_buffer, return the output’s **size in bytes**.
    ///
    /// Note that this method is used for the stream mode. It returns one token at a time.
    pub fn get_output_single<T: Sized>(
        &self,
        index: usize,
        out_buffer: &mut [T],
    ) -> Result<usize, WasiNnError> {
        self.context.get_output_single(index, out_buffer)
    }

    /// Clear the computation context.
    ///
    /// Note that this method is used for the stream mode. It clears the context after the stream mode is finished.
    pub fn finish_single(&mut self) -> Result<(), WasiNnError> {
        self.context.fini_single()
    }
}

/// Engine type
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]
pub enum EngineType {
    Ggml,
    Whisper,
    Piper,
}
"
crates/llama-core/src/images.rs,"//! Define APIs for image generation and edit.

use crate::{error::LlamaCoreError, SD_IMAGE_TO_IMAGE, SD_TEXT_TO_IMAGE};
use base64::{engine::general_purpose, Engine as _};
use endpoints::images::{
    ImageCreateRequest, ImageEditRequest, ImageObject, ImageVariationRequest, ListImagesResponse,
    ResponseFormat, SamplingMethod,
};
use std::{
    fs::{self, File},
    io::{self, Read},
    path::Path,
};
use wasmedge_stable_diffusion::{stable_diffusion_interface::ImageType, BaseFunction, Context};

/// Create an image given a prompt.
pub async fn image_generation(
    req: &mut ImageCreateRequest,
) -> Result<ListImagesResponse, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Processing the image generation request."");

    let sd = match SD_TEXT_TO_IMAGE.get() {
        Some(sd) => sd,
        None => {
            let err_msg = ""Fail to get the underlying value of `SD_TEXT_TO_IMAGE`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""llama_core"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let sd_locked = sd.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `SD_TEXT_TO_IMAGE`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""llama_core"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""create computation context."");

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""config of sd: {:?}"", &sd_locked);

    match sd_locked.create_context().map_err(|e| {
        let err_msg = format!(""Fail to create the context. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""llama_core"", ""{}"", &err_msg);

        LlamaCoreError::InitContext(err_msg)
    })? {
        Context::TextToImage(mut text_to_image) => {
            // create a unique file id
            let id = format!(""file_{}"", uuid::Uuid::new_v4());

            // save the file
            let path = Path::new(""archives"");
            if !path.exists() {
                fs::create_dir(path).unwrap();
            }
            let file_path = path.join(&id);
            if !file_path.exists() {
                fs::create_dir(&file_path).unwrap();
            }
            let filename = ""output.png"";
            let output_image_file = file_path.join(filename);
            let output_image_file = output_image_file.to_str().unwrap();

            // log
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""prompt: {}"", &req.prompt);

            // negative prompt
            let negative_prompt = req.negative_prompt.clone().unwrap_or_default();

            // log
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""negative prompt: {}"", &negative_prompt);

            // cfg_scale
            let cfg_scale = req.cfg_scale.unwrap_or(7.0);

            // log
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""cfg_scale: {}"", cfg_scale);

            // sampling method
            let sample_method = req.sample_method.unwrap_or(SamplingMethod::EulerA);

            // log
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""sample_method: {}"", sample_method);

            // convert sample method to value of `SampleMethodT` type
            let sample_method = match sample_method {
                SamplingMethod::Euler => {
                    wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::EULER
                }
                SamplingMethod::EulerA => {
                    wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::EULERA
                }
                SamplingMethod::Heun => {
                    wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::HEUN
                }
                SamplingMethod::Dpm2 => {
                    wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::DPM2
                }
                SamplingMethod::DpmPlusPlus2sA => {
                    wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::DPMPP2SA
                }
                SamplingMethod::DpmPlusPlus2m => {
                    wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::DPMPP2M
                }
                SamplingMethod::DpmPlusPlus2mv2 => {
                    wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::DPMPP2Mv2
                }
                SamplingMethod::Ipndm => {
                    wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::IPNDM
                }
                SamplingMethod::IpndmV => {
                    wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::IPNDMV
                }
                SamplingMethod::Lcm => {
                    wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::LCM
                }
            };

            // steps
            let steps = req.steps.unwrap_or(20);

            // log
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""steps: {}"", steps);

            // size
            let height = req.height.unwrap_or(512);
            let width = req.width.unwrap_or(512);

            // log
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""height: {}, width: {}"", height, width);

            // log
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""generate image"");

            text_to_image
                .set_prompt(&req.prompt)
                .set_negative_prompt(negative_prompt)
                .set_output_path(output_image_file)
                .set_cfg_scale(cfg_scale)
                .set_sample_method(sample_method)
                .set_sample_steps(steps as i32)
                .set_height(height as i32)
                .set_width(width as i32)
                .generate()
                .map_err(|e| {
                    let err_msg = format!(""Fail to dump the image. {}"", e);

                    #[cfg(feature = ""logging"")]
                    error!(target: ""llama_core"", ""{}"", &err_msg);

                    LlamaCoreError::Operation(err_msg)
                })?;

            // log
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

            let image = match req.response_format {
                Some(ResponseFormat::B64Json) => {
                    // convert the image to base64 string
                    let base64_string = match image_to_base64(output_image_file) {
                        Ok(base64_string) => base64_string,
                        Err(e) => {
                            let err_msg =
                                format!(""Fail to convert the image to base64 string. {}"", e);

                            #[cfg(feature = ""logging"")]
                            error!(target: ""llama_core"", ""{}"", &err_msg);

                            return Err(LlamaCoreError::Operation(err_msg));
                        }
                    };

                    // log
                    #[cfg(feature = ""logging"")]
                    info!(target: ""stdout"", ""base64 string: {}"", &base64_string.chars().take(10).collect::<String>());

                    // create an image object
                    ImageObject {
                        b64_json: Some(base64_string),
                        url: None,
                        prompt: Some(req.prompt.clone()),
                    }
                }
                Some(ResponseFormat::Url) | None => {
                    // create an image object
                    ImageObject {
                        b64_json: None,
                        url: Some(format!(""/archives/{}/{}"", &id, &filename)),
                        prompt: Some(req.prompt.clone()),
                    }
                }
            };

            let created: u64 =
                match std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        #[cfg(feature = ""logging"")]
                        error!(target: ""llama_core"", ""{}"", &err_msg);

                        return Err(LlamaCoreError::Operation(err_msg.into()));
                    }
                };

            let res = ListImagesResponse {
                created,
                data: vec![image],
            };

            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""End of the image generation."");

            Ok(res)
        }
        _ => {
            let err_msg = ""Fail to get the `TextToImage` context."";

            #[cfg(feature = ""logging"")]
            error!(target: ""llama_core"", ""{}"", &err_msg);

            Err(LlamaCoreError::Operation(err_msg.into()))
        }
    }
}

/// Create an edited or extended image given an original image and a prompt.
pub async fn image_edit(req: &mut ImageEditRequest) -> Result<ListImagesResponse, LlamaCoreError> {
    let sd = match SD_IMAGE_TO_IMAGE.get() {
        Some(sd) => sd,
        None => {
            let err_msg = ""Fail to get the underlying value of `SD_IMAGE_TO_IMAGE`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""llama_core"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let sd_locked = sd.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `SD_IMAGE_TO_IMAGE`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""llama_core"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    match sd_locked.create_context().map_err(|e| {
        let err_msg = format!(""Fail to create the context. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""llama_core"", ""{}"", &err_msg);

        LlamaCoreError::InitContext(err_msg)
    })? {
        Context::ImageToImage(mut image_to_image) => {
            // create a unique file id
            let id = format!(""file_{}"", uuid::Uuid::new_v4());

            // save the file
            let path = Path::new(""archives"");
            if !path.exists() {
                fs::create_dir(path).unwrap();
            }
            let file_path = path.join(&id);
            if !file_path.exists() {
                fs::create_dir(&file_path).unwrap();
            }
            let filename = ""output.png"";
            let output_image_file = file_path.join(filename);
            let output_image_file = output_image_file.to_str().unwrap();

            // get the path of the original image
            let origin_image_file = Path::new(""archives"")
                .join(&req.image.id)
                .join(&req.image.filename);
            let path_origin_image = origin_image_file.to_str().ok_or(LlamaCoreError::Operation(
                ""Fail to get the path of the original image."".into(),
            ))?;

            // create and dump the generated image
            image_to_image
                .set_prompt(&req.prompt)
                .set_image(ImageType::Path(path_origin_image))
                .set_output_path(output_image_file)
                .generate()
                .map_err(|e| {
                    let err_msg = format!(""Fail to dump the image. {}"", e);

                    #[cfg(feature = ""logging"")]
                    error!(target: ""llama_core"", ""{}"", &err_msg);

                    LlamaCoreError::Operation(err_msg)
                })?;

            // log
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

            let image = match req.response_format {
                Some(ResponseFormat::B64Json) => {
                    // convert the image to base64 string
                    let base64_string = match image_to_base64(output_image_file) {
                        Ok(base64_string) => base64_string,
                        Err(e) => {
                            let err_msg =
                                format!(""Fail to convert the image to base64 string. {}"", e);

                            #[cfg(feature = ""logging"")]
                            error!(target: ""llama_core"", ""{}"", &err_msg);

                            return Err(LlamaCoreError::Operation(err_msg));
                        }
                    };

                    // log
                    #[cfg(feature = ""logging"")]
                    info!(target: ""stdout"", ""base64 string: {}"", &base64_string.chars().take(10).collect::<String>());

                    // create an image object
                    ImageObject {
                        b64_json: Some(base64_string),
                        url: None,
                        prompt: Some(req.prompt.clone()),
                    }
                }
                Some(ResponseFormat::Url) | None => {
                    // create an image object
                    ImageObject {
                        b64_json: None,
                        url: Some(format!(""/archives/{}/{}"", &id, &filename)),
                        prompt: Some(req.prompt.clone()),
                    }
                }
            };

            let created: u64 =
                match std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        #[cfg(feature = ""logging"")]
                        error!(target: ""llama_core"", ""{}"", &err_msg);

                        return Err(LlamaCoreError::Operation(err_msg.into()));
                    }
                };

            Ok(ListImagesResponse {
                created,
                data: vec![image],
            })
        }
        _ => {
            let err_msg = ""Fail to get the `ImageToImage` context."";

            #[cfg(feature = ""logging"")]
            error!(target: ""llama_core"", ""{}"", &err_msg);

            Err(LlamaCoreError::Operation(err_msg.into()))
        }
    }
}

/// Create a variation of a given image.
pub async fn image_variation(
    _req: &mut ImageVariationRequest,
) -> Result<ListImagesResponse, LlamaCoreError> {
    unimplemented!(""image_variation"")
}

// convert an image file to a base64 string
fn image_to_base64(image_path: &str) -> io::Result<String> {
    // Open the file
    let mut image_file = File::open(image_path)?;

    // Read the file into a byte array
    let mut buffer = Vec::new();
    image_file.read_to_end(&mut buffer)?;

    Ok(general_purpose::STANDARD.encode(&buffer))
}
"
crates/llama-core/src/lib.rs,"//! Llama Core, abbreviated as `llama-core`, defines a set of APIs. Developers can utilize these APIs to build applications based on large models, such as chatbots, RAG, and more.

#[cfg(feature = ""logging"")]
#[macro_use]
extern crate log;

pub mod audio;
pub mod chat;
pub mod completions;
pub mod embeddings;
pub mod error;
pub mod graph;
pub mod images;
pub mod models;
pub mod rag;
#[cfg(feature = ""search"")]
pub mod search;
pub mod utils;

pub use error::LlamaCoreError;
pub use graph::{EngineType, Graph, GraphBuilder};

use chat_prompts::PromptTemplateType;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{
    collections::HashMap,
    path::Path,
    sync::{Mutex, RwLock},
};
use utils::get_output_buffer;
use wasmedge_stable_diffusion::*;

// key: model_name, value: Graph
pub(crate) static CHAT_GRAPHS: OnceCell<Mutex<HashMap<String, Graph>>> = OnceCell::new();
// key: model_name, value: Graph
pub(crate) static EMBEDDING_GRAPHS: OnceCell<Mutex<HashMap<String, Graph>>> = OnceCell::new();
// cache bytes for decoding utf8
pub(crate) static CACHED_UTF8_ENCODINGS: OnceCell<Mutex<Vec<u8>>> = OnceCell::new();
// running mode
pub(crate) static RUNNING_MODE: OnceCell<RwLock<RunningMode>> = OnceCell::new();
// stable diffusion context for the text-to-image task
pub(crate) static SD_TEXT_TO_IMAGE: OnceCell<Mutex<StableDiffusion>> = OnceCell::new();
// stable diffusion context for the image-to-image task
pub(crate) static SD_IMAGE_TO_IMAGE: OnceCell<Mutex<StableDiffusion>> = OnceCell::new();
// context for the audio task
pub(crate) static AUDIO_GRAPH: OnceCell<Mutex<Graph>> = OnceCell::new();
// context for the piper task
pub(crate) static PIPER_GRAPH: OnceCell<Mutex<Graph>> = OnceCell::new();

pub(crate) const MAX_BUFFER_SIZE: usize = 2usize.pow(14) * 15 + 128;
pub(crate) const OUTPUT_TENSOR: usize = 0;
const PLUGIN_VERSION: usize = 1;

/// Model metadata
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // this field not defined for the beckend plugin
    #[serde(skip_serializing)]
    pub model_name: String,
    // this field not defined for the beckend plugin
    #[serde(skip_serializing)]
    pub model_alias: String,
    // this field not defined for the beckend plugin
    #[serde(skip_serializing)]
    pub log_prompts: bool,
    // this field not defined for the beckend plugin
    #[serde(skip_serializing)]
    pub prompt_template: PromptTemplateType,

    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    #[serde(rename = ""enable-debug-log"")]
    pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    /// path to the multimodal projector file for llava
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub mmproj: Option<String>,
    /// Path to the image file for llava
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub image: Option<String>,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    /// The main GPU to use. Defaults to None.
    #[serde(rename = ""main-gpu"")]
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[serde(rename = ""tensor-split"")]
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub tensor_split: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    pub use_mmap: Option<bool>,
    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,
    #[serde(rename = ""threads"")]
    pub threads: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,

    // * grammar parameters
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir). Defaults to empty string.
    pub grammar: String,
    /// JSON schema to constrain generations (<https://json-schema.org/>), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub json_schema: Option<String>,

    // * parameters for whisper
    pub translate: bool,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub language: Option<String>,
}
impl Default for Metadata {
    fn default() -> Self {
        Self {
            model_name: String::new(),
            model_alias: String::new(),
            log_prompts: false,
            debug_log: false,
            prompt_template: PromptTemplateType::Llama2Chat,
            log_enable: false,
            embeddings: false,
            n_predict: 1024,
            reverse_prompt: None,
            mmproj: None,
            image: None,
            n_gpu_layers: 100,
            main_gpu: None,
            tensor_split: None,
            use_mmap: Some(true),
            ctx_size: 512,
            batch_size: 512,
            threads: 2,
            temperature: 1.0,
            top_p: 1.0,
            repeat_penalty: 1.1,
            presence_penalty: 0.0,
            frequency_penalty: 0.0,
            grammar: String::new(),
            json_schema: None,
            translate: false,
            language: None,
        }
    }
}

/// Builder for the `Metadata` struct
#[derive(Debug)]
pub struct MetadataBuilder {
    metadata: Metadata,
}
impl MetadataBuilder {
    pub fn new<S: Into<String>>(model_name: S, model_alias: S, pt: PromptTemplateType) -> Self {
        let metadata = Metadata {
            model_name: model_name.into(),
            model_alias: model_alias.into(),
            prompt_template: pt,
            ..Default::default()
        };

        Self { metadata }
    }

    pub fn with_prompt_template(mut self, template: PromptTemplateType) -> Self {
        self.metadata.prompt_template = template;
        self
    }

    pub fn enable_plugin_log(mut self, enable: bool) -> Self {
        self.metadata.log_enable = enable;
        self
    }

    pub fn enable_debug_log(mut self, enable: bool) -> Self {
        self.metadata.debug_log = enable;
        self
    }

    pub fn enable_prompts_log(mut self, enable: bool) -> Self {
        self.metadata.log_prompts = enable;
        self
    }

    pub fn enable_embeddings(mut self, enable: bool) -> Self {
        self.metadata.embeddings = enable;
        self
    }

    pub fn with_n_predict(mut self, n: u64) -> Self {
        self.metadata.n_predict = n;
        self
    }

    pub fn with_main_gpu(mut self, gpu: Option<u64>) -> Self {
        self.metadata.main_gpu = gpu;
        self
    }

    pub fn with_tensor_split(mut self, split: Option<String>) -> Self {
        self.metadata.tensor_split = split;
        self
    }

    pub fn with_threads(mut self, threads: u64) -> Self {
        self.metadata.threads = threads;
        self
    }

    pub fn with_reverse_prompt(mut self, prompt: Option<String>) -> Self {
        self.metadata.reverse_prompt = prompt;
        self
    }

    pub fn with_mmproj(mut self, path: Option<String>) -> Self {
        self.metadata.mmproj = path;
        self
    }

    pub fn with_image(mut self, path: impl Into<String>) -> Self {
        self.metadata.image = Some(path.into());
        self
    }

    pub fn with_n_gpu_layers(mut self, n: u64) -> Self {
        self.metadata.n_gpu_layers = n;
        self
    }

    pub fn disable_mmap(mut self, disable: Option<bool>) -> Self {
        self.metadata.use_mmap = disable.map(|v| !v);
        self
    }

    pub fn with_ctx_size(mut self, size: u64) -> Self {
        self.metadata.ctx_size = size;
        self
    }

    pub fn with_batch_size(mut self, size: u64) -> Self {
        self.metadata.batch_size = size;
        self
    }

    pub fn with_temperature(mut self, temp: f64) -> Self {
        self.metadata.temperature = temp;
        self
    }

    pub fn with_top_p(mut self, top_p: f64) -> Self {
        self.metadata.top_p = top_p;
        self
    }

    pub fn with_repeat_penalty(mut self, penalty: f64) -> Self {
        self.metadata.repeat_penalty = penalty;
        self
    }

    pub fn with_presence_penalty(mut self, penalty: f64) -> Self {
        self.metadata.presence_penalty = penalty;
        self
    }

    pub fn with_frequency_penalty(mut self, penalty: f64) -> Self {
        self.metadata.frequency_penalty = penalty;
        self
    }

    pub fn with_grammar(mut self, grammar: impl Into<String>) -> Self {
        self.metadata.grammar = grammar.into();
        self
    }

    pub fn with_json_schema(mut self, schema: Option<String>) -> Self {
        self.metadata.json_schema = schema;
        self
    }

    pub fn build(self) -> Metadata {
        self.metadata
    }
}

/// Builder for creating an audio metadata
#[derive(Debug)]
pub struct AudioMetadataBuilder {
    metadata: Metadata,
}
impl AudioMetadataBuilder {
    pub fn new<S: Into<String>>(model_name: S, model_alias: S) -> Self {
        let metadata = Metadata {
            model_name: model_name.into(),
            model_alias: model_alias.into(),
            prompt_template: PromptTemplateType::Null,
            ..Default::default()
        };

        Self { metadata }
    }

    pub fn enable_plugin_log(mut self, enable: bool) -> Self {
        self.metadata.log_enable = enable;
        self
    }

    pub fn enable_debug_log(mut self, enable: bool) -> Self {
        self.metadata.debug_log = enable;
        self
    }

    pub fn enable_translate(mut self, enable: bool) -> Self {
        self.metadata.translate = enable;
        self
    }

    pub fn target_language(mut self, language: Option<String>) -> Self {
        self.metadata.language = language;
        self
    }

    pub fn build(self) -> Metadata {
        self.metadata
    }
}

/// Initialize the core context
pub fn init_core_context(
    metadata_for_chats: Option<&[Metadata]>,
    metadata_for_embeddings: Option<&[Metadata]>,
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Initializing the core context"");

    if metadata_for_chats.is_none() && metadata_for_embeddings.is_none() {
        let err_msg = ""Failed to initialize the core context. Please set metadata for chat completions and/or embeddings."";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        return Err(LlamaCoreError::InitContext(err_msg.into()));
    }

    let mut mode = RunningMode::Embeddings;

    if let Some(metadata_chats) = metadata_for_chats {
        let mut chat_graphs = HashMap::new();
        for metadata in metadata_chats {
            let graph = Graph::new(metadata)?;

            chat_graphs.insert(graph.name().to_string(), graph);
        }
        CHAT_GRAPHS.set(Mutex::new(chat_graphs)).map_err(|_| {
            let err_msg = ""Failed to initialize the core context. Reason: The `CHAT_GRAPHS` has already been initialized"";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg.into())
        })?;

        mode = RunningMode::Chat
    }

    if let Some(metadata_embeddings) = metadata_for_embeddings {
        let mut embedding_graphs = HashMap::new();
        for metadata in metadata_embeddings {
            let graph = Graph::new(metadata)?;

            embedding_graphs.insert(graph.name().to_string(), graph);
        }
        EMBEDDING_GRAPHS
            .set(Mutex::new(embedding_graphs))
            .map_err(|_| {
                let err_msg = ""Failed to initialize the core context. Reason: The `EMBEDDING_GRAPHS` has already been initialized"";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                LlamaCoreError::InitContext(err_msg.into())
            })?;

        if mode == RunningMode::Chat {
            mode = RunningMode::ChatEmbedding;
        }
    }

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""running mode: {}"", mode);

    RUNNING_MODE.set(RwLock::new(mode)).map_err(|_| {
        let err_msg = ""Failed to initialize the core context. Reason: The `RUNNING_MODE` has already been initialized"";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        LlamaCoreError::InitContext(err_msg.into())
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""The core context has been initialized"");

    Ok(())
}

/// Initialize the core context for RAG scenarios.
pub fn init_rag_core_context(
    metadata_for_chats: &[Metadata],
    metadata_for_embeddings: &[Metadata],
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Initializing the core context for RAG scenarios"");

    // chat models
    if metadata_for_chats.is_empty() {
        let err_msg = ""The metadata for chat models is empty"";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        return Err(LlamaCoreError::InitContext(err_msg.into()));
    }
    let mut chat_graphs = HashMap::new();
    for metadata in metadata_for_chats {
        let graph = Graph::new(metadata)?;

        chat_graphs.insert(graph.name().to_string(), graph);
    }
    CHAT_GRAPHS.set(Mutex::new(chat_graphs)).map_err(|_| {
        let err_msg = ""Failed to initialize the core context. Reason: The `CHAT_GRAPHS` has already been initialized"";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        LlamaCoreError::InitContext(err_msg.into())
    })?;

    // embedding models
    if metadata_for_embeddings.is_empty() {
        let err_msg = ""The metadata for embeddings is empty"";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        return Err(LlamaCoreError::InitContext(err_msg.into()));
    }
    let mut embedding_graphs = HashMap::new();
    for metadata in metadata_for_embeddings {
        let graph = Graph::new(metadata)?;

        embedding_graphs.insert(graph.name().to_string(), graph);
    }
    EMBEDDING_GRAPHS
        .set(Mutex::new(embedding_graphs))
        .map_err(|_| {
            let err_msg = ""Failed to initialize the core context. Reason: The `EMBEDDING_GRAPHS` has already been initialized"";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg.into())
        })?;

    let running_mode = RunningMode::Rag;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""running mode: {}"", running_mode);

    // set running mode
    RUNNING_MODE.set(RwLock::new(running_mode)).map_err(|_| {
            let err_msg = ""Failed to initialize the core context. Reason: The `RUNNING_MODE` has already been initialized"";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg.into())
        })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""The core context for RAG scenarios has been initialized"");

    Ok(())
}

/// Get the plugin info
///
/// Note that it is required to call `init_core_context` before calling this function.
pub fn get_plugin_info() -> Result<PluginInfo, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Getting the plugin info"");

    match running_mode()? {
        RunningMode::Embeddings => {
            let embedding_graphs = match EMBEDDING_GRAPHS.get() {
                Some(embedding_graphs) => embedding_graphs,
                None => {
                    let err_msg = ""Fail to get the underlying value of `EMBEDDING_GRAPHS`."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", err_msg);

                    return Err(LlamaCoreError::Operation(err_msg.into()));
                }
            };

            let embedding_graphs = embedding_graphs.lock().map_err(|e| {
                let err_msg = format!(""Fail to acquire the lock of `EMBEDDING_GRAPHS`. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

            let graph = match embedding_graphs.values().next() {
                Some(graph) => graph,
                None => {
                    let err_msg = ""Fail to get the underlying value of `EMBEDDING_GRAPHS`."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", err_msg);

                    return Err(LlamaCoreError::Operation(err_msg.into()));
                }
            };

            get_plugin_info_by_graph(graph)
        }
        _ => {
            let chat_graphs = match CHAT_GRAPHS.get() {
                Some(chat_graphs) => chat_graphs,
                None => {
                    let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", err_msg);

                    return Err(LlamaCoreError::Operation(err_msg.into()));
                }
            };

            let chat_graphs = chat_graphs.lock().map_err(|e| {
                let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

            let graph = match chat_graphs.values().next() {
                Some(graph) => graph,
                None => {
                    let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", err_msg);

                    return Err(LlamaCoreError::Operation(err_msg.into()));
                }
            };

            get_plugin_info_by_graph(graph)
        }
    }
}

fn get_plugin_info_by_graph(graph: &Graph) -> Result<PluginInfo, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Getting the plugin info by the graph named {}"", graph.name());

    // get the plugin metadata
    let output_buffer = get_output_buffer(graph, PLUGIN_VERSION)?;
    let metadata: serde_json::Value = serde_json::from_slice(&output_buffer[..]).map_err(|e| {
        let err_msg = format!(""Fail to deserialize the plugin metadata. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    // get build number of the plugin
    let plugin_build_number = match metadata.get(""llama_build_number"") {
        Some(value) => match value.as_u64() {
            Some(number) => number,
            None => {
                let err_msg = ""Failed to convert the build number of the plugin to u64"";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                return Err(LlamaCoreError::Operation(err_msg.into()));
            }
        },
        None => {
            let err_msg = ""Metadata does not have the field `llama_build_number`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    // get commit id of the plugin
    let plugin_commit = match metadata.get(""llama_commit"") {
        Some(value) => match value.as_str() {
            Some(commit) => commit,
            None => {
                let err_msg = ""Failed to convert the commit id of the plugin to string"";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                return Err(LlamaCoreError::Operation(err_msg.into()));
            }
        },
        None => {
            let err_msg = ""Metadata does not have the field `llama_commit`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Plugin info: b{}(commit {})"", plugin_build_number, plugin_commit);

    Ok(PluginInfo {
        build_number: plugin_build_number,
        commit_id: plugin_commit.to_string(),
    })
}

/// Version info of the `wasi-nn_ggml` plugin, including the build number and the commit id.
#[derive(Debug, Clone)]
pub struct PluginInfo {
    pub build_number: u64,
    pub commit_id: String,
}
impl std::fmt::Display for PluginInfo {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            ""wasinn-ggml plugin: b{}(commit {})"",
            self.build_number, self.commit_id
        )
    }
}

/// Running mode
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]
pub enum RunningMode {
    Chat,
    Embeddings,
    ChatEmbedding,
    Rag,
}
impl std::fmt::Display for RunningMode {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            RunningMode::Chat => write!(f, ""chat""),
            RunningMode::Embeddings => write!(f, ""embeddings""),
            RunningMode::ChatEmbedding => write!(f, ""chat-embeddings""),
            RunningMode::Rag => write!(f, ""rag""),
        }
    }
}

/// Return the current running mode.
pub fn running_mode() -> Result<RunningMode, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get the running mode."");

    let mode = match RUNNING_MODE.get() {
        Some(mode) => match mode.read() {
            Ok(mode) => mode.to_owned(),
            Err(e) => {
                let err_msg = format!(""Fail to get the underlying value of `RUNNING_MODE`. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                return Err(LlamaCoreError::Operation(err_msg));
            }
        },
        None => {
            let err_msg = ""Fail to get the underlying value of `RUNNING_MODE`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""running mode: {}"", &mode);

    Ok(mode.to_owned())
}

/// Initialize the stable diffusion context
pub fn init_stable_diffusion_context_with_full_model(
    model_file: impl AsRef<str>,
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Initializing the stable diffusion context with the full model"");

    // create the stable diffusion context for the text-to-image task
    let sd = StableDiffusion::new(Task::TextToImage, model_file.as_ref());
    SD_TEXT_TO_IMAGE.set(Mutex::new(sd)).map_err(|_| {
        let err_msg = ""Failed to initialize the stable diffusion context. Reason: The `SD_TEXT_TO_IMAGE` has already been initialized"";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        LlamaCoreError::InitContext(err_msg.into())
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""The stable diffusion text-to-image context has been initialized"");

    // create the stable diffusion context for the image-to-image task
    let sd = StableDiffusion::new(Task::ImageToImage, model_file.as_ref());
    SD_IMAGE_TO_IMAGE.set(Mutex::new(sd)).map_err(|_| {
        let err_msg = ""Failed to initialize the stable diffusion context. Reason: The `SD_IMAGE_TO_IMAGE` has already been initialized"";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        LlamaCoreError::InitContext(err_msg.into())
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""The stable diffusion image-to-image context has been initialized"");

    Ok(())
}

pub fn init_stable_diffusion_context_with_standalone_diffusion_model(
    model_file: impl AsRef<str>,
    vae: impl AsRef<str>,
    clip_l: impl AsRef<str>,
    t5xxl: impl AsRef<str>,
    n_threads: i32,
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Initializing the stable diffusion context with the standalone diffusion model"");

    // create the stable diffusion context for the text-to-image task
    let sd = SDBuidler::new_with_standalone_model(Task::TextToImage, model_file.as_ref())
        .map_err(|e| {
            let err_msg = format!(
                ""Failed to initialize the stable diffusion context. Reason: {}"",
                e
            );

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg)
        })?
        .with_vae_path(vae.as_ref())
        .map_err(|e| {
            let err_msg = format!(
                ""Failed to initialize the stable diffusion context. Reason: {}"",
                e
            );

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg)
        })?
        .with_clip_l_path(clip_l.as_ref())
        .map_err(|e| {
            let err_msg = format!(
                ""Failed to initialize the stable diffusion context. Reason: {}"",
                e
            );

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg)
        })?
        .with_t5xxl_path(t5xxl.as_ref())
        .map_err(|e| {
            let err_msg = format!(
                ""Failed to initialize the stable diffusion context. Reason: {}"",
                e
            );

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg)
        })?
        .with_n_threads(n_threads)
        .build();

    SD_TEXT_TO_IMAGE.set(Mutex::new(sd)).map_err(|_| {
            let err_msg = ""Failed to initialize the stable diffusion context. Reason: The `SD_TEXT_TO_IMAGE` has already been initialized"";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg.into())
        })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""The stable diffusion text-to-image context has been initialized"");

    // create the stable diffusion context for the image-to-image task
    let sd = SDBuidler::new_with_standalone_model(Task::TextToImage, model_file.as_ref())
        .map_err(|e| {
            let err_msg = format!(
                ""Failed to initialize the stable diffusion context. Reason: {}"",
                e
            );

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg)
        })?
        .with_vae_path(vae.as_ref())
        .map_err(|e| {
            let err_msg = format!(
                ""Failed to initialize the stable diffusion context. Reason: {}"",
                e
            );

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg)
        })?
        .with_clip_l_path(clip_l.as_ref())
        .map_err(|e| {
            let err_msg = format!(
                ""Failed to initialize the stable diffusion context. Reason: {}"",
                e
            );

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg)
        })?
        .with_t5xxl_path(t5xxl.as_ref())
        .map_err(|e| {
            let err_msg = format!(
                ""Failed to initialize the stable diffusion context. Reason: {}"",
                e
            );

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg)
        })?
        .with_n_threads(n_threads)
        .build();

    SD_IMAGE_TO_IMAGE.set(Mutex::new(sd)).map_err(|_| {
        let err_msg = ""Failed to initialize the stable diffusion context. Reason: The `SD_IMAGE_TO_IMAGE` has already been initialized"";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        LlamaCoreError::InitContext(err_msg.into())
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""The stable diffusion image-to-image context has been initialized"");

    Ok(())
}

/// Initialize the whisper context
pub fn init_whisper_context(
    metadata: &Metadata,
    model_file: impl AsRef<Path>,
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Initializing the audio context"");

    // create and initialize the audio context
    let graph = GraphBuilder::new(EngineType::Whisper)?
        .with_config(metadata)?
        .use_cpu()
        .build_from_files([model_file.as_ref()])?;

    AUDIO_GRAPH.set(Mutex::new(graph)).map_err(|_| {
            let err_msg = ""Failed to initialize the audio context. Reason: The `AUDIO_GRAPH` has already been initialized"";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg.into())
        })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""The audio context has been initialized"");

    Ok(())
}

/// Initialize the piper context
///
/// # Arguments
///
/// * `voice_model` - Path to the voice model file.
///
/// * `voice_config` - Path to the voice config file.
///
/// * `espeak_ng_data` - Path to the espeak-ng data directory.
///
pub fn init_piper_context(
    voice_model: impl AsRef<Path>,
    voice_config: impl AsRef<Path>,
    espeak_ng_data: impl AsRef<Path>,
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Initializing the piper context"");

    let config = serde_json::json!({
        ""model"": voice_model.as_ref().to_owned(),
        ""config"": voice_config.as_ref().to_owned(),
        ""espeak_data"": espeak_ng_data.as_ref().to_owned(),
    });

    // create and initialize the audio context
    let graph = GraphBuilder::new(EngineType::Piper)?
        .use_cpu()
        .build_from_buffer([config.to_string()])?;

    PIPER_GRAPH.set(Mutex::new(graph)).map_err(|_| {
            let err_msg = ""Failed to initialize the piper context. Reason: The `PIPER_GRAPH` has already been initialized"";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg.into())
        })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""The piper context has been initialized"");

    Ok(())
}
"
crates/llama-core/src/models.rs,"//! Define APIs for querying models.

use crate::{error::LlamaCoreError, CHAT_GRAPHS, EMBEDDING_GRAPHS};
use endpoints::models::{ListModelsResponse, Model};

/// Lists models available
pub async fn models() -> Result<ListModelsResponse, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""List models"");

    let mut models = vec![];

    {
        if let Some(chat_graphs) = CHAT_GRAPHS.get() {
            let chat_graphs = chat_graphs.lock().map_err(|e| {
                let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

            for (name, graph) in chat_graphs.iter() {
                models.push(Model {
                    id: name.clone(),
                    created: graph.created.as_secs(),
                    object: String::from(""model""),
                    owned_by: String::from(""Not specified""),
                });
            }
        }
    }

    {
        if let Some(embedding_graphs) = EMBEDDING_GRAPHS.get() {
            let embedding_graphs = embedding_graphs.lock().map_err(|e| {
                LlamaCoreError::Operation(format!(
                    ""Fail to acquire the lock of `EMBEDDING_GRAPHS`. {}"",
                    e
                ))
            })?;

            if !embedding_graphs.is_empty() {
                for (name, graph) in embedding_graphs.iter() {
                    models.push(Model {
                        id: name.clone(),
                        created: graph.created.as_secs(),
                        object: String::from(""model""),
                        owned_by: String::from(""Not specified""),
                    });
                }
            }
        }
    }

    Ok(ListModelsResponse {
        object: String::from(""list""),
        data: models,
    })
}
"
crates/llama-core/src/rag.rs,"//! Define APIs for RAG operations.

use crate::{embeddings::embeddings, error::LlamaCoreError, running_mode, RunningMode};
use endpoints::{
    embeddings::{EmbeddingObject, EmbeddingsResponse, InputText},
    rag::{RagEmbeddingRequest, RagScoredPoint, RetrieveObject},
};
use qdrant::*;
use text_splitter::{MarkdownSplitter, TextSplitter};
use tiktoken_rs::cl100k_base;

/// Convert document chunks to embeddings.
///
/// # Arguments
///
/// * `embedding_request` - A reference to an `EmbeddingRequest` object.
///
/// * `qdrant_url` - URL of the Qdrant server.
///
/// * `qdrant_collection_name` - Name of the Qdrant collection to be created.
///
/// # Returns
///
/// Name of the Qdrant collection if successful.
pub async fn rag_doc_chunks_to_embeddings(
    rag_embedding_request: &RagEmbeddingRequest,
) -> Result<EmbeddingsResponse, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Convert document chunks to embeddings."");

    let running_mode = running_mode()?;
    if running_mode != RunningMode::Rag {
        let err_msg = format!(
            ""Creating knowledge base is not supported in the {} mode."",
            running_mode
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    let embedding_request = &rag_embedding_request.embedding_request;
    let qdrant_url = rag_embedding_request.qdrant_url.as_str();
    let qdrant_collection_name = rag_embedding_request.qdrant_collection_name.as_str();

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Compute embeddings for document chunks."");

    #[cfg(feature = ""logging"")]
    if let Ok(request_str) = serde_json::to_string(&embedding_request) {
        info!(target: ""stdout"", ""Embedding request: {}"", request_str);
    }

    // compute embeddings for the document
    let response = embeddings(embedding_request).await?;
    let embeddings = response.data.as_slice();
    let dim = embeddings[0].embedding.len();

    // create a Qdrant client
    let qdrant_client = qdrant::Qdrant::new_with_url(qdrant_url.to_string());

    // create a collection
    qdrant_create_collection(&qdrant_client, qdrant_collection_name, dim).await?;

    let chunks = match &embedding_request.input {
        InputText::String(text) => vec![text.clone()],
        InputText::ArrayOfStrings(texts) => texts.clone(),
        InputText::ArrayOfTokens(tokens) => tokens.iter().map(|t| t.to_string()).collect(),
        InputText::ArrayOfTokenArrays(token_arrays) => token_arrays
            .iter()
            .map(|tokens| tokens.iter().map(|t| t.to_string()).collect())
            .collect(),
    };

    // create and upsert points
    qdrant_persist_embeddings(
        &qdrant_client,
        qdrant_collection_name,
        embeddings,
        chunks.as_slice(),
    )
    .await?;

    Ok(response)
}

/// Convert a query to embeddings.
///
/// # Arguments
///
/// * `embedding_request` - A reference to an `EmbeddingRequest` object.
pub async fn rag_query_to_embeddings(
    rag_embedding_request: &RagEmbeddingRequest,
) -> Result<EmbeddingsResponse, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Compute embeddings for the user query."");

    let running_mode = running_mode()?;
    if running_mode != RunningMode::Rag {
        let err_msg = format!(""The RAG query is not supported in the {running_mode} mode."",);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    embeddings(&rag_embedding_request.embedding_request).await
}

/// Retrieve similar points from the Qdrant server using the query embedding
///
/// # Arguments
///
/// * `query_embedding` - A reference to a query embedding.
///
/// * `qdrant_url` - URL of the Qdrant server.
///
/// * `qdrant_collection_name` - Name of the Qdrant collection to be created.
///
/// * `limit` - Max number of retrieved result.
pub async fn rag_retrieve_context(
    query_embedding: &[f32],
    qdrant_url: impl AsRef<str>,
    qdrant_collection_name: impl AsRef<str>,
    limit: usize,
    score_threshold: Option<f32>,
) -> Result<RetrieveObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    {
        info!(target: ""stdout"", ""Retrieve context."");

        info!(target: ""stdout"", ""qdrant_url: {}, qdrant_collection_name: {}, limit: {}, score_threshold: {}"", qdrant_url.as_ref(), qdrant_collection_name.as_ref(), limit, score_threshold.unwrap_or_default());
    }

    let running_mode = running_mode()?;
    if running_mode != RunningMode::Rag {
        let err_msg = format!(
            ""The context retrieval is not supported in the {} mode."",
            running_mode
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    // create a Qdrant client
    let qdrant_client = qdrant::Qdrant::new_with_url(qdrant_url.as_ref().to_string());

    // search for similar points
    let scored_points = match qdrant_search_similar_points(
        &qdrant_client,
        qdrant_collection_name.as_ref(),
        query_embedding,
        limit,
        score_threshold,
    )
    .await
    {
        Ok(points) => points,
        Err(e) => {
            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", e.to_string());

            return Err(e);
        }
    };

    let ro = match scored_points.is_empty() {
        true => RetrieveObject {
            points: None,
            limit,
            score_threshold: score_threshold.unwrap_or(0.0),
        },
        false => {
            let mut points: Vec<RagScoredPoint> = vec![];
            for point in scored_points.iter() {
                if let Some(payload) = &point.payload {
                    if let Some(source) = payload.get(""source"") {
                        points.push(RagScoredPoint {
                            source: source.to_string(),
                            score: point.score,
                        })
                    }
                }
            }

            RetrieveObject {
                points: Some(points),
                limit,
                score_threshold: score_threshold.unwrap_or(0.0),
            }
        }
    };

    Ok(ro)
}

async fn qdrant_create_collection(
    qdrant_client: &qdrant::Qdrant,
    collection_name: impl AsRef<str>,
    dim: usize,
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Create a Qdrant collection named {} of {} dimensions."", collection_name.as_ref(), dim);

    if let Err(e) = qdrant_client
        .create_collection(collection_name.as_ref(), dim as u32)
        .await
    {
        let err_msg = e.to_string();

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    Ok(())
}

async fn qdrant_persist_embeddings(
    qdrant_client: &qdrant::Qdrant,
    collection_name: impl AsRef<str>,
    embeddings: &[EmbeddingObject],
    chunks: &[String],
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Persist embeddings to the Qdrant instance."");

    let mut points = Vec::<Point>::new();
    for embedding in embeddings {
        // convert the embedding to a vector
        let vector: Vec<_> = embedding.embedding.iter().map(|x| *x as f32).collect();

        // create a payload
        let payload = serde_json::json!({""source"": chunks[embedding.index as usize]})
            .as_object()
            .map(|m| m.to_owned());

        // create a point
        let p = Point {
            id: PointId::Num(embedding.index),
            vector,
            payload,
        };

        points.push(p);
    }

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Number of points to be upserted: {}"", points.len());

    if let Err(e) = qdrant_client
        .upsert_points(collection_name.as_ref(), points)
        .await
    {
        let err_msg = format!(""Failed to upsert points. Reason: {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    Ok(())
}

async fn qdrant_search_similar_points(
    qdrant_client: &qdrant::Qdrant,
    collection_name: impl AsRef<str>,
    query_vector: &[f32],
    limit: usize,
    score_threshold: Option<f32>,
) -> Result<Vec<ScoredPoint>, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Search similar points from the qdrant instance."");

    match qdrant_client
        .search_points(
            collection_name.as_ref(),
            query_vector.to_vec(),
            limit as u64,
            score_threshold,
        )
        .await
    {
        Ok(search_result) => {
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""Number of similar points found: {}"", search_result.len());

            Ok(search_result)
        }
        Err(e) => {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""Fail to search similar points from the qdrant instance. Reason: {}"", &err_msg);

            Err(LlamaCoreError::Operation(err_msg))
        }
    }
}

/// Generate a list of chunks from a given text. Each chunk will be up to the `chunk_capacity`.
///
/// # Arguments
///
/// * `text` - A reference to a text.
///
/// * `ty` - Type of the text, `txt` for text content or `md` for markdown content.
///
/// * `chunk_capacity` - The max tokens each chunk contains.
///
/// # Returns
///
/// A vector of strings.
///
/// # Errors
///
/// Returns an error if the operation fails.
pub fn chunk_text(
    text: impl AsRef<str>,
    ty: impl AsRef<str>,
    chunk_capacity: usize,
) -> Result<Vec<String>, LlamaCoreError> {
    if ty.as_ref().to_lowercase().as_str() != ""txt"" && ty.as_ref().to_lowercase().as_str() != ""md"" {
        let err_msg = ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported."";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        return Err(LlamaCoreError::Operation(err_msg.into()));
    }

    match ty.as_ref().to_lowercase().as_str() {
        ""txt"" => {
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""Chunk the plain text contents."");

            let tokenizer = cl100k_base().map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

            // create a text splitter
            let splitter = TextSplitter::new(tokenizer).with_trim_chunks(true);

            let chunks = splitter
                .chunks(text.as_ref(), chunk_capacity)
                .map(|s| s.to_string())
                .collect::<Vec<_>>();

            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""Number of chunks: {}"", chunks.len());

            Ok(chunks)
        }
        ""md"" => {
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""Chunk the markdown contents."");

            let tokenizer = cl100k_base().map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

            // create a markdown splitter
            let splitter = MarkdownSplitter::new(tokenizer).with_trim_chunks(true);

            let chunks = splitter
                .chunks(text.as_ref(), chunk_capacity)
                .map(|s| s.to_string())
                .collect::<Vec<_>>();

            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""Number of chunks: {}"", chunks.len());

            Ok(chunks)
        }
        _ => {
            let err_msg =
                ""Failed to upload the target file. Only text and markdown files are supported."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            Err(LlamaCoreError::Operation(err_msg.into()))
        }
    }
}
"
crates/llama-core/src/search.rs,"use crate::{error::LlamaCoreError, CHAT_GRAPHS};
use reqwest::{Client, Url};
use serde::{Deserialize, Serialize};
use serde_json::Value;

/// Possible input/output Content Types. Currently only supports JSON.
#[derive(Debug, Eq, PartialEq)]
pub enum ContentType {
    JSON,
}

impl std::fmt::Display for ContentType {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            ""{}"",
            match &self {
                ContentType::JSON => ""application/json"",
            }
        )
    }
}

/// The base Search Configuration holding all relevant information to access a search api and retrieve results.
#[derive(Debug)]
pub struct SearchConfig {
    /// The search engine we're currently focusing on. Currently only one supported, to ensure stability.
    #[allow(dead_code)]
    pub search_engine: String,
    /// The total number of results.
    pub max_search_results: u8,
    /// The size limit of every search result.
    pub size_limit_per_result: u16,
    /// The endpoint for the search API.
    pub endpoint: String,
    /// The content type of the input.
    pub content_type: ContentType,
    /// The (expected) content type of the output.
    pub output_content_type: ContentType,
    /// Method expected by the api endpoint.
    pub method: String,
    /// Additional headers for any other purpose.
    pub additional_headers: Option<std::collections::HashMap<String, String>>,
    /// Callback function to parse the output of the api-service. Implementation left to the user.
    pub parser: fn(&serde_json::Value) -> Result<SearchOutput, Box<dyn std::error::Error>>,
    /// Prompts for use with summarization functionality. If set to `None`, use hard-coded prompts.
    pub summarization_prompts: Option<(String, String)>,
    /// Context size for summary generation. If `None`, will use the 4 char ~ 1 token metric to generate summary.
    pub summarize_ctx_size: Option<usize>,
}

/// output format for individual results in the final output.
#[derive(Serialize, Deserialize)]
pub struct SearchResult {
    pub url: String,
    pub site_name: String,
    pub text_content: String,
}

/// Final output format for consumption by the LLM.
#[derive(Serialize, Deserialize)]
pub struct SearchOutput {
    pub results: Vec<SearchResult>,
}

impl SearchConfig {
    /// Wrapper for the parser() function.
    pub fn parse_into_results(
        &self,
        raw_results: &serde_json::Value,
    ) -> Result<SearchOutput, Box<dyn std::error::Error>> {
        (self.parser)(raw_results)
    }
    pub fn new(
        search_engine: String,
        max_search_results: u8,
        size_limit_per_result: u16,
        endpoint: String,
        content_type: ContentType,
        output_content_type: ContentType,
        method: String,
        additional_headers: Option<std::collections::HashMap<String, String>>,
        parser: fn(&serde_json::Value) -> Result<SearchOutput, Box<dyn std::error::Error>>,
        summarization_prompts: Option<(String, String)>,
        summarize_ctx_size: Option<usize>,
    ) -> SearchConfig {
        SearchConfig {
            search_engine,
            max_search_results,
            size_limit_per_result,
            endpoint,
            content_type,
            output_content_type,
            method,
            additional_headers,
            parser,
            summarization_prompts,
            summarize_ctx_size,
        }
    }
    /// Perform a web search with a `Serialize`-able input. The `search_input` is used as is to query the search endpoint.
    pub async fn perform_search<T: Serialize>(
        &self,
        search_input: &T,
    ) -> Result<SearchOutput, LlamaCoreError> {
        let client = Client::new();
        let url = match Url::parse(&self.endpoint) {
            Ok(url) => url,
            Err(_) => {
                let msg = ""Malformed endpoint url"";
                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""perform_search: {}"", msg);
                return Err(LlamaCoreError::Search(format!(
                    ""When parsing endpoint url: {}"",
                    msg
                )));
            }
        };

        let method_as_string = match reqwest::Method::from_bytes(self.method.as_bytes()) {
            Ok(method) => method,
            _ => {
                let msg = ""Non Standard or unknown method"";
                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""perform_search: {}"", msg);
                return Err(LlamaCoreError::Search(format!(
                    ""When converting method from bytes: {}"",
                    msg
                )));
            }
        };

        let mut req = client.request(method_as_string.clone(), url);

        // check headers.
        req = req.headers(
            match (&self
                .additional_headers
                .clone()
                .unwrap_or_else(|| std::collections::HashMap::new()))
                .try_into()
            {
                Ok(headers) => headers,
                Err(_) => {
                    let msg = ""Failed to convert headers from HashMaps to HeaderMaps"";
                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""perform_search: {}"", msg);
                    return Err(LlamaCoreError::Search(format!(
                        ""On converting headers: {}"",
                        msg
                    )));
                }
            },
        );

        // For POST requests, search_input goes into the request body. For GET requests, in the
        // params.
        req = match method_as_string {
            reqwest::Method::POST => match self.content_type {
                ContentType::JSON => req.json(search_input),
            },
            reqwest::Method::GET => req.query(search_input),
            _ => {
                let msg = format!(
                    ""Unsupported request method: {}"",
                    method_as_string.to_owned()
                );
                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""perform_search: {}"", msg);
                return Err(LlamaCoreError::Search(msg));
            }
        };

        let res = match req.send().await {
            Ok(r) => r,
            Err(e) => {
                let msg = e.to_string();
                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""perform_search: {}"", msg);
                return Err(LlamaCoreError::Search(format!(
                    ""When recieving response: {}"",
                    msg
                )));
            }
        };

        match res.content_length() {
            Some(length) => {
                if length == 0 {
                    let msg = ""Empty response from server"";
                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""perform_search: {}"", msg);
                    return Err(LlamaCoreError::Search(format!(
                        ""Unexpected content length: {}"",
                        msg
                    )));
                }
            }
            None => {
                let msg = ""Content length returned None"";
                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""perform_search: {}"", msg);
                return Err(LlamaCoreError::Search(format!(
                    ""Content length field not found: {}"",
                    msg
                )));
            }
        }

        // start parsing the output.
        //
        // only checking for JSON as the output content type since it's the most common and widely
        // supported.
        let raw_results: Value;
        match self.output_content_type {
            ContentType::JSON => {
                let body_text = match res.text().await {
                    Ok(body) => body,
                    Err(e) => {
                        let msg = e.to_string();
                        #[cfg(feature = ""logging"")]
                        error!(target: ""stdout"", ""perform_search: {}"", msg);
                        return Err(LlamaCoreError::Search(format!(
                            ""When accessing response body: {}"",
                            msg
                        )));
                    }
                };
                println!(""{}"", body_text);
                raw_results = match serde_json::from_str(body_text.as_str()) {
                    Ok(value) => value,
                    Err(e) => {
                        let msg = e.to_string();
                        #[cfg(feature = ""logging"")]
                        error!(target: ""stdout"", ""perform_search: {}"", msg);
                        return Err(LlamaCoreError::Search(format!(
                            ""When converting to a JSON object: {}"",
                            msg
                        )));
                    }
                };
            }
        };

        // start cleaning the output.

        // produce SearchOutput instance with the raw results obtained from the endpoint.
        let mut search_output: SearchOutput = match self.parse_into_results(&raw_results) {
            Ok(search_output) => search_output,
            Err(e) => {
                let msg = e.to_string();
                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""perform_search: {}"", msg);
                return Err(LlamaCoreError::Search(format!(
                    ""When calling parse_into_results: {}"",
                    msg
                )));
            }
        };

        // apply maximum search result limit.
        search_output
            .results
            .truncate(self.max_search_results as usize);

        // apply per result character limit.
        //
        // since the clipping only happens when split_at_checked() returns Some, the results will
        // remain unchanged should split_at_checked() return None.
        for result in search_output.results.iter_mut() {
            if let Some(clipped_content) = result
                .text_content
                .split_at_checked(self.size_limit_per_result as usize)
            {
                result.text_content = clipped_content.0.to_string();
            }
        }

        // Search Output cleaned and finalized.
        Ok(search_output)
    }
    /// Perform a search and summarize the corresponding search results
    pub async fn summarize_search<T: Serialize>(
        &self,
        search_input: &T,
    ) -> Result<String, LlamaCoreError> {
        let search_output = self.perform_search(&search_input).await?;

        let summarization_prompts = self.summarization_prompts.clone().unwrap_or((
            ""The following are search results I found on the internet:\n\n"".to_string(),
            ""\n\nTo sum up them up: "".to_string(),
        ));

        // the fallback context size limit for the search summary to be generated.
        let summarize_ctx_size = self
            .summarize_ctx_size
            .unwrap_or((self.size_limit_per_result * self.max_search_results as u16) as usize);

        summarize(
            search_output,
            summarize_ctx_size,
            summarization_prompts.0,
            summarization_prompts.1,
        )
    }
}

/// Summarize the search output provided
fn summarize(
    search_output: SearchOutput,
    summarize_ctx_size: usize,
    initial_prompt: String,
    final_prompt: String,
) -> Result<String, LlamaCoreError> {
    let mut search_output_string: String = String::new();

    // Add the text content of every result together.
    search_output
        .results
        .iter()
        .for_each(|result| search_output_string.push_str(result.text_content.as_str()));

    // Error on embedding running mode.
    if crate::running_mode()? == crate::RunningMode::Embeddings {
        let err_msg = ""Summarization is not supported in the EMBEDDINGS running mode."";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        return Err(LlamaCoreError::Search(err_msg.into()));
    }

    // Get graphs and pick the first graph.
    let chat_graphs = match CHAT_GRAPHS.get() {
        Some(chat_graphs) => chat_graphs,
        None => {
            let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Search(err_msg.into()));
        }
    };

    let mut chat_graphs = chat_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Search(err_msg)
    })?;

    // Prepare input prompt.
    let input = initial_prompt + search_output_string.as_str() + final_prompt.as_str();
    let tensor_data = input.as_bytes().to_vec();

    // Use first available chat graph
    let graph: &mut crate::Graph = match chat_graphs.values_mut().next() {
        Some(graph) => graph,
        None => {
            let err_msg = ""No available chat graph."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Search(err_msg.into()));
        }
    };

    graph
        .set_input(0, wasmedge_wasi_nn::TensorType::U8, &[1], &tensor_data)
        .expect(""Failed to set prompt as the input tensor"");

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Generating a summary for search results..."");
    // Execute the inference.
    graph.compute().expect(""Failed to complete inference"");

    // Retrieve the output.
    let mut output_buffer = vec![0u8; summarize_ctx_size];
    let mut output_size = graph
        .get_output(0, &mut output_buffer)
        .expect(""Failed to get output tensor"");
    output_size = std::cmp::min(summarize_ctx_size, output_size);

    // Compute lossy UTF-8 output (text only).
    let output = String::from_utf8_lossy(&output_buffer[..output_size]).to_string();

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Summary generated."");

    Ok(output)
}
"
crates/llama-core/src/utils.rs,"//! Define utility functions.

use crate::{
    error::{BackendError, LlamaCoreError},
    Graph, CHAT_GRAPHS, EMBEDDING_GRAPHS, MAX_BUFFER_SIZE,
};
use chat_prompts::PromptTemplateType;
use serde_json::Value;

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

/// Return the names of the chat models.
pub fn chat_model_names() -> Result<Vec<String>, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get the names of the chat models."");

    let chat_graphs = match CHAT_GRAPHS.get() {
        Some(chat_graphs) => chat_graphs,
        None => {
            let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let chat_graphs = chat_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    let mut model_names = Vec::new();
    for model_name in chat_graphs.keys() {
        model_names.push(model_name.clone());
    }

    Ok(model_names)
}

/// Return the names of the embedding models.
pub fn embedding_model_names() -> Result<Vec<String>, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get the names of the embedding models."");

    let embedding_graphs = match EMBEDDING_GRAPHS.get() {
        Some(embedding_graphs) => embedding_graphs,
        None => {
            return Err(LlamaCoreError::Operation(String::from(
                ""Fail to get the underlying value of `EMBEDDING_GRAPHS`."",
            )));
        }
    };

    let embedding_graphs = match embedding_graphs.lock() {
        Ok(embedding_graphs) => embedding_graphs,
        Err(e) => {
            let err_msg = format!(""Fail to acquire the lock of `EMBEDDING_GRAPHS`. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg));
        }
    };

    let mut model_names = Vec::new();
    for model_name in embedding_graphs.keys() {
        model_names.push(model_name.clone());
    }

    Ok(model_names)
}

/// Get the chat prompt template type from the given model name.
pub fn chat_prompt_template(name: Option<&str>) -> Result<PromptTemplateType, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    match name {
        Some(name) => {
            info!(target: ""stdout"", ""Get the chat prompt template type from the chat model named {}."", name)
        }
        None => {
            info!(target: ""stdout"", ""Get the chat prompt template type from the default chat model."")
        }
    }

    let chat_graphs = match CHAT_GRAPHS.get() {
        Some(chat_graphs) => chat_graphs,
        None => {
            let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let chat_graphs = chat_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    match name {
        Some(model_name) => match chat_graphs.contains_key(model_name) {
            true => {
                let graph = chat_graphs.get(model_name).unwrap();
                let prompt_template = graph.prompt_template();

                #[cfg(feature = ""logging"")]
                info!(target: ""stdout"", ""prompt_template: {}"", &prompt_template);

                Ok(prompt_template)
            }
            false => match chat_graphs.iter().next() {
                Some((_, graph)) => {
                    let prompt_template = graph.prompt_template();

                    #[cfg(feature = ""logging"")]
                    info!(target: ""stdout"", ""prompt_template: {}"", &prompt_template);

                    Ok(prompt_template)
                }
                None => {
                    let err_msg = ""There is no model available in the chat graphs."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    Err(LlamaCoreError::Operation(err_msg.into()))
                }
            },
        },
        None => match chat_graphs.iter().next() {
            Some((_, graph)) => {
                let prompt_template = graph.prompt_template();

                #[cfg(feature = ""logging"")]
                info!(target: ""stdout"", ""prompt_template: {}"", &prompt_template);

                Ok(prompt_template)
            }
            None => {
                let err_msg = ""There is no model available in the chat graphs."";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                Err(LlamaCoreError::Operation(err_msg.into()))
            }
        },
    }
}

/// Get output buffer generated by model.
pub(crate) fn get_output_buffer(graph: &Graph, index: usize) -> Result<Vec<u8>, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get the output buffer generated by the model named {}"", graph.name());

    let mut output_buffer: Vec<u8> = Vec::with_capacity(MAX_BUFFER_SIZE);

    let output_size: usize = graph.get_output(index, &mut output_buffer).map_err(|e| {
        let err_msg = format!(""Fail to get the generated output tensor. {msg}"", msg = e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Backend(BackendError::GetOutput(err_msg))
    })?;

    unsafe {
        output_buffer.set_len(output_size);
    }

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Output buffer size: {}"", output_size);

    Ok(output_buffer)
}

/// Get output buffer generated by model in the stream mode.
pub(crate) fn get_output_buffer_single(
    graph: &Graph,
    index: usize,
) -> Result<Vec<u8>, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get output buffer generated by the model named {} in the stream mode."", graph.name());

    let mut output_buffer: Vec<u8> = Vec::with_capacity(MAX_BUFFER_SIZE);

    let output_size: usize = graph
        .get_output_single(index, &mut output_buffer)
        .map_err(|e| {
            let err_msg = format!(""Fail to get plugin metadata. {msg}"", msg = e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Backend(BackendError::GetOutput(err_msg))
        })?;

    unsafe {
        output_buffer.set_len(output_size);
    }

    Ok(output_buffer)
}

pub(crate) fn set_tensor_data_u8(
    graph: &mut Graph,
    idx: usize,
    tensor_data: &[u8],
) -> Result<(), LlamaCoreError> {
    if graph
        .set_input(idx, wasmedge_wasi_nn::TensorType::U8, &[1], tensor_data)
        .is_err()
    {
        let err_msg = format!(""Fail to set input tensor at index {}"", idx);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    };

    Ok(())
}

/// Get the token information from the graph.
pub(crate) fn get_token_info_by_graph(graph: &Graph) -> Result<TokenInfo, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get token info from the model named {}."", graph.name());

    let output_buffer = get_output_buffer(graph, 1)?;
    let token_info: Value = match serde_json::from_slice(&output_buffer[..]) {
        Ok(token_info) => token_info,
        Err(e) => {
            let err_msg = format!(""Fail to deserialize token info: {msg}"", msg = e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg));
        }
    };

    let prompt_tokens = match token_info[""input_tokens""].as_u64() {
        Some(prompt_tokens) => prompt_tokens,
        None => {
            let err_msg = ""Fail to convert `input_tokens` to u64."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };
    let completion_tokens = match token_info[""output_tokens""].as_u64() {
        Some(completion_tokens) => completion_tokens,
        None => {
            let err_msg = ""Fail to convert `output_tokens` to u64."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""prompt tokens: {}, completion tokens: {}"", prompt_tokens, completion_tokens);

    Ok(TokenInfo {
        prompt_tokens,
        completion_tokens,
    })
}

/// Get the token information from the graph by the model name.
pub(crate) fn get_token_info_by_graph_name(
    name: Option<&String>,
) -> Result<TokenInfo, LlamaCoreError> {
    let chat_graphs = match CHAT_GRAPHS.get() {
        Some(chat_graphs) => chat_graphs,
        None => {
            let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let chat_graphs = chat_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    match name {
        Some(model_name) => match chat_graphs.contains_key(model_name) {
            true => {
                let graph = chat_graphs.get(model_name).unwrap();
                get_token_info_by_graph(graph)
            }
            false => match chat_graphs.iter().next() {
                Some((_, graph)) => get_token_info_by_graph(graph),
                None => {
                    let err_msg = ""There is no model available in the chat graphs."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    Err(LlamaCoreError::Operation(err_msg.into()))
                }
            },
        },
        None => match chat_graphs.iter().next() {
            Some((_, graph)) => get_token_info_by_graph(graph),
            None => {
                let err_msg = ""There is no model available in the chat graphs."";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                Err(LlamaCoreError::Operation(err_msg.into()))
            }
        },
    }
}

#[derive(Debug)]
pub(crate) struct TokenInfo {
    pub(crate) prompt_tokens: u64,
    pub(crate) completion_tokens: u64,
}

pub(crate) trait TensorType {
    fn tensor_type() -> wasmedge_wasi_nn::TensorType;
    fn shape(shape: impl AsRef<[usize]>) -> Vec<usize> {
        shape.as_ref().to_vec()
    }
}

impl TensorType for u8 {
    fn tensor_type() -> wasmedge_wasi_nn::TensorType {
        wasmedge_wasi_nn::TensorType::U8
    }
}

impl TensorType for f32 {
    fn tensor_type() -> wasmedge_wasi_nn::TensorType {
        wasmedge_wasi_nn::TensorType::F32
    }
}

pub(crate) fn set_tensor_data<T: TensorType>(
    graph: &mut Graph,
    idx: usize,
    tensor_data: &[T],
    shape: impl AsRef<[usize]>,
) -> Result<(), LlamaCoreError> {
    if graph
        .set_input(idx, T::tensor_type(), &T::shape(shape), tensor_data)
        .is_err()
    {
        let err_msg = format!(""Fail to set input tensor at index {}"", idx);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    };

    Ok(())
}
"
docker/Dockerfile,"# syntax=docker/dockerfile:1

FROM ubuntu:22.04
ARG CHAT_MODEL_FILE
ARG EMBEDDING_MODEL_FILE
ARG PROMPT_TEMPLATE
ENV CHAT_MODEL_FILE=${CHAT_MODEL_FILE}
ENV EMBEDDING_MODEL_FILE=${EMBEDDING_MODEL_FILE}
ENV PROMPT_TEMPLATE=${PROMPT_TEMPLATE}
RUN apt-get update && apt-get install -y curl
RUN mkdir /models
COPY $CHAT_MODEL_FILE /models/$CHAT_MODEL_FILE
COPY $EMBEDDING_MODEL_FILE /models/$EMBEDDING_MODEL_FILE
RUN curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.13.5
RUN curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
RUN curl -LO https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz; tar xzf chatbot-ui.tar.gz; rm chatbot-ui.tar.gz
COPY run.sh .
RUN chmod +x run.sh

ENTRYPOINT ./run.sh $CHAT_MODEL_FILE $EMBEDDING_MODEL_FILE $PROMPT_TEMPLATE ""$@""
"
docker/Dockerfile.cuda11,"# syntax=docker/dockerfile:1

FROM nvidia/cuda:11.4.3-base-ubuntu20.04
ARG CHAT_MODEL_FILE
ARG EMBEDDING_MODEL_FILE
ARG PROMPT_TEMPLATE
ENV CHAT_MODEL_FILE=${CHAT_MODEL_FILE}
ENV EMBEDDING_MODEL_FILE=${EMBEDDING_MODEL_FILE}
ENV PROMPT_TEMPLATE=${PROMPT_TEMPLATE}
RUN apt-get update && apt-get install -y curl libcublas-11-4
RUN mkdir /models
COPY $CHAT_MODEL_FILE /models/$CHAT_MODEL_FILE
COPY $EMBEDDING_MODEL_FILE /models/$EMBEDDING_MODEL_FILE
RUN curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- --version=0.13.5 --ggmlcuda=11
RUN curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
RUN curl -LO https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz; tar xzf chatbot-ui.tar.gz; rm chatbot-ui.tar.gz
COPY run.sh .
RUN chmod +x run.sh

ENTRYPOINT ./run.sh $CHAT_MODEL_FILE $EMBEDDING_MODEL_FILE $PROMPT_TEMPLATE ""$@""
"
docker/Dockerfile.cuda12,"# syntax=docker/dockerfile:1

FROM nvidia/cuda:12.5.0-base-ubuntu22.04
ARG CHAT_MODEL_FILE
ARG EMBEDDING_MODEL_FILE
ARG PROMPT_TEMPLATE
ENV CHAT_MODEL_FILE=${CHAT_MODEL_FILE}
ENV EMBEDDING_MODEL_FILE=${EMBEDDING_MODEL_FILE}
ENV PROMPT_TEMPLATE=${PROMPT_TEMPLATE}
RUN apt-get update && apt-get install -y curl libcublas-12-5
RUN mkdir /models
COPY $CHAT_MODEL_FILE /models/$CHAT_MODEL_FILE
COPY $EMBEDDING_MODEL_FILE /models/$EMBEDDING_MODEL_FILE
RUN curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- --version=0.13.5 --ggmlcuda=12
RUN curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
RUN curl -LO https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz; tar xzf chatbot-ui.tar.gz; rm chatbot-ui.tar.gz
COPY run.sh .
RUN chmod +x run.sh

ENTRYPOINT ./run.sh $CHAT_MODEL_FILE $EMBEDDING_MODEL_FILE $PROMPT_TEMPLATE ""$@""
"
docker/README.md,"# Getting started with Docker

You can run all the commands in this document without any change on any machine with the latest Docker and at least 8GB of RAM available to the container.
By default, the container uses the CPU to peform computations, which could be slow for large LLMs. For GPUs,

* Mac: Everything here works on [Docker Desktop for Mac](https://docs.docker.com/desktop/install/mac-install/). However, the Apple GPU cores will not be available inside Docker containers until [WebGPU is supported by Docker](webgpu.md) later in 2024.
* Windows and Linux with Nvidia GPU: You will need to install [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation) for Docker. In the instructions below, replace the `latest` tag with `cuda12` or `cuda11`, and add the `--device nvidia.com/gpu=all` flag, to use take advantage of the GPU. If you need to build the images yourself, replace `Dockerfile` with `Dockerfile.cuda12` or `Dockerfile.cuda11`.

## Quick start

Run the following Docker command to start an OpenAI-compatible LLM API server on your own device.

```
docker run --rm -p 8080:8080 --name api-server secondstate/qwen-2-0.5b-allminilm-2:latest
```

Go to http://localhost:8080 from your browser to chat with the model!

This container starts two models Qwen-2-0.5B is a very small but highly capable LLM chat model, and all-miniLM is 
a widely used embedding model. 
That allows the API server to support both `/chat` and `/embeddings` endpoints, which are crucial for most
LLM agent apps and frameworks based on OpenAI.

Alternatively, you can use the command below to start a server on an Nvidia CUDA 12 machine.

```
docker run --rm -p 8080:8080 --device nvidia.com/gpu=all --name api-server secondstate/qwen-2-0.5b-allminilm-2:cuda12
```

You can make an OpenAI style API request as follows.

```
curl -X POST http://localhost:8080/v1/chat/completions \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{""messages"":[{""role"":""system"", ""content"": ""You are a helpful assistant.""}, {""role"":""user"", ""content"": ""Where is Paris?""}]}'
```

Or, make an embedding request to turn a collection of text paragraphs into vectors. It is required for many RAG apps.

```
curl -X POST http://localhost:8080/v1/embeddings \
    -H 'accept:application/json' \
    -H 'Content-Type: application/json' \
    -d '{""model"":""all-MiniLM-L6-v2-ggml-model-f16.gguf"", ""input"":[""Paris is the capital of France."",""Paris occupies a central position in the rich agricultural region of 890 square miles (2,300 square km)."",""The population of Paris is 2,145,906""]}'
```

Stop and remove the container once you are done.

```
docker stop api-server
```

## Specify context window sizes

The memory consumption of the container is dependent on the context size you give to the model. You can specify the context size by appending two arguments at the end of the command. The following command starts the container with a context window of 1024 tokens for the chat LLM and a context window of 256 tokens for the embedding model. 

```
docker run --rm -p 8080:8080 --name api-server secondstate/qwen-2-0.5b-allminilm-2:latest ctx-size 1024 256
```

Each model comes with a maximum context size it can support. Your custom context size should not exceed that. Please refer to model documentation for this information. 

> If you set the embedding context size (i.e., the last argument in the above command) to 0, the container would load the chat LLM only.

## Build your own image

You can build nad publish a Docker image to use any models you like. First, download the model files (must be in GGUF format) you want from Huggingface. 
Of course, you could also your private finetuned model files here. 

```
curl -LO https://huggingface.co/second-state/Qwen2-0.5B-Instruct-GGUF/resolve/main/Qwen2-0.5B-Instruct-Q5_K_M.gguf
curl -LO https://huggingface.co/second-state/All-MiniLM-L6-v2-Embedding-GGUF/resolve/main/all-MiniLM-L6-v2-ggml-model-f16.gguf
```

Build a multi-platform image by passing the model files as `--build-arg`. The `PROMPT_TEMPLATE` is the specific text format the chat model is trained on to follow conversations. It differs for each model, and you will need to special attention. For all models published by the second-state organization, you can find the prompt-template in the model card. 

```
docker buildx build . --platform linux/arm64,linux/amd64 \
  --tag secondstate/qwen-2-0.5b-allminilm-2:latest -f Dockerfile \
  --build-arg CHAT_MODEL_FILE=Qwen2-0.5B-Instruct-Q5_K_M.gguf \
  --build-arg EMBEDDING_MODEL_FILE=all-MiniLM-L6-v2-ggml-model-f16.gguf \
  --build-arg PROMPT_TEMPLATE=chatml
```

Once it is built, you can publish it to Docker Hub.

```
docker login
docker push secondstate/qwen-2-0.5b-allminilm-2:latest
```

## What's next

Use the container as a drop-in replacement for the OpenAI API for your favorite agent app or framework! [See some examples here](https://llamaedge.com/docs/category/drop-in-replacement-for-openai). 

## A production-ready example

The quick start example uses very small models. They are great to get started but most production use cases require 
at least a 7B class LLM.
Let's build a new image for the Llama-3-8B chat LLM and the Nomic-1.5 embedding model. Both models are highly rated.
Download the two models in GGUF format.

```
curl -LO https://huggingface.co/second-state/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf
curl -LO https://huggingface.co/second-state/Nomic-embed-text-v1.5-Embedding-GGUF/resolve/main/nomic-embed-text-v1.5-f16.gguf
```

Build the image using the downloaded model files. The prompt template is `llama-3-chat`.

```
docker buildx build . --platform linux/arm64,linux/amd64 \
  --tag secondstate/llama-3-8b-nomic-1.5:latest -f Dockerfile \
  --build-arg CHAT_MODEL_FILE=Meta-Llama-3-8B-Instruct-Q5_K_M.gguf \
  --build-arg EMBEDDING_MODEL_FILE=nomic-embed-text-v1.5-f16.gguf \
  --build-arg PROMPT_TEMPLATE=llama-3-chat
```

Run an API server container using the models.
You will need at least 8GB of RAM for the container.

```
docker run --rm -p 8080:8080 --name api-server secondstate/llama-3-8b-nomic-1.5:latest
```

Run both models at their maximum context sizes. You will need at least 32GB of RAM for the container in order
to support these content sizes.

```
docker run --rm -p 8080:8080 --name api-server secondstate/llama-3-8b-nomic-1.5:latest ctx-size 8192 8192
```

## Why LlamaEdge?

LlamaEdge supports multiple types of models and their corresponding OpenAI-compatible APIs. 
We have seen LLM chat and embedding APIs here. 
It also supports multimodal vision models like [Llava](https://www.secondstate.io/articles/llava-v1.6-vicuna-7b/), 
[voice-to-text](https://github.com/WasmEdge/WasmEdge/issues/3170) models like Whisper, and text-to-image models like Stable Diffusion.

LlamaEdge supports multiple underlying GPU or CPU drivers and execution frameworks.

* GGML for generic Mac (Apple Silicon), Nvidia, and all CPUs
* [MLX](https://github.com/WasmEdge/WasmEdge/issues/3266) for advanced Apple Silicon
* TensorRT for advanced Nvidia
* [Intel Neural Speed](https://github.com/second-state/WasmEdge-WASINN-examples/pull/135) for advanced Intel CPUs
* Experimental WebGPU support in Docker

Finally, LlamaEdge enables developers to create applications on top of its component APIs. 
For example, [GaiaNet](https://docs.gaianet.ai/) built personalized and decentralized RAG services. 
The [Moxin](https://github.com/moxin-org/moxin) project is a rich Chatbot UI. Both projects are based on the LlamaEdge Rust SDK. 
The [OpenInterpreter](https://github.com/OpenInterpreter/01) project embeds LlamaEdge as an LLM provider in their Python app.
You can use model-specific Docker images as base images for your own apps.

"
docker/run.sh,"#!/bin/bash

source /root/.wasmedge/env

chat_file_name=$1
embedding_file_name=$2
prompt_template=$3
chat_ctx_size=$4
embedding_ctx_size=$5

if [ -z ""$chat_ctx_size"" ]; then
    chat_ctx_size=512
fi

if [ -z ""$embedding_ctx_size"" ]; then
    embedding_ctx_size=256
fi

if [ ""$embedding_ctx_size"" -eq ""0"" ]; then
    wasmedge --dir .:. --nn-preload default:GGML:AUTO:/models/$chat_file_name llama-api-server.wasm --prompt-template $prompt_template --ctx-size $chat_ctx_size --model-name $chat_file_name --socket-addr 0.0.0.0:8080
else
    wasmedge --dir .:. --nn-preload default:GGML:AUTO:/models/$chat_file_name --nn-preload embedding:GGML:AUTO:/models/$embedding_file_name llama-api-server.wasm --prompt-template $prompt_template,embedding --ctx-size $chat_ctx_size,$embedding_ctx_size --model-name $chat_file_name,$embedding_file_name --socket-addr 0.0.0.0:8080
fi
"
docker/webgpu.md,"# Docker + WebGPU preview

Docker is the leading solution for packaging and deploying portable applications. However, for AI and LLM
workloads, Docker containers are often not portable due to the lack of GPU abstraction -- you will need
a different container image for each GPU / driver combination. In some cases, the GPU is simply not
accessible from inside containers. For example, the ""impossible triangle of LLM app, Docker, and Mac GPU""
refers to the lack of Mac GPU access from containers.

Docker is supporting the WebGPU API for container apps. It will allow any underlying GPU or accelerator 
hardware to be accessed through WebGPU. That means container apps just need to write to the WebGPU API
and they will automatically become portable across all GPUs supported by Docker.
However, asking developers to rewrite existing LLM apps, which use the CUDA or Metal or other GPU APIs, 
to WebGPU is a challenge.

LlamaEdge provides an ecosystem of portable AI / LLM apps and components 
that can run on multiple inference backends including the WebGPU.
It supports any programming language that can be compiled into Wasm, such as Rust.
Furthermore, LlamaEdge apps are lightweight and binary portable across different CPUs and OSes, making it an ideal 
runtime to embed into container images.

> Based on the [WasmEdge runtime](https://github.com/WasmEdge/WasmEdge), LlamaEdge features a pluggable architecture that can easily switch between different inference backends without changing the compiled Wasm binary files.

In this article, we will showcase an [OpenAI-compatible speech-to-text API server](https://platform.openai.com/docs/guides/speech-to-text) implemented in LlamaEdge and running inside Docker taking advantage of GPUs through the WebGPU API. 

## Install Docker Desktop Preview with WebGPU

Download the preview Docker Desktop software:

* Mac with Apple Silicon (M series): https://desktop-stage.docker.com/mac/main/arm64/155220/Docker.dmg
* Linux with x86 CPU and any GPU:
  * https://desktop-stage.docker.com/linux/main/amd64/155220/docker-desktop-amd64.deb
  * https://desktop-stage.docker.com/linux/main/amd64/155220/docker-desktop-x86_64.rpm

> On Linux follow steps 1 and 3 from https://docs.docker.com/desktop/install/ubuntu/#install-docker-desktop to install the downloaded package.

Go to Settings,

* In ""General"", turn on `containerd` support
* In ""Features in development"", turn on ""Enable Wasm""

## Run the API server as a container

Pull the pre-made container image from Docker hub and run it.

```
docker run \
  --runtime=io.containerd.wasmedge.v1 \
  --platform=wasi/wasm \
  --device=""docker.com/gpu=webgpu"" \
  --env WASMEDGE_WASINN_PRELOAD=default:Burn:GPU:/tiny_en.mpk:/tiny_en.cfg:/tokenizer.json:en \
  -p 8080:8080 \
  secondstate/burn-whisper-server:latest
```

The [Docker image](https://hub.docker.com/r/secondstate/burn-whisper-server/tags) is only 90MB and it contains the entire model, runtime, and the API server.
It is also important to note that the image is for the `wasi/wasm` architecture. It can run on any OS and CPU
platform Docker supports.

## Use the API server

The API server is [OpenAI-compatible](https://platform.openai.com/docs/guides/speech-to-text).
You can use HTTP POST to submit a `.wav` file to transcribe. 
You can use [this file](https://huggingface.co/second-state/whisper-burn/resolve/main/audio16k.wav) as an example.

```
curl -LO https://huggingface.co/second-state/whisper-burn/resolve/main/audio16k.wav
```

You can now make an API request to the server.

```
curl http://localhost:8080/v1/audio/transcriptions \
  -H ""Content-Type: multipart/form-data"" \
  -F file=""@audio16k.wav""
```

The result is as follows.

```
{
    ""text"": "" Hello, I am the whisper machine learning model. If you see this as text then I am working properly.""
}
```

### Create your own audio file

The current demo requires `.wav` file in a specific format. 
It should use `lpcm` and the sample rate should be `16000.0`.

The [yt-dlp](https://github.com/yt-dlp/yt-dlp) program can download YouTube audio track in the above format.

```
yt-dlp -f bestaudio --extract-audio --audio-format wav --postprocessor-args ""-ss 25 -t 10 -ar 16000 -ac 1"" -o ""output.wav"" ""https://www.youtube.com/watch?v=UF8uR6Z6KLc""
```

## Build and publish the API server

The source code for the API server is [here](https://github.com/LlamaEdge/whisper-api-server/). 
It uses WasmEdge's [burn](https://github.com/second-state/wasmedge-burn-plugin) plugin to run
inference operations via WebGPU. But its source code has no dependency on `burn`. Instead, it uses the standard
and portable WASI-NN inferface to interact with the underlying inference runtime.
You can simply compile the Rust project to wasm.

```
cargo build --release --target wasm32-wasi
cp target/wasm32-wasi/release/whisper-api-server.wasm  .
```

Download the whispter AI model files for speech-to-text inference.

```
curl -LO https://huggingface.co/second-state/whisper-burn/resolve/main/tiny_en.tar.gz
tar -xvzf tiny_en.tar.gz
```

Use the following `Dockerfile` to build the image.

```
FROM scratch

# Copy the prepared files from the current directory to the image
COPY tiny_en.cfg /tiny_en.cfg
COPY tiny_en.mpk /tiny_en.mpk
COPY tokenizer.json /tokenizer.json
COPY whisper-api-server.wasm /app.wasm

# Set the entrypoint
ENTRYPOINT [ ""/app.wasm"" ]
```

Build and publish the Docker image.

```
docker build . --platform wasi/wasm -t secondstate/burn-whisper-server:latest
docker push secondstate/burn-whisper-server:latest
```


"
llama-api-server/.cargo/config.toml,"[build]
target = ""wasm32-wasip1""
rustflags = [""--cfg"", ""wasmedge"", ""--cfg"", ""tokio_unstable""]
"
llama-api-server/.gitignore,"/target
"
llama-api-server/Cargo.toml,"[package]
name = ""llama-api-server""
version = ""0.14.3""
edition = ""2021""

[dependencies]
llama-core.workspace = true
endpoints.workspace = true
chat-prompts.workspace = true
serde.workspace = true
serde_json.workspace = true
hyper = { version = ""0.14"", features = [""full""] }
tokio.workspace = true
thiserror.workspace = true
uuid.workspace = true
clap.workspace = true
once_cell.workspace = true
mime_guess = ""2.0.4""
futures-util = ""0.3""
anyhow.workspace = true
multipart-2021 = ""0.19.0""
wasi-logger.workspace = true
log.workspace = true
either.workspace = true
walkdir = ""2.5.0""

[features]
default = []
"
llama-api-server/README.md,"# LlamaEdge API Server

LlamaEdge API server offers OpenAI-compatible REST APIs. It can accelerate developers to build LLM-driven applications, AI infrastructure, and etc. In addition, LlamaEdge is also friendly to AI frameworks, such as LangChain and LlamaIndex.

<!-- @import ""[TOC]"" {cmd=""toc"" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [LlamaEdge API Server](#llamaedge-api-server)
  - [Dependencies](#dependencies)
  - [Get LlamaEdge API server](#get-llamaedge-api-server)
  - [Get model](#get-model)
  - [Run LlamaEdge API server](#run-llamaedge-api-server)
  - [Endpoints](#endpoints)
    - [`/v1/models` endpoint](#v1models-endpoint)
    - [`/v1/chat/completions` endpoint](#v1chatcompletions-endpoint)
    - [`/v1/files` endpoint](#v1files-endpoint)
    - [`/v1/chunks` endpoint](#v1chunks-endpoint)
    - [`/v1/embeddings` endpoint](#v1embeddings-endpoint)
    - [`/v1/completions` endpoint](#v1completions-endpoint)
  - [Add a web UI](#add-a-web-ui)
  - [CLI options for the API server](#cli-options-for-the-api-server)
  - [Set Log Level](#set-log-level)

<!-- /code_chunk_output -->

## Dependencies

Install the latest WasmEdge with plugins:

<details> <summary> For macOS (apple silicon) </summary>

```console
# install WasmEdge with wasi-nn-ggml plugin
curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s

# Assuming you use zsh (the default shell on macOS), run the following command to activate the environment
source $HOME/.zshenv
```

</details>

<details> <summary> For Ubuntu (>= 20.04) </summary>

```console
# install libopenblas-dev
apt update && apt install -y libopenblas-dev

# install WasmEdge with wasi-nn-ggml plugin
curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s

# Assuming you use bash (the default shell on Ubuntu), run the following command to activate the environment
source $HOME/.bashrc
```

</details>

<details> <summary> For General Linux </summary>

```console
# install WasmEdge with wasi-nn-ggml plugin
curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s

# Assuming you use bash (the default shell on Ubuntu), run the following command to activate the environment
source $HOME/.bashrc
```

</details>

## Get LlamaEdge API server

- Download LlamaEdge API server with the support for `HTTP` scheme only:

  ```console
  curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
  ```

- Download LlamaEdge API server with the support for `HTTP` and `WebSocket` schemes:

  ```console
  curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server-full.wasm
  ```

## Get model

[huggingface.co/second-state](https://huggingface.co/second-state?search_models=GGUF) maintains a group of GGUF models for different usages. In addition, you can also pick a GGUF model from the [https://huggingface.co/models?sort=trending&search=gguf](https://huggingface.co/models?sort=trending&search=gguf).

## Run LlamaEdge API server

Run the API server with the following command:

```bash
wasmedge --dir .:. --nn-preload default:GGML:AUTO:Meta-Llama-3-8B-Instruct-Q5_K_M.gguf \
  llama-api-server.wasm \
  --prompt-template llama-3-chat \
  --ctx-size 4096 \
  --model-name llama-3-8b

```

The command above starts the API server on the default socket address. Besides, there are also some other options specified in the command:

- The `--dir .:.` option specifies the current directory as the root directory of the WASI file system.

- The `--nn-preload default:GGML:AUTO:Meta-Llama-3-8B-Instruct-Q5_K_M.gguf` option specifies the Llama model to be used by the API server. The pattern of the argument is `<name>:<encoding>:<target>:<model path>`. Here, the model used is `Meta-Llama-3-8B-Instruct-Q5_K_M.gguf`; and we give it an alias `default` as its name in the runtime environment. You can change the model name here if you're not using llama-3-8b.
- The `--prompt-template llama-3-chat` is the prompt template for the model.
- The `--model-name llama-3-8b` specifies the model name. It is used in the chat request.

## Endpoints

### `/v1/models` endpoint

`/v1/models` endpoint is used to list models running on LlamaEdge API server.

<details> <summary> Example </summary>

You can use `curl` to test it on a new terminal:

```bash
curl -X GET http://localhost:8080/v1/models -H 'accept:application/json'
```

If the command is successful, you should see the similar output as below in your terminal:

```json
{
    ""object"":""list"",
    ""data"":[
        {
            ""id"":""llama-3-8b"",
            ""created"":1697084821,
            ""object"":""model"",
            ""owned_by"":""Not specified""
        }
    ]
}
```

</details>

### `/v1/chat/completions` endpoint

`/v1/chat/completions` endpoint is used for multi-turn conversations between human users and LLM models.

<details> <summary> Example </summary>

The following command sends a chat request with a user's question to the LLM model named `llama-3-8b`:

```bash
curl -X POST http://localhost:8080/v1/chat/completions \
    -H 'accept:application/json' \
    -H 'Content-Type: application/json' \
    -d '{""messages"":[{""role"":""system"", ""content"": ""You are a helpful assistant.""}, {""role"":""user"", ""content"": ""Who is Robert Oppenheimer?""}], ""model"":""llama-3-8b""}'
```

Here is the response from LlamaEdge API server:

```json
{
    ""id"":"""",
    ""object"":""chat.completion"",
    ""created"":1697092593,
    ""model"":""llama-3-8b"",
    ""choices"":[
        {
            ""index"":0,
            ""message"":{
                ""role"":""assistant"",
                ""content"":""Robert Oppenheimer was an American theoretical physicist and director of the Manhattan Project, which developed the atomic bomb during World War II. He is widely regarded as one of the most important physicists of the 20th century and is known for his contributions to the development of quantum mechanics and the theory of the atomic nucleus. Oppenheimer was also a prominent figure in the post-war nuclear weapons debate, advocating for international control and regulation of nuclear weapons.""
            },
            ""finish_reason"":""stop""
        }
    ],
    ""usage"":{
        ""prompt_tokens"":9,
        ""completion_tokens"":12,
        ""total_tokens"":21
    }
}
```

</details>

### `/v1/files` endpoint

`/v1/files` endpoint is used for uploading text and markdown files to LlamaEdge API server.

<details> <summary> Example: Upload files </summary>

The following command upload a text file [paris.txt](https://huggingface.co/datasets/gaianet/paris/raw/main/paris.txt) to the API server via the `/v1/files` endpoint:

```bash
curl -X POST http://127.0.0.1:8080/v1/files -F ""file=@paris.txt""
```

If the command is successful, you should see the similar output as below in your terminal:

```bash
{
    ""id"": ""file_4bc24593-2a57-4646-af16-028855e7802e"",
    ""bytes"": 2161,
    ""created_at"": 1711611801,
    ""filename"": ""paris.txt"",
    ""object"": ""file"",
    ""purpose"": ""assistants""
}
```

The `id` and `filename` fields are important for the next step, for example, to segment the uploaded file to chunks for computing embeddings.

If you'd like to build a RAG chatbot, it's strongly recommended to visit [LlamaEdge-RAG API Server](https://github.com/LlamaEdge/rag-api-server).

</details>

<details> <summary> Example: List files </summary>

The following command lists all files on the server via the `/v1/files` endpoint:

```bash
curl -X GET http://127.0.0.1:8080/v1/files
```

If the command is successful, you should see the similar output as below in your terminal:

```bash
{
    ""object"": ""list"",
    ""data"": [
        {
            ""id"": ""file_33d9188d-5060-4141-8c52-ae148fd15f6a"",
            ""bytes"": 17039,
            ""created_at"": 1718296362,
            ""filename"": ""test-123.m4a"",
            ""object"": ""file"",
            ""purpose"": ""assistants""
        },
        {
            ""id"": ""file_8c6439da-df59-4b9a-bb5e-dba4b2f23c04"",
            ""bytes"": 17039,
            ""created_at"": 1718294169,
            ""filename"": ""test-123.m4a"",
            ""object"": ""file"",
            ""purpose"": ""assistants""
        },
        {
            ""id"": ""file_6c601277-7deb-44c9-bfb3-57ce9da856c9"",
            ""bytes"": 17039,
            ""created_at"": 1718296350,
            ""filename"": ""test-123.m4a"",
            ""object"": ""file"",
            ""purpose"": ""assistants""
        },
        {
            ""id"": ""file_137b1ea2-c01d-44da-83ad-6b4aa2ff71de"",
            ""bytes"": 244596,
            ""created_at"": 1718337557,
            ""filename"": ""audio16k.wav"",
            ""object"": ""file"",
            ""purpose"": ""assistants""
        },
        {
            ""id"": ""file_21fde6a7-18dc-4d42-a5bb-1a27d4b7a32e"",
            ""bytes"": 17039,
            ""created_at"": 1718294739,
            ""filename"": ""test-123.m4a"",
            ""object"": ""file"",
            ""purpose"": ""assistants""
        },
        {
            ""id"": ""file_b892bc81-35e9-44a6-8c01-ae915c1d3832"",
            ""bytes"": 2161,
            ""created_at"": 1715832065,
            ""filename"": ""paris.txt"",
            ""object"": ""file"",
            ""purpose"": ""assistants""
        },
        {
            ""id"": ""file_6a6d8046-fd98-410a-b70e-0a0142ec9a39"",
            ""bytes"": 17039,
            ""created_at"": 1718332593,
            ""filename"": ""test-123.m4a"",
            ""object"": ""file"",
            ""purpose"": ""assistants""
        }
    ]
}
```

</details>

<details> <summary> Example: Retrieve information about a specific file </summary>

The following command retrieves information about a specific file on the server via the `/v1/files/{file_id}` endpoint:

```bash
curl -X GET http://localhost:10086/v1/files/file_b892bc81-35e9-44a6-8c01-ae915c1d3832
```

If the command is successful, you should see the similar output as below in your terminal:

```bash
{
    ""id"": ""file_b892bc81-35e9-44a6-8c01-ae915c1d3832"",
    ""bytes"": 2161,
    ""created_at"": 1715832065,
    ""filename"": ""paris.txt"",
    ""object"": ""file"",
    ""purpose"": ""assistants""
}
```

</details>

<details> <summary> Example: Delete a specific file </summary>

The following command deletes a specific file on the server via the `/v1/files/{file_id}` endpoint:

```bash
curl -X DELETE http://localhost:10086/v1/files/file_6a6d8046-fd98-410a-b70e-0a0142ec9a39
```

If the command is successful, you should see the similar output as below in your terminal:

```bash
{
    ""id"": ""file_6a6d8046-fd98-410a-b70e-0a0142ec9a39"",
    ""object"": ""file"",
    ""deleted"": true
}
```

</details>

### `/v1/chunks` endpoint

To segment the uploaded file to chunks for computing embeddings, use the `/v1/chunks` API.

<details> <summary> Example </summary>

The following command sends the uploaded file ID and filename to the API server and gets the chunks:

```bash
curl -X POST http://localhost:8080/v1/chunks \
    -H 'accept:application/json' \
    -H 'Content-Type: application/json' \
    -d '{""id"":""file_4bc24593-2a57-4646-af16-028855e7802e"", ""filename"":""paris.txt"", ""chunk_capacity"":100}'
```

The following is an example return with the generated chunks:

```json
{
    ""id"": ""file_4bc24593-2a57-4646-af16-028855e7802e"",
    ""filename"": ""paris.txt"",
    ""chunks"": [
        ""Paris, city and capital of France, ... and far beyond both banks of the Seine."",
        ""Paris occupies a central position in the rich agricultural region ... metropolitan area, 890 square miles (2,300 square km)."",
        ""Pop. (2020 est.) city, 2,145,906; (2020 est.) urban agglomeration, 10,858,874."",
        ""For centuries Paris has been one of the world’s ..., for Paris has retained its importance as a centre for education and intellectual pursuits."",
        ""Paris’s site at a crossroads of both water and land routes ... The Frankish king Clovis I had taken Paris from the Gauls by 494 CE and later made his capital there."",
        ""Under Hugh Capet (ruled 987–996) and the Capetian dynasty ..., drawing to itself much of the talent and vitality of the provinces.""
    ]
}
```

If you'd like to build a RAG chatbot, it's strongly recommended to visit [LlamaEdge-RAG API Server](https://github.com/LlamaEdge/rag-api-server).

</details>

### `/v1/embeddings` endpoint

To compute embeddings for user query or file chunks, use the `/v1/embeddings` API.

<details> <summary> Example </summary>

The following command sends a query to the API server and gets the embeddings as return:

```bash
curl -X POST http://localhost:8080/v1/embeddings \
    -H 'accept:application/json' \
    -H 'Content-Type: application/json' \
    -d '{""model"": ""e5-mistral-7b-instruct-Q5_K_M"", ""input"":[""Paris, city and capital of France, ... and far beyond both banks of the Seine."",""Paris occupies a central position in the rich agricultural region ... metropolitan area, 890 square miles (2,300 square km)."",""Pop. (2020 est.) city, 2,145,906; (2020 est.) urban agglomeration, 10,858,874."",""For centuries Paris has been one of the world’s ..., for Paris has retained its importance as a centre for education and intellectual pursuits."",""Paris’s site at a crossroads of both water and land routes ... The Frankish king Clovis I had taken Paris from the Gauls by 494 CE and later made his capital there."",""Under Hugh Capet (ruled 987–996) and the Capetian dynasty ..., drawing to itself much of the talent and vitality of the provinces.""]}'
```

The embeddings returned are like below:

```json
{
    ""object"": ""list"",
    ""data"": [
        {
            ""index"": 0,
            ""object"": ""embedding"",
            ""embedding"": [
                0.1477311701,
                -0.00002238310481,
                ...,
                0.01931835897,
                -0.02496444248
            ]
        },
        {
            ""index"": 1,
            ""object"": ""embedding"",
            ""embedding"": [
                0.1766036302,
                -0.009940749966,
                ...,
                0.0156990625,
                -0.02616829611
            ]
        },
        {
            ""index"": 2,
            ""object"": ""embedding"",
            ""embedding"": [
                0.04604972154,
                -0.07207781076,
                ...,
                0.00005568400593,
                0.04646552354
            ]
        },
        {
            ""index"": 3,
            ""object"": ""embedding"",
            ""embedding"": [
                0.1065238863,
                -0.04788689688,
                ...,
                0.0301867798,
                0.0275206212
            ]
        },
        {
            ""index"": 4,
            ""object"": ""embedding"",
            ""embedding"": [
                0.05383823439,
                0.03193736449,
                ...,
                0.01904040016,
                -0.02546775527
            ]
        },
        {
            ""index"": 5,
            ""object"": ""embedding"",
            ""embedding"": [
                0.05341234431,
                0.005945806392,
                ...,
                0.06845153868,
                0.02127391472
            ]
        }
    ],
    ""model"": ""all-MiniLM-L6-v2-ggml-model-f16"",
    ""usage"": {
        ""prompt_tokens"": 495,
        ""completion_tokens"": 0,
        ""total_tokens"": 495
    }
}
```

If you'd like to build a RAG chatbot, it's strongly recommended to visit [LlamaEdge-RAG API Server](https://github.com/LlamaEdge/rag-api-server).

</details>

### `/v1/completions` endpoint

To obtain the completion for a single prompt, use the `/v1/completions` API.

<details> <summary> Example </summary>

The following command sends a prompt to the API server and gets the completion:

```bash
curl -X POST http://localhost:8080/v1/completions \
    -H 'accept:application/json' \
    -H 'Content-Type: application/json' \
    -d '{""prompt"":[""Long long ago, ""], ""model"":""tinyllama""}'
```

The response looks like below:

```json
{
    ""id"": ""b68bfc92-8b23-4435-bbe1-492e11c973a3"",
    ""choices"": [
        {
            ""finish_reason"": ""stop"",
            ""index"": 0,
            ""logprobs"": null,
            ""text"": ""in a galaxy far, far away, a group of Jedi Knights lived and trained to defend the peace and security of the galaxy. They were the last hope for peace and justice in a world where the dark side of the force was rife with corruption and injustice. The Knights were a select few, and their training and abilities were the envy of the galaxy. They were the chosen ones. They were the ones who would bring peace and justice to the galaxy. ...""
        }
    ],
    ""created"": 1702046592,
    ""model"": ""tinyllama"",
    ""object"": ""text_completion"",
    ""usage"": {
        ""prompt_tokens"": 3,
        ""completion_tokens"": 804,
        ""total_tokens"": 807
    }
}
```

</details>

## Add a web UI

We provide a front-end Web UI for you to easily interact with the API. You can download and extract it by running:

```bash
curl -LO https://github.com/LlamaEdge/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz
tar xzf chatbot-ui.tar.gz
```

After that, you can use the same command line to create the API server

```bash
wasmedge --dir .:. --nn-preload default:GGML:AUTO:Meta-Llama-3-8B-Instruct-Q5_K_M.gguf \
  llama-api-server.wasm \
  --prompt-template llama-3-chat \
  --ctx-size 4096 \
  --model-name llama-3-8b
```

Then, you will be asked to open `http://127.0.0.1:8080` from your browser.

<a id=""cli-options""></a>

## CLI options for the API server

The `-h` or `--help` option can list the available options of the `llama-api-server` wasm app:

```console
$ wasmedge llama-api-server.wasm -h

LlamaEdge API Server

Usage: llama-api-server.wasm [OPTIONS] --prompt-template <PROMPT_TEMPLATE>

Options:
  -m, --model-name <MODEL_NAME>
          Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model [default: default]
  -a, --model-alias <MODEL_ALIAS>
          Model aliases for chat and embedding models [default: default,embedding]
  -c, --ctx-size <CTX_SIZE>
          Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model [default: 4096,384]
  -b, --batch-size <BATCH_SIZE>
          Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model [default: 512,512]
  -p, --prompt-template <PROMPT_TEMPLATE>
          Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model [possible values: llama-2-chat, llama-3-chat, llama-3-tool, mistral-instruct, mistral-tool, mistrallite, openchat, codellama-instruct, codellama-super-instruct, human-assistant, vicuna-1.0-chat, vicuna-1.1-chat, vicuna-llava, chatml, chatml-tool, internlm-2-tool, baichuan-2, wizard-coder, zephyr, stablelm-zephyr, intel-neural, deepseek-chat, deepseek-coder, deepseek-chat-2, solar-instruct, phi-2-chat, phi-2-instruct, phi-3-chat, phi-3-instruct, gemma-instruct, octopus, glm-4-chat, groq-llama3-tool, embedding]
  -r, --reverse-prompt <REVERSE_PROMPT>
          Halt generation at PROMPT, return control
  -n, --n-predict <N_PREDICT>
          Number of tokens to predict [default: 1024]
  -g, --n-gpu-layers <N_GPU_LAYERS>
          Number of layers to run on the GPU [default: 100]
      --main-gpu <MAIN_GPU>
          The main GPU to use
      --tensor-split <TENSOR_SPLIT>
          How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1
      --threads <THREADS>
          Number of threads to use during computation [default: 2]
      --no-mmap <NO_MMAP>
          Disable memory mapping for file access of chat models [possible values: true, false]
      --temp <TEMP>
          Temperature for sampling [default: 1.0]
      --top-p <TOP_P>
          An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled [default: 1.0]
      --repeat-penalty <REPEAT_PENALTY>
          Penalize repeat sequence of tokens [default: 1.1]
      --presence-penalty <PRESENCE_PENALTY>
          Repeat alpha presence penalty. 0.0 = disabled [default: 0.0]
      --frequency-penalty <FREQUENCY_PENALTY>
          Repeat alpha frequency penalty. 0.0 = disabled [default: 0.0]
      --grammar <GRAMMAR>
          BNF-like grammar to constrain generations (see samples in grammars/ dir) [default: ]
      --json-schema <JSON_SCHEMA>
          JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead
      --llava-mmproj <LLAVA_MMPROJ>
          Path to the multimodal projector file
      --socket-addr <SOCKET_ADDR>
          Socket address of LlamaEdge API Server instance [default: 0.0.0.0:8080]
      --web-ui <WEB_UI>
          Root path for the Web UI files [default: chatbot-ui]
      --log-prompts
          Deprecated. Print prompt strings to stdout
      --log-stat
          Deprecated. Print statistics to stdout
      --log-all
          Deprecated. Print all log information to stdout
  -h, --help
          Print help
  -V, --version
          Print version
```

Please guarantee that the port is not occupied by other processes. If the port specified is available on your machine and the command is successful, you should see the following output in the terminal:

```console
[INFO] LlamaEdge HTTP listening on 8080
```

If the Web UI is ready, you can navigate to `http://127.0.0.1:8080` to open the chatbot, it will interact with the API of your server.

## Set Log Level

You can set the log level of the API server by setting the `LLAMA_LOG` environment variable. For example, to set the log level to `debug`, you can run the following command:

```bash
wasmedge --dir .:. --env LLAMA_LOG=debug \
    --nn-preload default:GGML:AUTO:Meta-Llama-3-8B-Instruct-Q5_K_M.gguf \
    llama-api-server.wasm \
    --model-name llama-3-8b \
    --prompt-template llama-3-chat \
    --ctx-size 4096
```

The log level can be one of the following values: `trace`, `debug`, `info`, `warn`, `error`. The default log level is `info`.
"
llama-api-server/ToolUse.md,"# Tool Use

## Introduction

The following diagram shows the basic interactions between three roles (front end, LLM and toolkit) in the scenario of tool use.

<div align=center>
<img src=""image/tool_use.png"" alt=""Interactions in the senario of tool use"" width=""60%"" />
</div>

The following models with tool use capabilities are supported by LlamaEdge API server:

- [second-state/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/second-state/Mistral-7B-Instruct-v0.3-GGUF)
- [second-state/Hermes-2-Pro-Llama-3-8B-GGUF](https://huggingface.co/second-state/Hermes-2-Pro-Llama-3-8B-GGUF)
- [second-state/Llama-3-Groq-8B-Tool-Use-GGUF](https://huggingface.co/second-state/Llama-3-Groq-8B-Tool-Use-GGUF)
- [second-state/Meta-Llama-3.1-8B-Instruct-GGUF](https://huggingface.co/second-state/Meta-Llama-3.1-8B-Instruct-GGUF)
- [second-state/internlm2_5-7b-chat-GGUF](https://huggingface.co/second-state/internlm2_5-7b-chat-GGUF)

### Example

The examples involved in the explanation below are generated by [second-state/Mistral-7B-Instruct-v0.3-GGUF](https://huggingface.co/second-state/Mistral-7B-Instruct-v0.3-GGUF).

- Step 0: Start LlamaEdge API Server

  Start the LlamaEdge API server with the following command:

  ```bash
  wasmedge --dir .:. --nn-preload default:GGML:AUTO:Mistral-7B-Instruct-v0.3-Q5_K_M.gguf \
    llama-api-server.wasm \
    --prompt-template mistral-tool \
    --ctx-size 32000 \
    --model-name Mistral-7B-Instruct-v0.3
  ```

- Step 1: Send a request with user query and available tools
  The following shows the JSON format of a user request. The request includes a `user query`, available `tools`, and the `tool choice`. The tool choice can be `auto` (the model can pick between generating a message or calling one or more tools), `none` (the model will not call any tool and instead generates a message), or `required` (the model must call one or more tools).

  <details> <summary> Expand to see the example </summary>

    ```json
    {
        ""model"": ""Mistral-7B-Instruct-v0.3"",
        ""messages"": [
            {
                ""role"": ""user"",
                ""content"": ""Hey! What is the weather like in auckland?""
            }
        ],
        ""tools"": [
            {
                ""type"": ""function"",
                ""function"": {
                    ""name"": ""get_current_weather"",
                    ""description"": ""Get the current weather in a given location"",
                    ""parameters"": {
                        ""type"": ""object"",
                        ""properties"": {
                            ""location"": {
                                ""type"": ""string"",
                                ""description"": ""The city and state, e.g. San Francisco, CA""
                            },
                            ""format"": {
                                ""type"": ""string"",
                                ""enum"": [
                                    ""celsius"",
                                    ""fahrenheit""
                                ],
                                ""description"": ""The temperature unit to use. Infer this from the users location.""
                            }
                        },
                        ""required"": [
                            ""location"",
                            ""format""
                        ]
                    }
                }
            },
            {
                ""type"": ""function"",
                ""function"": {
                    ""name"": ""predict_weather"",
                    ""description"": ""Predict the weather in 24 hours"",
                    ""parameters"": {
                        ""type"": ""object"",
                        ""properties"": {
                            ""location"": {
                                ""type"": ""string"",
                                ""description"": ""The city and state, e.g. San Francisco, CA""
                            },
                            ""format"": {
                                ""type"": ""string"",
                                ""enum"": [
                                    ""celsius"",
                                    ""fahrenheit""
                                ],
                                ""description"": ""The temperature unit to use. Infer this from the users location.""
                            }
                        },
                        ""required"": [
                            ""location"",
                            ""format""
                        ]
                    }
                }
            }
        ],
        ""tool_choice"": ""auto"", // ""auto"", ""none"", ""required""
        ""stream"": true
    }
    ```

  </details>

- Step 2: Return a response of tool call

  The following shows the JSON format of a response from the LLM. The response includes the `selected tool` and the `tool output`. The tool output can be a `message` or a `function`.

  <details> <summary> Expand to see the example </summary>

    ```json
    {
        ""id"": ""chatcmpl-7c7953eb-0732-4216-b85b-90ad17412b5f"",
        ""object"": ""chat.completion"",
        ""created"": 1719235670,
        ""model"": ""Mistral-7B-Instruct-v0.3"",
        ""choices"": [
            {
                ""index"": 0,
                ""message"": {
                    ""content"": ""[TOOL_CALLS] [{\""name\"":\""get_current_weather\"",\""arguments\"":{\""location\"": \""Auckland, NZ\"", \""format\"": \""celsius\""}}]\n\nThe current weather in Auckland is being fetched...\n\n[Output from the function]\n\nAssistant: The current temperature in Auckland is 18 degrees Celsius.\n\nIs there anything else I can help you with?"",
                    ""tool_calls"": [
                        {
                            ""id"": ""call_abc123"",
                            ""type"": ""function"",
                            ""function"": {
                                ""name"": ""get_current_weather"",
                                ""arguments"": ""{\""format\"":\""celsius\"",\""location\"":\""Auckland, NZ\""}""
                            }
                        }
                    ],
                    ""role"": ""assistant""
                },
                ""finish_reason"": ""tool_calls"",
                ""logprobs"": null
            }
        ],
        ""usage"": {
            ""prompt_tokens"": 234,
            ""completion_tokens"": 91,
            ""total_tokens"": 325
        }
    }
    ```

  </details>

- Step 3: Execute tool

  The front end extracts the tool name and arguments from the tool call message, and then executes the tool. Note that tools could be a group of built-in functions or external APIs. The tool execution could be synchronous or asynchronous, depending on the tool implementation.

- Step 4: Return tool result

  After the tool execution, the front end receives the tool result.

- Step 5: Feed tool result to LLM

  Package the tool result into a tool message and send it to the LLM.

  <details> <summary> Expand to see the example </summary>

    ```json
    {
        ""model"": ""Mistral-7B-Instruct-v0.3"",
        ""messages"": [
            {
                ""role"": ""user"",
                ""content"": ""Hey! What is the weather like in auckland?""
            },
            {
                ""content"": ""[TOOL_CALLS] [{\""name\"":\""get_current_weather\"",\""arguments\"":{\""location\"": \""Auckland, NZ\"", \""format\"": \""celsius\""}}]\n\nHere is the current weather in Auckland, New Zealand: {(response from get_current_weather function)}\n\nI will also predict the weather for tomorrow. Here you go: {(response from predict_weather function)}"",
                ""tool_calls"": [
                    {
                        ""id"": ""call_abc123"",
                        ""type"": ""function"",
                        ""function"": {
                            ""name"": ""get_current_weather"",
                            ""arguments"": ""{\""format\"":\""celsius\"",\""location\"":\""Auckland, NZ\""}""
                        }
                    }
                ],
                ""role"": ""assistant""
            },
            {
                ""role"": ""tool"",
                ""content"": ""Fine, with a chance of showers.""
            }
        ],
        ""tools"": [
            {
                ""type"": ""function"",
                ""function"": {
                    ""name"": ""get_current_weather"",
                    ""description"": ""Get the current weather in a given location"",
                    ""parameters"": {
                        ""type"": ""object"",
                        ""properties"": {
                            ""location"": {
                                ""type"": ""string"",
                                ""description"": ""The city and state, e.g. San Francisco, CA""
                            },
                            ""unit"": {
                                ""type"": ""string"",
                                ""enum"": [
                                    ""celsius"",
                                    ""fahrenheit""
                                ]
                            }
                        },
                        ""required"": [
                            ""location""
                        ]
                    }
                }
            },
            {
                ""type"": ""function"",
                ""function"": {
                    ""name"": ""predict_weather"",
                    ""description"": ""Predict the weather in 24 hours"",
                    ""parameters"": {
                        ""type"": ""object"",
                        ""properties"": {
                            ""location"": {
                                ""type"": ""string"",
                                ""description"": ""The city and state, e.g. San Francisco, CA""
                            },
                            ""unit"": {
                                ""type"": ""string"",
                                ""enum"": [
                                    ""celsius"",
                                    ""fahrenheit""
                                ]
                            }
                        },
                        ""required"": [
                            ""location""
                        ]
                    }
                }
            }
        ],
        ""tool_choice"": ""auto"",
        ""stream"": true
    }
    ```

  </details>

- Step 6: Return chat completions

  LLM generates a chat completion with the user query and the tool result.

  <details> <summary> Expand to see the example </summary>

  For the purpose of demonstration, the following example is generated by the LLM in the `stream = false` mode.

    ```json
    {
        ""id"": ""chatcmpl-b0db39c1-67e9-457a-be82-f7b3ca1489e9"",
        ""object"": ""chat.completion"",
        ""created"": 1719211344,
        ""model"": ""Mistral-7B-Instruct-v0.3"",
        ""choices"": [
            {
                ""index"": 0,
                ""message"": {
                    ""content"": ""Today in Auckland, the current weather is fine but there's a chance of showers. Make sure to check the forecast for any potential changes throughout the day!"",
                    ""tool_calls"": [],
                    ""role"": ""assistant""
                },
                ""finish_reason"": ""stop"",
                ""logprobs"": null
            }
        ],
        ""usage"": {
            ""prompt_tokens"": 60,
            ""completion_tokens"": 36,
            ""total_tokens"": 96
        }
    }
    ```

  </details>
"
llama-api-server/image/tool_use.png,"�PNG

   
IHDR  �  �   e|�   gAMA  ���a    cHRM  z&  ��  �   ��  u0  �`  :�  p��Q<   �eXIfMM *                  J       R(       �i       Z       �      �    ��       ��       �      �      �    ASCII   Screenshot�4N�   	pHYs  %  %IR$�  
iTXtXML:com.adobe.xmp     <x:xmpmeta xmlns:x=""adobe:ns:meta/"" x:xmptk=""XMP Core 6.0.0"">
   <rdf:RDF xmlns:rdf=""http://www.w3.org/1999/02/22-rdf-syntax-ns#"">
      <rdf:Description rdf:about=""""
            xmlns:tiff=""http://ns.adobe.com/tiff/1.0/""
            xmlns:exif=""http://ns.adobe.com/exif/1.0/"">
         <tiff:ResolutionUnit>2</tiff:ResolutionUnit>
         <tiff:XResolution>144/1</tiff:XResolution>
         <tiff:YResolution>144/1</tiff:YResolution>
         <tiff:Orientation>1</tiff:Orientation>
         <exif:PixelXDimension>1266</exif:PixelXDimension>
         <exif:ColorSpace>1</exif:ColorSpace>
         <exif:UserComment>Screenshot</exif:UserComment>
         <exif:PixelYDimension>700</exif:PixelYDimension>
      </rdf:Description>
   </rdf:RDF>
</x:xmpmeta>
�j��  @ IDATx�������b���?�@TLJe�.l�VL��@����
""*�-X�(*������}���۵;�~��{�Μ93��]��s�iTSS3�Q@ @ @ @ @�A�נώ�C @ @ @ @ /@ �@ @ @ @ �ȋ�M�@ @ @ @  ��3�  �  �  �  �@���&q� �  �  �  � ��@ @ @ @ @ �bp�8E@ @ @ @ �� �  �  �  �  y1�I�"" �  �  �  � �x@ @ @ @ �� ���$N@ @ @ @ y< �  �  �  � �@�@^n��  �  �  �  � �<�@ @ @ @ b @ /7�SD @ @ @ @�@�  �  �  �  � 1 ����)""�  �  �  �  @ �g @ @ @ @ �ȋ�M�@ @ @ @  ��3�  �  �  �  �@���&q� �  �  �  � ��@ @ @ @ @ �bp�8E@ @ @ @ �� �  �  �  �  y1�I�"" �  �  �  � �x@ @ @ @ �� ���$N@ @ @ @ y< �  �  �  � �@�@^n��  �  �  �  � �<�@ @ @ @ b @ /7�SD @ @ @ @�@�  �  �  �  � 1 ����)""�  �  �  �  @ �g @ @ @ @ �ȋ�M�@ @ @ @  ��3�  �  �  �  �@���&q� �  �  �  � ��@ @ @ @ @ �bp�8E@ @ @ @ �� �  �  �  �  y1�I�"" �  �  �  � �x@ @ @ @ �� ���$N@ @ @ @ ��q!����˩r�ԛ@MMM��#� 87k�,7x�`7v�X7y�d7s�L�Ҽys׺ukW]]���g״iS�@ (� ��KF́b,��Ӎ���88�'w�s�o����o'�� C�O�>n���nΜ9)����]Ϟ=��S@ (� ��K��1�.��ӱ�͝��cܟ_�?f�5�g�_<1��t@ ��S�LqGq�{�7|�]v����瞮]�v�E�N���M��&L����}�Q_o��6r��z�kٲe��و  �@����t���_�q�<M ��B��������n� e+� �N;��O��ڴi㮸�
��V[�����ǻSO=ս��kn�Vp#F� ��R�� � �
��W���Y n��	���ȵU�@�~�T̍�B@��ڷo�3�7l�0�n��u��c�=|v�2���Lv� � 9	�ˉ��*D n�����BL.@ 
+�1�ԝV�x��t
�i��6/�����@ @ �r #�\�,�UQq����9\,���f�]u�U���<�L�����v뭷����S�2�m2(�#� �%@F^^|�\�q�<MF^�?�\ � ^`���>��.����Kut��6��VmR@ @ H%@ /��@ @ �cǎ�k5;m��ڰ6�m��@ @ �W�@^��[�@ �$0y�d�r�v��>��am�� 
 �  � �� �����\ � K`�̙��VZ)�C�h�·am�� 
 �  � �� �����\ � [`�ܹy�Xλ!@ @ ({ye��@@ (�@���}���y�MO�6ͷam�� 
 �  � �� �����\ � K�u�־�	&�}k��̻A@ @ ([ye{k�0@ (�@uu�oz�Сy�ڰ6�n�@ @ �V�QMMM����������cє�t9%����<����T�Y�f�UW]�͙3�=��3n���ʉb���n뭷v��?��:u�kڴiN�� � ��<�J�m�.���d�U���#�  �@�
������wꩧ��^��(�}U�A�l��  � T� y�wϹ�2��� ��-��@�Bڷo��x�
��.��aÆ�̺L(��c�=ܣ�>�6�h#����g�u@ �I�����ةB��y���
y0�L@ (������VXa��|�͝�ʦ+���
�i_�AA @ @ 2�2Q�
\ n����99=@ +�)S��#�8�g�iGe���瞮]�vn��Vr����?w��V[(���L<�Z�l���
@ �b	��W,Y�-��}�&�WO�P�q��S�7 (K�>}����/O[hL<է �  P
y�P�q���iyq}�8oBq��:u@ ���l��vcǎu�'OvӧO�ק.��[�v���n�}�ab����\ ���5�{�֟@�>Oȫ�g�##P0����)؅� ���SMMM?SN@���{� �@m��}�f�����w �  �  �  � 4Hy
�pR �  �  �  � � �Wۃw �  �  �  � 4H��
�8)@ @���{��mv�С��G� �  � �`��x�'���q�3�Ű�g��O#��8#)
@ ���6&�H�����i2�*�q��@ @ *PȌ7��+d����= �  �@���W9��+-c���B�
.
�%`d��b�
 �@���y%��p����i&���c�I""�  �  �  � T� ]k��	3f�{�7��9����""�,�O���/�	&��:�(��K��0y�d7d���⋻3�8#��|�A��矻6mڸN�:��;#�  �  � T��K/��}�٬/z��Vs{�G��e��}^o֬�;�C�����{���g�=z�m�>r?��S��	'��Zh���P��е��ߣ����s7�pCP��Gu;�s�>�B�.]��ѣ�M~��[c�5���^���܁�]vY7s�̼��ر�ӸC'�x�����j�v�[*p9�s
 �@&֕����hQ@�X������	_v�e��oE�K�o��{�'2��{�.�����n���r���o(��Ԋ��~����5i��/+)��7�t��7��q�s?�2�3n����+�/z�Nț1cF� ^t�#�  �  �  P�͛7w�[��sy�If%���W_�6��U	=�~�?�����w�2:�����r�-n�EMX���	׳@ @ @ �rPwU��ֽ��;jҤI�M�[�v�mݔ)S�y/��R�;N8� �]$v��Z�?g��'���:�<��_�_�Rh�/���N��fϞZ�z���v�D���w���n�� �  �  � ԣ�>�~��Y����\.S]��^{m�e�Gsi�}� ���u?Ҟ�ȑ#]������\����/L���C�ٮӧOw�<��v����D��̙����Ӡ�s��6scǎM��_w���v��)��34V�wܑ2��믿�3�<�m��~���^صo��]x�N�( �  �  � �*p�UW��чzh�M�i�:}�������n��s�{�I'��~�)�^���u��TˣF��A�3�A�jm���UWW;��������ܽz�
�R}}Ƨ�G�@^|�?S�Rs�=���o��������_n�����������р�*
�%��U�i�K�.���j�W^qJэ���8���/
e�
}�1�v�a��SO�Վ��$�n���߿��G�u�;�<�K'��1�X�""�  �  � T���O?�?G�y�y]�#�<�Z�l��3�5���_�k��ƭ��N�q�%������-�\�v��^�u����_�wQ�@I<�v��ӗ>s�\��>�S�#@ />�*�3�fx��D��X��8�5n�x��+���=��S��
�}��gN]^'N��ڵk��k����
��ׯ_?��U�V~Mw=d�?Sm����4��<�}�]�ꦛn���ǯl@��{���
@ @ @ ��hS9�e�]֍?�� ��/���_���s�QGU�GZ.��}c|{��|v��jr}FWv`���A}��۷o�Y�`#
R�@^��-�OJi�ʐ��Fm��b�f͜��V���_0�=��_�,8�����}�G:�VZi%߭v��7�v�f�bYv�����*����/�.]�8���{���o�~S֠E�t�/�VX�w�=���ܵ�^�wQv��) �  �  � d+��}��'hu饗���m�cǎ~h�W\�{����`�%����:�kSv�N;��o���nܸqN�de�����؊��[c�5�-�1 ���>E�;�T]}i��de����~�a��?���j��z�`}xA��`�SN���I�&�[�n��Rw��1����T3\t��fz��}5u���cx�}��7x�_R@ @ @ ��x��\�O��e4���V�EI,V�x�
����kk#��a��n�������1c��K/�j���@�~�erq�|h��Rm<�@�a�j5�E�n����T�u�Y�/G���5����l���X�U�\�����[�άY�| ���7�~�id
o@ @ @ (������MEc�'*�Ҫ���^�d�\>_+K.UQ���n�pC�yez��C��.�8e�)�M��iӦ9uQUI֭V�l6ܕW^Yo�p*��p� ���'*�~���O>�U�j��%�W�4E7@ @ @ �R|����!Օ6YQ֟m6ac.�����l�-���Ǟ�j���Ua}	�+���uKU ���w
�����f�m߾}�z�~�E�˚�:Y	�.�_b�m��ׄ)���}������Ԙx�J���T�ن  �  �  �@!^x�}�MT��b�3���������DmG����N�hh]��;̩�ۢ�.���2 �Wf74|9;�C�vĈ�����5aF�n����AP�L���6��o��M{ZQ /<΀R��{V/:�h�F�����z�}���Ժhy���ݟ��Z�h��{@ @ @ �.�I�X�Y{o��m�&���󵵑�U�9J�Y`���'�p����	(5!���좌��K,���k/��{n0;��Kv��q�Z����aԨQ~}�N�|Pp���
��~����-(�oРA�6x���4րv���k����_��4�@ @ @ �Rh�W���r��7�9s��9�p�����u��#+�� �J�=��e-_v�e�$�zO)Oy1��ʪ�`�
��M����g������[n�e�]\�.]����h
�_~9�������>:�r��s�����������`�m����ߜ� ��g�=����O���������{�/G����{��C8�0�� �  �  � iN9��9�M�6ij&�ܻwo�Qc��s�z�Y�,�g�q�ۡC�`8�\>_[�����ݍ7�T�g��yB��W�&M���b���.��Q�fj|:}���v�n��lѿjҋ��t�6��Fۯ��Z�N���7��闝~-��R�2�4��6�l4q�y��z�B���ܢ��ڿ2ď�T�wA�����׿�X~�>�}lT_KMVf }��z�G @ @ �D@�N�s��vʵhBɮ]���o��6״iS�9Y�66J��������������U�����k?��[l�E�Z3�F'�6�� �5�ے����k%<���_p�m����r
�Y��Vk�i{x�cǎN�Z�_�~ٍ=:ȪS��{�ך��͚5s�_�v����
:>���~M�1|��Z���jK���O�L�8�onժ���~
""��#�5��e@ @ @ ��ϻ�������D�����3�����U��<SQ�6}v�.�~�o�|����o�����SO9�v�,��k���}�UWu����}�gwJ|�����U@Je��X�nH3:G���iՍw���n�u���r�����V���ꫮI�&n�M6�(¯�4N���� ��
����+��e��Y�<\2 Po��~?��-�� � ������8|�����&u�d�뭷���f������D�j݌3ܬY�| P�9�Z����k�*�I��J n�x�
��A R�����0ť�	@ ����@^�o""�^4��}����f�Hh@ @ @ @ � ��� �#�  �  �  �  Py%@� �  �  �  � �+@ /_A�G @ \|�����r��@ @ �`��,���@C����
ѐsB 
)��sϹ��ۮV�#G�t:t���7 � �B��.J��1�*���d���I�@ @��
X^�޽��Tl]�=iN@ @���k�D @ �8	(`��<eߝ}���K�ZG0/Nw�sE @ � ���wO8#@ ���u����g� �[�6�y��+ �  �@���> �  �@@w�Sw��xxZ�.����@ @ � ����@ @ ��i���e���i��nx;� �  � � ��N�� �  �@����%��`��%�a= �  �@:y�؎  � �PϺ�&�ċ�ju//*�{@ @ �t��	�@ H""�j\�$���<�b�xyɔX�  �  �H�@^""�!�  � �Xw���K���%�a= �  �@*y�t؆  � $�t\�$��	1//��@ @ 	�K��:@ @ �@���%k���ɰ@ @ � ��D*�C @ ��2.^��//�@ @ 
�K��J@ @ �@���%n��.�6�����.�@ @ *[�@^e��@ ��w\�d�R[��K��z@ @  �g�""�  � )
5.^�C0^^2�#�  � � �<��@ H""P�q������@ @  �P��  � $���4��e�%���Z��xy��7 �  P�����rm �  ��@���Kvb���L�� �  � �x@ @ �$�/�a���~��9�LV�� �  � �#@ �r�5W�  � Y�b\�d��l����y:
 �  � �x@ @ ��	W�q�گb��d2�G @ *W�@^��{�@ ��z\�$���*;OyXLV�� �  � �/@ ���1W�  � Y(`��*Ŝ�6�S�s`��LŨ�  � �� ���\ � d)P���%;U��K&�z@ @����5j�S�|@ @ f�5.^2�pV޳�>�ܣ �  �� ����>
O�QMM�܆wZuϨ����J� �@-�y?ϵ��@ {u���Y�{�����R����$�Wۘw � ���Q���@�r
��b�&�Wl��N����A��ꨍ  �@a�{YGZA @����A9���_c�ޜ#""�  �  �  �  �� �����@ @ @ @ ���+�9GD @ @ @ @ kyY�� �  �  �  � �`��қsD@ @ @ @ � #/k2v@ @ @ @ @���Jo�@ @ @ @ �Z�@^�d�  �  �  �  �@��ޜ#""�  �  �  �  �� �����@ @ @ @ ���+�yY����鋂  � ��{�܆- � T� �>��;_��&�WGZA @ @ @ @����K� �  �  �  � F�@^ai@ @ @ @ ��
�+*/�#�  �  �  �  Py�q�@ @ @ @ �*Ш��fnQ�@� �  �  �  �  �� yy�  �  �  �  � � �W|c��  �  �  �  �@���&�@ @ @ @ �/@ ���@ @ @ @ ����MH �  �  �  � _�@^���UUUN_@ H.����6lA @�R��A����\7���8�
 �  �  �  � E �WT^G @ @ @ @�0�
�H+ �  �  �  � U�@^Qyi@ @ @ @ ���+�#� �  �  �  �  PT�F555s�zG @ @ @ @ ����˛�@ @ @ @ (� ���s@ @ @ @ � ��7!
 �  �  �  �  P|y�7� �  �  �  � �-@ /oB@ @ @ @ @���o\�G���r�� �  �@r�^&�a � �*��*���	�ƑV@ @ @ @ (� �����8 �  �  �  � � �WGZA @ @ @ @����K� �  �  �  � F�@^ai@ @ @ @ ��
4����[�#�8 �  �  �  � �-@F^ބ4�  �  �  �  �@��ߘ# �  �  �  �  �� ���	i @ @ @ @ ���+�1G@ @ @ @ @ oyy�  �  �  �  � � �W|�<BUU��@ ���2�
[@ �T�}P�w�0�ݸ0��
�""�袋�� � %���_Kv,�  �  � ���K��V@ @ @ @ � ��q8	@ @ @ @ Rе6�[(������k���w,�@�k�;w���?���^8i=6���=�����g�v���[h��\�F�<p�u��Ϛ5˷���Kg��Y��D?/�R
aY.\ �  � q #/�w�s/[�;�ӝu�YNA�B�.��-��""�e˖)�7n����_|�Eʺl,��7�<��/��B�\�xz�^|����-Z���Ə��f᫯�r�,��kڴ����?3��裏�Ǽ��3��J�Z��:9O@ @ �]�@^���""]_MM����X��Ot�]v����
r e�eR���˙�K�x
�������_N����c!�s�h]��MY��*?����T�a}�KiY,Ǹ���˸�1�@ ������������b'��7߸#�<����ϱ;wN8���{o��뮻��յ6����������k�H��|8H8gΜ�;��  �  � � ��[ǉ������i��-��N��\�ȴ�b��x��FCد�_���
7��z���_D9rd��4�\���~��C9����ݖ��h}��q mk���d��o�?�
��s@ @ @ ���~9����t��
��,����ݺ��7n�\pA�|�)��_~����*1h� ץK��K��͛�8�=��#u�^~����?�|�I߮&x�w�ԍ�P7P;�l�����N����g���/�������}�nݢM��{ｷ߮n��R���۷�o[�HTF���:�
0)7p���� ѱ��tl �{u�־�=��S����O_��mTp���+����v��?�<B���N;�=��.�ų{��n�=�tS�L	�O��M�ֆ�]?�p�,jB���^۝y�N]�3-cƌ���Yϵ������O$��
��y睃j��?X1oA����n���r�-��?�C=�M�>=\-X�������|���v۹�=*'X���:A�B @ @ �0km��9�����/�|��'�]�v����:<0����;����O�7�x�-��~�{�￿1b�U��""e�W�^.r�����#(`7z�h�2dH�O�l���>����/�?y�d�/�����\���?�c�#�@S@K���:V\q��ؓ&Mr
8��m���w�����ޖn����U S�'ۢn�*
*)xt�A��O?�
6̏�h3�*��{��`��n�2|��`��?�7�;�u��5��gF�j}]q��w��.<��NU�m���h����B�>��׿��c���':�_�� ����""}) ���z����n�=�\�J�R�saE��X��~(x��[�f'�]v�%x��z��z�)�u�u�9{>�J��l~�#��@ @ �I���z�o�����r��j���PΫI�&n����y�<4(��4�����4h�2�
����VQO���C���
H\z饾�%�\�lՃW�π|pe��W�E4�������o���t�ċ���B]�2���HT�ɇz�o����""~S��N��{�GpD�����*n뭷�o�|m�^�;�QG�(�`��1��~��7���Y���َ�O�z.4����
��x�������+�r�o�ĳ ��mֆ2U<�s����T��ʛo��~��G�Vϳ�0��i�έU�V>@w���~m;����;�1c��U枭?���݇~h�����u�XQ�����KA @ @ � ��T:lC �n��V[Yc�p���}��L�:����)0��B�f͚�@��:*��z��~S֗�^*8a~�:zoA��v���6m�VXa�l�t�D�E��:��Q�#U,�e�S �J�ۥ�+��<�v�m��V]u� �K�`�r�a����@����M
""Y��X����TWVe'*�̊ձ��^�iW��g��\rI�9��}W[���ϯ��7|.'�t���K��ֆ�2�Tn���]�Yd���e@��PQ�*{���?s4��:��0�l�k���� V�\]}U�_~y�^�?��7�o��\Gv�- �  �@��<�g3�^��*�Ƨ�l�_U��t�a{��1�V^y��$ՅS�e�(��_��1���_֋Ϭw��+�}�`��O?���l�^|�|��ë.+�K]Uu�u�]���B_���SQ0歷�
Nłi�R��N����ҫ�Y�5~���0a����mu�n��NmŲ�V[m5׶m[�Z&�'e�-��2~�Ƣ{����-��b�f��O��{l��Y SfΜYg�V(ب�8��-�����D�z�q�y�_E?OѢ��l����W�ѣG$��t/5^�J�Y�����ϵ��kb�B���߷���Z@ @ �h��""K P��5{�씗��h��l�ҩ����RwZ
���q�4�{qT,3Ocu)k)Zlb��	
RXI��m�ׯ��*x����=�J9,�:����Zk�@��׮���~���NU�UʆKT`Uf�ݳ믿���y�X���*0�O���l1սS��) ��KT��>�� ��Ȭ�YR]MH��l�יִ�k�톟5k�cǎ��M�滶+�]w��r�-��k���mB5��z��j��l��f>���{��U6�MNb����a�}NV���N��@ @ @���Jo�(�����h\,��A��Ʉ��
P��k���gd���QjC[���x��v�c�Y{��-�u᠏�Y�0�O���v���s�%�]ݖ轂�� �����9'Z��uq��k���뻗Z�Z�Mt��(K���Jt�������(��2�l��j�bMJaרl=甅��L��f8�����S�7|^o����Xg�u�Yv��^�iW@͆.6��%{n�ς��k��h�g$�	G³ '���X�*ꦮ��c�D�r�ט�*�Λ�d�s��
�#�  �  �@��~"")�9pD(��M���(Kc�%*����R8���ꆨ/Sl�YM�,$u[T o�y�""XQ��D�6۞�k8�J�d/\��e��ɖ?��:��q��Bc��{�g�LE��e����F�X PV&i{� ��)pi=(�R�|��ꫯ��
�}饗�q�4ᅞ%�suo��x*�^ڢE��eZO�c��<T�_���|ڝ:u�[{���sԂ��J���:����J�[e*;�����_М�Me��?��Yc�5���N���'�F�-���J�z��зL�C���  �  � �,�y�|8<�h߾}p_.X��&0�m
�Y��
ج��bR�О={�VX����f�er��p��h�t�A�h}��%�)����~1�Cc��̯ʈ��/l�����L�Wۮ����릛nj��*�25����~�Ǔ��TC�FSf�ʰaÂn�6���0�f7V9蠃��K&N����-�v�_��R=S��=�p`Mݐ�E]r�L����?�nN�>�<���h�gVπ���g��z7n�k]l��l9��kۇW@ @ �y�8��馛| �2��5�`��E���4Ⱦ# �.{͊�*�L��g+�vu�TQ���S����Әz�1��P�Omm��&~�\�٤ ����>
��1N<���-h<0+ʪ���3�^-ź����K.�Ըy�L�=�|�[0L]_��p��iπf�
�G<j�<e��۳emXPL��s���lX��v��|���(�lM�7g??�>��hF1$Uԭ8<F�&���gZA�pVb���]{�c�{��fcք V��f�U枍#y�!��*
��d#��&���D��X�l~�m^@ @ 
#��Sl��´H+�$@ ���6�Z6�9��+����ezQ��`�-���եR]g��׼ys�l1�V����z���Ω`��
�)קOW]]8N>�d߮ƹ�I���̽c�9�)��e������)�o
x蜔Y��k�nݺ�v��w��l�p��j�Pu����M�6�c��ź�茺��>�b.+�Kώ��o�����3jԨ��Ƹ�xwV�
g�i��N�gF�j��ӳ�g�������2""�-Qɧ]]��
�n��v~&X�S05�}P�[ϛ�_]�2���-���������n���r�^�|��?ʦ�D""���=�ܣ*n��Al=��<뙒����� ���}#�o��\�vc@ @ �Y�@^=� �@T < �M2��N�n�N����:� ��UԵк�)�NA�h���K/���OYw�� b߾}����dEjSAMʠ��TR�
F(h���ۻ1c����hYJɮUmF���x����N�n�ጻ�~Zo�սS���Yf�ÞŸe�Y�L�f3���3�e����N��&*�b����24�}�s.�~x�ֲ���9�gDE��A��gMY��&Mr
��(�I<T�X������˶]�O�Yݙ���9l��N�:���ug%���u�:=��R����|p[�����4�RA;�,�,�z.4�d�|���,3�:+����k��_u/�S1b���f����}����,��Ϳߢ���\��b@ @ �O�Ѽtι�wx����m�]ޒ]�}('m8�P4�)q�¯Մ~��{��7�f�U�>ا*�^���^4	���-�\��T_u5(�ƨSvP���	�`��Su�T7Ie��a���5.��VF����R���
}�hT0T�dc���7����S L���;�RK-\��ey���N�.�ڮ�{��̙3�Fm�Q�m�z��5i�lZ��ٱ�s��
oSV�&
���2�,lR��~��S�?����򫟙D3��N��W`PϺ~β����ϵ�S�
����	k@ @ �/@ ��Ʊ:LJ{����]�3��� Iȫ�+S@D�Oeu)3Qݓ)4ty��/��@ @�����<�#�X`�ر>�m۶�+��2PWޟ~�ɩ{��x�[�v��IA @ @ @ ���F��y(�g����
 M4ҵkW?��NYc�e�3&��i""�  �  �@e�[v��lF�� yuHXQ��Zg������4��&ߘ3gN�q�J%��ӯ��ʵh��i�	Mj@A @ @ @ [���V�����
z��Dl
�Uz�Z��K�d�� c�%���{��P��Y�  �@9	ؿ#��_Nw�t�BF^�+�H�F�r��k��b����� �  �  �  P��*���-���SO�4�  �  �  � T� ��J��E�fxE@�I@ @ @ @ $@ /��b��7c@ @ @ ��`l�ʽ���r&�(�b�a�n��� ^�t.@��������5���?�z+h@ ���|��. @ @ @ @ �
 #�nr6��iF���Of��ѣ]۶m�)^@ ��@�/�]y�؎  �   2�x��ҥ�9r��,�]�vͫ-vF @ @ @ H.@ /�
[� ��U@ @ @ @ ����.��%�a �  �  � �Æ�@�lg���D@=}٘?��C@ @ @ @ ��d�%�aK��:w�\�i
@ @ @ �,y�u���j����#�<����-�� �  �  �  s����������رc�v�/ �  �  �  P��ꕿr�`@ @ @ @ sy�[Q@ @ @ �K���&��ٹ�#���?W�  �  �  �  y1�Q�& �  �  �  �@eȫ����#�  �  �  � �D�@^Ln��  �  �  �  P��*��s� �  �  �  � 1 ���i""�  �  �  �@������(�""@ /5�A @ @ @ @���J��@ @ @ @ �E�@^.j�  �  �  �  �@����!�  �  �  �  �� ��\��@ @ @ @ �4.��8 �  �  �  P�555{�\x�d��oH �  �  �  � ]�@^щ9  �  �  �  � ���ߐ@ @ @ @ (� ���s @ @ @ @ � ���!- �  �  �  �  PtyE'�  �  �  �  ��?UUUN_r ���� �  �  �  �  Pby%�p �  �  �  � �""@ /5�A @ @ @ @��Y�\�  @ IDAT�J��@ @ @ @ �E�@^.j�  �  �  �  �@���x@ @ @ �X������v.<2��7�@ @ @ @ �.@ ��� @ @ @ @ ����oH �  �  �  � ]�@^щ9  �  �  �  � ���ߐ@ @ @ @ (� ���s @ @ @ @����*�/
���E�}@ @ @ @ (� ���s8@ @ @ @ r ���� �  �  �  �  Pby%�p �  �  �  � �""@ /5�A @ @ @ @���K|<�  �  �  � T�@MMM�^;�� y�� �  �  �  � E �Wtb�  �  �  �  �@�t��ߐhp�.�h�;'N�+�믿� ��  �  � �.@F^��N @ @ @ @ ����Q@ @ @ @ �z�km��N ��	���n�ܹ�x�F��B-�q�l+����n���n���w,�@��S@ @ (�@UU�?�ז���D ��n'�@]���~۽����^s���[{��]�v�ܦ�nZ�rk���/�A��U~�������>׽{wסC����>0�@ @ @ (yer#�	0��q��6��={��.��5n\�_�d�%<�+��v�ò	��
���On�%��m~������_Aۧ1@ @ ��@i>��߉+@ v7�xc��ܹ��a��W_}���N���_���5�\����#�kS�O�G�g�r�{��'��u���jS]ks���o(su�2g�[�@ @ �, P^L�����w��Ǐ�^|�Ů[�n��lyj�Y�fz�U]tQ�^��.�l�m�7����[p�3����?��*0��c&E݋��dL�~�m��O}�ѹ+ ��	3-�G%S�L�ͦ^2�d�i��u�M��?�>�G @ @ ����6w;�D��
���	���m8Ȣ �	'��������/�,�u������O��jR��6��i|�DE��+��µi���Yd?�_�>}�o���h���=�裮K�.�r�~��;��C�ꫯ��o>׾}{w���'lgԨQ>�q�%����:��Ӝڳ����뮻�c|��n��j���s��r��?i�$���{��͛��
�N�8њ��:f���{��:��^}��~��Z�xs�
7���_~�%�����@��7��KϔL/��B���/�jOY�ڦ�����QG�w�}�
ꩽ�/��u���_����>��3z���<x�o�������_|��ٲ�>���N���s\p��`�
�9t�AN�*C���G�ӢE?�2O�E�4ȷ�{��w��Gy$Z�K�>n�8��s�>vǎ�����;���l壏>�������6�����z�""�  �  �@y4�7KJ�SO��W7O�fω&rű�f��U0���}��`2���;
^����%+��Rn
��@Ͱa��.��R'������ ��**�/��SъZ>�`����=s�5�����Z�^pM�6���qx��g4مu��S
2��Î� ��Vp�v�m�m��W�I��V��1>��[�����ǖ�a-[��*pٷo��}x�k׮�m���
��h��1�����yY���'�x�m���~����Xc
o�@��?����^r�ݰ�n��ѣG[s��ƈSpn��VsjGE�:�u�Yn��vrÇ�jaʔ)n�u���T [�
7�Яkݺ��<y�_SV���G��+��������p1bD��߫W/w�E���V�~���[l�4H������g�9昄���
+pl�K���;�pz��sC)� /Kg͑@ @ � #�?�(��6��g).��E*�JEe��t�INA<fXQf�����+���	g�]{��AO�u��={v�N��3�<3�Ӷ �� +��@�.*
R)��,oTlQ`T� ���ׯ����+�����z��1c��>}��ԩ�ߪq
U.��R�L+b)P� �\-�� �I�TQ�3_;V��
�Y﮻��>:���{�GՎ��Eצ �p<��;�s|=v��O�E��Vƅ,
��~����{��7�.�
�j���zt|{.U_�>[�߿����F|�|U֞��%�\�1-
*��,Y���6T����1+-Э�:?���h��6�xc�I0���9Գ��Im( �  �  P�*�>s�&�L�&M�3`���ÿ��2įR��2��N�_�}��  � ���tL2N>�dw���CZWI/�q���ꎨ��ꊫn�
���v�m>����������5����䡢�ɷ�~��j],�-��+���;��s}}�:��y�JS�Qu�\~��ݑGlֵ)�n���;���6p�( �r�)����>��Oe��2U�ͥ{��9�o
ة�>v����KuGU�駟�Y�&�m�w�AHu��
 ���U�v��PM¢ 릛n�;�E�ԕZ�RսVsV�ݧ����3b�<Z O]��uXE�{챇[h���x���
��z��5�MϺ�ym۶��}zU�l�U����Y+ʲ�y�(�TE��6���C=��ڵ���y���  �  � e+@ �lo-�@m���z�뮻�oPW<>ԮY�w6�믿~�ƭ˪����i�h�j���U�U��bAR��.�ʲb�P
�h�>e�-��2~��S݅5�]�b{VGA;+��g�XRQƕ���""�r�����7u������U�`�8~������_��Ɏ{�a��
+�ϊ��pQ���O��{Yݿ�EC=�{�@����XV�+(����b��=���u�pT��p�N��ς���~nU�M����u�U Z]s_|����L��@ @ @ ���}��=�PWAeٸ^
Dh��DA�tm�]]GU�-0Q����M�:5M��H
�����R��?�b�M�}[l���`Y�+u�T�Peh%ʾ*����t�,�T%��r�-�LU5��(��Ʀ�6����	+ʬK6��Q�S]B�a�`�23)6.���d#*:~4Х�ʒ,dQe�h�d��}
ױe�S��k]r�%mu�6Ф�c-��rA=[0seɦ+�Q?�:������5������?[��c; �  � �W�@^|�g�@ZM����dP@u[�er��KR��^Ɏd(�b]�^z�-*���@LiV�@�i��L<e���+AU%�]w]�e݀����d�AUQw�T�T�Y�f	5ꮫ��8ڮ`�2���ui{�1���}�R@I>�*�������X���L>ˌ��c�9�>��o��&�&�>z|�����E_�����u���Օ9�Oѳ������gZ�L�z��:���9�""�  �  �@� ����g�@N#G�t֕P�8h����^;����i�UW��[��h[���j�Jo��ጴ�¼e�Yi�UV	o*ʲ2�,إ��4Λf4�E�@^>'djC���m�Y��i�=eo��e�i��p*Q�Yx?��&e�i��M6�$2j��T��p;Z��V��jt�u-���{uu�����/�����[o�dA�B+Q�֭@�� U�Z�#�g�������` �  �@���M�GA [��)!ٶF}h0�KEA<}Я� ����k��4�g�h|/u�� �6V�2��x�hugc�iC)��;���p�A����+�i����+��9�Ye�eڴi��e�,Hj����:~B
�Gh_�Mc�:���`®Y�o��
w�Tp)����+�����N�]�K�ز�
;Z�ܢ��ޞc�e�O���Lº�q�urYV&��՗e0����k]�S=s9.� �  �  �@� ��0�g�@^�L�.��T
$��i,��={�/�̶���T��u�y��A����^{-�0�3��5��'t>?���Uw{�_�~��fy]a��m�Z���ƍ�����0+�f��Ju�-d���)x������ꫠyM��� 4هi��T(�fWUQp8\�ꮹ��*��]V�Q]rU�?���[���ǁ{ꩧ����[�R��p6�f��&�N�iV(@gc�iٳg{�ώ�2s-�.��%W�iӦ�ޫmeO����uXF @ @ �� �W^���A��3�4~�ƛK�e����J��.��*
�Y�MA��7��O�����P0G�9�w�UW��g�yƏCw�QG����Q G㲝s�9�N���l������c�=֏�I:lf[��ևK�8��.��_�������#���� S�L��,6I��jk�<�|�1ܔ��IHTt�Zod��OAHM���{���jS�V���p7=+6Q����f�řh&ل
�R٢6	�Ʃ���5v���/�5iƹ瞛����C��*
�w�����kݷO>�d�뮻� ��u[�σ��9����  �  � � ����I""��@��i�=m`~{U�t1$j����p�^�z��n�ɯ�xs�h�xj*��0a�[h���{}S�Q�4�P �[n�c�)�P�u�U`ˊ3��dۢ�v��Ot����
YPI��t���aҤIN�6�4�����
�r޷T�gu�U�h�:C�RF��g��@&�Z�rO>���Z�ټ&;�tm\x�~�TՓÍ7��I�v���jZ�@g�z�>|��m��k�z��5�M�����<#֝V�i���ム���/��ԾV�X�j�ï�-|����٫�o��v~�=�ʐ�5�s�=��5s�1cj;kݎ�mX�vl�p���YQѳn]�5.�G���9;����s`?�=Xc�Q@ @ @���\�����߬����,T7A��� ��}ll2��5����Xh����єɕlvZ�������G�0�ۭ���J���P�LA5�I��W��j{˖-]40S�3�xqN3�jY/(pUv=���7ʸ�p�
������?v���HĲz�W�ou������j�Ǯ]��'P���\�6o���|�ͱ�����SA[e�)Ȩ�^Ȯ�
�Θ1�w�5���g��5k����s��bu���N������H�h��.��l����[��}~��>'��X�{ȫ�;���
��$@1Y�?�cr�*�45���
U�mU�gXQ�R�HG��z���.��b��k>%��a��/5������^&�a=u�wI]�8��-�@ 7�����^ � y
h�9��_�w�qn����K�2�^A<u�� P*�5���y��N�R���8 �  � $ #/�J�#à<n>��]���Bc&n��fI/u�ر�S�NI���?2���wI�<��Gm)3/�����|�ٿ�J��9s�8�|�i�p�9LE��Y.�44�2�͝�[�rg�(� ��R�7������ߠO�T�`��t��@J�s�	M4���5v�f7�ܹ���$��l�@Pl!U@���c��
��D��'N��gz��_�ܚ�H_���3�8#�f4��
��NT�Y@c�j��q��9Mr�߭�)�*\�q�_?��  P�-Z�p\pA=��G������e=�r�j켺��L`ԨQ~#{~�q��СCkM�ޞ�r�c<g[?�sȷ�y��4C���﮽��|����&��b*
�j�R���^j�� �#@ /=�E��
�?�
��pZ K�D=��g�y�g��{^@ ���?�,�*�Z�r����kܸ�2d�+���w�s�����7����V[�)h���G+�9����f/FQ�V+�LA �� �]��q� �  P
����������l��ӗzH.�@�ʦ�n��*hw�Yg��^{�g�i�&=*D�dJ���z�/�ՖoQv���ʭ(��wٔ��9���E��4D�
�Q�\|N�rbc'@ �T��EA<za��+�����p�[|��kU�}����/������ﵶ����v���%�w��n�vp�쳏���߂ә<y��֭�寧���4c�]wu�<�����vU/Q�{�����~�}���~��;��U}�Q�������]\4h�_��K8�9'�Gy$��j�ꫯv;�sPe�}��������kuu�[j���b�-�q衇��ӧ���S�Lq�:���4�&�� � G�@^q\i@ *@@�8���RzpùĜ�Zk-w�A�����(���'�MWʢqZ�^{m7z�h��<~����O>����K�^;v��Yd�Mc�m��>��BS�m�
7tv=?���oײ���]9r�߮z
�4��^�~ڴi��&Q���c���u\չ����n���3���f�N�o:/��iE������s������~ک}\u�;�ӵl��gRھz>|�[g�u��?�멾�{ꩧ����_}�:� � `��A�K3�ޛ� ����D%���Qj���'Y�>���������������<������������feT�O��J��e��lG n�Yk�ܕy��W_�����WkrM�k�߿�;��3ݲ�.��뮔ͨ��k���(H���'�|�4��[o��4V�|
dil�'�x��7�|���w+���Xu����p�
n��V���SO=շ���2�^x�`V���
;1e�)��Z���}ۚ����AOe���݂.�l˔��,?u����}e�����LV,�1c��F�y��7���:���w�i���t���s�=���U&��$���NAP��&*����|V��f���{�i|�����Z�@����9ٝb}%ؿe�}֫�1w&��ݎ=@ @ ��[m��S�^&������2�:ujL�������Gmo�zU�m���Oن����*�Bz�=�����y
d)KN�(p�`��x*
6Z�ڃ>�5k��+[N�5%ǌ�r�H�I�&�|+3Q�U�����\��n����:���}���r{뭷��)������>� ��
��^{Y�Z��;M��A�S���䚃���ن�_~y������������-
 �@a�Ʊ""[I�?�� �����tY@�'�a���F�e|Ꚅ����rч�p&�u��z
�P��@
�YQ6Y۶m�q�g��zM@_u�Uk��c����w�0`@�M�f+��B�^�k*΂x�Qc�Y�a��N�6�����~�����K�*�Nc�)�MYw��N�$+���/���|�QGթ��+�����g݆��y_�G�u�C/��~?�Wv#@���
�Ik �  PA�%V�L ��n<����C3g�t�}��{饗�M7��4���ժ�k��W��Z�r�ƍ˺
��<�R9���2���w��o�7Z�E�n��.~��oR���X#Z�ފL�MQƟ���[�k�n��f������͡u�ֵ���QW\
���?E���J �W�w�kG @ ��.���Z�����2Xz�}X���q甡���
�e|�����4+��-��YW%�`Y����/��n�׌�V�u5Z����������Y��M��6X��4>��9$
,j���mv��� � � ��ڇ� �  ��P �_�~�4ģm-� PG@]>�����?���5n\�#�2ٔ��YZ'M�Tg�R�иy�ʐ!C���ӘpV4����+�C���U�?������TE��eZVYe������kr!�ƿ���*4H��l�dU֣2�O���߿���U�P@ 
+�Ϩ��m��@ @ ��P O��Y�w�����(X@ ��fu��h�4e�Y�M�꣨+�W^���hm���zj]FG�Y�45S�f��̱ʌ�$V��jݫ��j�ҾZ�Lm���N���Yǎë3^�x�V4;o�(�h��/T�U��&*ֵټ�a �@n�rsc/@ (s�D<�lv�2�|.��	(�l��V��t�I����V�駟t��l�V4v^Ϟ=ݍ7�h�2z����ݴi��~��믾=�P۽{w�ܮ];?{����~���+l2�nb����w�}w7���-[�6����o�zmv\������s�kEA6)Kp�M6��I_u^V�J���.�Ƚ���V�Ͼ{�����ܳ�6,QAD}��-����BM^n�e@����`n�#p��	(�@����ߖܖl�W#�V�@ ��	D��Z^8��������䪠�{��/F��jVY�i�SoM�������a5f���&ʀ*���}�<�����o��V�;��Cݝw���)3�p����#���L���u?��_ߩS'�YZu�*��7�|3�u���wֶ-����^�u�
�s�=�ڷo��6,�0C�e����-KP�{u\����G��F�4�D�&M�v�-({O�?e�m������U�S����a��v�kL�0�����w3_��k&`����ʱ�뮿�z��o͛7��}�j�q-�T`@ #2�2b� � T�@8�� �e���x���5""Ph���D
�1?� �G
�XF�۲����'{ʹ�����F�
�x�F� ��v�a�s�ξz�=�.���Ypڠ����y�=��
~) �2q�D� �h�և�[:ꬾe�)��ci_�X��{|�M>7� ���ի�o_�e�q��}5m�|����4h������A<�� ��u*���\sM�SPUك�bc���7��e*I@��a�)XI�͵F����8�M+�˄�����\ �@�
橤
���2KT�W��2�(룏>r~��GNY\
\�ϔՋë�Ec�)3Nס.��%z��|��i�<]o��\5{��z�����Aԭvʔ)~�	���cx��q�����݌3\ӦM�2�,T���ݧ�~��~�m��I�]w]�~����.̜9��W`P�~:��=�ַ�d���(`���V�w?�k&���aY�`�P��m�b@ 
,�����\Yd�)k ..� ���,���w��
���L����@ @ @ @��4s�رc��¸�X	ȋ���d@ @ @ @��v�yg�뮻Ы��q�<@ @ @ @ C��L@/C,�\�@^�Ii@ @ @ �] ׀���c|�r:�w}���4-#�  �  �@�0�A�?\?�/����:w��N8�W]]]��֛ y�Fρ@ @ @ @�\r��+���:J#@ �4�@ @ @ *@��^��z�Dy��ϡ@ @ @ @�<��}��j4o�Ź�}��TUU���;�0�E�F�|{��p�1g�  ��@6/�oa�G�@ �D>OW�]O~�d�%�a �  �  �  � 
F�Yk̭�D@ @�2�r��=�ǽ� �	@�\^x�?3m����o�6���}�I]�T� y�s��R@ @ @ @ �ȋ����@ @ @ @ *G�@^��k�@ @ @ @ ��b|�8u@ @ @ �_�Ν;��Ç��Ip�`�����\( �  �  � J@�N8�UWW�I�A � ���DT@ @ @ @ � �ǓP���S�c#�  �  �  �@lԅ���ܮ�<Qyey[�(@ @ @ (�@۶m�m!�� �]���N �  �  �  � �V�@^i�9 �  �  �  � 9	�ˉ��@ @ @ @ (� ���zs4@ @ @ @ r ��;!�  �  �  �  PZy���h �  �  �  � �$@ /'6vB @ @ @ @���J���@ @ @ @ �I�@^Nl�  �  �  �  �@i�֛�!�  �  �  �  �� �����	@ @ @ @ ��
�+�7GC @ @ @ @ 'y9�� �  �  �  � � �WZo��  �  �  �  ����	��S�?�u�JݫI����""2V$�_E�	~\�%2d�dH��T��teh0��""�+CH�d��}W���g�}�=��g?w�����g����Yk=�������E��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+ ȫ�[k @�  @� *	�*�9�  @�  @�@���z��F�  @�  @��� ��� @�  @� �+0���F�  @����4i��jZ���c�=6����T���NO�  @�  @�A^
Ț @�  @�  Щ����
:�  @� �!O?�t��pe�y�I�}�s����.	��_�JO>�d��k0^J�~�}5^��� @`vF��N�g @� D��L[l�E�����G=/x��}L�81������|z��;nO#��7����k�1��d�v�!��A�'=�MT�U�s$ @� �q!p�Yg�����_�ַ�����Z��W��U:��C�
+���㎎�\y�ӄ	�9��Q=u<��;s�̺N��vb�ak��O~��'^��׷~�=�Z@��חO�	 @� t&p�w����U2��n�a��?�1���-����M��>��[oMӦM���m~����1m�J���L[���SO�MV�m�� �~p��~���O�  @��@�[l����A[�*f{����4�|�Ͳϛ������+�|����?��,��ʆ����?�!F��h�n���0�""��Dx��3Ϥ�?��#�}����b�F���t������ Щ��?�:��� @� ��@�S�+�H묳Nz��^W{_�[n��������Oi���oO�m�]�����=���{�t���7��>}zZk���o~�-F���86��k����v�m�}�C9$v�g66�_���?�����(-��2ya�o�1}���ϟu�Q��c��K,�C��*{�q�5��Ŝ��ň�w�1���ɓsص��˧��]�����������/�Mf���_*#��7�m?��O���h�[n�%��JN<��ơ1�r�=�Hѿ�F�n���k�m�S^Dh|�)�d�&#���~����G-����^�!?�f�m�M��{��[�=
ƃ����3��9 @�  �����s����0��[m������܏/�y睷���ô���7�ǋ��^<N?��<�o饗N��w_�袋��4�x<��#y[�W�7v|����_��m>�좾���C���""��b�>��3dc���0r�]vinf��9�7�u�]�q���6�]u�Ui�-�L�w^�ౌ �{��f�S�NMg�qF��/~�\p�!�ɛ?����|����/}�K� 5,�D_7�`��]�+��{l:��s��o\>��q���qn7�tS~\p�9��4iR�|�׭Q��������)�[�F)���=�-`D^���O�  @��b��~���W���7�+^�Z{�׿�5e��x����?��i��7��7�d���FL��k���""�)��Ї�C=�""ԋr�	'��o}�[���_bь�;��b��+%ª(�6cƌ�?�!/��b�W�<�̩�;��S�^��W����w9�������<��\��g�=dd�1������p��� ,�+wR""�Q�1r�{��^Zo����G?�����W_��;,6�t��\�zF�kVB�SO=5E8�#)_����zbq��*q�1�D��=�<�r��Q憀 on�k�  @��\�a�r8�O|""���VW.���<�2�^�GL�\h���OL��k��]8����(����my��[,�#.�_|��y�X�6F�͗��%��Vq?�x?�{�5�Ëŵ뮻��H�rH�{[l�E���׼&E����)�+��綾^��b�ڲ�.�G�E����;�O}�S��}��7?�(Ø����Om�Q�����T����=�����4#���K�z�4��%� Ǵޘv1��vF���s|y����}�����vӟ��E��#JL���k8�*�N� ����̥� @� ����ň�]�p�,eZhL
-�2����Ps)�ɶ�~�Yy��j�z?��Ϛ��D���l[g�Kp�UW�Y	�bc�i�Cv�ML��w�+��Mo�e�5�\3o�3���|�͍}""Tl-���j^������������}����&����D�eʔ)�9�Ĩ�=#��u��/�v�m������ o�>>%@�  0n�f1%2�Yg��b1�n���ܕW^9�#�bz�{���t�Ȋ)�QbZj�7-J�|ы^4�#�(��{o~�ƗD��;����ne��}���,�#ە�p��;��Szc�é�Ĕ�xD)u��3����N�.!b��kw��ſ��/����Z*}�3�ɯc:mL����S,ar��
�L�b#s� @� ��8����p衇6^ǋ2j+Vs�{1,FKuRb
mA�\�*���>򑏤%�\2�?-F����b_)15�5@��ʶ���2� pv#t+�,���B�J�
�1]�L�]`����cW�����v%��:��h��l+�g,�q��G�z�XI�q��""!1/�h]ह��zv׭�� �Y@�7���s#@�  �$D�a]�#��Qe��v�
�-V���0������y���Yx�Ľ�>���
w���7�w9���me��|~��_���~v��D)1�Lin4[��+��^��r�<�}���|���_�HJ�yq��+ߖ����/��5�= [��~�n#9w� @`,L�KMu @� �a����bk��{�E����睮v:��^8������2�䗱�Bk�����CIe:n�>��Ǫ�������Q��:F'F��w��Wum�O�G.JL��Qre%��w��׶3���5���w����{[��Kb�=��3ӧ�}������T�]v�%}��_�����1ox�K?\��W��K@�W��v @� �e����8�{�)�q?���yW���Х�;V�Q��>����*�t��.�,���Ӂc5٘ܼE���5�y�{ߛ���_O�<�L~El�������Ү�o�q�N�Y��O�[�w�5פ�N:)��}�������T�צ,>��Ciڴiy�-��2-��""��X}i^��NR�����*���(�A���;.=���C�/������J�cq݊q��i���� �/�. @� �S��#�HGydj��9��0���{�m��6y�֨�WL�2�?������޹��K�[lXa���=��#�([wX}����X%5V���f#<�{���Ү����������;���;ޑ���˦�Ĩ���:*���KR�d�t���Kox��!�����O�g,�D�����)���l�Ac�X0�,�0�Ak��~���Xc���V[�zv�y��_�⺽�-o��E0�g�� @`<���Ut @� :(#�"" ����c�~q_�ٕ��[)e��8��/̫��gq/��?>�j�E���N;�����[o���
1�����k��m��E�o%�*}��Š]�˶�Ѓ1  >�IDAT���2��c���SQO<��\e�����?6l��f9��{���\~��yu��b�^�s0��F�Sn�4�8����g�����=��s�����?��rU���'S��,f� �\��}� v�ԩ)9��""�(e,���_��J��>��� @`<Lxv���x;��9t$P�7���l���o�٫�9� zU���^�2�5'�I�&�i���<F�Ţ
1U4F�-�����p�:��SO���}.�袍)���Eo����؋`+՘�ev��Qd�D�,�2eJm7��qO>�d��[�1��?�i�l��Ży���믿>�y�)���跗���m��~F�V7�;b�d�n��X\���>��O��ö�����\�y�����߃m���/�. @�@�	̍��tPv(S{E��B�߂��@�ɾ��#*�)07~?�����7&�f��  @ J�W�ۄy�� @� O@�7x�� @� } �.���H	�����"" @�K��.���  PE@�WE�1 @� ���
�uv� @�@D��<���>�h�H� �Q@�W#�� @� ����_cS	�x
/�X����Tu @�fA^���#@� �@	�N�� �5(� @� h#0aƌ3�l�i@,�=��i @���:�y�.�+�?�{Yy� �3�N~?hw�&Lț�~�Ngp��7��ޙ @� 5
����w�<2���^cw4E�  Ї��>�h�L� ����ֳ(�/ۇ�|�~�/�~��o�?����������C�@=����n�L#g���T�9#��~ @� �Xm�������� @� -��2�o�_���/�/��pн�?��@����\rI��-⹔֩�~^� @����~0����ӳ���y�{�۞�xڲ�H� �t��]�W*�	� @��:�����
�ک�fj�� @�@�1�v�������A���k @� C���፿ � @� s˟�1Bo��W��^{���{�!ۼ!@� z[`,?�35""������ysK^� @� �g�G孲�*�dڴi)�g�=�"" @� ���f
�	 @� sQ��/L���ŋ�i @�@�z��� 0x�	��;c @��Hy#Q� @�f��=�mk��#@� ����.z�b�BWܜ���>�F`ҤI��ݾ&��c�
�;ݺ�����.�,�3/F�) @�@�	̍�zOA��- ��p����>�`��y�	f' ț���:��=� @� ��)����y]{��L���+�? @�  @� �Ll��&�?��3g�'�xbT�}�s�<��3�cF��O>�����y�7M�0a4�������z]� @� �+A^_].�%@`8�����鷿��p�����i�5����>����Ӫ��:��m��
�/��Ҷ�����[,���i���i��V�*{���z^=�� @� �A޸��N� ��}�k�Cb�喫�ň�і-WG�ҷ:��i����+�+��""�}��i�M6�j� @�  @��y}}�t� �""p�m�����ֲ�RK�n���VZ)�w�}�����mS�NM���'�|���=o�{o�<������v� @�  @`�y����	���o�9�əg��^�׌�YM�81��e/������ǁ���?�_��a�h� VZ�00��F���yN�kE�}��:9�g�y&�3���%��%@�  @��X	t�rc�� @��@97�tS>z���X������>�b�ދ^���X{���UW]ն����oi�wL���S,[����/����o�����b���Zk�?���_�b�ϋ_��!��~��i����m��+��b�{��}������g|����6ba�����������v�[o��*��M7�4~�
7��<�����7�ɟ����}�5��O?�N9唴�2��01��x���~6=��y_ @�  0��3�c�)9zZ���s���K/�C�3f�{�'�M��]�;Ｔ�6���x1*��?�q~D���O���-�ܒ�]w�t�w�me����r���H�*+����?́ܞ{���G���?L믿~�/��W��U~�~�鹿�%F
����p�����k��&�z�y��.����ݗ/�0��~�JLgn>��[oM�x�G������G�84��a<.���t��צI�&5>��  @� �A����p�����;�@�w1*k���Kox�����漺k]D4E�������{�����ac����|&�{��~;��S�^��W����w9��c�<���8/���ȼ#�;餓�����\�<�6�|��:�����SO=��׽�u�O�S��H����""��i��E��N;������}(=��C��q�	'��߿��o��*!^����?�q�-�AG�����d�-ڝ  @�������16�Q�x䍷+�|�@���bDV�ΊP,�u�]������|�+��s�1y�YJ�����T�h��xE�
ZF�� ��K/ͣ�b�I�����q;�s�ԧ>�}��7?W��]y��c�XZ}��s5|p��������o}+�`�{����/�8���_~y~}��W��a�3���D���ۧVX!
7U68�/я���%���~>*��xm6���w�;O�""�;��r?���Q4kW @� ��� ��/�� 0�?��y��~�����~8O�����*�Qv�a�!����.|)�W�d[B��L��.�������1�5ʻ�����7�)�n���k����֪%��:�8��F��ua�B�]�F���E��Ľ��\�j�'�xb��/���杺�eʔ)��c��w���<""/6ƹD?v�m��>^ @� �C n�3�G��H��}���f
�	�K���\L���.F��Q""x:��s�T��Ɔ.����s�1b�]���Qb�`��xC��kW�ý2���~sڶ�:��%��ыQ""t,r4?��9bzp���y�[��鴫��J
��F�i����.�˲�RK���%��Ƣ""�z��+���(#4ǲMu @� +���U}�,A�`]ogK`\
���/��AYd�Y�/F����jy{	�f�i7�}�D�Ԯ<���y�^���|�����֕d˱e�x+�V-��ls�iȥ�HƘ�������c��ʻF�w�%���}�k�9�{�UoG��(���K?F��y�>���V[m�X��+��#�Xb���`��ُ  @� }# ��K���x�g�H����ڕ��%���R�WV�mm/�}F)�T_��W���PG�R�K�-��b�v��-,""���ߋ�s�{����6""�z����b���.�,�
�Q""��6mZc��^�u�]�}4���Ŵ�XY7FFH{����.��ro�WhG @� � �.�. 0����>�W��Uj�M�}���ȭ�!��v)m4O�mns����m�b��K�����w��������ȸ�{��;����o���m������""p�����?Zw�u���F�z1���CI�����>e�c,NQJuX�7��e�h�cA��W�%��)Ȼ�K��W������NF��J|!@�  @�@�	�z����)��Zj�}��'�w�}�
bq�22+F�m�ᆍ�bq�~N8�ƶ�x�����j""k
�N:�T���}��l����9B���̙3������c�����y,���QbT]Yt#�G=����z���|gl�#��}ص�~,�ǋ��E)����3Έ�\b�m,<2�R�wX�⸘��8����&�u=��#�m�\F6�5 @� �Y`b?w^�	 :���y�׾��y�e���o% :��#�L���Nc��������XI���z��}o�b���_?��ٛo��120��t�}�b:�^{�;�|��""�<�sr�b�-��r��بg�m�I'�|r�{˭��i�M6�!Y,rQ���w^�w�1/r�����^{���O��*#�>���}�~~�a�wNg�yf^�6�M��6���{���??�r�)i�M7��f\�p�t��}H�v�M�  @� �M@�7ޮ��!0��zj�r���g�J�ҕ���:�!�3�g1��9�ŗ�7Fѕc͇�T��v�-��,��㮻�ڼ{�s�=ӂ.�G�5�h��f�m�G�5/VQ�(�0���7��7����s_cdb	��8��S���vکQcL��򗿜>��O�д9������l�1/���WF��H�I���m��Ю�q?����EP#�b:�\�>�����E��\""
�  @�@/
L�<9w+V�U�V`³�8���5ڣ�?��?(���:9ɸ!}�@u����	L�4��q,|q�M7����W�Gm��o�5��Uw�
7�)��ǚ���t�7�xc����Ӕ)SR��n�hݭ+���޼`�C=�b����/?�=�""L����õp��1j���x͝|���T��<��WV�m�gv��z�|M�ߧE]4�Wb��c��(q�c��XŸ��^�膀���PU'��{��O��=�g)�k�>����""���9@ݟ]06@NuA�006w,��eǄ* @� +P~��
�@G'n����܉@����/~�I�%@�  @� �� o`.u�hx�m�ݐ{i�N��  @�  Л����2.{�1��W��'� @�  @� ]�uW���xg�}�6xE�  @� P�����)��SUC�����!^�1������  @� zV�=�z���o�""��|���r�-׿'�� @�  @� �0""��.H?w'�#�8""�q��|�N�  @� �IA^O^��� ����� @�  @� �)0�ٛ,��Ϯ�u7&O����9s���\sM:���;�7���y晧��N �5�������Zh�m������|8a��ͯ��g @� �vF�S�mD��v[����G��p;=��#�}d;�y�x����� @�@o	�4���[ץ_zcD^�\���Y�AɈ�ҥ3�<3�磌�*�F�|�9��_~��ۑ ���)Sr7�4�nN#�ʹ�W$< @� ��/P��[�7��u7�P��
�>����2� ��n�@.J+��R��3��N~^�;AA^;� @���(�G������Y=��
�p6�l�t�W�{�-��r�s�Δ  @�  P�� ��Ak�9��sw� @�  @� �n	�%���^)o���K� @�  @� y�2z�K/�4�v�iI�7z;G @�  @����{�?����u�� �n�no�M6I��p� @�  @�@GV��o�\Vϩ�jm;�٭�w�9�E]Ԫ���l#@� �����eOC� @�@���Z�{�1�c���H	 @`��y9��� @� Q���A��Ι  @�  @��y}w�t�  @�  @`y�x՝3 @�  0W��6s��kA^__>�'@�  @� Aޠ\i�I�  @�  ������|:O�  @�  0(��A��Γ  @�  @��y}}�t�  @�  @`P&ʉ:O @�  @����1c�����X���>�x�N�  @�  08�����Δ  @�  @��y}|�t�  @�  @`py�s��) @�  @�@�����: @�  @�����Z;S @�  @`.L�<9�C!PE@�WE�1 @�  @� j���9 @�  @� UyU�C�  @�  @�fA^���#@�  @�  PE@�WE�1 @�  @� j�Xs{�#@�  @� ���3�ܝx�F�un� @�  @� ]�u�X @�  @� :�un� @�  @� ]�u�X @�  @� :�un� @�  @� ]�u�X @�  @� �L�<9�C!PE@�WE�1 @�  @� j���9 @�  @� UyU�C�  @�  @�fA^���#@�  @�  PE@�WE�1 @�  @� j�Xs{�#@�  @� ���3�ܝx�F�un� @�  @� ]�u�X @�  @� :�un� @�  @� ]�u�X @�  @� :�un� @�  @� ]�u�X @�  @� �L�<9�C!PE`b��C� �v�&Mj��6%��c�
��:Y @� �0""�>k- @�  @� �, ȫL�@ @�  @� �	�Z[������>����t�w�\0-��i�%�L/x�fi�'�H�>�h�w�y�|��7��6���?���4al=v����'�|2��_��uG
 @� A@�7W�9��}����#�<��L��V[m���Zk�Yo~��_�m��&�z뭳�����,~��i��7O�y���_M���g��믟�;�Y��al""T�@5J���{��Ʀ���%��(W\qEZi���o�' @� �o������>OgG�@����o�""�,�>��O�/���[��V:����������V[�T�tg�̙�K_�RZq�!�+_�ʴ��UVY%W}���綶�~�N��t��+��G��s�9���5*��u�6��}��'[��m���H�  @��3f�H�PT�UQs�
D�#����׎>�����~7�jqP�ҋ`��r�禝w�9W���K�o�1�s�=�{��^���K�<�6�l����'��N?��N����?�x>&�*�	���\�{��SOM�}�k�7{M�  @� �q-`j����N�@��sL�bj�/~��b��Nl��F��K���wr���[V��SO=�v�}�|��.�~��_�I�&
��/}i	x����|�_H[o���}Z�<��3��}n��Y����7�H����n�i�q����1mu�đ�s�0�}�f�Y��?��ӣj�]U�}�#��aUl�Xi�q��?��M[�$@�  @�@w��뮯�	�@\1�5JL�,!^A�m���G?�������l��Yg����:�YB�R�_��<0��t�My�^��<_}��i�v��� ,�Ğt�I���s�U_��Wr��~{�o���HW^yec��ӧ��1�8������~��}�{�׿�5}�c�}y�_��X~�������Y�����b���_��4y��4�<�������,��>D=?�я�[l���|.x`z���C=�>��O�{�����ꪳ�b��~�CJ���_��[�.�����c�s��i�њ{�G�s�qq>�^{m����u%��c4f�f�?�������M\�xD����(?�[o�t���j�q-�p��ak��k������}� @�  @�@�&<;/�?72�V+������}�ʨ����[����}���
oxC�#¡���%y�Z,v�ǢD�t�G�i��^�i��w\^�""²�ڕ�?>�߯|6u��`����^p�i�u�Ig�yfj7R,�م�4����)�%�+�Ex%B�u�]�f��e�M7��(.�����׽�u��	6�����7�u�]m?;�3S����o�M6�$E{Q���t�6Q���?䕃�u�\+�SD�7�`���ž�m���y���/����?�)�w�嗧w��]y��6+��B���]i����n�ױ�{�}���n�myڮ.�Ɨ�����z: @�@���/WJ?	��o���˘V����%����)F�-��y�S�~���::�2�MozS#̩Ra�1q��Aӟ����V�N8�Qe��Т|�k_��d�����S�cQN<���#�""����E�z��[����~�/1-B�p�UX�p �� Qbѐ��e��v�!^��w��]�����{#@;��ێ̋�}���w#�\t�E9T�����#	���S���en�)��#�b�e���:j�@,>�?ľї0��3F;F�d�#X�ќa+G�ER�}��T��/�e��t�x.�eamq��������1��~�����r�;��f; @� �D@�7&�*!@ ""h��T�r�����OLO��	""4�Zb�U�W��U���/1�,��-�袋6­뮻.�iQw_Q""܊�1,��.��R)�
G)�Ĕ�yX�6�|�����p%�t��g?��*�+��R�*�̎=�ؼ=�n�ᆼ�G�F�Q�ѧ�O�N1J��\��oާ�K�Ӓ��1�ӟ�t���""���1B���\u�U�}�_Ĩ�����Xb�#�(v
Wbzl�dQ""@�Q�q�׼�59|,h	ò�z
�x�lWb��Nl""���7a<�뮹�KK�{>Fy�[ޒC�x����q��5�+ @� f'���sg������.��mf#�[k.1:,V�QNRE�P*�W���(�tRb�[���D�UJLۍ�<c4\m��<c�Z�_���e�Ž�˒K.���[h����""4���e�5�̛��Y�wƈ��7^)\�RF���o|���dcq��R������)���W��<���?�/�/��f#�p�
��򍶔��Ul��f�Y�~�}YJq9�SR��1%:F�b+1�2^��V @� tK@��-Y�p��n�T��'B��vZ�s�ȩ��X�,��2��;�sD��
��(SS�A��Bki��X��15�JĨ��*��bZj�%��FYn���V#�n�E��Xc�2����>��V����c.�}�+^O#*�6��roĘ6���+e�b����8����a1-{��,(R���/�,͕���ߧ{��Wc�m��Ű�`�?�A�&<KE6 @�  @`yc��*�.�ł����cS	^F��p1�kN���*��ģ�|+͔�}��p�1�vʔ)9�+uĨ��o���o?�a#�^B��~�UP���͡S��L��wvS[��{�+1�6J�3\}�\c�[q��xT�6݉M��I��.F;r�!��5F��|��y����=�ꑴo @� �| 0f�!Ok��4�x�g�y*�[��@eN#�N;��FsZp��cˋ�WZ�5q��:��i�q_�2���Q�-��e�]���K.�$�������~���>��sO�}c�RF�}G�+��+徇e��v��6ݸ��p��/�����V�ML����?�������p�y�5�k��( @� 薀 �[��%0�qO�R�� �}<G UJ,�P�D�Z�������f��Aߩ���?�l������Ӷrﵽ��;/��r+%�NK�/_������b���W_=���z��{�M�7¢Xi��D�%�β�F�>���P��h�r�r>��)A^|̘1c�]b�j��{��ٜ6t�&���ţ�2�X��S,�R���}� @� tK@��-Y�@������3��O�����!U	�b��2""﮻�J��K~��l�?܋�Yꉩ��|�;SL-�l��l����y$]L����8\us�^�h1��o;}������~zH=�k�2Jmȇ-ob�2�5�Y�#�}��'��������Q�ge��v�5פ�N:)^��w�=?w�K\�'�|�Q}�x�""m���ƇM/��8��>I)V�{��=�⚖R���G�����n�l\p�S����~���#T-�j	���
 @� �&���v�?l7��%�ay���� �*��g�� �B
�Z1�,B��:�Qu�#Gyd~��Hk���""Vw�m����h��o~s^t ���ǭ��*y*j�u�Yg��Z[����>�4�X�6��J��<""`����H�VX!o��=���F����S5�G8��-o�S5c!�2�-��)b��iӦ�C;�x���y�w��y{Lm�r�-K�]y��+R������Zk�՘Z_�}[��u%�,�h���l���rQb�ڲzm��EE��)Ξ{��}�nۄY�e�wNm�Q�/b����߇��� @�  @�[��nɪ���
�=�b%��btTL�G���-��^��Ӽzj}��p6/=������1�6�z�%�����:����׮��YP>?���RX1�,����r@WB����F�z�S�_`��+��m�D����=�K���7����b�i{Q�|�=����1�8�8���S�
\Χ�AӋ�Ϛ�7��!ƈ�X\""V�-�)���:�F���q���>ǎq�����%��phn3�r_Ę�\F�Q�����M���Ȍ�E���<^��Z�Ba.\pAf�� @�  @���9����O�b�S�:9��?Æ
w��?�6���C1m��o��h�^z�F�5�gSic
kLyl��Ys_Ƣ�8�X�!F~Ũ��{�Ep��O�Q�%�zꩧ�H��o*F��fח��mĔ���l�hH��e�)F�Ū�1Jp��]�}S�cbV�b�c�=�|x��GS\�r�6�����ӝwޙZh�|���خ<��3��s��\��v�ƶn�����.|��l�u�
-S��F�~^��k�	 @� �( ��ū2��L�""�8hz�óq@2nN�5�7'օ�u��*�^{���(�ኟ����N�  �MSk���n @��➕1%>¼x�.t�W'�� @� �N@�7�.�""@� �
�H��(�@���� @� ��	���2�v���@;A^;� @`�@,8��.��X�V!0�""̋{�
����v~ @� ��p����f]�q����.��<n+w��q{i��(�#oX}�kL��6mZ��q�2����� @����w[r�`v"" ���M�����^ @� CZ��t���i��V�c�week��W�s�3����~��م��Z�#B7 @� ��h�r�]}����K.���� @���0""o\\Ʊ;��#���TM 0�����3��y��o�zF�5KzM� Ʒ@�=��}��uvF�uKV� @����?|Ex%F��7�/�$@� D�'��q`+0""o`/}�/Ɉ�2z����c�>�T��u���������������;|?�~���C�ϼ���/M�:�ѵь�k�y٨� @� -F䵀xK� �w���y��Xԥ @� !`D��!e�K��!;����;�@���֑xe�mli��r�R�#@� ����>}gO� �.P���J�W�eG @� �� o��s&@� *t�AC��
���  @����.⪚ ?�M�6m�	E�7�)�C�� H�r���6H'=b�]��ʎ @�� 
D��p�o���j0�^ @� � `��������]��U�W�@�������9؋6��솼:	 @���(��mD�����:;Sk�%�^ @�/f���	�4 @���䍛K�D @� �N�C��x���x @� c! �Eu @� �B �͖���l�L�  @`n
�G�������\}������K �3~^�̥� @��@	X�v�.��%@�  @� �WA^�^9�&@�  @� (A�@]n'K�  @�  Я��~�r�M�  @�  0P�����N�  @�  @�_y�z���  @� �;�ɓ'�x(��9�  @�  @�@�����5G�  @�  @����*9�  @ �	&` @�  P��y�Qk�  @�  @�@u#��9� P�3f���ˍ�G���*� @���
���;q @���n�� �������R�M�����/ @�  @� ��
c3 @�  @� �^����� @�  @� �򆁱�  @�  @�@/	�z�j� @�  @� �ay���L� �T`���)
 @����Hx�"" ȫ�� @�  @� 5�j� @�  @� �*��*j�!@�  @�  P�� �fp� @�  @� �"" ȫ�� @�  @� 5L��=� @� F`ƌs�N� ���F�d��F�w��  @�  @�@O	�z�r� @�  @� �����.� @�  @� �)A^O]�!@�  @�  �^@����V @�  @� =% ��ˡ3 @��x�<yr��B� (~?(���9�  @�  @�@�����5G�  @�  @��� ���c @�  @� �, ȫ\s @�  @� ��9�  @�  @�@�knOs @� ���1c����%@� F&����9٫��y�]l%@�  @�  �S����:C�  @�  @��� ���� @�  @� zJ@��S�Cg @�  @� ��w��  @�  @�@O	�z�r� 0�&O��� @� ������*��*j�!@�  @�  P�� �fp� @�  @� �"" ȫ�� @�  @� 5�j� @�  @� �*��*j�!@�  @�  P��Ě�� @``f̘10��D	 @� ��	��`dN�j/ �k�2�['L�0�  @�  @� �$`jm/]
}!@�  @�  0��y���fC|��;o @�  @������+� @�  @� ���6 @�  @�  ���>�H���ɓ'�x( @���~^o� 0�~?�+?6�-�G� @�  @� 誀 ���*'@�  @�  06���qT @�  @� ��
�ʫr @�  @� c# �G� @�  @� ���3f��j*'@�  @�  @�c#�:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o<.[�<yr��B� /����6>!@� �*���A��csނ��qT @�  @� ��
�ʫr @�  @� c# �G� @�  @� ���3f��j*'@�  @�  @�c#�:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� �:&T @�  @� ���o� @�  @� ��l�Zv�4�    IEND�B`�"
llama-api-server/src/backend/ggml.rs,"use crate::{error, utils::gen_chat_id, SERVER_INFO};
use endpoints::{
    chat::ChatCompletionRequest,
    completions::CompletionRequest,
    embeddings::EmbeddingRequest,
    files::{DeleteFileStatus, FileObject, ListFilesResponse},
    rag::{ChunksRequest, ChunksResponse},
};
use futures_util::TryStreamExt;
use hyper::{body::to_bytes, Body, Method, Request, Response};
use multipart::server::{Multipart, ReadEntry, ReadEntryResult};
use multipart_2021 as multipart;
use std::{
    fs::{self, File},
    io::{Cursor, Read, Write},
    path::Path,
    time::SystemTime,
};
use walkdir::{DirEntry, WalkDir};

/// List all models available.
pub(crate) async fn models_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming model list request."");

    let list_models_response = match llama_core::models::models().await {
        Ok(list_models_response) => list_models_response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // serialize response
    let s = match serde_json::to_string(&list_models_response) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Failed to serialize the model list result. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the model list response."");

    res
}

/// Compute embeddings for the input text and return the embeddings object.
pub(crate) async fn embeddings_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming embeddings request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""embeddings_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut embedding_request: EmbeddingRequest = match serde_json::from_slice(&body_bytes) {
        Ok(embedding_request) => embedding_request,
        Err(e) => {
            let err_msg = format!(""Fail to deserialize embedding request: {msg}"", msg = e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if embedding_request.user.is_none() {
        embedding_request.user = Some(gen_chat_id())
    };
    let id = embedding_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::embeddings::embeddings(&embedding_request).await {
        Ok(embedding_response) => {
            // serialize embedding object
            match serde_json::to_string(&embedding_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .header(""user"", id)
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize embedding object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the embeddings response"");

    res
}

/// Process a completion request and returns a completion response with the answer from the model.
pub(crate) async fn completions_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming completions request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""completions_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut completion_request: CompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(completion_request) => completion_request,
        Err(e) => {
            let err_msg = format!(""Fail to deserialize completions request: {msg}"", msg = e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if completion_request.user.is_none() {
        completion_request.user = Some(gen_chat_id())
    };
    let id = completion_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::completions::completions(&completion_request).await {
        Ok(completion_object) => {
            // serialize completion object
            let s = match serde_json::to_string(&completion_object) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Fail to serialize completion object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .header(""user"", id)
                .body(Body::from(s));
            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the completions response."");

    res
}

/// Process a chat-completion request and returns a chat-completion response with the answer from the model.
pub(crate) async fn chat_completions_handler(mut req: Request<Body>) -> Response<Body> {
    info!(target: ""stdout"", ""Handling the coming chat completion request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    info!(target: ""stdout"", ""Prepare the chat completion request."");

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut chat_request: ChatCompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chat_request) => chat_request,
        Err(e) => {
            let err_msg = format!(
                ""Fail to deserialize chat completion request: {msg}"",
                msg = e
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the user id is provided
    if chat_request.user.is_none() {
        chat_request.user = Some(gen_chat_id())
    };
    let id = chat_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", chat_request.user.clone().unwrap());

    let res = match llama_core::chat::chat(&mut chat_request).await {
        Ok(result) => match result {
            either::Left(stream) => {
                let stream = stream.map_err(|e| e.to_string());

                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""text/event-stream"")
                    .header(""Cache-Control"", ""no-cache"")
                    .header(""Connection"", ""keep-alive"")
                    .header(""user"", id)
                    .body(Body::wrap_stream(stream));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""finish chat completions in stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            either::Right(chat_completion_object) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&chat_completion_object) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize chat completion object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .header(""user"", id)
                    .body(Body::from(s));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""Finish chat completions in non-stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in non-stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
        },
        Err(e) => {
            let err_msg = format!(""Failed to get chat completions. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the chat completion response."");

    res
}

/// Upload files and return the file object.
pub(crate) async fn files_handler(req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming files request"");

    let res = if req.method() == Method::POST {
        let boundary = ""boundary="";

        let boundary = req.headers().get(""content-type"").and_then(|ct| {
            let ct = ct.to_str().ok()?;
            let idx = ct.find(boundary)?;
            Some(ct[idx + boundary.len()..].to_string())
        });

        let req_body = req.into_body();
        let body_bytes = match to_bytes(req_body).await {
            Ok(body_bytes) => body_bytes,
            Err(e) => {
                let err_msg = format!(""Fail to read buffer from request body. {}"", e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        let cursor = Cursor::new(body_bytes.to_vec());

        let mut multipart = Multipart::with_body(cursor, boundary.unwrap());

        let mut file_object: Option<FileObject> = None;
        while let ReadEntryResult::Entry(mut field) = multipart.read_entry_mut() {
            if &*field.headers.name == ""file"" {
                let filename = match field.headers.filename {
                    Some(filename) => filename,
                    None => {
                        let err_msg =
                            ""Failed to upload the target file. The filename is not provided."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                if !((filename).to_lowercase().ends_with("".txt"")
                    || (filename).to_lowercase().ends_with("".md""))
                    || (filename).to_lowercase().ends_with("".png"")
                {
                    let err_msg = format!(
                        ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported. The file extension is {}."",
                        &filename
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }

                let mut buffer = Vec::new();
                let size_in_bytes = match field.data.read_to_end(&mut buffer) {
                    Ok(size_in_bytes) => size_in_bytes,
                    Err(e) => {
                        let err_msg = format!(""Failed to read the target file. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a unique file id
                let id = format!(""file_{}"", uuid::Uuid::new_v4());

                // save the file
                let path = Path::new(""archives"");
                if !path.exists() {
                    fs::create_dir(path).unwrap();
                }
                let file_path = path.join(&id);
                if !file_path.exists() {
                    fs::create_dir(&file_path).unwrap();
                }
                let mut file = match File::create(file_path.join(&filename)) {
                    Ok(file) => file,
                    Err(e) => {
                        let err_msg =
                            format!(""Failed to create archive document {}. {}"", &filename, e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };
                file.write_all(&buffer[..]).unwrap();

                // log
                info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

                let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a file object
                file_object = Some(FileObject {
                    id,
                    bytes: size_in_bytes as u64,
                    created_at,
                    filename,
                    object: ""file"".to_string(),
                    purpose: ""assistants"".to_string(),
                });

                break;
            }
        }

        match file_object {
            Some(fo) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&fo) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize file object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .body(Body::from(s));

                match result {
                    Ok(response) => response,
                    Err(e) => {
                        let err_msg = e.to_string();

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            None => {
                let err_msg = ""Failed to upload the target file. Not found the target file."";

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::GET {
        let uri_path = req.uri().path();

        if uri_path == ""/v1/files"" {
            let mut file_objects: Vec<FileObject> = Vec::new();
            for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let id = entry
                        .path()
                        .parent()
                        .and_then(|p| p.file_name())
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .to_string();

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    let fo = FileObject {
                        id,
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    };

                    file_objects.push(fo);
                }
            }

            info!(target: ""stdout"", ""Found {} archive files"", file_objects.len());

            let file_objects = ListFilesResponse {
                object: ""list"".to_string(),
                data: file_objects,
            };

            // serialize chat completion object
            let s = match serde_json::to_string(&file_objects) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Failed to serialize file object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(s));

            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        } else {
            let id = uri_path.trim_start_matches(""/v1/files/"");
            let root = format!(""archives/{}"", id);
            let mut file_object: Option<FileObject> = None;
            for entry in WalkDir::new(root).into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    file_object = Some(FileObject {
                        id: id.into(),
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    });

                    break;
                }
            }

            match file_object {
                Some(fo) => {
                    // serialize chat completion object
                    let s = match serde_json::to_string(&fo) {
                        Ok(s) => s,
                        Err(e) => {
                            let err_msg = format!(""Failed to serialize file object. {}"", e);

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            return error::internal_server_error(err_msg);
                        }
                    };

                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));

                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                None => {
                    let err_msg = format!(
                        ""Failed to retrieve the target file. Not found the target file with id {}."",
                        id
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
    } else if req.method() == Method::DELETE {
        let id = req.uri().path().trim_start_matches(""/v1/files/"");
        let root = format!(""archives/{}"", id);
        let status = match fs::remove_dir_all(root) {
            Ok(_) => {
                info!(target: ""stdout"", ""Successfully deleted the target file with id {}."", id);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: true,
                }
            }
            Err(e) => {
                let err_msg = format!(""Failed to delete the target file with id {}. {}"", id, e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: false,
                }
            }
        };

        // serialize status
        let s = match serde_json::to_string(&status) {
            Ok(s) => s,
            Err(e) => {
                let err_msg = format!(
                    ""Failed to serialize the status of the file deletion operation. {}"",
                    e
                );

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        // return response
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::from(s));

        match result {
            Ok(response) => response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::OPTIONS {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""files_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    } else {
        let err_msg = ""Invalid HTTP Method."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        error::internal_server_error(err_msg)
    };

    info!(target: ""stdout"", ""Send the files response"");

    res
}

/// Segment the text into chunks and return the chunks response.
pub(crate) async fn chunks_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming chunks request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""chunks_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    let chunks_request: ChunksRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chunks_request) => chunks_request,
        Err(e) => {
            let err_msg = format!(""Fail to deserialize chunks request: {msg}"", msg = e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the archives directory exists
    let path = Path::new(""archives"");
    if !path.exists() {
        let err_msg = ""The `archives` directory does not exist."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the archive id exists
    let archive_path = path.join(&chunks_request.id);
    if !archive_path.exists() {
        let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the file exists
    let file_path = archive_path.join(&chunks_request.filename);
    if !file_path.exists() {
        let err_msg = format!(
            ""Not found file: {} in archive id: {}"",
            &chunks_request.filename, &chunks_request.id
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // log
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &chunks_request.id, &chunks_request.filename);

    // get the extension of the archived file
    let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
        Some(extension) => extension,
        None => {
            let err_msg = format!(
                ""Failed to get the extension of the archived `{}`."",
                &chunks_request.filename
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // open the file
    let mut file = match File::open(&file_path) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to open `{}`. {}"", &chunks_request.filename, e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // read the file
    let mut contents = String::new();
    if let Err(e) = file.read_to_string(&mut contents) {
        let err_msg = format!(""Failed to read `{}`. {}"", &chunks_request.filename, e);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
    {
        Ok(chunks) => {
            let chunks_response = ChunksResponse {
                id: chunks_request.id,
                filename: chunks_request.filename,
                chunks,
            };

            // serialize embedding object
            match serde_json::to_string(&chunks_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize chunks response. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the chunks response."");

    res
}

/// Return the server info.
pub(crate) async fn server_info_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming server info request."");

    // get the server info
    let server_info = match SERVER_INFO.get() {
        Some(server_info) => server_info,
        None => {
            let err_msg = ""The server info is not set."";

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(""The server info is not set."");
        }
    };

    // serialize server info
    let s = match serde_json::to_string(&server_info) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Fail to serialize server info. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the server info response."");

    res
}

fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}
"
llama-api-server/src/backend/mod.rs,"pub(crate) mod ggml;

use crate::error;
use hyper::{Body, Request, Response};

pub(crate) async fn handle_llama_request(req: Request<Body>) -> Response<Body> {
    match req.uri().path() {
        ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
        ""/v1/completions"" => ggml::completions_handler(req).await,
        ""/v1/models"" => ggml::models_handler().await,
        ""/v1/embeddings"" => ggml::embeddings_handler(req).await,
        ""/v1/files"" => ggml::files_handler(req).await,
        ""/v1/chunks"" => ggml::chunks_handler(req).await,
        ""/v1/info"" => ggml::server_info_handler().await,
        path => {
            if path.starts_with(""/v1/files/"") {
                ggml::files_handler(req).await
            } else {
                error::invalid_endpoint(path)
            }
        }
    }
}
"
llama-api-server/src/error.rs,"use hyper::{Body, Response};
use thiserror::Error;

#[allow(dead_code)]
pub(crate) fn not_implemented() -> Response<Body> {
    // log error
    error!(target: ""stdout"", ""501 Not Implemented"");

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_IMPLEMENTED)
        .body(Body::from(""501 Not Implemented""))
        .unwrap()
}

pub(crate) fn internal_server_error(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""500 Internal Server Error"".to_string(),
        false => format!(""500 Internal Server Error: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::INTERNAL_SERVER_ERROR)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn bad_request(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""400 Bad Request"".to_string(),
        false => format!(""400 Bad Request: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::BAD_REQUEST)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn invalid_endpoint(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""404 The requested service endpoint is not found"".to_string(),
        false => format!(
            ""404 The requested service endpoint is not found: {}"",
            msg.as_ref()
        ),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND)
        .body(Body::from(err_msg))
        .unwrap()
}

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum ServerError {
    /// Error returned while parsing socket address failed
    #[error(""Failed to parse socket address: {0}"")]
    SocketAddr(String),
    /// Error returned while parsing CLI options failed
    #[error(""{0}"")]
    ArgumentError(String),
    /// Generic error returned while performing an operation
    #[error(""{0}"")]
    Operation(String),
}
"
llama-api-server/src/main.rs,"#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default socket address of LlamaEdge API Server instance
const DEFAULT_SOCKET_ADDRESS: &str = ""0.0.0.0:8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance
    #[arg(long, default_value = DEFAULT_SOCKET_ADDRESS)]
    socket_addr: String,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = cli
        .socket_addr
        .parse::<SocketAddr>()
        .map_err(|e| ServerError::SocketAddr(e.to_string()))?;
    let port = addr.port().to_string();

    // log socket address
    info!(target: ""stdout"", ""socket_address: {}"", addr.to_string());

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}
"
llama-api-server/src/utils.rs,"use serde::{Deserialize, Serialize};

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

#[derive(
    Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, clap::ValueEnum, Serialize, Deserialize,
)]
#[serde(rename_all = ""lowercase"")]
pub(crate) enum LogLevel {
    /// Describes messages about the values of variables and the flow of
    /// control within a program.
    Trace,

    /// Describes messages likely to be of interest to someone debugging a
    /// program.
    Debug,

    /// Describes messages likely to be of interest to someone monitoring a
    /// program.
    Info,

    /// Describes messages indicating hazardous situations.
    Warn,

    /// Describes messages indicating serious errors.
    Error,

    /// Describes messages indicating fatal errors.
    Critical,
}
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error,
        }
    }
}
impl std::fmt::Display for LogLevel {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            LogLevel::Trace => write!(f, ""trace""),
            LogLevel::Debug => write!(f, ""debug""),
            LogLevel::Info => write!(f, ""info""),
            LogLevel::Warn => write!(f, ""warn""),
            LogLevel::Error => write!(f, ""error""),
            LogLevel::Critical => write!(f, ""critical""),
        }
    }
}
impl std::str::FromStr for LogLevel {
    type Err = String;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""trace"" => Ok(LogLevel::Trace),
            ""debug"" => Ok(LogLevel::Debug),
            ""info"" => Ok(LogLevel::Info),
            ""warn"" => Ok(LogLevel::Warn),
            ""error"" => Ok(LogLevel::Error),
            ""critical"" => Ok(LogLevel::Critical),
            _ => Err(format!(""Invalid log level: {}"", s)),
        }
    }
}
"
llama-chat/.cargo/config.toml,"[build]
target = ""wasm32-wasip1""
rustflags = [""--cfg"", ""wasmedge"", ""--cfg"", ""tokio_unstable""]
"
llama-chat/Cargo.toml,"[package]
name = ""llama-chat""
version = ""0.14.3""
edition = ""2021""

[dependencies]
chat-prompts.workspace = true
endpoints.workspace = true
llama-core.workspace = true
clap.workspace = true
serde.workspace = true
serde_json.workspace = true
anyhow.workspace = true
tokio.workspace = true
futures.workspace = true
either.workspace = true
"
llama-chat/README.md,"# Run the LLM via CLI

[See it in action!](https://x.com/juntao/status/1705588244602114303)

<!-- @import ""[TOC]"" {cmd=""toc"" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [Run the LLM via CLI](#run-the-llm-via-cli)
  - [Dependencies](#dependencies)
  - [Get `llama-chat` wasm app](#get-llama-chat-wasm-app)
  - [Get Model](#get-model)
  - [Execute](#execute)
  - [CLI options](#cli-options)
  - [Optional: Build the `llama-chat` wasm app yourself](#optional-build-the-llama-chat-wasm-app-yourself)

<!-- /code_chunk_output -->

## Dependencies

Install the latest WasmEdge with plugins:

<details> <summary> For macOS (apple silicon) </summary>

```console
# install WasmEdge-0.13.4 with wasi-nn-ggml plugin
curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s

# Assuming you use zsh (the default shell on macOS), run the following command to activate the environment
source $HOME/.zshenv
```

</details>

<details> <summary> For Ubuntu (>= 20.04) </summary>

```console
# install libopenblas-dev
apt update && apt install -y libopenblas-dev

# install WasmEdge-0.13.4 with wasi-nn-ggml plugin
curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s

# Assuming you use bash (the default shell on Ubuntu), run the following command to activate the environment
source $HOME/.bashrc
```

</details>

<details> <summary> For General Linux </summary>

```console
# install WasmEdge-0.13.4 with wasi-nn-ggml plugin
curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s

# Assuming you use bash (the default shell on Ubuntu), run the following command to activate the environment
source $HOME/.bashrc
```

</details>

## Get `llama-chat` wasm app

Download the `llama-chat.wasm`:

```bash
curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm
```

## Get Model

Click [here](../models.md) to see the download link and commands to run the model.

## Execute

Execute the WASM with the `wasmedge` using the named model feature to preload large model. Here we use the `Llama-2-7B-Chat` model as an example:

```console
# download model
curl -LO https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf

# run the `llama-chat` wasm app with the model
wasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-7b-chat.Q5_K_M.gguf llama-chat.wasm --prompt-template llama-2-chat
```

After executing the command, you may need to wait a moment for the input prompt to appear.
You can enter your question once you see the `[USER]:` prompt:

```console
[USER]:
What's the capital of France?
[ASSISTANT]:
The capital of France is Paris.
[USER]:
what about Norway?
[ASSISTANT]:
The capital of Norway is Oslo.
[USER]:
I have two apples, each costing 5 dollars. What is the total cost of these apples?
[ASSISTANT]:
The total cost of the two apples is 10 dollars.
[USER]:
What if I have 3 apples?
[ASSISTANT]:
If you have 3 apples, each costing 5 dollars, the total cost of the apples is 15 dollars.
```

## CLI options

The options for `llama-chat` wasm app are:

```console
~/LlamaEdge/chat$ wasmedge llama-chat.wasm -h

Usage: llama-chat.wasm [OPTIONS] --prompt-template <PROMPT_TEMPLATE>

Options:
  -m, --model-name <MODEL_NAME>
          Model name [default: default]
  -a, --model-alias <MODEL_ALIAS>
          Model alias [default: default]
  -c, --ctx-size <CTX_SIZE>
          Size of the prompt context [default: 512]
  -n, --n-predict <N_PREDICT>
          Number of tokens to predict [default: 1024]
  -g, --n-gpu-layers <N_GPU_LAYERS>
          Number of layers to run on the GPU [default: 100]
      --main-gpu <MAIN_GPU>
          The main GPU to use
      --tensor-split <TENSOR_SPLIT>
          How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1
      --threads <THREADS>
          Number of threads to use during computation [default: 2]
      --no-mmap <NO_MMAP>
          Disable memory mapping for file access of chat models [possible values: true, false]
  -b, --batch-size <BATCH_SIZE>
          Batch size for prompt processing [default: 512]
      --temp <TEMP>
          Temperature for sampling
      --top-p <TOP_P>
          An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
      --repeat-penalty <REPEAT_PENALTY>
          Penalize repeat sequence of tokens [default: 1.1]
      --presence-penalty <PRESENCE_PENALTY>
          Repeat alpha presence penalty. 0.0 = disabled [default: 0.0]
      --frequency-penalty <FREQUENCY_PENALTY>
          Repeat alpha frequency penalty. 0.0 = disabled [default: 0.0]
      --grammar <GRAMMAR>
          BNF-like grammar to constrain generations (see samples in grammars/ dir) [default: ]
      --json-schema <JSON_SCHEMA>
          JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead
  -p, --prompt-template <PROMPT_TEMPLATE>
          Sets the prompt template [possible values: llama-2-chat, llama-3-chat, llama-3-tool, mistral-instruct, mistral-tool, mistrallite, openchat, codellama-instruct, codellama-super-instruct, human-assistant, vicuna-1.0-chat, vicuna-1.1-chat, vicuna-llava, chatml, chatml-tool, internlm-2-tool, baichuan-2, wizard-coder, zephyr, stablelm-zephyr, intel-neural, deepseek-chat, deepseek-coder, deepseek-chat-2, solar-instruct, phi-2-chat, phi-2-instruct, phi-3-chat, phi-3-instruct, gemma-instruct, octopus, glm-4-chat, groq-llama3-tool, embedding]
  -r, --reverse-prompt <REVERSE_PROMPT>
          Halt generation at PROMPT, return control
  -s, --system-prompt <SYSTEM_PROMPT>
          System prompt message string
      --log-prompts
          Print prompt strings to stdout
      --log-stat
          Print statistics to stdout
      --log-all
          Print all log information to stdout
      --disable-stream
          enable streaming stdout
  -h, --help
          Print help
  -V, --version
          Print version
```

## Optional: Build the `llama-chat` wasm app yourself

Run the following command:

```console
cargo build --target wasm32-wasi --release
```

The `llama-chat.wasm` will be generated in the `target/wasm32-wasi/release` folder.
"
llama-chat/src/main.rs,"use anyhow::bail;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use either::{Left, Right};
use endpoints::chat::{
    ChatCompletionChunk, ChatCompletionRequestBuilder, ChatCompletionRequestMessage,
    ChatCompletionRequestSampling, ChatCompletionUserMessageContent,
};
use futures::TryStreamExt;
use llama_core::{init_core_context, MetadataBuilder};
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Debug, Parser)]
#[command(author, about, version, long_about=None)]
struct Cli {
    /// Model name
    #[arg(short, long, default_value = ""default"")]
    model_name: String,
    /// Model alias
    #[arg(short = 'a', long, default_value = ""default"")]
    model_alias: String,
    /// Size of the prompt context
    #[arg(short, long, default_value = ""512"")]
    ctx_size: u64,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Batch size for prompt processing
    #[arg(short, long, default_value = ""512"")]
    batch_size: u64,
    /// Temperature for sampling
    #[arg(long, conflicts_with = ""top_p"")]
    temp: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, conflicts_with = ""temp"")]
    top_p: Option<f64>,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Sets the prompt template.
    #[arg(short, long, value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: PromptTemplateType,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// System prompt message string.
    #[arg(short, long)]
    system_prompt: Option<String>,
    /// Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Print all log information to stdout
    #[arg(long)]
    log_all: bool,
    /// enable streaming stdout
    #[arg(long, default_value = ""false"")]
    disable_stream: bool,
}

#[allow(unreachable_code)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> anyhow::Result<()> {
    // get the environment variable `PLUGIN_DEBUG`
    let plugin_debug = std::env::var(""PLUGIN_DEBUG"").unwrap_or_default();
    let plugin_debug = match plugin_debug.is_empty() {
        true => false,
        false => plugin_debug.to_lowercase().parse::<bool>().unwrap_or(false),
    };

    // parse the command line arguments
    let cli = Cli::parse();

    // log version
    log(format!(
        ""\n[INFO] llama-chat version: {}"",
        env!(""CARGO_PKG_VERSION"")
    ));

    // log the cli options
    log(format!(""[INFO] Model name: {}"", &cli.model_name));
    log(format!(""[INFO] Model alias: {}"", &cli.model_alias));
    log(format!(""[INFO] Prompt template: {}"", &cli.prompt_template));
    // ctx size
    log(format!(""[INFO] Context size: {}"", &cli.ctx_size));
    // reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        log(format!(""[INFO] reverse prompt: {}"", reverse_prompt));
    }
    // system prompt
    if let Some(system_prompt) = &cli.system_prompt {
        log(format!(""[INFO] system prompt: {}"", system_prompt));
    }
    // n_predict
    log(format!(
        ""[INFO] Number of tokens to predict: {}"",
        &cli.n_predict
    ));
    // n_gpu_layers
    log(format!(
        ""[INFO] Number of layers to run on the GPU: {}"",
        &cli.n_gpu_layers
    ));
    // main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        log(format!(""[INFO] Main GPU to use: {}"", main_gpu));
    }
    // tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        log(format!(""[INFO] Tensor split: {}"", tensor_split));
    }
    log(format!(""[INFO] Threads: {}"", &cli.threads));
    // no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        log(format!(
            ""[INFO] Disable memory mapping for file access of chat models : {}"",
            &no_mmap
        ));
    }
    // batch size
    log(format!(
        ""[INFO] Batch size for prompt processing: {}"",
        &cli.batch_size
    ));
    // temp and top_p
    if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
    }
    // repeat penalty
    log(format!(
        ""[INFO] Penalize repeat sequence of tokens: {}"",
        &cli.repeat_penalty
    ));
    // presence penalty
    log(format!(
        ""[INFO] Presence penalty (0.0 = disabled): {}"",
        &cli.presence_penalty
    ));
    // frequency penalty
    log(format!(
        ""[INFO] Frequency penalty (0.0 = disabled): {}"",
        &cli.frequency_penalty
    ));
    // grammar
    log(format!(""[INFO] BNF-like grammar: {}"", &cli.grammar));
    // json schema
    if let Some(json_schema) = &cli.json_schema {
        log(format!(""[INFO] JSON schema: {}"", json_schema));
    }
    // log prompts
    log(format!(""[INFO] Enable prompt log: {}"", &cli.log_prompts));
    // log statistics
    log(format!(""[INFO] Enable plugin log: {}"", &cli.log_stat));

    // create a MetadataBuilder instance
    let builder = MetadataBuilder::new(&cli.model_name, &cli.model_alias, cli.prompt_template)
        .with_ctx_size(cli.ctx_size)
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_batch_size(cli.batch_size)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .enable_prompts_log(cli.log_prompts || cli.log_all)
        .enable_plugin_log(cli.log_stat || cli.log_all)
        .enable_debug_log(plugin_debug);
    // temp and top_p
    let builder = if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
        builder.with_top_p(top_p)
    } else {
        let temp = cli.temp.unwrap();
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    };
    // create a Metadata instance
    let metadata = builder.build();

    // initialize the core context
    init_core_context(Some(&[metadata]), None)?;

    // get the plugin version info
    let plugin_info = llama_core::get_plugin_info()?;
    log(format!(
        ""[INFO] Wasi-nn-ggml plugin: b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    ));

    // create a ChatCompletionRequestSampling instance
    let sampling = if cli.temp.is_none() && cli.top_p.is_none() {
        ChatCompletionRequestSampling::Temperature(1.0)
    } else if let Some(temp) = cli.temp {
        ChatCompletionRequestSampling::Temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        ChatCompletionRequestSampling::TopP(top_p)
    } else {
        let temp = cli.temp.unwrap();
        ChatCompletionRequestSampling::Temperature(temp)
    };

    // create a chat request
    let mut chat_request = ChatCompletionRequestBuilder::new(&cli.model_name, vec![])
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_sampling(sampling)
        .enable_stream(!cli.disable_stream)
        .build();

    // add system message if provided
    if let Some(system_prompt) = &cli.system_prompt {
        let system_message = ChatCompletionRequestMessage::new_system_message(system_prompt, None);

        chat_request.messages.push(system_message);
    }

    let readme = ""
================================== Running in interactive mode. ===================================\n
    - Press [Ctrl+C] to interject at any time.
    - Press [Return] to end the input.
    - For multi-line inputs, end each line with '\\' and press [Return] to get another line.\n"";
    log(readme);

    loop {
        println!(""\n[You]: "");
        let user_input = read_input();

        // put the user message into the messages sequence of chat_request
        let user_message = ChatCompletionRequestMessage::new_user_message(
            ChatCompletionUserMessageContent::Text(user_input),
            None,
        );

        chat_request.messages.push(user_message);

        if cli.log_stat || cli.log_all {
            print_log_begin_separator(""STATISTICS (Set Input)"", Some(""*""), None);
        }

        if cli.log_stat || cli.log_all {
            print_log_end_separator(Some(""*""), None);
        }

        println!(""\n[Bot]:"");
        let mut assistant_answer = String::new();
        match llama_core::chat::chat(&mut chat_request).await {
            Ok(res) => match res {
                Left(mut stream) => {
                    while let Some(data) = stream.try_next().await? {
                        if let Some(chunk) = parse_sse_event(&data) {
                            if let Some(content) = &chunk.choices[0].delta.content {
                                if content.is_empty() {
                                    continue;
                                }
                                if assistant_answer.is_empty() {
                                    let content = content.trim_start();
                                    print!(""{}"", content);
                                    assistant_answer.push_str(content);
                                } else {
                                    print!(""{content}"");
                                    assistant_answer.push_str(content);
                                }
                                io::stdout().flush().unwrap();
                            }
                        }
                    }
                    println!();
                }
                Right(completion) => {
                    let chat_completion = completion.choices[0]
                        .message
                        .content
                        .to_owned()
                        .unwrap_or_default();
                    println!(""{chat_completion}"");
                    assistant_answer = chat_completion;
                }
            },
            Err(e) => {
                bail!(""Fail to generate chat completion. Reason: {msg}"", msg = e)
            }
        };

        let assistant_message = ChatCompletionRequestMessage::new_assistant_message(
            Some(assistant_answer.trim().to_string()),
            None,
            None,
        );
        chat_request.messages.push(assistant_message);
    }

    Ok(())
}

// For single line input, just press [Return] to end the input.
// For multi-line input, end your input with '\\' and press [Return].
//
// For example:
//  [You]:
//  what is the capital of France?[Return]
//
//  [You]:
//  Count the words in the following sentence: \[Return]
//  \[Return]
//  You can use Git to save new files and any changes to already existing files as a bundle of changes called a commit, which can be thought of as a “revision” to your project.[Return]
//
fn read_input() -> String {
    let mut answer = String::new();
    loop {
        let mut temp = String::new();
        std::io::stdin()
            .read_line(&mut temp)
            .expect(""The read bytes are not valid UTF-8"");

        if temp.ends_with(""\\\n"") {
            temp.pop();
            temp.pop();
            temp.push('\n');
            answer.push_str(&temp);
            continue;
        } else if temp.ends_with('\n') {
            answer.push_str(&temp);
            return answer;
        } else {
            return answer;
        }
    }
}

fn print_log_begin_separator(
    title: impl AsRef<str>,
    ch: Option<&str>,
    len: Option<usize>,
) -> usize {
    let title = format!("" [LOG: {}] "", title.as_ref());

    let total_len: usize = len.unwrap_or(100);
    let separator_len: usize = (total_len - title.len()) / 2;

    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push_str(&title);
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push('\n');
    println!(""{}"", separator);
    total_len
}

fn print_log_end_separator(ch: Option<&str>, len: Option<usize>) {
    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(len.unwrap_or(100)).as_str());
    separator.push('\n');
    println!(""{}"", separator);
}

#[derive(Debug, Default, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    // #[serde(rename = ""enable-debug-log"")]
    // pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    // pub mmproj: String,
    // pub image: String,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    // #[serde(rename = ""main-gpu"")]
    // pub main_gpu: u64,
    // #[serde(rename = ""tensor-split"")]
    // pub tensor_split: String,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,

    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,
}

fn log(msg: impl std::fmt::Display) {
    println!(""{}"", msg);
}

fn parse_sse_event(s: &str) -> Option<ChatCompletionChunk> {
    let lines: Vec<&str> = s.split('\n').collect();
    // let mutevent = None;
    let mut data = None;

    for line in lines {
        if line.starts_with(""data:"") {
            data = Some(line.trim_start_matches(""data:"").trim());
        }
    }

    match data {
        Some(s) => {
            if s.trim() == ""[DONE]"" {
                return None;
            }

            match serde_json::from_str(s) {
                Ok(chunk) => Some(chunk),
                Err(e) => {
                    log(format!(
                        ""[ERROR] Fail to parse SSE data. Reason: {msg}. Data: {data}"",
                        msg = e,
                        data = s
                    ));
                    None
                }
            }
        }
        _ => None,
    }
}
"
llama-simple/.cargo/config.toml,"[build]
target = ""wasm32-wasip1""
rustflags = [""--cfg"", ""wasmedge"", ""--cfg"", ""tokio_unstable""]
"
llama-simple/Cargo.toml,"[package]
name = ""llama-simple""
version = ""0.14.3""
edition = ""2021""

[dependencies]
wasmedge-wasi-nn.workspace = true
clap.workspace = true
once_cell.workspace = true
serde.workspace = true
serde_json.workspace = true
"
llama-simple/README.md,"# Simple text completion

## Dependencies

Install the latest wasmedge with plugins:

```bash
curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s
```

## Get the compiled wasm binary program

Download the wasm file:

```bash
curl -LO https://github.com/LlamaEdge/LlmaEdge/releases/latest/download/llama-simple.wasm
```

## Get Model

Download llama model:

```bash
curl -LO https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q5_K_M.gguf
```

## Execute

Execute the WASM with the `wasmedge` using the named model feature to preload large model:

```bash
wasmedge --dir .:. --nn-preload default:GGML:AUTO:llama-2-7b-chat.Q5_K_M.gguf llama-simple.wasm \
  --prompt 'Robert Oppenheimer most important achievement is ' --ctx-size 4096
```

- The CLI options of `llama-simple` wasm app:

  ```console
  ~/llama-utils/simple$ wasmedge llama-simple.wasm -h
  Usage: llama-simple.wasm [OPTIONS] --prompt <PROMPT>

  Options:
    -p, --prompt <PROMPT>
            Sets the prompt string, including system message if required.
    -m, --model-alias <ALIAS>
            Sets the model alias [default: default]
    -c, --ctx-size <CTX_SIZE>
            Sets the prompt context size [default: 4096]
    -n, --n-predict <N_PRDICT>
            Number of tokens to predict [default: 1024]
    -g, --n-gpu-layers <N_GPU_LAYERS>
            Number of layers to run on the GPU [default: 100]
        --no-mmap
            Disable memory mapping for file access of chat models
    -b, --batch-size <BATCH_SIZE>
            Batch size for prompt processing [default: 4096]
    -r, --reverse-prompt <REVERSE_PROMPT>
            Halt generation at PROMPT, return control.
        --log-enable
            Enable trace logs
    -h, --help
            Print help
    -V, --version
          Print version
  ```

After executing the command, it takes some time to wait for the output.
Once the execution is complete, the following output will be generated:

```console
...................................................................................................
[2023-10-08 23:13:10.272] [info] [WASI-NN] GGML backend: set n_ctx to 4096
llama_new_context_with_model: kv self size  = 2048.00 MB
llama_new_context_with_model: compute buffer total size =  297.47 MB
llama_new_context_with_model: max tensor size =   102.54 MB
[2023-10-08 23:13:10.472] [info] [WASI-NN] GGML backend: llama_system_info: AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 |
[2023-10-08 23:13:10.472] [info] [WASI-NN] GGML backend: set n_predict to 128
[2023-10-08 23:13:16.014] [info] [WASI-NN] GGML backend: llama_get_kv_cache_token_count 128

llama_print_timings:        load time =  1431.58 ms
llama_print_timings:      sample time =     3.53 ms /   118 runs   (    0.03 ms per token, 33446.71 tokens per second)
llama_print_timings: prompt eval time =  1230.69 ms /    11 tokens (  111.88 ms per token,     8.94 tokens per second)
llama_print_timings:        eval time =  4295.81 ms /   117 runs   (   36.72 ms per token,    27.24 tokens per second)
llama_print_timings:       total time =  5742.71 ms
Robert Oppenheimer most important achievement is
1945 Manhattan Project.
Robert Oppenheimer was born in New York City on April 22, 1904. He was the son of Julius Oppenheimer, a wealthy German-Jewish textile merchant, and Ella Friedman Oppenheimer.
Robert Oppenheimer was a brilliant student. He attended the Ethical Culture School in New York City and graduated from the Ethical Culture Fieldston School in 1921. He then attended Harvard University, where he received his bachelor's degree
```

## Optional: Build the wasm file yourself

Compile the application to WebAssembly:

```bash
cargo build --target wasm32-wasi --release
```

The output wasm file will be at `target/wasm32-wasi/release/`.
"
llama-simple/src/main.rs,"use clap::{crate_version, Arg, ArgAction, Command};
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};

const DEFAULT_CTX_SIZE: &str = ""4096"";
static CTX_SIZE: OnceCell<usize> = OnceCell::new();

fn main() -> Result<(), String> {
    let matches = Command::new(""llama-simple"")
        .version(crate_version!())
        .arg(
            Arg::new(""prompt"")
                .short('p')
                .long(""prompt"")
                .value_name(""PROMPT"")
                .help(""Sets the prompt string, including system message if required."")
                .required(true),
        )
        .arg(
            Arg::new(""model_alias"")
                .short('m')
                .long(""model-alias"")
                .value_name(""ALIAS"")
                .help(""Sets the model alias"")
                .default_value(""default""),
        )
        .arg(
            Arg::new(""ctx_size"")
                .short('c')
                .long(""ctx-size"")
                .value_parser(clap::value_parser!(u32))
                .value_name(""CTX_SIZE"")
                .help(""Sets the prompt context size"")
                .default_value(DEFAULT_CTX_SIZE),
        )
        .arg(
            Arg::new(""n_predict"")
                .short('n')
                .long(""n-predict"")
                .value_parser(clap::value_parser!(u32))
                .value_name(""N_PRDICT"")
                .help(""Number of tokens to predict"")
                .default_value(""1024""),
        )
        .arg(
            Arg::new(""n_gpu_layers"")
                .short('g')
                .long(""n-gpu-layers"")
                .value_parser(clap::value_parser!(u32))
                .value_name(""N_GPU_LAYERS"")
                .help(""Number of layers to run on the GPU"")
                .default_value(""100""),
        )
        .arg(
            Arg::new(""no_mmap"")
                .long(""no-mmap"")
                .value_name(""NO_MMAP"")
                .help(""Disable memory mapping for file access of chat models"")
                .action(ArgAction::SetFalse),
        )
        .arg(
            Arg::new(""batch_size"")
                .short('b')
                .long(""batch-size"")
                .value_parser(clap::value_parser!(u32))
                .value_name(""BATCH_SIZE"")
                .help(""Batch size for prompt processing"")
                .default_value(""4096""),
        )
        .arg(
            Arg::new(""reverse_prompt"")
                .short('r')
                .long(""reverse-prompt"")
                .value_name(""REVERSE_PROMPT"")
                .help(""Halt generation at PROMPT, return control.""),
        )
        .arg(
            Arg::new(""log_enable"")
                .long(""log-enable"")
                .value_name(""LOG_ENABLE"")
                .help(""Enable trace logs"")
                .action(ArgAction::SetTrue),
        )
        .get_matches();

    // model alias
    let model_name = matches
        .get_one::<String>(""model_alias"")
        .unwrap()
        .to_string();

    // prompt
    let prompt = matches.get_one::<String>(""prompt"").unwrap().to_string();

    // create an `Options` instance
    let mut options = Options::default();

    // prompt context size
    let ctx_size = matches.get_one::<u32>(""ctx_size"").unwrap();
    CTX_SIZE
        .set(*ctx_size as usize * 6)
        .expect(""Fail to parse prompt context size"");
    println!(""[INFO] prompt context size: {size}"", size = ctx_size);

    // number of tokens to predict
    let n_predict = matches.get_one::<u32>(""n_predict"").unwrap();
    println!(""[INFO] Number of tokens to predict: {n}"", n = n_predict);
    options.n_predict = *n_predict as u64;

    // n_gpu_layers
    let n_gpu_layers = matches.get_one::<u32>(""n_gpu_layers"").unwrap();
    println!(
        ""[INFO] Number of layers to run on the GPU: {n}"",
        n = n_gpu_layers
    );
    options.n_gpu_layers = *n_gpu_layers as u64;

    // no_mmap
    let no_mmap = matches.get_flag(""no_mmap"");
    println!(""[INFO] no mmap: {nommap}"", nommap = !no_mmap);
    options.use_mmap = Some(!no_mmap);

    // batch size
    let batch_size = matches.get_one::<u32>(""batch_size"").unwrap();
    println!(
        ""[INFO] Batch size for prompt processing: {size}"",
        size = batch_size
    );
    options.batch_size = *batch_size as u64;

    // reverse_prompt
    if let Some(reverse_prompt) = matches.get_one::<String>(""reverse_prompt"") {
        println!(""[INFO] Reverse prompt: {prompt}"", prompt = &reverse_prompt);
        options.reverse_prompt = Some(reverse_prompt.to_string());
    }

    // log
    let log_enable = matches.get_flag(""log_enable"");
    println!(""[INFO] Log enable: {enable}"", enable = log_enable);
    options.log_enable = log_enable;

    // load the model into wasi-nn
    let graph = wasmedge_wasi_nn::GraphBuilder::new(
        wasmedge_wasi_nn::GraphEncoding::Ggml,
        wasmedge_wasi_nn::ExecutionTarget::AUTO,
    )
    .build_from_cache(&model_name)
    .expect(""Failed to load the model"");

    // initialize the execution context
    let mut context = graph
        .init_execution_context()
        .expect(""Failed to init context"");

    // set metadata
    let metadata = serde_json::to_string(&options).expect(""Fail to serialize options"");
    context
        .set_input(
            1,
            wasmedge_wasi_nn::TensorType::U8,
            &[1],
            metadata.as_bytes(),
        )
        .expect(""Fail to set metadata"");

    // set input tensor
    let tensor_data = prompt.as_bytes().to_vec();
    context
        .set_input(0, wasmedge_wasi_nn::TensorType::U8, &[1], &tensor_data)
        .expect(""Failed to set prompt as the input tensor"");

    // execute the inference
    context.compute().expect(""Failed to complete inference"");

    // retrieve the output
    let mut output_buffer = vec![0u8; *CTX_SIZE.get().unwrap()];
    let mut output_size = context
        .get_output(0, &mut output_buffer)
        .expect(""Failed to get output tensor"");
    output_size = std::cmp::min(*CTX_SIZE.get().unwrap(), output_size);
    let output = String::from_utf8_lossy(&output_buffer[..output_size]).to_string();

    println!(""\n[Answer]:\n\n{}"", output);

    Ok(())
}

#[derive(Debug, Default, Deserialize, Serialize)]
struct Options {
    #[serde(rename = ""enable-log"")]
    log_enable: bool,
    #[serde(rename = ""ctx-size"")]
    ctx_size: u64,
    #[serde(rename = ""n-predict"")]
    n_predict: u64,
    #[serde(rename = ""n-gpu-layers"")]
    n_gpu_layers: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,
    #[serde(rename = ""batch-size"")]
    batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    reverse_prompt: Option<String>,
}
"
models.md,
run-llm.exp,"#!/usr/bin/expect -f

set MODEL [lindex $argv 0]
set RUN_MODE [lindex $argv 1]

set timeout -1
spawn ./run-llm.sh
match_max 100000

# Install WasmEdge
send -- ""1\r""

# Select model
send -- ""$MODEL\r""

# Select running mode 1:CLI / 2:API-SERVER
send -- ""$RUN_MODE\r""

# Do not show the log
send -- ""2\r""

expect eof
"
run-llm.sh,"#!/bin/bash
#
# Helper script for deploying LlamaEdge API Server with a single Bash command
#
# - Works on Linux and macOS
# - Supports: CPU, CUDA, Metal, OpenCL
# - Can run GGUF models from https://huggingface.co/second-state/
#

set -e

# required utils: curl, git, make
if ! command -v curl &> /dev/null; then
    printf ""[-] curl not found\n""
    exit 1
fi
if ! command -v git &> /dev/null; then
    printf ""[-] git not found\n""
    exit 1
fi
if ! command -v make &> /dev/null; then
    printf ""[-] make not found\n""
    exit 1
fi

# parse arguments
port=8080
repo=""""
wtype=""""
backend=""cpu""
ctx_size=512
n_predict=1024
n_gpu_layers=100

# if macOS, use metal backend by default
if [[ ""$OSTYPE"" == ""darwin""* ]]; then
    backend=""metal""
elif command -v nvcc &> /dev/null; then
    backend=""cuda""
fi

gpu_id=0
n_parallel=8
n_kv=4096
verbose=0
log_prompts=0
log_stat=0
# 0: server mode
# 1: local mode
# mode=0
# 0: non-interactive
# 1: interactive
interactive=0
model=""""
# ggml version: latest or bxxxx
ggml_version=""latest""

function print_usage {
    printf ""Usage:\n""
    printf ""  ./run-llm.sh [--port]\n\n""
    printf ""  --model:        model name\n""
    printf ""  --interactive:  run in interactive mode\n""
    printf ""  --port:         port number, default is 8080\n""
    printf ""  --ggml-version: ggml version (for example, b2963). If the option is not used, then install the latest version.\n""
    printf ""Example:\n\n""
    printf '  bash <(curl -sSfL 'https://code.flows.network/webhook/iwYN1SdN3AmPgR5ao5Gt/run-llm.sh')""\n\n'
}

while [[ $# -gt 0 ]]; do
    key=""$1""
    case $key in
        --model)
            model=""$2""
            shift
            shift
            ;;
        --interactive)
            interactive=1
            shift
            ;;
        --port)
            port=""$2""
            shift
            shift
            ;;
        --ggml-version)
            ggml_version=""$2""
            shift
            shift
            ;;
        --help)
            print_usage
            exit 0
            ;;
        *)
            echo ""Unknown argument: $key""
            print_usage
            exit 1
            ;;
    esac
done

# available weights types
wtypes=(""Q2_K"" ""Q3_K_L"" ""Q3_K_M"" ""Q3_K_S"" ""Q4_0"" ""Q4_K_M"" ""Q4_K_S"" ""Q5_0"" ""Q5_K_M"" ""Q5_K_S"" ""Q6_K"" ""Q8_0"")

wfiles=()
for wt in ""${wtypes[@]}""; do
    wfiles+=("""")
done

ss_urls=(
    ""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-1.5-9B-Chat-GGUF/resolve/main/Yi-1.5-9B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/stablelm-2-zephyr-1.6b-GGUF/resolve/main/stablelm-2-zephyr-1_6b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/OpenChat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34B-Chat-GGUF/resolve/main/Yi-34B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34Bx2-MoE-60B-GGUF/resolve/main/Yi-34Bx2-MoE-60B-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-LLM-7B-Chat-GGUF/resolve/main/deepseek-llm-7b-chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-Coder-6.7B-Instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/Mistral-7B-Instruct-v0.2-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Orca-2-13B-GGUF/resolve/main/Orca-2-13b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/SOLAR-10.7B-Instruct-v1.0-GGUF/resolve/main/SOLAR-10.7B-Instruct-v1.0-Q5_K_M.gguf""
)

# sample models
ss_models=(
    ""gemma-2-9b-it""
    ""yi-1.5-9b-chat""
    ""phi-3-mini-4k""
    ""llama-3-8b-instruct""
    ""llama-2-7b-chat""
    ""stablelm-2-zephyr-1.6b""
    ""openchat-3.5-0106""
    ""yi-34b-chat""
    ""yi-34bx2-moe-60b""
    ""deepseek-llm-7b-chat""
    ""deepseek-coder-6.7b-instruct""
    ""mistral-7b-instruct-v0.2""
    ""dolphin-2.6-mistral-7b""
    ""orca-2-13b""
    ""tinyllama-1.1b-chat-v1.0""
    ""solar-10.7b-instruct-v1.0""
)

# prompt types
prompt_types=(
    ""gemma-instruct""
    ""chatml""
    ""phi-3-chat""
    ""llama-3-chat""
    ""llama-2-chat""
    ""chatml""
    ""openchat""
    ""zephyr""
    ""codellama-instruct""
    ""mistral-instruct""
    ""mistrallite""
    ""vicuna-chat""
    ""vicuna-1.1-chat""
    ""wizard-coder""
    ""intel-neural""
    ""deepseek-chat""
    ""deepseek-coder""
    ""solar-instruct""
    ""belle-llama-2-chat""
    ""human-assistant""
)


if [ -n ""$model"" ]; then
    printf ""\n""

    # Check if the model is in the list of supported models
    if [[ ! "" ${ss_models[@]} "" =~ "" ${model} "" ]]; then

        printf ""[+] ${model} is an invalid name or a unsupported model. Please check the model list:\n\n""

        for i in ""${!ss_models[@]}""; do
            printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
        done
        printf ""\n""

        # ask for repo until index of sample repo is provided or an URL
        while [[ -z ""$repo"" ]]; do

            read -p ""[+] Please select a number from the list above: "" repo

            # check if the input is a number
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        done
    else
        # Find the index of the model in the list of supported models
        for i in ""${!ss_models[@]}""; do
            if [[ ""${ss_models[$i]}"" = ""${model}"" ]]; then
                ss_model=""${ss_models[$i]}""
                repo=""${ss_urls[$i]}""

                break
            fi
        done

    fi

    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    ss_url=$repo

    repo=${repo%/resolve/main/*}

    # check file if the model has been downloaded before
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * prompt type and reverse prompt

    readme_url=""$repo/resolve/main/README.md""

    # Download the README.md file
    curl -s $readme_url -o README.md

    # Extract the ""Prompt type: xxxx"" line
    prompt_type_line=$(grep -i ""Prompt type:"" README.md)

    # Extract the xxxx part
    prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

    printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

    # Check if ""Reverse prompt"" exists
    if grep -q ""Reverse prompt:"" README.md; then
        # Extract the ""Reverse prompt: xxxx"" line
        reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

        # Extract the xxxx part
        reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
    else
        printf ""[+] No reverse prompt required\n""
    fi

    # Clean up
    rm README.md

    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Install WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    model_name=${wfile%-Q*}

    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

    # Add reverse prompt if it exists
    if [ -n ""$reverse_prompt"" ]; then
        cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
    fi

    printf ""\n""
    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    printf ""********************* [LOG: MODEL INFO (Load Model & Init Execution Context)] *********************""
    eval $cmd

elif [ ""$interactive"" -eq 0 ]; then

    printf ""\n""
    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Installing WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download gemma-2-9b-it-Q5_K_M.gguf
    ss_url=""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading %s ...\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    # * start llama-api-server
    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:gemma-2-9b-it-Q5_K_M.gguf llama-api-server.wasm -p gemma-instruct -c 4096 --model-name gemma-2-9b-it --socket-addr 0.0.0.0:${port}""

    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    eval $cmd

elif [ ""$interactive"" -eq 1 ]; then

    printf ""\n""
    printf ""[I] This is a helper script for deploying LlamaEdge API Server on this machine.\n\n""
    printf ""    The following tasks will be done:\n""
    printf ""    - Download GGUF model\n""
    printf ""    - Install WasmEdge Runtime and the wasi-nn_ggml plugin\n""
    printf ""    - Download LlamaEdge API Server\n""
    printf ""\n""
    printf ""    Upon the tasks done, an HTTP server will be started and it will serve the selected\n""
    printf ""    model.\n""
    printf ""\n""
    printf ""    Please note:\n""
    printf ""\n""
    printf ""    - All downloaded files will be stored in the current folder\n""
    printf ""    - The server will be listening on all network interfaces\n""
    printf ""    - The server will run with default settings which are not always optimal\n""
    printf ""    - Do not judge the quality of a model based on the results from this script\n""
    printf ""    - This script is only for demonstration purposes\n""
    printf ""\n""
    printf ""    During the whole process, you can press Ctrl-C to abort the current process at any time.\n""
    printf ""\n""
    printf ""    Press Enter to continue ...\n\n""

    read

    # * install WasmEdge + wasi-nn_ggml plugin

    printf ""[+] Installing WasmEdge ...\n\n""

    # Check if WasmEdge has been installed
    reinstall_wasmedge=1
    if command -v wasmedge &> /dev/null
    then
        printf ""    1) Install the latest version of WasmEdge and wasi-nn_ggml plugin (recommended)\n""
        printf ""    2) Keep the current version\n\n""
        read -p ""[+] Select a number from the list above: "" reinstall_wasmedge
    fi

    while [[ ""$reinstall_wasmedge"" -ne 1 && ""$reinstall_wasmedge"" -ne 2 ]]; do
        printf ""    Invalid number. Please enter number 1 or 2\n""
        read reinstall_wasmedge
    done

    if [[ ""$reinstall_wasmedge"" == ""1"" ]]; then
        # install WasmEdge + wasi-nn_ggml plugin
        if [ ""$ggml_version"" = ""latest"" ]; then
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        else
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        fi


    elif [[ ""$reinstall_wasmedge"" == ""2"" ]]; then
        wasmedge_path=$(which wasmedge)
        wasmedge_root_path=${wasmedge_path%""/bin/wasmedge""}

        found=0
        for file in ""$wasmedge_root_path/plugin/libwasmedgePluginWasiNN.""*; do
        if [[ -f $file ]]; then
            found=1
            break
        fi
        done

        if [[ $found -eq 0 ]]; then
            printf ""\n    * Not found wasi-nn_ggml plugin. Please download it from https://github.com/WasmEdge/WasmEdge/releases/ and move it to %s. After that, please rerun the script. \n\n"" ""$wasmedge_root_path/plugin/""

            exit 1
        fi

    fi

    printf ""[+] The most popular models at https://huggingface.co/second-state:\n\n""

    for i in ""${!ss_models[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
    done

    # ask for repo until index of sample repo is provided or an URL
    while [[ -z ""$repo"" ]]; do
        printf ""\n    Or choose one from: https://huggingface.co/models?sort=trending&search=gguf\n\n""

        read -p ""[+] Please select a number from the list above or enter an URL: "" repo

        # check if the input is a number
        if [[ ""$repo"" =~ ^[0-9]+$ ]]; then
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        elif [[ ""$repo"" =~ ^https?:// ]]; then
            repo=""$repo""
        else
            printf ""[-] Invalid repo URL: %s\n"" ""$repo""
            repo=""""
        fi
    done


    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    if [ -n ""$ss_model"" ]; then
        ss_url=$repo
        repo=${repo%/resolve/main/*}

        # check file if the model has been downloaded before
        wfile=$(basename ""$ss_url"")
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$ss_url""
        fi

    else

        printf ""[+] Checking for GGUF model files in %s\n\n"" ""$repo""

        # find GGUF files in the source
        model_tree=""${repo%/}/tree/main""
        model_files=$(curl -s ""$model_tree"" | grep -i ""\\.gguf</span>"" | sed -E 's/.*<span class=""truncate group-hover:underline"">(.*)<\/span><\/a>/\1/g')
        # Convert model_files into an array
        model_files_array=($model_files)

        while IFS= read -r line; do
            sizes+=(""$line"")
        done < <(curl -s ""$model_tree"" | awk -F 'download=true"">' '/download=true"">[0-9\.]+ (GB|MB)/ {print $2}' | awk '{print $1, $2}')

        # list all files in the provided git repo
        length=${#model_files_array[@]}
        for ((i=0; i<$length; i++)); do
            file=${model_files_array[i]}
            size=${sizes[i]}
            iw=-1
            is=0
            for wt in ""${wtypes[@]}""; do
                # uppercase
                ufile=$(echo ""$file"" | tr '[:lower:]' '[:upper:]')
                if [[ ""$ufile"" =~ ""$wt"" ]]; then
                    iw=$is
                    break
                fi
                is=$((is+1))
            done

            if [[ $iw -eq -1 ]]; then
                continue
            fi

            wfiles[$iw]=""$file""

            have="" ""
            if [[ -f ""$file"" ]]; then
                have=""*""
            fi

            printf ""    %2d) %s %7s   %s\n"" $iw ""$have"" ""$size"" ""$file""
        done

        # ask for weights type until provided and available
        while [[ -z ""$wtype"" ]]; do
            printf ""\n""
            read -p ""[+] Please select a number from the list above: "" wtype
            wfile=""${wfiles[$wtype]}""

            if [[ -z ""$wfile"" ]]; then
                printf ""[-] Invalid number: %s\n"" ""$wtype""
                wtype=""""
            fi
        done

        url=""${repo%/}/resolve/main/$wfile""

        # check file if the model has been downloaded before
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$url""
        fi

    fi

    # * prompt type and reverse prompt

    if [[ $repo =~ ^https://huggingface\.co/second-state ]]; then
        readme_url=""$repo/resolve/main/README.md""

        # Download the README.md file
        curl -s $readme_url -o README.md

        # Extract the ""Prompt type: xxxx"" line
        prompt_type_line=$(grep -i ""Prompt type:"" README.md)

        # Extract the xxxx part
        prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

        # Check if ""Reverse prompt"" exists
        if grep -q ""Reverse prompt:"" README.md; then
            # Extract the ""Reverse prompt: xxxx"" line
            reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

            # Extract the xxxx part
            reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

            printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
        else
            printf ""[+] No reverse prompt required\n""
        fi

        # Clean up
        rm README.md
    else
        printf ""[+] Please select a number from the list below:\n""
        printf ""    The definitions of the prompt types below can be found at https://github.com/LlamaEdge/LlamaEdge/raw/main/api-server/chat-prompts/README.md\n\n""

        is=0
        for r in ""${prompt_types[@]}""; do
            printf ""    %2d) %s\n"" $is ""$r""
            is=$((is+1))
        done
        printf ""\n""

        prompt_type_index=-1
        while ((prompt_type_index < 0 || prompt_type_index >= ${#prompt_types[@]})); do
            read -p ""[+] Select prompt type: "" prompt_type_index
            # Check if the input is a number
            if ! [[ ""$prompt_type_index"" =~ ^[0-9]+$ ]]; then
                echo ""Invalid input. Please enter a number.""
                prompt_type_index=-1
            fi
        done
        prompt_type=""${prompt_types[$prompt_type_index]}""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $need_reverse_prompt =~ ^[yYnN]$ ]]; do
            read -p ""[+] Need reverse prompt? (y/n): "" need_reverse_prompt
        done

        # If user answered yes, ask them to input a string
        if [[ ""$need_reverse_prompt"" == ""y"" || ""$need_reverse_prompt"" == ""Y"" ]]; then
            read -p ""    Enter the reverse prompt: "" reverse_prompt
            printf ""\n""
        fi
    fi

    # * running mode

    printf ""[+] Running mode: \n\n""

    running_modes=(""API Server with Chatbot web app"" ""CLI Chat"")

    for i in ""${!running_modes[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${running_modes[$i]}""
    done

    while [[ -z ""$running_mode_index"" ]]; do
        printf ""\n""
        read -p ""[+] Select a number from the list above: "" running_mode_index
        running_mode=""${running_modes[$running_mode_index - 1]}""

        if [[ -z ""$running_mode"" ]]; then
            printf ""[-] Invalid number: %s\n"" ""$running_mode_index""
            running_mode_index=""""
        fi
    done
    printf ""[+] Selected running mode: %s (%s)\n"" ""$running_mode_index"" ""$running_mode""

    # * download llama-api-server.wasm or llama-chat.wasm

    repo=""second-state/LlamaEdge""
    releases=$(curl -s ""https://api.github.com/repos/$repo/releases"")
    if [[ ""$running_mode_index"" == ""1"" ]]; then

        # * Download llama-api-server.wasm

        if [ -f ""llama-api-server.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-api-server.wasm. Download the latest llama-api-server.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-api-server.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

            printf ""\n""
        fi

        # * chatbot-ui

        if [ -d ""chatbot-ui"" ]; then
            printf ""[+] Using cached Chatbot web app\n""
        else
            printf ""[+] Downloading Chatbot web app ...\n""
            files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
            curl -LO $files_tarball
            if [ $? -ne 0 ]; then
                printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
                exit 1
            fi
            tar xzf chatbot-ui.tar.gz
            rm chatbot-ui.tar.gz
            printf ""\n""
        fi

        model_name=${wfile%-Q*}

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start the server:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_server =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start the server? (y/n): "" start_server
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_server"" == ""y"" || ""$start_server"" == ""Y"" ]]; then
            printf ""\n""
            printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
            printf ""*********************************** LlamaEdge API Server ********************************\n\n""
            eval $cmd

        fi

    elif [[ ""$running_mode_index"" == ""2"" ]]; then

        # * Download llama-chat.wasm

        if [ -f ""llama-chat.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-chat.wasm. Download the latest llama-chat.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-chat.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-chat.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-chat.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

            printf ""\n""
        fi

        # * prepare the command

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-chat.wasm --prompt-template $prompt_type""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start CLI Chat:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_chat =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start CLI Chat? (y/n): "" start_chat
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_chat"" == ""y"" || ""$start_chat"" == ""Y"" ]]; then
            printf ""\n""

            # Execute the command
            printf ""********************* LlamaEdge *********************\n\n""
            eval $cmd

        fi

    else
        printf ""[-] Invalid running mode: %s\n"" ""$running_mode_index""
        exit 1
    fi

else
    echo ""Invalid value for interactive""
fi

exit 0
"
rustfmt.toml,"array_width = 60
attr_fn_like_width = 70
binop_separator = ""Front""
blank_lines_lower_bound = 0
blank_lines_upper_bound = 1
brace_style = ""SameLineWhere""
chain_width = 60
color = ""Auto""
combine_control_expr = true
comment_width = 80
condense_wildcard_suffixes = false
control_brace_style = ""AlwaysSameLine""
disable_all_formatting = false
edition = ""2021""
emit_mode = ""Files""
empty_item_single_line = true
enum_discrim_align_threshold = 0
error_on_line_overflow = false
error_on_unformatted = false
fn_params_layout = ""Tall""
fn_call_width = 60
fn_single_line = false
force_explicit_abi = true
force_multiline_blocks = false
format_code_in_doc_comments = false
format_macro_bodies = true
format_macro_matchers = true
format_strings = false
group_imports = ""Preserve""
hard_tabs = false
hide_parse_errors = false
ignore = []
imports_granularity = ""Crate""
imports_indent = ""Block""
imports_layout = ""Mixed""
indent_style = ""Block""
inline_attribute_width = 0
make_backup = false
match_arm_blocks = true
match_arm_leading_pipes = ""Never""
match_block_trailing_comma = false
max_width = 100
merge_derives = true
newline_style = ""Auto""
normalize_comments = false
normalize_doc_attributes = false
overflow_delimited_expr = false
remove_nested_parens = true
reorder_impl_items = false
reorder_imports = true
reorder_modules = true
single_line_if_else_max_width = 50
skip_children = false
space_after_colon = true
space_before_colon = false
spaces_around_ranges = false
struct_field_align_threshold = 0
struct_lit_single_line = true
struct_lit_width = 18
struct_variant_width = 35
tab_spaces = 4
trailing_comma = ""Vertical""
trailing_semicolon = true
type_punctuation_density = ""Wide""
unstable_features = false
use_field_init_shorthand = false
use_small_heuristics = ""Default""
use_try_shorthand = false
version = ""One""
where_single_line = false
wrap_comments = false
"
tests/test_chat.hurl,"# test /v1/models endpoint
GET http://localhost:8080/v1/models
screencapability: low
HTTP 200
[Asserts]
jsonpath ""$.data[0].id"" == ""Qwen2-1.5B-Instruct""

# test /v1/chat/completions endpoint
POST http://localhost:8080/v1/chat/completions
Accept: application/json
Content-Type: application/json
```json
{
    ""messages"": [
        {
            ""role"": ""user"",
            ""content"": ""What is the capital of France?""
        }
    ],
    ""model"": ""Qwen2-1.5B-Instruct"",
    ""stream"": false
}
```
HTTP 200
[Asserts]
jsonpath ""$.model"" == ""Qwen2-1.5B-Instruct""
jsonpath ""$.choices[0].message.content"" contains ""Paris""


# test /v1/chat/completions endpoint
# Test purpose: The model name is incorrect
POST http://localhost:8080/v1/chat/completions
Accept: application/json
Content-Type: application/json
```json
{
    ""messages"": [
        {
            ""role"": ""user"",
            ""content"": ""What is the capital of France?""
        }
    ],
    ""model"": ""Qwen2-1.5B-Instruct-invalid"",
    ""stream"": false
}
```
HTTP 200
[Asserts]
jsonpath ""$.model"" == ""Qwen2-1.5B-Instruct""
jsonpath ""$.choices[0].message.content"" contains ""Paris"""
tests/test_embeddings.hurl,"
# test /v1/embeddings endpoint
POST http://localhost:8080/v1/embeddings
Accept: application/json
Content-Type: application/json
```json
{
    ""model"": ""nomic-embed-text-v1.5"",
    ""input"": [
        ""Gaianet is revolutionizing the AI landscape with a distributed AI infrastructure that seeks to decentralize the dominance of major players such as OpenAI, Google, and Anthropic. By leveraging a network of edge-computing nodes owned by individuals around the world, Gaianet enables hosting of both open-source and finely-tuned models. This infrastructure is designed to cater to diverse AI demands, offering a scalable alternative to traditional centralized servers."",
        ""The inception of Gaianet is driven by the necessity to address key issues in the current AI industry: censorship and bias in AI outputs, lack of privacy for user data, and the high costs associated with accessing and developing on centralized AI models. These challenges have restricted the dissemination of unbiased information, compromised data security, and erected barriers to innovation and broader application of AI technologies."",
        ""In an effort to mitigate these issues, Gaianet proposes a model that is inherently more inclusive and secure. It offers a platform where individuals can contribute their computing resources, expertise, and proprietary data to foster a collaborative advancement of AI technology. This approach not only democratizes access to AI but also aligns with principles of privacy and equitable distribution of resources."",
        ""The Gaianet ecosystem comprises four key components:\nComponent 1, The capability for individuals to run an Agent style API service on private devices, utilizing proprietary knowledge and finely-tuned models.\nComponent 2, Compatibility with the OpenAI API to ensure seamless integration with existing toolchains.\nComponent 3, A decentralized marketplace for AI services and components.\nComponent 4, Open-source development tools for creating LLM/AI applications."",
        ""Features of Gaia nodes are extensive and include integrated data, knowledge, and computing resources; support for open-source and OpenAI models; customization with fine-tuned LLMs; and the ability to chain multiple specialized models for enhanced functionality. Additionally, nodes offer knowledge bases for LLM enhancements, memorized chat history for extended conversations, and the capability for actions and function calls to external systems."",
        ""Gaianet also introduces a suite of ancillary offerings aimed at developers, including tools for fine-tuning LLMs, marketplaces for fine-tuned models and embeddings, and SDKs for various integrations."",
        ""The team behind Gaianet is spearheaded by CEO Matt Wright, previously Director of Fellowship Accelerator at Consensys and Director of DAOs. Chief Scientist Allen Yang brings his expertise from the University of California at Berkeley, while COO Shashank Sripada contributes his entrepreneurial experience. Advisors Michael Yuan and Brian Shin add further depth to the project's foundation."",
        ""Gaianet's official website can be accessed at [https://www.gaianet.ai/](https://www.gaianet.ai/), where you can find comprehensive information and resources about their initiatives.\n\nFor community engagement and discussions, Gaianet has a Discord server available at [https://discord.com/invite/gaianet-ai](https://discord.com/invite/gaianet-ai), welcoming members to join."",
        ""To stay updated with Gaianet's latest news and insights, you can follow their Telegram channel at [https://t.me/Gaianet_AI](https://t.me/Gaianet_AI).\n\nInsightful articles and updates from Gaianet are regularly published on their Medium blog at [https://medium.com/@Gaianet.ai](https://medium.com/@Gaianet.ai)."",
        ""For the latest announcements and engagements, follow Gaianet on Twitter at [https://twitter.com/Gaianet_AI](https://twitter.com/Gaianet_AI).\n\nDevelopers and contributors can explore Gaianet's GitHub repository at [https://github.com/GaiaNet-AI/](https://github.com/GaiaNet-AI/).""
    ]
}
```
HTTP 200
[Asserts]
jsonpath ""$.model"" == ""nomic-embed-text-v1.5""
jsonpath ""$.data"" count > 0

# test /v1/embeddings endpoint
# Test purpose: The model name is incorrect
POST http://localhost:8080/v1/embeddings
Accept: application/json
Content-Type: application/json
```json
{
    ""model"": ""nomic-embed-text-v1.5-invalid"",
    ""input"": [
        ""Gaianet is revolutionizing the AI landscape with a distributed AI infrastructure that seeks to decentralize the dominance of major players such as OpenAI, Google, and Anthropic. By leveraging a network of edge-computing nodes owned by individuals around the world, Gaianet enables hosting of both open-source and finely-tuned models. This infrastructure is designed to cater to diverse AI demands, offering a scalable alternative to traditional centralized servers."",
        ""The inception of Gaianet is driven by the necessity to address key issues in the current AI industry: censorship and bias in AI outputs, lack of privacy for user data, and the high costs associated with accessing and developing on centralized AI models. These challenges have restricted the dissemination of unbiased information, compromised data security, and erected barriers to innovation and broader application of AI technologies."",
        ""In an effort to mitigate these issues, Gaianet proposes a model that is inherently more inclusive and secure. It offers a platform where individuals can contribute their computing resources, expertise, and proprietary data to foster a collaborative advancement of AI technology. This approach not only democratizes access to AI but also aligns with principles of privacy and equitable distribution of resources."",
        ""The Gaianet ecosystem comprises four key components:\nComponent 1, The capability for individuals to run an Agent style API service on private devices, utilizing proprietary knowledge and finely-tuned models.\nComponent 2, Compatibility with the OpenAI API to ensure seamless integration with existing toolchains.\nComponent 3, A decentralized marketplace for AI services and components.\nComponent 4, Open-source development tools for creating LLM/AI applications."",
        ""Features of Gaia nodes are extensive and include integrated data, knowledge, and computing resources; support for open-source and OpenAI models; customization with fine-tuned LLMs; and the ability to chain multiple specialized models for enhanced functionality. Additionally, nodes offer knowledge bases for LLM enhancements, memorized chat history for extended conversations, and the capability for actions and function calls to external systems."",
        ""Gaianet also introduces a suite of ancillary offerings aimed at developers, including tools for fine-tuning LLMs, marketplaces for fine-tuned models and embeddings, and SDKs for various integrations."",
        ""The team behind Gaianet is spearheaded by CEO Matt Wright, previously Director of Fellowship Accelerator at Consensys and Director of DAOs. Chief Scientist Allen Yang brings his expertise from the University of California at Berkeley, while COO Shashank Sripada contributes his entrepreneurial experience. Advisors Michael Yuan and Brian Shin add further depth to the project's foundation."",
        ""Gaianet's official website can be accessed at [https://www.gaianet.ai/](https://www.gaianet.ai/), where you can find comprehensive information and resources about their initiatives.\n\nFor community engagement and discussions, Gaianet has a Discord server available at [https://discord.com/invite/gaianet-ai](https://discord.com/invite/gaianet-ai), welcoming members to join."",
        ""To stay updated with Gaianet's latest news and insights, you can follow their Telegram channel at [https://t.me/Gaianet_AI](https://t.me/Gaianet_AI).\n\nInsightful articles and updates from Gaianet are regularly published on their Medium blog at [https://medium.com/@Gaianet.ai](https://medium.com/@Gaianet.ai)."",
        ""For the latest announcements and engagements, follow Gaianet on Twitter at [https://twitter.com/Gaianet_AI](https://twitter.com/Gaianet_AI).\n\nDevelopers and contributors can explore Gaianet's GitHub repository at [https://github.com/GaiaNet-AI/](https://github.com/GaiaNet-AI/).""
    ]
}
```
HTTP 200
[Asserts]
jsonpath ""$.model"" == ""nomic-embed-text-v1.5""
jsonpath ""$.data"" count > 0"