"language: run-llm.sh
#!/bin/bash
#
# Helper script for deploying LlamaEdge API Server with a single Bash command
#
# - Works on Linux and macOS
# - Supports: CPU, CUDA, Metal, OpenCL
# - Can run GGUF models from https://huggingface.co/second-state/
#

set -e

# required utils: curl, git, make
if ! command -v curl &> /dev/null; then
    printf ""[-] curl not found\n""
    exit 1
fi
if ! command -v git &> /dev/null; then
    printf ""[-] git not found\n""
    exit 1
fi
if ! command -v make &> /dev/null; then
    printf ""[-] make not found\n""
    exit 1
fi

# parse arguments
port=8080
repo=""""
wtype=""""
backend=""cpu""
ctx_size=512
n_predict=1024
n_gpu_layers=100

# if macOS, use metal backend by default
if [[ ""$OSTYPE"" == ""darwin""* ]]; then
    backend=""metal""
elif command -v nvcc &> /dev/null; then
    backend=""cuda""
fi

gpu_id=0
n_parallel=8
n_kv=4096
verbose=0
log_prompts=0
log_stat=0
# 0: server mode
# 1: local mode
# mode=0
# 0: non-interactive
# 1: interactive
interactive=0
model=""""
# ggml version: latest or bxxxx
ggml_version=""latest""

function print_usage {
    printf ""Usage:\n""
    printf ""  ./run-llm.sh [--port]\n\n""
    printf ""  --model:        model name\n""
    printf ""  --interactive:  run in interactive mode\n""
    printf ""  --port:         port number, default is 8080\n""
    printf ""  --ggml-version: ggml version (for example, b2963). If the option is not used, then install the latest version.\n""
    printf ""Example:\n\n""
    printf '  bash <(curl -sSfL 'https://code.flows.network/webhook/iwYN1SdN3AmPgR5ao5Gt/run-llm.sh')""\n\n'
}

while [[ $# -gt 0 ]]; do
    key=""$1""
    case $key in
        --model)
            model=""$2""
            shift
            shift
            ;;
        --interactive)
            interactive=1
            shift
            ;;
        --port)
            port=""$2""
            shift
            shift
            ;;
        --ggml-version)
            ggml_version=""$2""
            shift
            shift
            ;;
        --help)
            print_usage
            exit 0
            ;;
        *)
            echo ""Unknown argument: $key""
            print_usage
            exit 1
            ;;
    esac
done

# available weights types
wtypes=(""Q2_K"" ""Q3_K_L"" ""Q3_K_M"" ""Q3_K_S"" ""Q4_0"" ""Q4_K_M"" ""Q4_K_S"" ""Q5_0"" ""Q5_K_M"" ""Q5_K_S"" ""Q6_K"" ""Q8_0"")

wfiles=()
for wt in ""${wtypes[@]}""; do
    wfiles+=("""")
done

ss_urls=(
    ""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-1.5-9B-Chat-GGUF/resolve/main/Yi-1.5-9B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/stablelm-2-zephyr-1.6b-GGUF/resolve/main/stablelm-2-zephyr-1_6b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/OpenChat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34B-Chat-GGUF/resolve/main/Yi-34B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34Bx2-MoE-60B-GGUF/resolve/main/Yi-34Bx2-MoE-60B-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-LLM-7B-Chat-GGUF/resolve/main/deepseek-llm-7b-chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-Coder-6.7B-Instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/Mistral-7B-Instruct-v0.2-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Orca-2-13B-GGUF/resolve/main/Orca-2-13b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/SOLAR-10.7B-Instruct-v1.0-GGUF/resolve/main/SOLAR-10.7B-Instruct-v1.0-Q5_K_M.gguf""
)

# sample models
ss_models=(
    ""gemma-2-9b-it""
    ""yi-1.5-9b-chat""
    ""phi-3-mini-4k""
    ""llama-3-8b-instruct""
    ""llama-2-7b-chat""
    ""stablelm-2-zephyr-1.6b""
    ""openchat-3.5-0106""
    ""yi-34b-chat""
    ""yi-34bx2-moe-60b""
    ""deepseek-llm-7b-chat""
    ""deepseek-coder-6.7b-instruct""
    ""mistral-7b-instruct-v0.2""
    ""dolphin-2.6-mistral-7b""
    ""orca-2-13b""
    ""tinyllama-1.1b-chat-v1.0""
    ""solar-10.7b-instruct-v1.0""
)

# prompt types
prompt_types=(
    ""gemma-instruct""
    ""chatml""
    ""phi-3-chat""
    ""llama-3-chat""
    ""llama-2-chat""
    ""chatml""
    ""openchat""
    ""zephyr""
    ""codellama-instruct""
    ""mistral-instruct""
    ""mistrallite""
    ""vicuna-chat""
    ""vicuna-1.1-chat""
    ""wizard-coder""
    ""intel-neural""
    ""deepseek-chat""
    ""deepseek-coder""
    ""solar-instruct""
    ""belle-llama-2-chat""
    ""human-assistant""
)


if [ -n ""$model"" ]; then
    printf ""\n""

    # Check if the model is in the list of supported models
    if [[ ! "" ${ss_models[@]} "" =~ "" ${model} "" ]]; then

        printf ""[+] ${model} is an invalid name or a unsupported model. Please check the model list:\n\n""

        for i in ""${!ss_models[@]}""; do
            printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
        done
        printf ""\n""

        # ask for repo until index of sample repo is provided or an URL
        while [[ -z ""$repo"" ]]; do

            read -p ""[+] Please select a number from the list above: "" repo

            # check if the input is a number
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        done
    else
        # Find the index of the model in the list of supported models
        for i in ""${!ss_models[@]}""; do
            if [[ ""${ss_models[$i]}"" = ""${model}"" ]]; then
                ss_model=""${ss_models[$i]}""
                repo=""${ss_urls[$i]}""

                break
            fi
        done

    fi

    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    ss_url=$repo

    repo=${repo%/resolve/main/*}

    # check file if the model has been downloaded before
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * prompt type and reverse prompt

    readme_url=""$repo/resolve/main/README.md""

    # Download the README.md file
    curl -s $readme_url -o README.md

    # Extract the ""Prompt type: xxxx"" line
    prompt_type_line=$(grep -i ""Prompt type:"" README.md)

    # Extract the xxxx part
    prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

    printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

    # Check if ""Reverse prompt"" exists
    if grep -q ""Reverse prompt:"" README.md; then
        # Extract the ""Reverse prompt: xxxx"" line
        reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

        # Extract the xxxx part
        reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
    else
        printf ""[+] No reverse prompt required\n""
    fi

    # Clean up
    rm README.md

    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Install WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    model_name=${wfile%-Q*}

    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

    # Add reverse prompt if it exists
    if [ -n ""$reverse_prompt"" ]; then
        cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
    fi

    printf ""\n""
    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    printf ""********************* [LOG: MODEL INFO (Load Model & Init Execution Context)] *********************""
    eval $cmd

elif [ ""$interactive"" -eq 0 ]; then

    printf ""\n""
    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Installing WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download gemma-2-9b-it-Q5_K_M.gguf
    ss_url=""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading %s ...\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    # * start llama-api-server
    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:gemma-2-9b-it-Q5_K_M.gguf llama-api-server.wasm -p gemma-instruct -c 4096 --model-name gemma-2-9b-it --socket-addr 0.0.0.0:${port}""

    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    eval $cmd

elif [ ""$interactive"" -eq 1 ]; then

    printf ""\n""
    printf ""[I] This is a helper script for deploying LlamaEdge API Server on this machine.\n\n""
    printf ""    The following tasks will be done:\n""
    printf ""    - Download GGUF model\n""
    printf ""    - Install WasmEdge Runtime and the wasi-nn_ggml plugin\n""
    printf ""    - Download LlamaEdge API Server\n""
    printf ""\n""
    printf ""    Upon the tasks done, an HTTP server will be started and it will serve the selected\n""
    printf ""    model.\n""
    printf ""\n""
    printf ""    Please note:\n""
    printf ""\n""
    printf ""    - All downloaded files will be stored in the current folder\n""
    printf ""    - The server will be listening on all network interfaces\n""
    printf ""    - The server will run with default settings which are not always optimal\n""
    printf ""    - Do not judge the quality of a model based on the results from this script\n""
    printf ""    - This script is only for demonstration purposes\n""
    printf ""\n""
    printf ""    During the whole process, you can press Ctrl-C to abort the current process at any time.\n""
    printf ""\n""
    printf ""    Press Enter to continue ...\n\n""

    read

    # * install WasmEdge + wasi-nn_ggml plugin

    printf ""[+] Installing WasmEdge ...\n\n""

    # Check if WasmEdge has been installed
    reinstall_wasmedge=1
    if command -v wasmedge &> /dev/null
    then
        printf ""    1) Install the latest version of WasmEdge and wasi-nn_ggml plugin (recommended)\n""
        printf ""    2) Keep the current version\n\n""
        read -p ""[+] Select a number from the list above: "" reinstall_wasmedge
    fi

    while [[ ""$reinstall_wasmedge"" -ne 1 && ""$reinstall_wasmedge"" -ne 2 ]]; do
        printf ""    Invalid number. Please enter number 1 or 2\n""
        read reinstall_wasmedge
    done

    if [[ ""$reinstall_wasmedge"" == ""1"" ]]; then
        # install WasmEdge + wasi-nn_ggml plugin
        if [ ""$ggml_version"" = ""latest"" ]; then
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        else
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        fi


    elif [[ ""$reinstall_wasmedge"" == ""2"" ]]; then
        wasmedge_path=$(which wasmedge)
        wasmedge_root_path=${wasmedge_path%""/bin/wasmedge""}

        found=0
        for file in ""$wasmedge_root_path/plugin/libwasmedgePluginWasiNN.""*; do
        if [[ -f $file ]]; then
            found=1
            break
        fi
        done

        if [[ $found -eq 0 ]]; then
            printf ""\n    * Not found wasi-nn_ggml plugin. Please download it from https://github.com/WasmEdge/WasmEdge/releases/ and move it to %s. After that, please rerun the script. \n\n"" ""$wasmedge_root_path/plugin/""

            exit 1
        fi

    fi

    printf ""[+] The most popular models at https://huggingface.co/second-state:\n\n""

    for i in ""${!ss_models[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
    done

    # ask for repo until index of sample repo is provided or an URL
    while [[ -z ""$repo"" ]]; do
        printf ""\n    Or choose one from: https://huggingface.co/models?sort=trending&search=gguf\n\n""

        read -p ""[+] Please select a number from the list above or enter an URL: "" repo

        # check if the input is a number
        if [[ ""$repo"" =~ ^[0-9]+$ ]]; then
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        elif [[ ""$repo"" =~ ^https?:// ]]; then
            repo=""$repo""
        else
            printf ""[-] Invalid repo URL: %s\n"" ""$repo""
            repo=""""
        fi
    done


    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    if [ -n ""$ss_model"" ]; then
        ss_url=$repo
        repo=${repo%/resolve/main/*}

        # check file if the model has been downloaded before
        wfile=$(basename ""$ss_url"")
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$ss_url""
        fi

    else

        printf ""[+] Checking for GGUF model files in %s\n\n"" ""$repo""

        # find GGUF files in the source
        model_tree=""${repo%/}/tree/main""
        model_files=$(curl -s ""$model_tree"" | grep -i ""\\.gguf</span>"" | sed -E 's/.*<span class=""truncate group-hover:underline"">(.*)<\/span><\/a>/\1/g')
        # Convert model_files into an array
        model_files_array=($model_files)

        while IFS= read -r line; do
            sizes+=(""$line"")
        done < <(curl -s ""$model_tree"" | awk -F 'download=true"">' '/download=true"">[0-9\.]+ (GB|MB)/ {print $2}' | awk '{print $1, $2}')

        # list all files in the provided git repo
        length=${#model_files_array[@]}
        for ((i=0; i<$length; i++)); do
            file=${model_files_array[i]}
            size=${sizes[i]}
            iw=-1
            is=0
            for wt in ""${wtypes[@]}""; do
                # uppercase
                ufile=$(echo ""$file"" | tr '[:lower:]' '[:upper:]')
                if [[ ""$ufile"" =~ ""$wt"" ]]; then
                    iw=$is
                    break
                fi
                is=$((is+1))
            done

            if [[ $iw -eq -1 ]]; then
                continue
            fi

            wfiles[$iw]=""$file""

            have="" ""
            if [[ -f ""$file"" ]]; then
                have=""*""
            fi

            printf ""    %2d) %s %7s   %s\n"" $iw ""$have"" ""$size"" ""$file""
        done

        # ask for weights type until provided and available
        while [[ -z ""$wtype"" ]]; do
            printf ""\n""
            read -p ""[+] Please select a number from the list above: "" wtype
            wfile=""${wfiles[$wtype]}""

            if [[ -z ""$wfile"" ]]; then
                printf ""[-] Invalid number: %s\n"" ""$wtype""
                wtype=""""
            fi
        done

        url=""${repo%/}/resolve/main/$wfile""

        # check file if the model has been downloaded before
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$url""
        fi

    fi

    # * prompt type and reverse prompt

    if [[ $repo =~ ^https://huggingface\.co/second-state ]]; then
        readme_url=""$repo/resolve/main/README.md""

        # Download the README.md file
        curl -s $readme_url -o README.md

        # Extract the ""Prompt type: xxxx"" line
        prompt_type_line=$(grep -i ""Prompt type:"" README.md)

        # Extract the xxxx part
        prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

        # Check if ""Reverse prompt"" exists
        if grep -q ""Reverse prompt:"" README.md; then
            # Extract the ""Reverse prompt: xxxx"" line
            reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

            # Extract the xxxx part
            reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

            printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
        else
            printf ""[+] No reverse prompt required\n""
        fi

        # Clean up
        rm README.md
    else
        printf ""[+] Please select a number from the list below:\n""
        printf ""    The definitions of the prompt types below can be found at https://github.com/LlamaEdge/LlamaEdge/raw/main/api-server/chat-prompts/README.md\n\n""

        is=0
        for r in ""${prompt_types[@]}""; do
            printf ""    %2d) %s\n"" $is ""$r""
            is=$((is+1))
        done
        printf ""\n""

        prompt_type_index=-1
        while ((prompt_type_index < 0 || prompt_type_index >= ${#prompt_types[@]})); do
            read -p ""[+] Select prompt type: "" prompt_type_index
            # Check if the input is a number
            if ! [[ ""$prompt_type_index"" =~ ^[0-9]+$ ]]; then
                echo ""Invalid input. Please enter a number.""
                prompt_type_index=-1
            fi
        done
        prompt_type=""${prompt_types[$prompt_type_index]}""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $need_reverse_prompt =~ ^[yYnN]$ ]]; do
            read -p ""[+] Need reverse prompt? (y/n): "" need_reverse_prompt
        done

        # If user answered yes, ask them to input a string
        if [[ ""$need_reverse_prompt"" == ""y"" || ""$need_reverse_prompt"" == ""Y"" ]]; then
            read -p ""    Enter the reverse prompt: "" reverse_prompt
            printf ""\n""
        fi
    fi

    # * running mode

    printf ""[+] Running mode: \n\n""

    running_modes=(""API Server with Chatbot web app"" ""CLI Chat"")

    for i in ""${!running_modes[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${running_modes[$i]}""
    done

    while [[ -z ""$running_mode_index"" ]]; do
        printf ""\n""
        read -p ""[+] Select a number from the list above: "" running_mode_index
        running_mode=""${running_modes[$running_mode_index - 1]}""

        if [[ -z ""$running_mode"" ]]; then
            printf ""[-] Invalid number: %s\n"" ""$running_mode_index""
            running_mode_index=""""
        fi
    done
    printf ""[+] Selected running mode: %s (%s)\n"" ""$running_mode_index"" ""$running_mode""

    # * download llama-api-server.wasm or llama-chat.wasm

    repo=""second-state/LlamaEdge""
    releases=$(curl -s ""https://api.github.com/repos/$repo/releases"")
    if [[ ""$running_mode_index"" == ""1"" ]]; then

        # * Download llama-api-server.wasm

        if [ -f ""llama-api-server.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-api-server.wasm. Download the latest llama-api-server.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-api-server.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

            printf ""\n""
        fi

        # * chatbot-ui

        if [ -d ""chatbot-ui"" ]; then
            printf ""[+] Using cached Chatbot web app\n""
        else
            printf ""[+] Downloading Chatbot web app ...\n""
            files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
            curl -LO $files_tarball
            if [ $? -ne 0 ]; then
                printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
                exit 1
            fi
            tar xzf chatbot-ui.tar.gz
            rm chatbot-ui.tar.gz
            printf ""\n""
        fi

        model_name=${wfile%-Q*}

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start the server:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_server =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start the server? (y/n): "" start_server
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_server"" == ""y"" || ""$start_server"" == ""Y"" ]]; then
            printf ""\n""
            printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
            printf ""*********************************** LlamaEdge API Server ********************************\n\n""
            eval $cmd

        fi

    elif [[ ""$running_mode_index"" == ""2"" ]]; then

        # * Download llama-chat.wasm

        if [ -f ""llama-chat.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-chat.wasm. Download the latest llama-chat.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-chat.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-chat.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-chat.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

            printf ""\n""
        fi

        # * prepare the command

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-chat.wasm --prompt-template $prompt_type""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start CLI Chat:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_chat =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start CLI Chat? (y/n): "" start_chat
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_chat"" == ""y"" || ""$start_chat"" == ""Y"" ]]; then
            printf ""\n""

            # Execute the command
            printf ""********************* LlamaEdge *********************\n\n""
            eval $cmd

        fi

    else
        printf ""[-] Invalid running mode: %s\n"" ""$running_mode_index""
        exit 1
    fi

else
    echo ""Invalid value for interactive""
fi

exit 0
"
"language: llama-chat/src/main.rs
use anyhow::bail;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use either::{Left, Right};
use endpoints::chat::{
    ChatCompletionChunk, ChatCompletionRequestBuilder, ChatCompletionRequestMessage,
    ChatCompletionRequestSampling, ChatCompletionUserMessageContent,
};
use futures::TryStreamExt;
use llama_core::{init_core_context, MetadataBuilder};
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Debug, Parser)]
#[command(author, about, version, long_about=None)]
struct Cli {
    /// Model name
    #[arg(short, long, default_value = ""default"")]
    model_name: String,
    /// Model alias
    #[arg(short = 'a', long, default_value = ""default"")]
    model_alias: String,
    /// Size of the prompt context
    #[arg(short, long, default_value = ""512"")]
    ctx_size: u64,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Batch size for prompt processing
    #[arg(short, long, default_value = ""512"")]
    batch_size: u64,
    /// Temperature for sampling
    #[arg(long, conflicts_with = ""top_p"")]
    temp: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, conflicts_with = ""temp"")]
    top_p: Option<f64>,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Sets the prompt template.
    #[arg(short, long, value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: PromptTemplateType,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// System prompt message string.
    #[arg(short, long)]
    system_prompt: Option<String>,
    /// Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Print all log information to stdout
    #[arg(long)]
    log_all: bool,
    /// enable streaming stdout
    #[arg(long, default_value = ""false"")]
    disable_stream: bool,
}

#[allow(clippy::needless_return)]
#[allow(unreachable_code)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> anyhow::Result<()> {
    // get the environment variable `PLUGIN_DEBUG`
    let plugin_debug = std::env::var(""PLUGIN_DEBUG"").unwrap_or_default();
    let plugin_debug = match plugin_debug.is_empty() {
        true => false,
        false => plugin_debug.to_lowercase().parse::<bool>().unwrap_or(false),
    };

    // parse the command line arguments
    let cli = Cli::parse();

    // log version
    log(format!(
        ""\n[INFO] llama-chat version: {}"",
        env!(""CARGO_PKG_VERSION"")
    ));

    // log the cli options
    log(format!(""[INFO] Model name: {}"", &cli.model_name));
    log(format!(""[INFO] Model alias: {}"", &cli.model_alias));
    log(format!(""[INFO] Prompt template: {}"", &cli.prompt_template));
    // ctx size
    log(format!(""[INFO] Context size: {}"", &cli.ctx_size));
    // reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        log(format!(""[INFO] reverse prompt: {}"", reverse_prompt));
    }
    // system prompt
    if let Some(system_prompt) = &cli.system_prompt {
        log(format!(""[INFO] system prompt: {}"", system_prompt));
    }
    // n_predict
    log(format!(
        ""[INFO] Number of tokens to predict: {}"",
        &cli.n_predict
    ));
    // n_gpu_layers
    log(format!(
        ""[INFO] Number of layers to run on the GPU: {}"",
        &cli.n_gpu_layers
    ));
    // main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        log(format!(""[INFO] Main GPU to use: {}"", main_gpu));
    }
    // tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        log(format!(""[INFO] Tensor split: {}"", tensor_split));
    }
    log(format!(""[INFO] Threads: {}"", &cli.threads));
    // no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        log(format!(
            ""[INFO] Disable memory mapping for file access of chat models : {}"",
            &no_mmap
        ));
    }
    // batch size
    log(format!(
        ""[INFO] Batch size for prompt processing: {}"",
        &cli.batch_size
    ));
    // temp and top_p
    if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
    }
    // repeat penalty
    log(format!(
        ""[INFO] Penalize repeat sequence of tokens: {}"",
        &cli.repeat_penalty
    ));
    // presence penalty
    log(format!(
        ""[INFO] Presence penalty (0.0 = disabled): {}"",
        &cli.presence_penalty
    ));
    // frequency penalty
    log(format!(
        ""[INFO] Frequency penalty (0.0 = disabled): {}"",
        &cli.frequency_penalty
    ));
    // grammar
    log(format!(""[INFO] BNF-like grammar: {}"", &cli.grammar));
    // json schema
    if let Some(json_schema) = &cli.json_schema {
        log(format!(""[INFO] JSON schema: {}"", json_schema));
    }
    // log prompts
    log(format!(""[INFO] Enable prompt log: {}"", &cli.log_prompts));
    // log statistics
    log(format!(""[INFO] Enable plugin log: {}"", &cli.log_stat));

    // create a MetadataBuilder instance
    let builder = MetadataBuilder::new(&cli.model_name, &cli.model_alias, cli.prompt_template)
        .with_ctx_size(cli.ctx_size)
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_batch_size(cli.batch_size)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .enable_prompts_log(cli.log_prompts || cli.log_all)
        .enable_plugin_log(cli.log_stat || cli.log_all)
        .enable_debug_log(plugin_debug);
    // temp and top_p
    let builder = if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
        builder.with_top_p(top_p)
    } else {
        let temp = cli.temp.unwrap();
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    };
    // create a Metadata instance
    let metadata = builder.build();

    // initialize the core context
    init_core_context(Some(&[metadata]), None)?;

    // get the plugin version info
    let plugin_info = llama_core::get_plugin_info()?;
    log(format!(
        ""[INFO] Wasi-nn-ggml plugin: b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    ));

    // create a ChatCompletionRequestSampling instance
    let sampling = if cli.temp.is_none() && cli.top_p.is_none() {
        ChatCompletionRequestSampling::Temperature(1.0)
    } else if let Some(temp) = cli.temp {
        ChatCompletionRequestSampling::Temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        ChatCompletionRequestSampling::TopP(top_p)
    } else {
        let temp = cli.temp.unwrap();
        ChatCompletionRequestSampling::Temperature(temp)
    };

    // create a chat request
    let mut chat_request = ChatCompletionRequestBuilder::new(&cli.model_name, vec![])
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_sampling(sampling)
        .enable_stream(!cli.disable_stream)
        .build();

    // add system message if provided
    if let Some(system_prompt) = &cli.system_prompt {
        let system_message = ChatCompletionRequestMessage::new_system_message(system_prompt, None);

        chat_request.messages.push(system_message);
    }

    let readme = ""
================================== Running in interactive mode. ===================================\n
    - Press [Ctrl+C] to interject at any time.
    - Press [Return] to end the input.
    - For multi-line inputs, end each line with '\\' and press [Return] to get another line.\n"";
    log(readme);

    loop {
        println!(""\n[You]: "");
        let user_input = read_input();

        // put the user message into the messages sequence of chat_request
        let user_message = ChatCompletionRequestMessage::new_user_message(
            ChatCompletionUserMessageContent::Text(user_input),
            None,
        );

        chat_request.messages.push(user_message);

        if cli.log_stat || cli.log_all {
            print_log_begin_separator(""STATISTICS (Set Input)"", Some(""*""), None);
        }

        if cli.log_stat || cli.log_all {
            print_log_end_separator(Some(""*""), None);
        }

        println!(""\n[Bot]:"");
        let mut assistant_answer = String::new();
        match llama_core::chat::chat(&mut chat_request).await {
            Ok(res) => match res {
                Left(mut stream) => {
                    while let Some(data) = stream.try_next().await? {
                        if let Some(chunk) = parse_sse_event(&data) {
                            if let Some(content) = &chunk.choices[0].delta.content {
                                if content.is_empty() {
                                    continue;
                                }
                                if assistant_answer.is_empty() {
                                    let content = content.trim_start();
                                    print!(""{}"", content);
                                    assistant_answer.push_str(content);
                                } else {
                                    print!(""{content}"");
                                    assistant_answer.push_str(content);
                                }
                                io::stdout().flush().unwrap();
                            }
                        }
                    }
                    println!();
                }
                Right(completion) => {
                    let chat_completion = completion.choices[0]
                        .message
                        .content
                        .to_owned()
                        .unwrap_or_default();
                    println!(""{chat_completion}"");
                    assistant_answer = chat_completion;
                }
            },
            Err(e) => {
                bail!(""Fail to generate chat completion. Reason: {msg}"", msg = e)
            }
        };

        let assistant_message = ChatCompletionRequestMessage::new_assistant_message(
            Some(assistant_answer.trim().to_string()),
            None,
            None,
        );
        chat_request.messages.push(assistant_message);
    }

    Ok(())
}

// For single line input, just press [Return] to end the input.
// For multi-line input, end your input with '\\' and press [Return].
//
// For example:
//  [You]:
//  what is the capital of France?[Return]
//
//  [You]:
//  Count the words in the following sentence: \[Return]
//  \[Return]
//  You can use Git to save new files and any changes to already existing files as a bundle of changes called a commit, which can be thought of as a “revision” to your project.[Return]
//
fn read_input() -> String {
    let mut answer = String::new();
    loop {
        let mut temp = String::new();
        std::io::stdin()
            .read_line(&mut temp)
            .expect(""The read bytes are not valid UTF-8"");

        if temp.ends_with(""\\\n"") {
            temp.pop();
            temp.pop();
            temp.push('\n');
            answer.push_str(&temp);
            continue;
        } else if temp.ends_with('\n') {
            answer.push_str(&temp);
            return answer;
        } else {
            return answer;
        }
    }
}

fn print_log_begin_separator(
    title: impl AsRef<str>,
    ch: Option<&str>,
    len: Option<usize>,
) -> usize {
    let title = format!("" [LOG: {}] "", title.as_ref());

    let total_len: usize = len.unwrap_or(100);
    let separator_len: usize = (total_len - title.len()) / 2;

    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push_str(&title);
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push('\n');
    println!(""{}"", separator);
    total_len
}

fn print_log_end_separator(ch: Option<&str>, len: Option<usize>) {
    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(len.unwrap_or(100)).as_str());
    separator.push('\n');
    println!(""{}"", separator);
}

#[derive(Debug, Default, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    // #[serde(rename = ""enable-debug-log"")]
    // pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    // pub mmproj: String,
    // pub image: String,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    // #[serde(rename = ""main-gpu"")]
    // pub main_gpu: u64,
    // #[serde(rename = ""tensor-split"")]
    // pub tensor_split: String,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,

    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,
}

fn log(msg: impl std::fmt::Display) {
    println!(""{}"", msg);
}

fn parse_sse_event(s: &str) -> Option<ChatCompletionChunk> {
    let lines: Vec<&str> = s.split('\n').collect();
    // let mutevent = None;
    let mut data = None;

    for line in lines {
        if line.starts_with(""data:"") {
            data = Some(line.trim_start_matches(""data:"").trim());
        }
    }

    match data {
        Some(s) => {
            if s.trim() == ""[DONE]"" {
                return None;
            }

            match serde_json::from_str(s) {
                Ok(chunk) => Some(chunk),
                Err(e) => {
                    log(format!(
                        ""[ERROR] Fail to parse SSE data. Reason: {msg}. Data: {data}"",
                        msg = e,
                        data = s
                    ));
                    None
                }
            }
        }
        _ => None,
    }
}
"
"language: llama-api-server/src/main.rs
#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::{ArgGroup, Parser};
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default port
const DEFAULT_PORT: &str = ""8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
#[command(group = ArgGroup::new(""socket_address_group"").multiple(false).args(&[""socket_addr"", ""port""]))]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance. For example, `0.0.0.0:8080`.
    #[arg(long, default_value = None, value_parser = clap::value_parser!(SocketAddr), group = ""socket_address_group"")]
    socket_addr: Option<SocketAddr>,
    /// Port number
    #[arg(long, default_value = DEFAULT_PORT, value_parser = clap::value_parser!(u16), group = ""socket_address_group"")]
    port: u16,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[allow(clippy::needless_return)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = match cli.socket_addr {
        Some(addr) => addr,
        None => SocketAddr::from(([0, 0, 0, 0], cli.port)),
    };
    let port = addr.port().to_string();

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    info!(target: ""stdout"", ""Listening on {}"", addr);

    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}
"
"language: llama-api-server/src/utils.rs
use serde::{Deserialize, Serialize};

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

#[derive(
    Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, clap::ValueEnum, Serialize, Deserialize,
)]
#[serde(rename_all = ""lowercase"")]
pub(crate) enum LogLevel {
    /// Describes messages about the values of variables and the flow of
    /// control within a program.
    Trace,

    /// Describes messages likely to be of interest to someone debugging a
    /// program.
    Debug,

    /// Describes messages likely to be of interest to someone monitoring a
    /// program.
    Info,

    /// Describes messages indicating hazardous situations.
    Warn,

    /// Describes messages indicating serious errors.
    Error,

    /// Describes messages indicating fatal errors.
    Critical,
}
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error,
        }
    }
}
impl std::fmt::Display for LogLevel {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            LogLevel::Trace => write!(f, ""trace""),
            LogLevel::Debug => write!(f, ""debug""),
            LogLevel::Info => write!(f, ""info""),
            LogLevel::Warn => write!(f, ""warn""),
            LogLevel::Error => write!(f, ""error""),
            LogLevel::Critical => write!(f, ""critical""),
        }
    }
}
impl std::str::FromStr for LogLevel {
    type Err = String;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""trace"" => Ok(LogLevel::Trace),
            ""debug"" => Ok(LogLevel::Debug),
            ""info"" => Ok(LogLevel::Info),
            ""warn"" => Ok(LogLevel::Warn),
            ""error"" => Ok(LogLevel::Error),
            ""critical"" => Ok(LogLevel::Critical),
            _ => Err(format!(""Invalid log level: {}"", s)),
        }
    }
}
"
"language: llama-api-server/src/error.rs
use hyper::{Body, Response};
use thiserror::Error;

#[allow(dead_code)]
pub(crate) fn not_implemented() -> Response<Body> {
    // log error
    error!(target: ""stdout"", ""501 Not Implemented"");

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_IMPLEMENTED)
        .body(Body::from(""501 Not Implemented""))
        .unwrap()
}

pub(crate) fn internal_server_error(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""500 Internal Server Error"".to_string(),
        false => format!(""500 Internal Server Error: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::INTERNAL_SERVER_ERROR)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn bad_request(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""400 Bad Request"".to_string(),
        false => format!(""400 Bad Request: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::BAD_REQUEST)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn invalid_endpoint(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""404 The requested service endpoint is not found"".to_string(),
        false => format!(
            ""404 The requested service endpoint is not found: {}"",
            msg.as_ref()
        ),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND)
        .body(Body::from(err_msg))
        .unwrap()
}

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum ServerError {
    /// Error returned while parsing CLI options failed
    #[error(""{0}"")]
    ArgumentError(String),
    /// Generic error returned while performing an operation
    #[error(""{0}"")]
    Operation(String),
}
"
"language: llama-api-server/src/backend/mod.rs
pub(crate) mod ggml;

use crate::error;
use hyper::{Body, Request, Response};

pub(crate) async fn handle_llama_request(req: Request<Body>) -> Response<Body> {
    match req.uri().path() {
        ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
        ""/v1/completions"" => ggml::completions_handler(req).await,
        ""/v1/models"" => ggml::models_handler().await,
        ""/v1/embeddings"" => ggml::embeddings_handler(req).await,
        ""/v1/files"" => ggml::files_handler(req).await,
        ""/v1/chunks"" => ggml::chunks_handler(req).await,
        ""/v1/info"" => ggml::server_info_handler().await,
        path => {
            if path.starts_with(""/v1/files/"") {
                ggml::files_handler(req).await
            } else {
                error::invalid_endpoint(path)
            }
        }
    }
}
"
"language: llama-api-server/src/backend/ggml.rs
use crate::{error, utils::gen_chat_id, SERVER_INFO};
use endpoints::{
    chat::ChatCompletionRequest,
    completions::CompletionRequest,
    embeddings::EmbeddingRequest,
    files::{DeleteFileStatus, FileObject, ListFilesResponse},
    rag::{ChunksRequest, ChunksResponse},
};
use futures_util::TryStreamExt;
use hyper::{body::to_bytes, Body, Method, Request, Response};
use multipart::server::{Multipart, ReadEntry, ReadEntryResult};
use multipart_2021 as multipart;
use std::{
    fs::{self, File},
    io::{Cursor, Read, Write},
    path::Path,
    time::SystemTime,
};
use walkdir::{DirEntry, WalkDir};

/// List all models available.
pub(crate) async fn models_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming model list request."");

    let list_models_response = match llama_core::models::models().await {
        Ok(list_models_response) => list_models_response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // serialize response
    let s = match serde_json::to_string(&list_models_response) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Failed to serialize the model list result. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the model list response."");

    res
}

/// Compute embeddings for the input text and return the embeddings object.
pub(crate) async fn embeddings_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming embeddings request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""embeddings_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut embedding_request: EmbeddingRequest = match serde_json::from_slice(&body_bytes) {
        Ok(embedding_request) => embedding_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize embedding request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if embedding_request.user.is_none() {
        embedding_request.user = Some(gen_chat_id())
    };
    let id = embedding_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::embeddings::embeddings(&embedding_request).await {
        Ok(embedding_response) => {
            // serialize embedding object
            match serde_json::to_string(&embedding_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .header(""user"", id)
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize embedding object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the embeddings response"");

    res
}

/// Process a completion request and returns a completion response with the answer from the model.
pub(crate) async fn completions_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming completions request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""completions_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut completion_request: CompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(completion_request) => completion_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize completions request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if completion_request.user.is_none() {
        completion_request.user = Some(gen_chat_id())
    };
    let id = completion_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::completions::completions(&completion_request).await {
        Ok(completion_object) => {
            // serialize completion object
            let s = match serde_json::to_string(&completion_object) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Fail to serialize completion object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .header(""user"", id)
                .body(Body::from(s));
            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the completions response."");

    res
}

/// Process a chat-completion request and returns a chat-completion response with the answer from the model.
pub(crate) async fn chat_completions_handler(mut req: Request<Body>) -> Response<Body> {
    info!(target: ""stdout"", ""Handling the coming chat completion request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    info!(target: ""stdout"", ""Prepare the chat completion request."");

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut chat_request: ChatCompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chat_request) => chat_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chat completion request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the user id is provided
    if chat_request.user.is_none() {
        chat_request.user = Some(gen_chat_id())
    };
    let id = chat_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", chat_request.user.clone().unwrap());

    let res = match llama_core::chat::chat(&mut chat_request).await {
        Ok(result) => match result {
            either::Left(stream) => {
                let stream = stream.map_err(|e| e.to_string());

                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""text/event-stream"")
                    .header(""Cache-Control"", ""no-cache"")
                    .header(""Connection"", ""keep-alive"")
                    .header(""user"", id)
                    .body(Body::wrap_stream(stream));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""finish chat completions in stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            either::Right(chat_completion_object) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&chat_completion_object) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize chat completion object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .header(""user"", id)
                    .body(Body::from(s));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""Finish chat completions in non-stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in non-stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
        },
        Err(e) => {
            let err_msg = format!(""Failed to get chat completions. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the chat completion response."");

    res
}

/// Upload files and return the file object.
pub(crate) async fn files_handler(req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming files request"");

    let res = if req.method() == Method::POST {
        let boundary = ""boundary="";

        let boundary = req.headers().get(""content-type"").and_then(|ct| {
            let ct = ct.to_str().ok()?;
            let idx = ct.find(boundary)?;
            Some(ct[idx + boundary.len()..].to_string())
        });

        let req_body = req.into_body();
        let body_bytes = match to_bytes(req_body).await {
            Ok(body_bytes) => body_bytes,
            Err(e) => {
                let err_msg = format!(""Fail to read buffer from request body. {}"", e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        let cursor = Cursor::new(body_bytes.to_vec());

        let mut multipart = Multipart::with_body(cursor, boundary.unwrap());

        let mut file_object: Option<FileObject> = None;
        while let ReadEntryResult::Entry(mut field) = multipart.read_entry_mut() {
            if &*field.headers.name == ""file"" {
                let filename = match field.headers.filename {
                    Some(filename) => filename,
                    None => {
                        let err_msg =
                            ""Failed to upload the target file. The filename is not provided."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                if !((filename).to_lowercase().ends_with("".txt"")
                    || (filename).to_lowercase().ends_with("".md""))
                    || (filename).to_lowercase().ends_with("".png"")
                {
                    let err_msg = format!(
                        ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported. The file extension is {}."",
                        &filename
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }

                let mut buffer = Vec::new();
                let size_in_bytes = match field.data.read_to_end(&mut buffer) {
                    Ok(size_in_bytes) => size_in_bytes,
                    Err(e) => {
                        let err_msg = format!(""Failed to read the target file. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a unique file id
                let id = format!(""file_{}"", uuid::Uuid::new_v4());

                // save the file
                let path = Path::new(""archives"");
                if !path.exists() {
                    fs::create_dir(path).unwrap();
                }
                let file_path = path.join(&id);
                if !file_path.exists() {
                    fs::create_dir(&file_path).unwrap();
                }
                let mut file = match File::create(file_path.join(&filename)) {
                    Ok(file) => file,
                    Err(e) => {
                        let err_msg =
                            format!(""Failed to create archive document {}. {}"", &filename, e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };
                file.write_all(&buffer[..]).unwrap();

                // log
                info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

                let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a file object
                file_object = Some(FileObject {
                    id,
                    bytes: size_in_bytes as u64,
                    created_at,
                    filename,
                    object: ""file"".to_string(),
                    purpose: ""assistants"".to_string(),
                });

                break;
            }
        }

        match file_object {
            Some(fo) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&fo) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize file object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .body(Body::from(s));

                match result {
                    Ok(response) => response,
                    Err(e) => {
                        let err_msg = e.to_string();

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            None => {
                let err_msg = ""Failed to upload the target file. Not found the target file."";

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::GET {
        let uri_path = req.uri().path();

        if uri_path == ""/v1/files"" {
            let mut file_objects: Vec<FileObject> = Vec::new();
            for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let id = entry
                        .path()
                        .parent()
                        .and_then(|p| p.file_name())
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .to_string();

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    let fo = FileObject {
                        id,
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    };

                    file_objects.push(fo);
                }
            }

            info!(target: ""stdout"", ""Found {} archive files"", file_objects.len());

            let file_objects = ListFilesResponse {
                object: ""list"".to_string(),
                data: file_objects,
            };

            // serialize chat completion object
            let s = match serde_json::to_string(&file_objects) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Failed to serialize file object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(s));

            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        } else {
            let id = uri_path.trim_start_matches(""/v1/files/"");
            let root = format!(""archives/{}"", id);
            let mut file_object: Option<FileObject> = None;
            for entry in WalkDir::new(root).into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    file_object = Some(FileObject {
                        id: id.into(),
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    });

                    break;
                }
            }

            match file_object {
                Some(fo) => {
                    // serialize chat completion object
                    let s = match serde_json::to_string(&fo) {
                        Ok(s) => s,
                        Err(e) => {
                            let err_msg = format!(""Failed to serialize file object. {}"", e);

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            return error::internal_server_error(err_msg);
                        }
                    };

                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));

                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                None => {
                    let err_msg = format!(
                        ""Failed to retrieve the target file. Not found the target file with id {}."",
                        id
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
    } else if req.method() == Method::DELETE {
        let id = req.uri().path().trim_start_matches(""/v1/files/"");
        let root = format!(""archives/{}"", id);
        let status = match fs::remove_dir_all(root) {
            Ok(_) => {
                info!(target: ""stdout"", ""Successfully deleted the target file with id {}."", id);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: true,
                }
            }
            Err(e) => {
                let err_msg = format!(""Failed to delete the target file with id {}. {}"", id, e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: false,
                }
            }
        };

        // serialize status
        let s = match serde_json::to_string(&status) {
            Ok(s) => s,
            Err(e) => {
                let err_msg = format!(
                    ""Failed to serialize the status of the file deletion operation. {}"",
                    e
                );

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        // return response
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::from(s));

        match result {
            Ok(response) => response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::OPTIONS {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""files_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    } else {
        let err_msg = ""Invalid HTTP Method."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        error::internal_server_error(err_msg)
    };

    info!(target: ""stdout"", ""Send the files response"");

    res
}

/// Segment the text into chunks and return the chunks response.
pub(crate) async fn chunks_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming chunks request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""chunks_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    let chunks_request: ChunksRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chunks_request) => chunks_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chunks request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the archives directory exists
    let path = Path::new(""archives"");
    if !path.exists() {
        let err_msg = ""The `archives` directory does not exist."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the archive id exists
    let archive_path = path.join(&chunks_request.id);
    if !archive_path.exists() {
        let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the file exists
    let file_path = archive_path.join(&chunks_request.filename);
    if !file_path.exists() {
        let err_msg = format!(
            ""Not found file: {} in archive id: {}"",
            &chunks_request.filename, &chunks_request.id
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // log
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &chunks_request.id, &chunks_request.filename);

    // get the extension of the archived file
    let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
        Some(extension) => extension,
        None => {
            let err_msg = format!(
                ""Failed to get the extension of the archived `{}`."",
                &chunks_request.filename
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // open the file
    let mut file = match File::open(&file_path) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to open `{}`. {}"", &chunks_request.filename, e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // read the file
    let mut contents = String::new();
    if let Err(e) = file.read_to_string(&mut contents) {
        let err_msg = format!(""Failed to read `{}`. {}"", &chunks_request.filename, e);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
    {
        Ok(chunks) => {
            let chunks_response = ChunksResponse {
                id: chunks_request.id,
                filename: chunks_request.filename,
                chunks,
            };

            // serialize embedding object
            match serde_json::to_string(&chunks_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize chunks response. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the chunks response."");

    res
}

/// Return the server info.
pub(crate) async fn server_info_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming server info request."");

    // get the server info
    let server_info = match SERVER_INFO.get() {
        Some(server_info) => server_info,
        None => {
            let err_msg = ""The server info is not set."";

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(""The server info is not set."");
        }
    };

    // serialize server info
    let s = match serde_json::to_string(&server_info) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Fail to serialize server info. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the server info response."");

    res
}

fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}
"
"language: docker/run.sh
#!/bin/bash

source /root/.wasmedge/env

chat_file_name=$1
embedding_file_name=$2
prompt_template=$3
chat_ctx_size=$4
embedding_ctx_size=$5

if [ -z ""$chat_ctx_size"" ]; then
    chat_ctx_size=512
fi

if [ -z ""$embedding_ctx_size"" ]; then
    embedding_ctx_size=256
fi

if [ ""$embedding_ctx_size"" -eq ""0"" ]; then
    wasmedge --dir .:. --nn-preload default:GGML:AUTO:/models/$chat_file_name llama-api-server.wasm --prompt-template $prompt_template --ctx-size $chat_ctx_size --model-name $chat_file_name --socket-addr 0.0.0.0:8080
else
    wasmedge --dir .:. --nn-preload default:GGML:AUTO:/models/$chat_file_name --nn-preload embedding:GGML:AUTO:/models/$embedding_file_name llama-api-server.wasm --prompt-template $prompt_template,embedding --ctx-size $chat_ctx_size,$embedding_ctx_size --model-name $chat_file_name,$embedding_file_name --socket-addr 0.0.0.0:8080
fi
"
"language: crates/chat-prompts/src/error.rs
use endpoints::chat::ChatCompletionRole;
use thiserror::Error;

pub type Result<T> = std::result::Result<T, PromptError>;

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum PromptError {
    #[error(""There must be at least one user message to create a prompt from."")]
    NoMessages,
    #[error(""No user message to create prompt from."")]
    NoUserMessage,
    #[error(""No content in the assistant message when the `tool_calls` is not specified."")]
    NoAssistantMessage,
    #[error(""No available tools to create prompt from."")]
    NoAvailableTools,
    #[error(""Bad messages. {0}"")]
    BadMessages(String),
    #[error(""Unknown chat completion role: {0:?}"")]
    UnknownRole(ChatCompletionRole),
    #[error(""Unknown prompt template type: {0}"")]
    UnknownPromptTemplateType(String),
    #[error(""Failed to build prompt. Reason: {0}"")]
    Operation(String),
}
"
"language: crates/chat-prompts/src/lib.rs
//! `chat-prompts` is part of [LlamaEdge API Server](https://github.com/LlamaEdge/LlamaEdge/tree/main/api-server) project. It provides a collection of prompt templates that are used to generate prompts for the LLMs (See models in [huggingface.co/second-state](https://huggingface.co/second-state)).
//!
//! For the details of available prompt templates, see [README.md](https://github.com/LlamaEdge/LlamaEdge/tree/main/api-server/chat-prompts).

pub mod chat;
pub mod error;

use clap::ValueEnum;
use endpoints::chat::ChatCompletionRequestMessage;
use serde::{Deserialize, Serialize};
use std::str::FromStr;

/// Define the chat prompt template types.
#[derive(Clone, Debug, Copy, PartialEq, Eq, Serialize, Deserialize, ValueEnum)]
pub enum PromptTemplateType {
    #[value(name = ""llama-2-chat"")]
    Llama2Chat,
    #[value(name = ""llama-3-chat"")]
    Llama3Chat,
    #[value(name = ""llama-3-tool"")]
    Llama3Tool,
    #[value(name = ""mistral-instruct"")]
    MistralInstruct,
    #[value(name = ""mistral-tool"")]
    MistralTool,
    #[value(name = ""mistrallite"")]
    MistralLite,
    #[value(name = ""openchat"")]
    OpenChat,
    #[value(name = ""codellama-instruct"")]
    CodeLlama,
    #[value(name = ""codellama-super-instruct"")]
    CodeLlamaSuper,
    #[value(name = ""human-assistant"")]
    HumanAssistant,
    #[value(name = ""vicuna-1.0-chat"")]
    VicunaChat,
    #[value(name = ""vicuna-1.1-chat"")]
    Vicuna11Chat,
    #[value(name = ""vicuna-llava"")]
    VicunaLlava,
    #[value(name = ""chatml"")]
    ChatML,
    #[value(name = ""chatml-tool"")]
    ChatMLTool,
    #[value(name = ""internlm-2-tool"")]
    InternLM2Tool,
    #[value(name = ""baichuan-2"")]
    Baichuan2,
    #[value(name = ""wizard-coder"")]
    WizardCoder,
    #[value(name = ""zephyr"")]
    Zephyr,
    #[value(name = ""stablelm-zephyr"")]
    StableLMZephyr,
    #[value(name = ""intel-neural"")]
    IntelNeural,
    #[value(name = ""deepseek-chat"")]
    DeepseekChat,
    #[value(name = ""deepseek-coder"")]
    DeepseekCoder,
    #[value(name = ""deepseek-chat-2"")]
    DeepseekChat2,
    #[value(name = ""deepseek-chat-25"")]
    DeepseekChat25,
    #[value(name = ""solar-instruct"")]
    SolarInstruct,
    #[value(name = ""phi-2-chat"")]
    Phi2Chat,
    #[value(name = ""phi-2-instruct"")]
    Phi2Instruct,
    #[value(name = ""phi-3-chat"")]
    Phi3Chat,
    #[value(name = ""phi-3-instruct"")]
    Phi3Instruct,
    #[value(name = ""gemma-instruct"")]
    GemmaInstruct,
    #[value(name = ""octopus"")]
    Octopus,
    #[value(name = ""glm-4-chat"")]
    Glm4Chat,
    #[value(name = ""groq-llama3-tool"")]
    GroqLlama3Tool,
    #[value(name = ""mediatek-breeze"")]
    BreezeInstruct,
    #[value(name = ""nemotron-chat"")]
    NemotronChat,
    #[value(name = ""nemotron-tool"")]
    NemotronTool,
    #[value(name = ""embedding"")]
    Embedding,
    #[value(name = ""none"")]
    Null,
}
impl PromptTemplateType {
    pub fn has_system_prompt(&self) -> bool {
        match self {
            PromptTemplateType::Llama2Chat
            | PromptTemplateType::Llama3Chat
            | PromptTemplateType::Llama3Tool
            | PromptTemplateType::CodeLlama
            | PromptTemplateType::CodeLlamaSuper
            | PromptTemplateType::VicunaChat
            | PromptTemplateType::VicunaLlava
            | PromptTemplateType::ChatML
            | PromptTemplateType::ChatMLTool
            | PromptTemplateType::InternLM2Tool
            | PromptTemplateType::Baichuan2
            | PromptTemplateType::WizardCoder
            | PromptTemplateType::Zephyr
            | PromptTemplateType::IntelNeural
            | PromptTemplateType::DeepseekCoder
            | PromptTemplateType::DeepseekChat2
            | PromptTemplateType::Octopus
            | PromptTemplateType::Phi3Chat
            | PromptTemplateType::Glm4Chat
            | PromptTemplateType::GroqLlama3Tool
            | PromptTemplateType::BreezeInstruct
            | PromptTemplateType::DeepseekChat25
            | PromptTemplateType::NemotronChat
            | PromptTemplateType::NemotronTool => true,
            PromptTemplateType::MistralInstruct
            | PromptTemplateType::MistralTool
            | PromptTemplateType::MistralLite
            | PromptTemplateType::HumanAssistant
            | PromptTemplateType::DeepseekChat
            | PromptTemplateType::GemmaInstruct
            | PromptTemplateType::OpenChat
            | PromptTemplateType::Phi2Chat
            | PromptTemplateType::Phi2Instruct
            | PromptTemplateType::Phi3Instruct
            | PromptTemplateType::SolarInstruct
            | PromptTemplateType::Vicuna11Chat
            | PromptTemplateType::StableLMZephyr
            | PromptTemplateType::Embedding
            | PromptTemplateType::Null => false,
        }
    }
}
impl FromStr for PromptTemplateType {
    type Err = error::PromptError;

    fn from_str(template: &str) -> std::result::Result<Self, Self::Err> {
        match template {
            ""llama-2-chat"" => Ok(PromptTemplateType::Llama2Chat),
            ""llama-3-chat"" => Ok(PromptTemplateType::Llama3Chat),
            ""llama-3-tool"" => Ok(PromptTemplateType::Llama3Tool),
            ""mistral-instruct"" => Ok(PromptTemplateType::MistralInstruct),
            ""mistral-tool"" => Ok(PromptTemplateType::MistralTool),
            ""mistrallite"" => Ok(PromptTemplateType::MistralLite),
            ""codellama-instruct"" => Ok(PromptTemplateType::CodeLlama),
            ""codellama-super-instruct"" => Ok(PromptTemplateType::CodeLlamaSuper),
            ""belle-llama-2-chat"" => Ok(PromptTemplateType::HumanAssistant),
            ""human-assistant"" => Ok(PromptTemplateType::HumanAssistant),
            ""vicuna-1.0-chat"" => Ok(PromptTemplateType::VicunaChat),
            ""vicuna-1.1-chat"" => Ok(PromptTemplateType::Vicuna11Chat),
            ""vicuna-llava"" => Ok(PromptTemplateType::VicunaLlava),
            ""chatml"" => Ok(PromptTemplateType::ChatML),
            ""chatml-tool"" => Ok(PromptTemplateType::ChatMLTool),
            ""internlm-2-tool"" => Ok(PromptTemplateType::InternLM2Tool),
            ""openchat"" => Ok(PromptTemplateType::OpenChat),
            ""baichuan-2"" => Ok(PromptTemplateType::Baichuan2),
            ""wizard-coder"" => Ok(PromptTemplateType::WizardCoder),
            ""zephyr"" => Ok(PromptTemplateType::Zephyr),
            ""stablelm-zephyr"" => Ok(PromptTemplateType::StableLMZephyr),
            ""intel-neural"" => Ok(PromptTemplateType::IntelNeural),
            ""deepseek-chat"" => Ok(PromptTemplateType::DeepseekChat),
            ""deepseek-coder"" => Ok(PromptTemplateType::DeepseekCoder),
            ""deepseek-chat-2"" => Ok(PromptTemplateType::DeepseekChat2),
            ""deepseek-chat-25"" => Ok(PromptTemplateType::DeepseekChat25),
            ""solar-instruct"" => Ok(PromptTemplateType::SolarInstruct),
            ""phi-2-chat"" => Ok(PromptTemplateType::Phi2Chat),
            ""phi-2-instruct"" => Ok(PromptTemplateType::Phi2Instruct),
            ""phi-3-chat"" => Ok(PromptTemplateType::Phi3Chat),
            ""phi-3-instruct"" => Ok(PromptTemplateType::Phi3Instruct),
            ""gemma-instruct"" => Ok(PromptTemplateType::GemmaInstruct),
            ""octopus"" => Ok(PromptTemplateType::Octopus),
            ""glm-4-chat"" => Ok(PromptTemplateType::Glm4Chat),
            ""groq-llama3-tool"" => Ok(PromptTemplateType::GroqLlama3Tool),
            ""mediatek-breeze"" => Ok(PromptTemplateType::BreezeInstruct),
            ""nemotron-chat"" => Ok(PromptTemplateType::NemotronChat),
            ""nemotron-tool"" => Ok(PromptTemplateType::NemotronTool),
            ""embedding"" => Ok(PromptTemplateType::Embedding),
            ""none"" => Ok(PromptTemplateType::Null),
            _ => Err(error::PromptError::UnknownPromptTemplateType(
                template.to_string(),
            )),
        }
    }
}
impl std::fmt::Display for PromptTemplateType {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            PromptTemplateType::Llama2Chat => write!(f, ""llama-2-chat""),
            PromptTemplateType::Llama3Chat => write!(f, ""llama-3-chat""),
            PromptTemplateType::Llama3Tool => write!(f, ""llama-3-tool""),
            PromptTemplateType::MistralInstruct => write!(f, ""mistral-instruct""),
            PromptTemplateType::MistralTool => write!(f, ""mistral-tool""),
            PromptTemplateType::MistralLite => write!(f, ""mistrallite""),
            PromptTemplateType::OpenChat => write!(f, ""openchat""),
            PromptTemplateType::CodeLlama => write!(f, ""codellama-instruct""),
            PromptTemplateType::HumanAssistant => write!(f, ""human-asistant""),
            PromptTemplateType::VicunaChat => write!(f, ""vicuna-1.0-chat""),
            PromptTemplateType::Vicuna11Chat => write!(f, ""vicuna-1.1-chat""),
            PromptTemplateType::VicunaLlava => write!(f, ""vicuna-llava""),
            PromptTemplateType::ChatML => write!(f, ""chatml""),
            PromptTemplateType::ChatMLTool => write!(f, ""chatml-tool""),
            PromptTemplateType::InternLM2Tool => write!(f, ""internlm-2-tool""),
            PromptTemplateType::Baichuan2 => write!(f, ""baichuan-2""),
            PromptTemplateType::WizardCoder => write!(f, ""wizard-coder""),
            PromptTemplateType::Zephyr => write!(f, ""zephyr""),
            PromptTemplateType::StableLMZephyr => write!(f, ""stablelm-zephyr""),
            PromptTemplateType::IntelNeural => write!(f, ""intel-neural""),
            PromptTemplateType::DeepseekChat => write!(f, ""deepseek-chat""),
            PromptTemplateType::DeepseekCoder => write!(f, ""deepseek-coder""),
            PromptTemplateType::DeepseekChat2 => write!(f, ""deepseek-chat-2""),
            PromptTemplateType::DeepseekChat25 => write!(f, ""deepseek-chat-25""),
            PromptTemplateType::SolarInstruct => write!(f, ""solar-instruct""),
            PromptTemplateType::Phi2Chat => write!(f, ""phi-2-chat""),
            PromptTemplateType::Phi2Instruct => write!(f, ""phi-2-instruct""),
            PromptTemplateType::Phi3Chat => write!(f, ""phi-3-chat""),
            PromptTemplateType::Phi3Instruct => write!(f, ""phi-3-instruct""),
            PromptTemplateType::CodeLlamaSuper => write!(f, ""codellama-super-instruct""),
            PromptTemplateType::GemmaInstruct => write!(f, ""gemma-instruct""),
            PromptTemplateType::Octopus => write!(f, ""octopus""),
            PromptTemplateType::Glm4Chat => write!(f, ""glm-4-chat""),
            PromptTemplateType::GroqLlama3Tool => write!(f, ""groq-llama3-tool""),
            PromptTemplateType::BreezeInstruct => write!(f, ""mediatek-breeze""),
            PromptTemplateType::NemotronChat => write!(f, ""nemotron-chat""),
            PromptTemplateType::NemotronTool => write!(f, ""nemotron-tool""),
            PromptTemplateType::Embedding => write!(f, ""embedding""),
            PromptTemplateType::Null => write!(f, ""none""),
        }
    }
}

/// Trait for merging RAG context into chat messages
pub trait MergeRagContext: Send {
    /// Merge RAG context into chat messages.
    ///
    /// Note that the default implementation simply merges the RAG context into the system message. That is, to use the default implementation, `has_system_prompt` should be set to `true` and `policy` set to `MergeRagContextPolicy::SystemMessage`.
    ///
    /// # Arguments
    ///
    /// * `messages` - The chat messages to merge the context into.
    ///
    /// * `context` - The RAG context to merge into the chat messages.
    ///
    /// * `has_system_prompt` - Whether the chat template has a system prompt.
    ///
    /// * `policy` - The policy for merging RAG context into chat messages.
    fn build(
        messages: &mut Vec<endpoints::chat::ChatCompletionRequestMessage>,
        context: &[String],
        has_system_prompt: bool,
        policy: MergeRagContextPolicy,
    ) -> error::Result<()> {
        if (policy == MergeRagContextPolicy::SystemMessage) && has_system_prompt {
            if messages.is_empty() {
                return Err(error::PromptError::NoMessages);
            }

            if context.is_empty() {
                return Err(error::PromptError::Operation(
                    ""No context provided."".to_string(),
                ));
            }

            let context = context[0].trim_end();

            // update or insert system message
            match messages[0] {
                ChatCompletionRequestMessage::System(ref message) => {
                    // compose new system message content
                    let content = format!(""{original_system_message}\nUse the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{context}"", original_system_message=message.content().trim(), context=context.trim_end());
                    // create system message
                    let system_message = ChatCompletionRequestMessage::new_system_message(
                        content,
                        messages[0].name().cloned(),
                    );
                    // replace the original system message
                    messages[0] = system_message;
                }
                _ => {
                    // prepare system message
                    let content = format!(""Use the following pieces of context to answer the user's question.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n{}"", context.trim_end());

                    // create system message
                    let system_message = ChatCompletionRequestMessage::new_system_message(
                        content,
                        messages[0].name().cloned(),
                    );
                    // insert system message
                    messages.insert(0, system_message);
                }
            };
        }

        Ok(())
    }
}

/// Define the strategy for merging RAG context into chat messages.
#[derive(Clone, Debug, Copy, Default, PartialEq, Eq, Serialize, Deserialize, ValueEnum)]
pub enum MergeRagContextPolicy {
    /// Merge RAG context into the system message.
    ///
    /// Note that this policy is only applicable when the chat template has a system message.
    #[default]
    SystemMessage,
    /// Merge RAG context into the last user message.
    LastUserMessage,
}
impl std::fmt::Display for MergeRagContextPolicy {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            MergeRagContextPolicy::SystemMessage => write!(f, ""system-message""),
            MergeRagContextPolicy::LastUserMessage => write!(f, ""last-user-message""),
        }
    }
}
"
"language: crates/chat-prompts/src/chat/glm.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate chat prompt for the `microsoft/phi-2` model.
#[derive(Debug, Default, Clone)]
pub struct Glm4ChatPrompt;
impl Glm4ChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""[gMASK]<|system|>\nYou are a friendly chatbot.""),
            false => format!(""[gMASK]<|system|>\n{content}""),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}<|user|>\n{user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<|user|>\n{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}<|assistant|>\n{assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Glm4ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""[gMASK]<|system|>\nYou are a friendly chatbot.""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|assistant|>"");

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/phi.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate instruct prompt for the `microsoft/phi-2` model.
#[derive(Debug, Default, Clone)]
pub struct Phi2InstructPrompt;
impl Phi2InstructPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(&self, message: &ChatCompletionUserMessage) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        format!(""Instruct: {user_message}"", user_message = content.trim(),)
    }
}
impl BuildChatPrompt for Phi2InstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        let mut prompt = if let Some(user_message) = messages.last() {
            match user_message {
                ChatCompletionRequestMessage::User(message) => self.append_user_message(message),
                _ => {
                    return Err(crate::error::PromptError::NoUserMessage);
                }
            }
        } else {
            return Err(crate::error::PromptError::NoMessages);
        };

        prompt.push_str(""\nOutput:"");

        Ok(prompt)
    }
}

/// Generate chat prompt for the `microsoft/phi-2` model.
#[derive(Debug, Default, Clone)]
pub struct Phi2ChatPrompt;
impl Phi2ChatPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(""Alice: {user_message}"", user_message = content.trim(),),
            false => format!(
                ""{chat_history}\nAlice: {user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\nBob: {assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Phi2ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\nBob:"");

        Ok(prompt)
    }
}

/// Generate chat prompt for the `microsoft/phi-3` model.
#[derive(Debug, Default, Clone)]
pub struct Phi3InstructPrompt;
impl Phi3InstructPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(&self, message: &ChatCompletionUserMessage) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        format!(
            ""<|user|>\n {user_message} <|end|>"",
            user_message = content.trim(),
        )
    }
}
impl BuildChatPrompt for Phi3InstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        let mut prompt = if let Some(user_message) = messages.last() {
            match user_message {
                ChatCompletionRequestMessage::User(message) => self.append_user_message(message),
                _ => {
                    return Err(crate::error::PromptError::NoUserMessage);
                }
            }
        } else {
            return Err(crate::error::PromptError::NoMessages);
        };

        prompt.push_str(""\n <|assistant|>"");

        Ok(prompt)
    }
}

/// Generate chat prompt for the `microsoft/phi-2` model.
#[derive(Debug, Default, Clone)]
pub struct Phi3ChatPrompt;
impl Phi3ChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|system|>\nYou are a friendly chatbot.<|end|>""),
            false => format!(""<|system|>\n{content}<|end|>""),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n<|user|>\n{user_message}<|end|>"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n<|user|>\n{user_message}<|end|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n<|assistant|>\n{assistant_message}<|end|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Phi3ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<|system|>\nYou are a friendly chatbot.<|end|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|assistant|>"");

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/intel.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `wizard-vicuna` model.
#[derive(Debug, Default, Clone)]
pub struct NeuralChatPrompt;
impl NeuralChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""### System:\nYou are a chatbot developed by Intel. Please answer all questions to the best of your ability.""),
            false => format!(""### System:\n{content}""),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n### User:\n{user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n### User:\n{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n### Assistant:\n{assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for NeuralChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""### System:\nYou are a chatbot developed by Intel. Please answer all questions to the best of your ability."")
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n### Assistant:"");

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/vicuna.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use base64::{engine::general_purpose, Engine as _};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};
use image::io::Reader as ImageReader;
use std::io::Cursor;

/// Vicuna-1.0 Prompt Template
#[derive(Debug, Default, Clone)]
pub struct VicunaChatPrompt;
impl VicunaChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.""),
            false => content.to_string(),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt} USER: {user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history} USER: {user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history} ASSISTANT: {assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for VicunaChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str("" ASSISTANT:"");

        Ok(prompt)
    }
}

/// Vicuna-1.1 Prompt Template
#[derive(Debug, Default, Clone)]
pub struct Vicuna11ChatPrompt;
impl Vicuna11ChatPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(""USER: {user_message}"", user_message = content.trim(),),
            false => format!(
                ""{chat_history}\nUSER: {user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\nASSISTANT: {assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Vicuna11ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }
        prompt.push_str("" ASSISTANT:"");

        Ok(prompt)
    }
}

/// Vicuna-1.0 Prompt Template
#[derive(Debug, Default, Clone)]
pub struct VicunaLlavaPrompt;
impl VicunaLlavaPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""You are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.""),
            false => content.to_string(),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> Result<String> {
        let prompt = match message.content() {
            ChatCompletionUserMessageContent::Text(content) => {
                match chat_history.as_ref().is_empty() {
                    true => format!(
                        ""{system_prompt}\nUSER: {user_message}"",
                        system_prompt = system_prompt.as_ref().trim(),
                        user_message = content.trim(),
                    ),
                    false => format!(
                        ""{chat_history}\nUSER: {user_message}"",
                        chat_history = chat_history.as_ref().trim(),
                        user_message = content.trim(),
                    ),
                }
            }
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                let mut image_content = String::new();
                for part in parts {
                    match part {
                        ContentPart::Text(text_content) => {
                            content.push_str(text_content.text());
                            content.push('\n');
                        }
                        ContentPart::Image(part) => {
                            image_content = match part.image().is_url() {
                                true => String::from(""<image>""),
                                false => {
                                    let base64_str = part.image().url.as_str();
                                    let format = is_image_format(base64_str)?;
                                    format!(
                                        r#""<img src=""data:image/{};base64,{}"">""#,
                                        format, base64_str
                                    )
                                }
                            };
                        }
                    }
                }

                match chat_history.as_ref().is_empty() {
                    true => format!(
                        ""{system_prompt}\nUSER:{image_embeddings}\n{user_message}"",
                        system_prompt = system_prompt.as_ref().trim(),
                        image_embeddings = image_content.trim(),
                        user_message = content.trim(),
                    ),
                    false => format!(
                        ""{chat_history}\nUSER:{image_embeddings}\n{user_message}"",
                        chat_history = chat_history.as_ref().trim(),
                        image_embeddings = image_content.trim(),
                        user_message = content.trim(),
                    ),
                }
            }
        };

        Ok(prompt)
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\nASSISTANT: {assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for VicunaLlavaPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""You are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message)?;
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\nASSISTANT:"");

        Ok(prompt)
    }
}

fn is_image_format(base64_str: &str) -> Result<String> {
    let image_data = match general_purpose::STANDARD.decode(base64_str) {
        Ok(data) => data,
        Err(_) => {
            return Err(PromptError::Operation(
                ""Failed to decode base64 string."".to_string(),
            ))
        }
    };

    let format = ImageReader::new(Cursor::new(image_data))
        .with_guessed_format()
        .unwrap()
        .format();

    let image_format = match format {
        Some(image::ImageFormat::Png) => ""png"".to_string(),
        Some(image::ImageFormat::Jpeg) => ""jpeg"".to_string(),
        Some(image::ImageFormat::Tga) => ""tga"".to_string(),
        Some(image::ImageFormat::Bmp) => ""bmp"".to_string(),
        Some(image::ImageFormat::Gif) => ""gif"".to_string(),
        Some(image::ImageFormat::Hdr) => ""hdr"".to_string(),
        Some(image::ImageFormat::Pnm) => ""pnm"".to_string(),
        _ => {
            return Err(PromptError::Operation(
                ""Unsupported image format."".to_string(),
            ))
        }
    };

    Ok(image_format)
}
"
"language: crates/chat-prompts/src/chat/deepseek.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `DeepSeek-LLM-Chat` model.
#[derive(Debug, Default, Clone)]
pub struct DeepseekChatPrompt;
impl DeepseekChatPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(""User: {user_message}"", user_message = content.trim(),),
            false => format!(
                ""{chat_history}User: {user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n\nAssistant: {assistant_message}<|end_of_sentence|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for DeepseekChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n\nAssistant:"");

        Ok(prompt)
    }
}

/// Generate prompts for the `DeepSeek-Coder` model.
#[derive(Debug, Default, Clone)]
pub struct DeepseekCoderPrompt;
impl DeepseekCoderPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.""),
            false => content.to_string(),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n### Instruction:\n{user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n### Instruction:\n{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_histroy}\n### Response:\n{assistant_message}\n<|EOT|>"",
            chat_histroy = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for DeepseekCoderPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.""),
        };

        // append user/assistant messages
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n### Response:"");

        Ok(prompt)
    }
}

/// Generate prompts for the `DeepSeek-V2` models.
#[derive(Debug, Default, Clone)]
pub struct DeepseekChat2Prompt;
impl DeepseekChat2Prompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|begin▁of▁sentence|>You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.""),
            false => format!(""<|begin▁of▁sentence|>{system_message}"", system_message=content),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n\nUser: {user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}User: {user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_histroy}\n\nAssistant: {assistant_message}<|end_of_sentence|>"",
            chat_histroy = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for DeepseekChat2Prompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""<|begin▁of▁sentence|>You are an AI programming assistant, utilizing the DeepSeek Coder model, developed by DeepSeek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.""),
        };

        // append user/assistant messages
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n\nAssistant:"");

        Ok(prompt)
    }
}

/// Generate prompts for the `DeepSeek-V2.5` models.
#[derive(Debug, Default, Clone)]
pub struct DeepseekChat25Prompt;
impl DeepseekChat25Prompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|begin▁of▁sentence|>""),
            false => format!(
                ""<|begin▁of▁sentence|>{system_message}"",
                system_message = content
            ),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}<|User|>{user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<|User|>{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_histroy}<|Assistant|>{assistant_message}<|end_of_sentence|>"",
            chat_histroy = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for DeepseekChat25Prompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<|begin▁of▁sentence|>""),
        };

        // append user/assistant messages
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|Assistant|>"");

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/mediatek.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `Breeze-7B-Instruct-v1_0` model
#[derive(Debug, Default, Clone)]
pub struct BreezeInstructPrompt;
impl BreezeInstructPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<s>You are a helpful AI assistant built by MediaTek Research. The user you are helping speaks Traditional Chinese and comes from Taiwan.""),
            false => format!(""<s>{content}""),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}  [INST] {user_message} [/INST]"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history} [INST] {user_message} [/INST]"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history} {assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for BreezeInstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<s>You are a helpful AI assistant built by MediaTek Research. The user you are helping speaks Traditional Chinese and comes from Taiwan.""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/mod.rs
pub mod baichuan;
pub mod belle;
pub mod chatml;
pub mod deepseek;
pub mod gemma;
pub mod glm;
pub mod groq;
pub mod intel;
pub mod llama;
pub mod mediatek;
pub mod mistral;
pub mod nvidia;
pub mod octopus;
pub mod openchat;
pub mod phi;
pub mod solar;
pub mod vicuna;
pub mod wizard;
pub mod zephyr;

use crate::{error::Result, PromptTemplateType};
use baichuan::*;
use belle::*;
use chatml::*;
use deepseek::*;
use endpoints::chat::{ChatCompletionRequestMessage, Tool};
use gemma::*;
use glm::*;
use groq::*;
use intel::*;
use llama::*;
use mediatek::BreezeInstructPrompt;
use mistral::*;
use nvidia::{NemotronChatPrompt, NemotronToolPrompt};
use octopus::*;
use openchat::*;
use phi::*;
use solar::*;
use vicuna::*;
use wizard::*;
use zephyr::*;

/// Trait for building prompts for chat completions.
#[enum_dispatch::enum_dispatch]
pub trait BuildChatPrompt: Send {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String>;

    fn build_with_tools(
        &self,
        messages: &mut Vec<ChatCompletionRequestMessage>,
        _tools: Option<&[Tool]>,
    ) -> Result<String> {
        self.build(messages)
    }
}

#[enum_dispatch::enum_dispatch(BuildChatPrompt)]
pub enum ChatPrompt {
    Llama2ChatPrompt,
    Llama3ChatPrompt,
    Llama3ToolPrompt,
    MistralInstructPrompt,
    MistralToolPrompt,
    MistralLitePrompt,
    OpenChatPrompt,
    CodeLlamaInstructPrompt,
    CodeLlamaSuperInstructPrompt,
    HumanAssistantChatPrompt,
    /// Vicuna 1.0
    VicunaChatPrompt,
    /// Vicuna 1.1
    Vicuna11ChatPrompt,
    VicunaLlavaPrompt,
    ChatMLPrompt,
    ChatMLToolPrompt,
    InternLM2ToolPrompt,
    Baichuan2ChatPrompt,
    WizardCoderPrompt,
    ZephyrChatPrompt,
    StableLMZephyrChatPrompt,
    NeuralChatPrompt,
    DeepseekChatPrompt,
    DeepseekCoderPrompt,
    DeepseekChat2Prompt,
    DeepseekChat25Prompt,
    SolarInstructPrompt,
    Phi2ChatPrompt,
    Phi2InstructPrompt,
    Phi3ChatPrompt,
    Phi3InstructPrompt,
    GemmaInstructPrompt,
    OctopusPrompt,
    Glm4ChatPrompt,
    GroqLlama3ToolPrompt,
    BreezeInstructPrompt,
    NemotronChatPrompt,
    NemotronToolPrompt,
}
impl From<PromptTemplateType> for ChatPrompt {
    fn from(ty: PromptTemplateType) -> Self {
        match ty {
            PromptTemplateType::Llama2Chat => ChatPrompt::Llama2ChatPrompt(Llama2ChatPrompt),
            PromptTemplateType::Llama3Chat => ChatPrompt::Llama3ChatPrompt(Llama3ChatPrompt),
            PromptTemplateType::Llama3Tool => ChatPrompt::Llama3ToolPrompt(Llama3ToolPrompt),
            PromptTemplateType::MistralInstruct => {
                ChatPrompt::MistralInstructPrompt(MistralInstructPrompt)
            }
            PromptTemplateType::MistralTool => ChatPrompt::MistralToolPrompt(MistralToolPrompt),
            PromptTemplateType::MistralLite => ChatPrompt::MistralLitePrompt(MistralLitePrompt),
            PromptTemplateType::OpenChat => ChatPrompt::OpenChatPrompt(OpenChatPrompt),
            PromptTemplateType::CodeLlama => {
                ChatPrompt::CodeLlamaInstructPrompt(CodeLlamaInstructPrompt)
            }
            PromptTemplateType::CodeLlamaSuper => {
                ChatPrompt::CodeLlamaSuperInstructPrompt(CodeLlamaSuperInstructPrompt)
            }
            PromptTemplateType::HumanAssistant => {
                ChatPrompt::HumanAssistantChatPrompt(HumanAssistantChatPrompt)
            }
            PromptTemplateType::VicunaChat => ChatPrompt::VicunaChatPrompt(VicunaChatPrompt),
            PromptTemplateType::Vicuna11Chat => ChatPrompt::Vicuna11ChatPrompt(Vicuna11ChatPrompt),
            PromptTemplateType::VicunaLlava => ChatPrompt::VicunaLlavaPrompt(VicunaLlavaPrompt),
            PromptTemplateType::ChatML => ChatPrompt::ChatMLPrompt(ChatMLPrompt),
            PromptTemplateType::ChatMLTool => ChatPrompt::ChatMLToolPrompt(ChatMLToolPrompt),
            PromptTemplateType::InternLM2Tool => {
                ChatPrompt::InternLM2ToolPrompt(InternLM2ToolPrompt)
            }
            PromptTemplateType::Baichuan2 => ChatPrompt::Baichuan2ChatPrompt(Baichuan2ChatPrompt),
            PromptTemplateType::WizardCoder => ChatPrompt::WizardCoderPrompt(WizardCoderPrompt),
            PromptTemplateType::Zephyr => ChatPrompt::ZephyrChatPrompt(ZephyrChatPrompt),
            PromptTemplateType::StableLMZephyr => {
                ChatPrompt::StableLMZephyrChatPrompt(StableLMZephyrChatPrompt)
            }
            PromptTemplateType::IntelNeural => ChatPrompt::NeuralChatPrompt(NeuralChatPrompt),
            PromptTemplateType::DeepseekChat => ChatPrompt::DeepseekChatPrompt(DeepseekChatPrompt),
            PromptTemplateType::DeepseekCoder => {
                ChatPrompt::DeepseekCoderPrompt(DeepseekCoderPrompt)
            }
            PromptTemplateType::DeepseekChat2 => {
                ChatPrompt::DeepseekChat2Prompt(DeepseekChat2Prompt)
            }
            PromptTemplateType::DeepseekChat25 => {
                ChatPrompt::DeepseekChat25Prompt(DeepseekChat25Prompt)
            }
            PromptTemplateType::SolarInstruct => {
                ChatPrompt::SolarInstructPrompt(SolarInstructPrompt)
            }
            PromptTemplateType::Phi2Chat => ChatPrompt::Phi2ChatPrompt(Phi2ChatPrompt),
            PromptTemplateType::Phi2Instruct => ChatPrompt::Phi2InstructPrompt(Phi2InstructPrompt),
            PromptTemplateType::Phi3Chat => ChatPrompt::Phi3ChatPrompt(Phi3ChatPrompt),
            PromptTemplateType::Phi3Instruct => ChatPrompt::Phi3InstructPrompt(Phi3InstructPrompt),
            PromptTemplateType::GemmaInstruct => {
                ChatPrompt::GemmaInstructPrompt(GemmaInstructPrompt)
            }
            PromptTemplateType::Octopus => ChatPrompt::OctopusPrompt(OctopusPrompt),
            PromptTemplateType::Glm4Chat => ChatPrompt::Glm4ChatPrompt(Glm4ChatPrompt),
            PromptTemplateType::GroqLlama3Tool => {
                ChatPrompt::GroqLlama3ToolPrompt(GroqLlama3ToolPrompt)
            }
            PromptTemplateType::BreezeInstruct => {
                ChatPrompt::BreezeInstructPrompt(BreezeInstructPrompt)
            }
            PromptTemplateType::NemotronChat => ChatPrompt::NemotronChatPrompt(NemotronChatPrompt),
            PromptTemplateType::NemotronTool => ChatPrompt::NemotronToolPrompt(NemotronToolPrompt),
            PromptTemplateType::Embedding => {
                panic!(""Embedding prompt template is not used for building chat prompts"")
            }
            PromptTemplateType::Null => {
                panic!(""Null prompt template is not used for building chat prompts"")
            }
        }
    }
}
"
"language: crates/chat-prompts/src/chat/gemma.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionUserMessage,
    ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `gemma-7b-it` model.
#[derive(Debug, Default, Clone)]
pub struct GemmaInstructPrompt;
impl GemmaInstructPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<bos><start_of_turn>user\n{user_message}<end_of_turn>\n<start_of_turn>model"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n<start_of_turn>user\n{user_message}<end_of_turn>\n<start_of_turn>model"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n{assistant_message}<end_of_turn>model"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for GemmaInstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/zephyr.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

#[derive(Debug, Default, Clone)]
pub struct ZephyrChatPrompt;
impl ZephyrChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|system|>\nYou are a friendly chatbot.</s>""),
            false => format!(""<|system|>\n{content}</s>""),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n<|user|>\n{user_message}</s>"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n<|user|>\n{user_message}</s>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n<|assistant|>\n{assistant_message}</s>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for ZephyrChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<|system|>\nYou are a friendly chatbot.</s>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|assistant|>"");

        Ok(prompt)
    }
}

#[derive(Debug, Default, Clone)]
pub struct StableLMZephyrChatPrompt;
impl StableLMZephyrChatPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<|user|>\n{user_message}<|endoftext|>"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n<|user|>\n{user_message}<|endoftext|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n<|assistant|>\n{assistant_message}<|endoftext|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for StableLMZephyrChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|assistant|>"");

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/groq.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionToolMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart, Tool,
};

/// Generate prompts for the `second-state/Llama-3-Groq-8B-Tool-Use-GGUF` model.
#[derive(Debug, Default, Clone)]
pub struct GroqLlama3ToolPrompt;
impl GroqLlama3ToolPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt_tool(&self, tools: Option<&[Tool]>) -> Result<String> {
        match tools {
            Some(tools) => {
                let mut available_tools = String::new();
                for tool in tools {
                    if available_tools.is_empty() {
                        available_tools
                            .push_str(&serde_json::to_string_pretty(&tool.function).unwrap());
                    } else {
                        available_tools.push('\n');
                        available_tools
                            .push_str(&serde_json::to_string_pretty(&tool.function).unwrap());
                    }
                }

                let tools = format!(
                    ""Here are the available tools:\n<tools> {} </tools>"",
                    available_tools
                );

                let format = r#""{""name"": <function-name>,""arguments"": <args-dict>}""#;
                let begin = format!(""<|start_header_id|>system<|end_header_id|>\n\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{}\n</tool_call>"", format);

                let end = r#""<|eot_id|>""#;

                Ok(format!(""{}\n\n{}{}"", begin, tools, end))
            }
            None => Err(PromptError::NoAvailableTools),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}<|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|>"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}<|start_header_id|>assistant<|end_header_id|>\n\n{assistant_message}<|eot_id|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }

    /// create a tool prompt from a chat completion request message.
    fn append_tool_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionToolMessage,
    ) -> String {
        format!(
            ""{chat_history}<|start_header_id|>tool<|end_header_id|>\n\n<tool_response>\n{tool_message}\n</tool_response><|eot_id|>"",
            chat_history = chat_history.as_ref().trim(),
            tool_message = message.content().trim(),
        )
    }
}
impl BuildChatPrompt for GroqLlama3ToolPrompt {
    fn build(&self, _messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        Err(PromptError::Operation(""The GroqToolPrompt struct is only designed for `Groq/Llama-3-Groq-8B-Tool-Use` model, which is for tool use ONLY instead of general knowledge or open-ended tasks."".to_string()))
    }

    fn build_with_tools(
        &self,
        messages: &mut Vec<ChatCompletionRequestMessage>,
        tools: Option<&[Tool]>,
    ) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = self.create_system_prompt_tool(tools)?;

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|start_header_id|>assistant<|end_header_id|>"");

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/solar.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionUserMessage,
    ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `Mistral-instruct-v0.1` model.
#[derive(Debug, Default, Clone)]
pub struct SolarInstructPrompt;
impl SolarInstructPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<s> ### User:\n{user_message}"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n\n<s> ### User:\n{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n\n### Assistant:\n{assistant_message}</s>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for SolarInstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/nvidia.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionToolMessage, ChatCompletionUserMessage, ChatCompletionUserMessageContent,
    ContentPart, Tool,
};

/// Generate prompts for the `nemotron-mini-instruct` model.
#[derive(Debug, Default, Clone)]
pub struct NemotronChatPrompt;
impl NemotronChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<extra_id_0>System\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.""),
            false =>format!(
                ""<extra_id_0>System\n{system_prompt}"", system_prompt=content
            )
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n<extra_id_1>User\n{user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n<extra_id_1>User\n{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}<extra_id_1>Assistant\n{assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for NemotronChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""<extra_id_0>System\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<extra_id_1>Assistant\n"");

        Ok(prompt)
    }
}

/// Generate prompts for the models using ChatML template.
#[derive(Debug, Default, Clone)]
pub struct NemotronToolPrompt;
impl NemotronToolPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
            false => format!(
                ""<|im_start|>system\n{system_prompt}<|im_end|>"",
                system_prompt = content
            ),
        }
    }

    fn create_system_prompt_tool(
        &self,
        message: &ChatCompletionSystemMessage,
        tools: Option<&[Tool]>,
    ) -> String {
        let content = message.content();
        match content.is_empty() {
            true => match tools {
                Some(tools) => {
                    let available_tools = serde_json::to_string(tools).unwrap();
                    let tools = format!(""<tool> {} </tool>"", available_tools);

                    let begin = r#""<extra_id_0>System\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.""#;

                    format!(""{}\n\n{}"", begin, tools)
                }
                None => {
                    String::from(""<extra_id_0>System\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe."")
                }
            },
            false => match tools {
                Some(tools) => {
                    let available_tools = serde_json::to_string(tools).unwrap();
                    let tools = format!(""<tool> {} </tool>"", available_tools);

                    let begin = format!(
                        ""<extra_id_0>System\n{system_prompt}"", system_prompt=content
                    );

                    format!(""{}\n\n{}"", begin, tools)
                }
                None => {
                    format!(
                        ""<extra_id_0>System\n{system_prompt}"", system_prompt=content
                    )
                }
            },
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n\n<extra_id_1>User\n{user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n<extra_id_1>User\n{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}<extra_id_1>Assistant\n{assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }

    /// create a tool prompt from a chat completion request message.
    fn append_tool_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionToolMessage,
    ) -> String {
        format!(
            ""{chat_history}\n<extra_id_1>Tool\n{tool_message}"",
            chat_history = chat_history.as_ref().trim(),
            tool_message = message.content().trim(),
        )
    }
}
impl BuildChatPrompt for NemotronToolPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|im_start|>assistant"");

        Ok(prompt)
    }

    fn build_with_tools(
        &self,
        messages: &mut Vec<ChatCompletionRequestMessage>,
        tools: Option<&[Tool]>,
    ) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt_tool(message, tools)
            }
            _ => match tools {
                Some(tools) => {
                    let mut tools_s = String::new();
                    for tool in tools {
                        let available_tool = serde_json::to_string(&tool.function).unwrap();

                        let tool = format!(""<tool> {} </tool>\n"", available_tool);

                        tools_s.push_str(&tool);
                    }

                    let begin = r#""<extra_id_0>System\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.""#;

                    format!(""{}\n{}"", begin, tools_s.trim())
                }
                None => {
                    String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>"")
                }
            },
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<extra_id_1>Assistant\n"");

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/belle.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionUserMessage,
    ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `BELLE-Llama2-13B-chat` model.
#[derive(Debug, Default, Clone)]
pub struct HumanAssistantChatPrompt;
impl HumanAssistantChatPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(""Human: \n{user_message}"", user_message = content.trim(),),
            false => format!(
                ""{chat_history}\nHuman: \n{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{prompt}\n\nAssistant:{assistant_message}"",
            prompt = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for HumanAssistantChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n\nAssistant:\n"");

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/chatml.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionToolMessage, ChatCompletionUserMessage, ChatCompletionUserMessageContent,
    ContentPart, Tool,
};

/// Generate prompts for the models using ChatML template.
#[derive(Debug, Default, Clone)]
pub struct ChatMLPrompt;
impl ChatMLPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
            false => format!(
                ""<|im_start|>system\n{system_prompt}<|im_end|>"",
                system_prompt = content
            ),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => match system_prompt.as_ref().is_empty() {
                true => {
                    format!(
                        ""<|im_start|>user\n{user_message}<|im_end|>"",
                        user_message = content.trim(),
                    )
                }
                false => {
                    format!(
                        ""{system_prompt}\n<|im_start|>user\n{user_message}<|im_end|>"",
                        system_prompt = system_prompt.as_ref().trim(),
                        user_message = content.trim(),
                    )
                }
            },
            false => format!(
                ""{chat_history}\n<|im_start|>user\n{user_message}<|im_end|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n<|im_start|>assistant\n{assistant_message}<|im_end|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }

    /// create a tool prompt from a chat completion request message.
    fn append_tool_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionToolMessage,
    ) -> Result<String> {
        let content = message.content();

        Ok(format!(
            ""{chat_history}\n<|im_start|>tool\n{tool_message}<|im_end|>"",
            chat_history = chat_history.as_ref().trim(),
            tool_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for ChatMLPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|im_start|>assistant"");

        Ok(prompt)
    }
}

/// Generate prompts for the models using ChatML template.
#[derive(Debug, Default, Clone)]
pub struct ChatMLToolPrompt;
impl ChatMLToolPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
            false => format!(
                ""<|im_start|>system\n{system_prompt}<|im_end|>"",
                system_prompt = content
            ),
        }
    }

    fn create_system_prompt_tool(
        &self,
        message: &ChatCompletionSystemMessage,
        tools: Option<&[Tool]>,
    ) -> String {
        let content = message.content();
        match content.is_empty() {
            true => match tools {
                Some(tools) => {
                    let available_tools = serde_json::to_string(tools).unwrap();
                    let tools = format!(""<tools> {} </tools>"", available_tools);

                    let begin = r#""<|im_start|>system\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:""#;

                    let end = r#""Use the following pydantic model json schema for each tool call you will make: {""properties"": {""arguments"": {""title"": ""Arguments"", ""type"": ""object""}, ""name"": {""title"": ""Name"", ""type"": ""string""}}, ""required"": [""arguments"", ""name""], ""title"": ""FunctionCall"", ""type"": ""object""} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{""arguments"": <args-dict>, ""name"": <function-name>}\n</tool_call><|im_end|>""#;

                    format!(""{} {} {}"", begin, tools, end)
                }
                None => {
                    String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>"")
                }
            },
            false => match tools {
                Some(tools) => {
                    let available_tools = serde_json::to_string(tools).unwrap();
                    let tools = format!(""<tools> {} </tools>"", available_tools);

                    let begin = format!(
                            ""<|im_start|>system\n{system_prompt}\nYou are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:"",
                            system_prompt = content
                        );

                    let end = r#""Use the following pydantic model json schema for each tool call you will make: {""properties"": {""arguments"": {""title"": ""Arguments"", ""type"": ""object""}, ""name"": {""title"": ""Name"", ""type"": ""string""}}, ""required"": [""arguments"", ""name""], ""title"": ""FunctionCall"", ""type"": ""object""} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{""arguments"": <args-dict>, ""name"": <function-name>}\n</tool_call><|im_end|>""#;

                    format!(""{} {} {}"", begin, tools, end)
                }
                None => {
                    format!(
                        ""<|im_start|>system\n{system_prompt}<|im_end|>"",
                        system_prompt = content
                    )
                }
            },
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => match system_prompt.as_ref().is_empty() {
                true => {
                    format!(
                        ""<|im_start|>user\n{user_message}<|im_end|>"",
                        user_message = content.trim(),
                    )
                }
                false => {
                    format!(
                        ""{system_prompt}\n<|im_start|>user\n{user_message}<|im_end|>"",
                        system_prompt = system_prompt.as_ref().trim(),
                        user_message = content.trim(),
                    )
                }
            },
            false => format!(
                ""{chat_history}\n<|im_start|>user\n{user_message}<|im_end|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n<|im_start|>assistant\n{assistant_message}<|im_end|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }

    /// create a tool prompt from a chat completion request message.
    fn append_tool_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionToolMessage,
    ) -> String {
        format!(
            ""{chat_history}\n<|im_start|>tool\n<tool_response>\n{tool_message}\n</tool_response>\n<|im_end|>"",
            chat_history = chat_history.as_ref().trim(),
            tool_message = message.content().trim(),
        )
    }
}
impl BuildChatPrompt for ChatMLToolPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|im_start|>assistant"");

        Ok(prompt)
    }

    fn build_with_tools(
        &self,
        messages: &mut Vec<ChatCompletionRequestMessage>,
        tools: Option<&[Tool]>,
    ) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt_tool(message, tools)
            }
            _ => match tools {
                Some(tools) => {
                    let available_tools = serde_json::to_string(tools).unwrap();
                    let tools = format!(""<tools> {} </tools>"", available_tools);

                    let begin = r#""<|im_start|>system\nYou are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:""#;

                    let end = r#""Use the following pydantic model json schema for each tool call you will make: {""properties"": {""arguments"": {""title"": ""Arguments"", ""type"": ""object""}, ""name"": {""title"": ""Name"", ""type"": ""string""}}, ""required"": [""arguments"", ""name""], ""title"": ""FunctionCall"", ""type"": ""object""} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n<tool_call>\n{""arguments"": <args-dict>, ""name"": <function-name>}\n</tool_call><|im_end|>""#;

                    format!(""{} {} {}"", begin, tools, end)
                }
                None => {
                    String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>"")
                }
            },
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|im_start|>assistant"");

        Ok(prompt)
    }
}

/// Generate prompts for InternLM-2.5 models in tool use scenario.
pub struct InternLM2ToolPrompt;
impl InternLM2ToolPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
            false => format!(
                ""<|im_start|>system\n{system_prompt}<|im_end|>"",
                system_prompt = content
            ),
        }
    }

    fn create_system_prompt_tool(
        &self,
        message: &ChatCompletionSystemMessage,
        tools: Option<&[Tool]>,
    ) -> String {
        let content = message.content();
        match content.is_empty() {
            true => match tools {
                Some(tools) => {
                    let begin = ""<|im_start|>system\nYou are InternLM2-Chat, a harmless AI assistant.<|im_end|>"";

                    let available_tools = serde_json::to_string_pretty(tools).unwrap();
                    let tools = format!(""<|im_start|>system name=<|plugin|>\n{}\n<|im_end|>"", available_tools);

                    format!(""{}\n{}"", begin, tools)
                }
                None => {
                    String::from(""<|im_start|>system\nYou are InternLM2-Chat, a harmless AI assistant.<|im_end|>"")
                }
            },
            false => match tools {
                Some(tools) => {
                    let begin = format!(""<|im_start|>system\n{}<|im_end|>"", content);

                    let available_tools = serde_json::to_string_pretty(tools).unwrap();
                    let tools = format!(""<|im_start|>system name=<|plugin|>\n{}\n<|im_end|>"", available_tools);

                    format!(""{}\n{}"", begin, tools)
                }
                None => {
                    format!(""<|im_start|>system\n{}<|im_end|>"", content)
                }
            },
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => match system_prompt.as_ref().is_empty() {
                true => {
                    format!(
                        ""<|im_start|>user\n{user_message}<|im_end|>"",
                        user_message = content.trim(),
                    )
                }
                false => {
                    format!(
                        ""{system_prompt}\n<|im_start|>user\n{user_message}<|im_end|>"",
                        system_prompt = system_prompt.as_ref().trim(),
                        user_message = content.trim(),
                    )
                }
            },
            false => format!(
                ""{chat_history}\n<|im_start|>user\n{user_message}<|im_end|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n<|im_start|>assistant\n{assistant_message}<|im_end|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }

    /// create a tool prompt from a chat completion request message.
    fn append_tool_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionToolMessage,
    ) -> String {
        format!(
            ""{chat_history}\n<|im_start|>environment name=<|plugin|>\n{tool_message}<|im_end|>"",
            chat_history = chat_history.as_ref().trim(),
            tool_message = message.content().trim(),
        )
    }
}
impl BuildChatPrompt for InternLM2ToolPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""<|im_start|>system\nAnswer as concisely as possible.<|im_end|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|im_start|>assistant"");

        Ok(prompt)
    }

    fn build_with_tools(
        &self,
        messages: &mut Vec<ChatCompletionRequestMessage>,
        tools: Option<&[Tool]>,
    ) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt_tool(message, tools)
            }
            _ => match tools {
                Some(tools) => {
                    let begin = ""<|im_start|>system\nYou are InternLM2-Chat, a harmless AI assistant.<|im_end|>"";

                    let available_tools = serde_json::to_string_pretty(tools).unwrap();
                    let tools = format!(""<|im_start|>system name=<|plugin|>\n{}\n<|im_end|>"", available_tools);

                    format!(""{}\n{}"", begin, tools)
                }
                None => {
                    String::from(""<|im_start|>system\nYou are InternLM2-Chat, a harmless AI assistant.<|im_end|>"")
                }
            },
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n<|im_start|>assistant"");

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/baichuan.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `Baichuan-2-chat` model.
#[derive(Debug, Default, Clone)]
pub struct Baichuan2ChatPrompt;
impl Baichuan2ChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        message.content().to_string()
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => match system_prompt.as_ref().is_empty() {
                true => {
                    format!(""用户:{user_message}"", user_message = content.trim(),)
                }
                false => {
                    format!(
                        ""{system_prompt}\n\n用户:{user_message}"",
                        system_prompt = system_prompt.as_ref().trim(),
                        user_message = content.trim(),
                    )
                }
            },
            false => format!(
                ""{chat_history}用户:{user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}\n助手:{assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Baichuan2ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => self.create_system_prompt(message),
            _ => String::from(""以下内容为人类用户与一位智能助手的对话。""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""\n助手:"");

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/octopus.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `Llama-2-chat` model.
#[derive(Debug, Default, Clone)]
pub struct OctopusPrompt;
impl OctopusPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""Below is the query from the users, please call the correct function and generate the parameters to call the function.""),
            false => content.to_string(),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}\n\nQuery: {user_message}"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}\n\nQuery: {user_message}"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history} \n\nResponse: {assistant_message}"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for OctopusPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""Below is the query from the users, please call the correct function and generate the parameters to call the function.""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str("" \n\nResponse:"");

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/mistral.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionToolMessage,
    ChatCompletionUserMessage, ChatCompletionUserMessageContent, ContentPart, Tool,
};

/// Generate prompts for the `Mistral-instruct` model.
#[derive(Debug, Default, Clone)]
pub struct MistralInstructPrompt;
impl MistralInstructPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<s>[INST] {user_message} [/INST]"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}[INST] {user_message} [/INST]"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}{assistant_message}</s>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for MistralInstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}

/// Generate prompts for the amazon `MistralLite-7B` model.
#[derive(Debug, Default, Clone)]
pub struct MistralLitePrompt;
impl MistralLitePrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<|prompter|>{user_message}</s>"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<|prompter|>{user_message}</s>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}<|assistant|>{assistant_message}</s>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for MistralLitePrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        // append user/assistant messages
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|assistant|>"");

        Ok(prompt)
    }
}

/// Generate prompts for the `Mistral-instruct` model.
#[derive(Debug, Default, Clone)]
pub struct MistralToolPrompt;
impl MistralToolPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<s>[INST] {user_message} [/INST]"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}[INST] {user_message} [/INST]"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message_tool(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
        tools: Option<&[Tool]>,
        last_user_message: bool,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => match last_user_message {
                true => match tools {
                    Some(tools) => {
                        let json = serde_json::to_string(tools).unwrap();

                        format!(
                            ""<s>[AVAILABLE_TOOLS] {available_tools}[/AVAILABLE_TOOLS][INST] {user_message}[/INST]"",
                            available_tools = json,
                            user_message = content.trim(),
                        )
                    }
                    None => format!(
                        ""<s>[INST] {user_message} [/INST]"",
                        user_message = content.trim(),
                    ),
                },
                false => format!(
                    ""{chat_history}[INST] {user_message} [/INST]"",
                    chat_history = chat_history.as_ref().trim(),
                    user_message = content.trim(),
                ),
            },
            false => match last_user_message {
                true => match tools {
                    Some(tools) => {
                        let json = serde_json::to_string(tools).unwrap();

                        format!(
                            ""{chat_history}[AVAILABLE_TOOLS] {available_tools}[/AVAILABLE_TOOLS][INST] {user_message}[/INST]"",
                            chat_history = chat_history.as_ref().trim(),
                            available_tools = json,
                            user_message = content.trim(),
                        )
                    }
                    None => format!(
                        ""{chat_history}[INST] {user_message} [/INST]"",
                        chat_history = chat_history.as_ref().trim(),
                        user_message = content.trim(),
                    ),
                },
                false => format!(
                    ""{chat_history}[INST] {user_message} [/INST]"",
                    chat_history = chat_history.as_ref().trim(),
                    user_message = content.trim(),
                ),
            },
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        let content = content.split(""\n"").next().unwrap_or_default();

        Ok(format!(
            ""{chat_history}{assistant_message}</s>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }

    fn append_tool_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionToolMessage,
    ) -> String {
        format!(
            ""{chat_history}[TOOL_RESULTS]{tool_result}[/TOOL_RESULTS]"",
            chat_history = chat_history.as_ref().trim(),
            tool_result = message.content().trim()
        )
    }
}
impl BuildChatPrompt for MistralToolPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }

    fn build_with_tools(
        &self,
        messages: &mut Vec<ChatCompletionRequestMessage>,
        tools: Option<&[endpoints::chat::Tool]>,
    ) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for (idx, message) in messages.iter().enumerate() {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    let last = idx == messages.len() - 1;
                    prompt = self.append_user_message_tool(&prompt, message, tools, last);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/openchat.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionUserMessage,
    ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the amazon `MistralLite-7B` model.
#[derive(Debug, Default, Clone)]
pub struct OpenChatPrompt;
impl OpenChatPrompt {
    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""GPT4 User: {user_message}<|end_of_turn|>"",
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}GPT4 User: {user_message}<|end_of_turn|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history}GPT4 Assistant: {assistant_message}<|end_of_turn|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for OpenChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        // append user/assistant messages
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""GPT4 Assistant:"");

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/wizard.rs
use super::BuildChatPrompt;
use crate::error::Result;
use endpoints::chat::{
    ChatCompletionRequestMessage, ChatCompletionSystemMessage, ChatCompletionUserMessage,
    ChatCompletionUserMessageContent, ContentPart,
};

/// Generate prompts for the `wizard-vicuna` model.
#[derive(Debug, Default, Clone)]
pub struct WizardCoderPrompt;
impl WizardCoderPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""Below is an instruction that describes a task. Write a response that appropriately completes the request.""),
            false => content.to_string(),
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        format!(
            ""{system_prompt}\n\n### Instruction:\n{user_message}"",
            system_prompt = system_prompt.as_ref().trim(),
            user_message = content.trim(),
        )
    }
}
impl BuildChatPrompt for WizardCoderPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""Below is an instruction that describes a task. Write a response that appropriately completes the request.""),
        };

        let message = messages.last().unwrap();
        let mut prompt = match message {
            ChatCompletionRequestMessage::User(ref message) => {
                self.append_user_message(system_prompt, message)
            }
            _ => return Err(crate::error::PromptError::NoUserMessage),
        };

        prompt.push_str(""\n\n### Response:"");

        Ok(prompt)
    }
}
"
"language: crates/chat-prompts/src/chat/llama.rs
use super::BuildChatPrompt;
use crate::error::{PromptError, Result};
use endpoints::chat::{
    ChatCompletionAssistantMessage, ChatCompletionRequestMessage, ChatCompletionSystemMessage,
    ChatCompletionToolMessage, ChatCompletionUserMessage, ChatCompletionUserMessageContent,
    ContentPart, Tool,
};

/// Generate prompts for the `Llama-2-chat` model.
#[derive(Debug, Default, Clone)]
pub struct Llama2ChatPrompt;
impl Llama2ChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe. <</SYS>>""),
            false =>format!(
                ""<<SYS>>\n{content} <</SYS>>""
            )
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => match system_prompt.as_ref().is_empty() {
                true => {
                    format!(
                        ""<s>[INST] {user_message} [/INST]"",
                        user_message = content.trim(),
                    )
                }
                false => {
                    format!(
                        ""<s>[INST] {system_prompt}\n\n{user_message} [/INST]"",
                        system_prompt = system_prompt.as_ref().trim(),
                        user_message = content.trim(),
                    )
                }
            },
            false => format!(
                ""{chat_history}<s>[INST] {user_message} [/INST]"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{prompt} {assistant_message} </s>"",
            prompt = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Llama2ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""<<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe. <</SYS>>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}

/// Generate prompts for the `Codellama-instruct` model.
#[derive(Debug, Default, Clone)]
pub struct CodeLlamaInstructPrompt;
impl CodeLlamaInstructPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<<SYS>>\nWrite code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```: <</SYS>>""),
            false => format!(
                ""<<SYS>>\n{system_prompt} <</SYS>>"", system_prompt=content
            )
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""<s>[INST] {system_prompt}\n\n{user_message} [/INST]"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<s>[INST] {user_message} [/INST]"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{prompt} {assistant_message} </s>"",
            prompt = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for CodeLlamaInstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""<<SYS>>\nWrite code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```: <</SYS>>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        Ok(prompt)
    }
}

/// Generate prompts for the `Codellama-70b-instruct-hf` model.
#[derive(Debug, Default, Clone)]
pub struct CodeLlamaSuperInstructPrompt;
impl CodeLlamaSuperInstructPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<s>Source: system\n\n Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```: <step>""),
            false => format!(
                ""<s>Source: system\n\n {content} <step>""
            )
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt} Source: user\n\n {user_message} <step>"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history} Source: user\n\n {user_message} <step>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{prompt} Source: assistant\n\n {assistant_message} <step>"",
            prompt = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for CodeLlamaSuperInstructPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""<s>Source: system\n\n Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```: <step>"")
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str("" Source: assistant\nDestination: user\n\n "");

        Ok(prompt)
    }
}

/// Generate prompts for the `Llama-3-chat` model.
///
/// Reference: <https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/>
#[derive(Debug, Default, Clone)]
pub struct Llama3ChatPrompt;
impl Llama3ChatPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.<|eot_id|>""),
            false =>format!(
                ""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>"", system_prompt=content
            )
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}<|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|>"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history} <|start_header_id|>assistant<|end_header_id|>\n\n{assistant_message}<|eot_id|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }
}
impl BuildChatPrompt for Llama3ChatPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.<|eot_id|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|start_header_id|>assistant<|end_header_id|>"");

        Ok(prompt)
    }
}

/// Generate prompts for the `Llama-3.1-instruct` model.
///
/// Reference: <https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/#json-based-tool-calling>
#[derive(Debug, Default, Clone)]
pub struct Llama3ToolPrompt;
impl Llama3ToolPrompt {
    /// Create a system prompt from a chat completion request message.
    fn create_system_prompt(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.<|eot_id|>""),
            false =>format!(
                ""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>"", system_prompt=content
            )
        }
    }

    /// Create a system prompt for tool use.
    fn create_system_prompt_tool(&self, message: &ChatCompletionSystemMessage) -> String {
        let content = message.content();
        match content.is_empty() {
            true => String::from(""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful assistant with tool calling capabilities. When you receive a tool call response, use the output to format an answer to the orginal use question.<|eot_id|>""),
            false =>format!(
                ""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>"", system_prompt=content
            )
        }
    }

    /// Create a user prompt from a chat completion request message.
    fn append_user_message(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => format!(
                ""{system_prompt}<|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|>"",
                system_prompt = system_prompt.as_ref().trim(),
                user_message = content.trim(),
            ),
            false => format!(
                ""{chat_history}<|start_header_id|>user<|end_header_id|>\n\n{user_message}<|eot_id|>"",
                chat_history = chat_history.as_ref().trim(),
                user_message = content.trim(),
            ),
        }
    }

    /// create a user prompt for tool use.
    fn append_user_message_tool(
        &self,
        chat_history: impl AsRef<str>,
        system_prompt: impl AsRef<str>,
        message: &ChatCompletionUserMessage,
        tools: impl AsRef<[Tool]>,
    ) -> String {
        let content = match message.content() {
            ChatCompletionUserMessageContent::Text(text) => text.to_string(),
            ChatCompletionUserMessageContent::Parts(parts) => {
                let mut content = String::new();
                for part in parts {
                    if let ContentPart::Text(text_content) = part {
                        content.push_str(text_content.text());
                        content.push('\n');
                    }
                }
                content
            }
        };

        match chat_history.as_ref().is_empty() {
            true => {
                let json = serde_json::to_string(tools.as_ref()).unwrap();

                format!(
                    ""{system_prompt}<|start_header_id|>user<|end_header_id|>\n\nGiven the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n\nRespond in the format {format}. Do not use variables.\n\n{available_tools}\n\nQuestion: {user_message}<|eot_id|>"",
                    system_prompt = system_prompt.as_ref().trim(),
                    format = r#""{""name"": function name, ""parameters"": dictionary of argument name and its value}""#,
                    available_tools = json,
                    user_message = content.trim(),
                )
            }
            false => {
                let json = serde_json::to_string(tools.as_ref()).unwrap();

                format!(
                    ""{chat_history}<|start_header_id|>user<|end_header_id|>\n\nGiven the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n\nRespond in the format {format}. Do not use variables.\n\n{available_tools}\n\nQuestion: {user_message}<|eot_id|>"",
                    chat_history = chat_history.as_ref().trim(),
                    format = r#""{""name"": function name, ""parameters"": dictionary of argument name and its value}""#,
                    available_tools = json,
                    user_message = content.trim(),
                )
            }
        }
    }

    /// Create an assistant prompt from a chat completion request message.
    fn append_assistant_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionAssistantMessage,
    ) -> Result<String> {
        let content = match message.content() {
            Some(content) => content.to_string(),
            // Note that the content is optional if `tool_calls` is specified.
            None => match message.tool_calls().is_some() {
                true => String::new(),
                false => return Err(PromptError::NoAssistantMessage),
            },
        };

        Ok(format!(
            ""{chat_history} <|start_header_id|>assistant<|end_header_id|>\n\n{assistant_message}<|eot_id|>"",
            chat_history = chat_history.as_ref().trim(),
            assistant_message = content.trim(),
        ))
    }

    /// Create a tool prompt.
    fn append_tool_message(
        &self,
        chat_history: impl AsRef<str>,
        message: &ChatCompletionToolMessage,
    ) -> String {
        format!(
            ""{chat_history}<|start_header_id|>ipython<|end_header_id|>\n\n{tool_result}<|eot_id|>"",
            chat_history = chat_history.as_ref().trim(),
            tool_result = message.content().trim()
        )
    }
}
impl BuildChatPrompt for Llama3ToolPrompt {
    fn build(&self, messages: &mut Vec<ChatCompletionRequestMessage>) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                self.create_system_prompt(message)
            }
            _ => String::from(""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.<|eot_id|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = self.append_user_message(&prompt, &system_prompt, message);
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|start_header_id|>assistant<|end_header_id|>"");

        Ok(prompt)
    }

    fn build_with_tools(
        &self,
        messages: &mut Vec<ChatCompletionRequestMessage>,
        tools: Option<&[endpoints::chat::Tool]>,
    ) -> Result<String> {
        if messages.is_empty() {
            return Err(crate::error::PromptError::NoMessages);
        }

        // system prompt
        let system_prompt = match messages[0] {
            ChatCompletionRequestMessage::System(ref message) => {
                match tools {
                    Some(available_tools) => match available_tools.is_empty() {
                        true => self.create_system_prompt(message),
                        false => self.create_system_prompt_tool(message),
                    },
                    None => self.create_system_prompt(message)
                }
            }
            _ => String::from(""<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a helpful, respectful and honest assistant. Always answer as short as possible, while being safe.<|eot_id|>""),
        };

        // append user/assistant messages
        let mut prompt = String::new();
        for message in messages {
            match message {
                ChatCompletionRequestMessage::User(message) => {
                    prompt = match tools {
                        Some(available_tools) => match available_tools.is_empty() {
                            true => self.append_user_message(&prompt, &system_prompt, message),
                            false => self.append_user_message_tool(
                                &prompt,
                                &system_prompt,
                                message,
                                available_tools,
                            ),
                        },
                        None => self.append_user_message(&prompt, &system_prompt, message),
                    };
                }
                ChatCompletionRequestMessage::Assistant(message) => {
                    prompt = self.append_assistant_message(&prompt, message)?;
                }
                ChatCompletionRequestMessage::Tool(message) => {
                    prompt = self.append_tool_message(&prompt, message);
                }
                _ => continue,
            }
        }

        prompt.push_str(""<|start_header_id|>assistant<|end_header_id|>"");

        Ok(prompt)
    }
}
"
"language: crates/llama-core/src/embeddings.rs
//! Define APIs for computing embeddings.

use crate::{
    error::{BackendError, LlamaCoreError},
    running_mode,
    utils::{get_output_buffer, get_token_info_by_graph},
    Graph, RunningMode, CHAT_GRAPHS, EMBEDDING_GRAPHS, OUTPUT_TENSOR,
};
use endpoints::{
    common::Usage,
    embeddings::{EmbeddingObject, EmbeddingRequest, EmbeddingsResponse, InputText},
};
use serde::{Deserialize, Serialize};

/// Compute embeddings for the given input.
///
/// # Argument
///
/// * `embedding_request` - The embedding request.
///
/// # Returns
///
/// The embeddings response.
pub async fn embeddings(
    embedding_request: &EmbeddingRequest,
) -> Result<EmbeddingsResponse, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Computing embeddings"");

    let running_mode = running_mode()?;
    if running_mode == RunningMode::Chat {
        let err_msg = format!(
            ""Computing embeddings is not supported in the {} mode."",
            running_mode
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    let model_name = &embedding_request.model;

    // For general embedding scenario, the embedding model is the same as the chat model.
    // For RAG scenario, the embedding model is different from the chat model.
    let embedding_graphs = match EMBEDDING_GRAPHS.get() {
        Some(embedding_graphs) => embedding_graphs,
        None => match CHAT_GRAPHS.get() {
            Some(chat_graphs) => chat_graphs,
            None => {
                let err_msg = ""No embedding model is available."";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                return Err(LlamaCoreError::Operation(err_msg.into()));
            }
        },
    };

    let mut embedding_graphs = embedding_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `EMBEDDING_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    let graph = match embedding_graphs.contains_key(model_name) {
        true => embedding_graphs.get_mut(model_name).unwrap(),
        false => match embedding_graphs.iter_mut().next() {
            Some((_, graph)) => graph,
            None => {
                let err_msg = ""There is no model available in the chat graphs."";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                return Err(LlamaCoreError::Operation(err_msg.into()));
            }
        },
    };

    // check if the `embedding` option of metadata is enabled
    if !graph.metadata.embeddings {
        graph.metadata.embeddings = true;
        graph.update_metadata()?;
    }

    // compute embeddings
    let (data, usage) = match &embedding_request.input {
        InputText::String(text) => compute_embeddings(graph, &[text.to_owned()])?,
        InputText::ArrayOfStrings(texts) => compute_embeddings(graph, texts.as_slice())?,
        InputText::ArrayOfTokens(tokens) => {
            let texts: Vec<String> = tokens.iter().map(|t| t.to_string()).collect();
            compute_embeddings(graph, texts.as_slice())?
        }
        InputText::ArrayOfTokenArrays(token_arrays) => {
            let texts: Vec<String> = token_arrays
                .iter()
                .map(|tokens| {
                    tokens
                        .iter()
                        .map(|t| t.to_string())
                        .collect::<Vec<String>>()
                        .join("" "")
                })
                .collect();
            compute_embeddings(graph, texts.as_slice())?
        }
    };

    let embedding_reponse = EmbeddingsResponse {
        object: String::from(""list""),
        data,
        model: graph.name().to_owned(),
        usage,
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Embeddings computed successfully."");

    Ok(embedding_reponse)
}

fn compute_embeddings(
    graph: &mut Graph,
    input: &[String],
) -> Result<(Vec<EmbeddingObject>, Usage), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Compute embeddings for {} chunks"", input.len());

    // compute embeddings
    let mut embeddings: Vec<EmbeddingObject> = Vec::new();
    let mut usage = Usage::default();
    for (idx, input) in input.iter().enumerate() {
        // set input
        let tensor_data = input.as_bytes().to_vec();
        graph
            .set_input(0, wasmedge_wasi_nn::TensorType::U8, &[1], &tensor_data)
            .map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Backend(BackendError::SetInput(err_msg))
            })?;

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""compute embeddings for chunk {}"", idx + 1);

        match graph.compute() {
            Ok(_) => {
                // Retrieve the output.
                let output_buffer = get_output_buffer(graph, OUTPUT_TENSOR)?;

                // convert inference result to string
                let output = std::str::from_utf8(&output_buffer[..]).map_err(|e| {
                    let err_msg = format!(
                        ""Failed to decode the buffer of the inference result to a utf-8 string. Reason: {}"",
                        e
                    );

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    LlamaCoreError::Operation(err_msg)
                })?;

                // deserialize the embedding data
                let embedding = serde_json::from_str::<Embedding>(output).map_err(|e| {
                    let err_msg =
                        format!(""Failed to deserialize the embedding data. Reason: {}"", e);

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    LlamaCoreError::Operation(err_msg)
                })?;

                let embedding_object = EmbeddingObject {
                    index: idx as u64,
                    object: String::from(""embedding""),
                    embedding: embedding.data,
                };

                embeddings.push(embedding_object);

                // retrieve the number of prompt and completion tokens
                let token_info = get_token_info_by_graph(graph)?;

                usage.prompt_tokens += token_info.prompt_tokens;
                usage.completion_tokens += token_info.completion_tokens;
                usage.total_tokens = usage.prompt_tokens + usage.completion_tokens;
            }
            Err(e) => {
                let err_msg = format!(""Failed to compute embeddings. Reason: {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                return Err(LlamaCoreError::Backend(BackendError::Compute(err_msg)));
            }
        }
    }

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""token usage of embeddings: {} prompt tokens, {} comletion tokens"", usage.prompt_tokens, usage.completion_tokens);

    Ok((embeddings, usage))
}

/// Get the dimension of the embedding model.
///
/// # Arguments
///
/// * `name` - The name of the embedding model. If `None`, the dimension of the first model will be returned.
///
/// # Returns
///
/// The dimension of the embedding model.
///
/// # Errors
///
/// * The model does not exist in the embedding graphs.
/// * No embedding model is available.
pub fn dimension(name: Option<&str>) -> Result<u64, LlamaCoreError> {
    let embedding_graphs = match EMBEDDING_GRAPHS.get() {
        Some(embedding_graphs) => embedding_graphs,
        None => {
            let err_msg = ""Fail to get the underlying value of `EMBEDDING_GRAPHS`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let embedding_graphs = embedding_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `EMBEDDING_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    match name {
        Some(model_name) => match embedding_graphs.get(model_name) {
            Some(graph) => Ok(graph.metadata.ctx_size),
            None => {
                let err_msg = format!(
                    ""The model `{}` does not exist in the embedding graphs."",
                    model_name
                );

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                Err(LlamaCoreError::Operation(err_msg))
            }
        },
        None => {
            if !embedding_graphs.is_empty() {
                let graph = match embedding_graphs.values().next() {
                    Some(graph) => graph,
                    None => {
                        let err_msg = ""Fail to get the underlying value of `EMBEDDING_GRAPHS`."";

                        #[cfg(feature = ""logging"")]
                        error!(target: ""stdout"", ""{}"", err_msg);

                        return Err(LlamaCoreError::Operation(err_msg.into()));
                    }
                };

                Ok(graph.metadata.ctx_size)
            } else {
                let err_msg = ""There is no model available in the embedding graphs."";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                Err(LlamaCoreError::Operation(err_msg.into()))
            }
        }
    }
}

#[derive(Debug, Serialize, Deserialize)]
struct Embedding {
    #[serde(rename = ""n_embedding"")]
    len: u64,
    #[serde(rename = ""embedding"")]
    data: Vec<f64>,
}
"
"language: crates/llama-core/src/completions.rs
//! Define APIs for completions.

use crate::{
    error::{BackendError, LlamaCoreError},
    running_mode,
    utils::{get_output_buffer, get_token_info_by_graph},
    Graph, RunningMode, CHAT_GRAPHS, OUTPUT_TENSOR,
};
use endpoints::{
    common::{FinishReason, Usage},
    completions::{CompletionChoice, CompletionObject, CompletionPrompt, CompletionRequest},
};
use std::time::SystemTime;

/// Given a prompt, the model will return one or more predicted completions along with the probabilities of alternative tokens at each position.
pub async fn completions(request: &CompletionRequest) -> Result<CompletionObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Generate completions"");

    let running_mode = running_mode()?;
    if running_mode == RunningMode::Embeddings || running_mode == RunningMode::Rag {
        let err_msg = format!(
            ""The completion is not supported in the {} mode."",
            running_mode
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    let prompt = match &request.prompt {
        CompletionPrompt::SingleText(prompt) => prompt.to_owned(),
        CompletionPrompt::MultiText(prompts) => prompts.join("" ""),
    };

    compute(prompt.trim(), request.model.as_ref())
}

fn compute(
    prompt: impl AsRef<str>,
    model_name: Option<&String>,
) -> std::result::Result<CompletionObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Compute completions"");

    let chat_graphs = match CHAT_GRAPHS.get() {
        Some(chat_graphs) => chat_graphs,
        None => {
            let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let mut chat_graphs = chat_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    match model_name {
        Some(model_name) => match chat_graphs.contains_key(model_name) {
            true => {
                let graph = chat_graphs.get_mut(model_name).unwrap();
                compute_by_graph(graph, prompt)
            }
            false => match chat_graphs.iter_mut().next() {
                Some((_, graph)) => compute_by_graph(graph, prompt),
                None => {
                    let err_msg = ""There is no model available in the chat graphs."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    Err(LlamaCoreError::Operation(err_msg.into()))
                }
            },
        },
        None => match chat_graphs.iter_mut().next() {
            Some((_, graph)) => compute_by_graph(graph, prompt),
            None => {
                let err_msg = ""There is no model available in the chat graphs."";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                Err(LlamaCoreError::Operation(err_msg.into()))
            }
        },
    }
}

/// Runs inference on the model with the given name and returns the output.
fn compute_by_graph(
    graph: &mut Graph,
    prompt: impl AsRef<str>,
) -> std::result::Result<CompletionObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Compute completions by graph"");

    // check if the `embedding` model is disabled or not
    if graph.metadata.embeddings {
        graph.metadata.embeddings = false;

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""The `embedding` field of metadata sets to false."");

        graph.update_metadata()?;
    }

    // set input
    let tensor_data = prompt.as_ref().as_bytes().to_vec();
    graph
        .set_input(0, wasmedge_wasi_nn::TensorType::U8, &[1], &tensor_data)
        .map_err(|e| {
            let err_msg = format!(""Failed to set the input tensor. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Backend(BackendError::SetInput(err_msg))
        })?;

    // execute the inference
    graph.compute().map_err(|e| {
        let err_msg = format!(""Failed to execute the inference. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Backend(BackendError::Compute(err_msg))
    })?;

    // Retrieve the output
    let buffer = get_output_buffer(graph, OUTPUT_TENSOR)?;

    // convert inference result to string
    let model_answer = String::from_utf8(buffer).map_err(|e| {
        let err_msg = format!(
            ""Failed to decode the buffer of the inference result to a utf-8 string. {}"",
            e
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;
    let answer = model_answer.trim();

    // retrieve the number of prompt and completion tokens
    let token_info = get_token_info_by_graph(graph)?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Prompt tokens: {}, Completion tokens: {}"", token_info.prompt_tokens, token_info.completion_tokens);

    let created = SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .map_err(|e| {
            let err_msg = format!(""Failed to get the current time. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Completions generated successfully."");

    Ok(CompletionObject {
        id: uuid::Uuid::new_v4().to_string(),
        object: String::from(""text_completion""),
        created: created.as_secs(),
        model: graph.name().to_string(),
        choices: vec![CompletionChoice {
            index: 0,
            text: String::from(answer),
            finish_reason: FinishReason::stop,
            logprobs: None,
        }],
        usage: Usage {
            prompt_tokens: token_info.prompt_tokens,
            completion_tokens: token_info.completion_tokens,
            total_tokens: token_info.prompt_tokens + token_info.completion_tokens,
        },
    })
}
"
"language: crates/llama-core/src/utils.rs
//! Define utility functions.

use crate::{
    error::{BackendError, LlamaCoreError},
    Graph, CHAT_GRAPHS, EMBEDDING_GRAPHS, MAX_BUFFER_SIZE,
};
use chat_prompts::PromptTemplateType;
use serde_json::Value;

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

/// Return the names of the chat models.
pub fn chat_model_names() -> Result<Vec<String>, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get the names of the chat models."");

    let chat_graphs = match CHAT_GRAPHS.get() {
        Some(chat_graphs) => chat_graphs,
        None => {
            let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let chat_graphs = chat_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    let mut model_names = Vec::new();
    for model_name in chat_graphs.keys() {
        model_names.push(model_name.clone());
    }

    Ok(model_names)
}

/// Return the names of the embedding models.
pub fn embedding_model_names() -> Result<Vec<String>, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get the names of the embedding models."");

    let embedding_graphs = match EMBEDDING_GRAPHS.get() {
        Some(embedding_graphs) => embedding_graphs,
        None => {
            return Err(LlamaCoreError::Operation(String::from(
                ""Fail to get the underlying value of `EMBEDDING_GRAPHS`."",
            )));
        }
    };

    let embedding_graphs = match embedding_graphs.lock() {
        Ok(embedding_graphs) => embedding_graphs,
        Err(e) => {
            let err_msg = format!(""Fail to acquire the lock of `EMBEDDING_GRAPHS`. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg));
        }
    };

    let mut model_names = Vec::new();
    for model_name in embedding_graphs.keys() {
        model_names.push(model_name.clone());
    }

    Ok(model_names)
}

/// Get the chat prompt template type from the given model name.
pub fn chat_prompt_template(name: Option<&str>) -> Result<PromptTemplateType, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    match name {
        Some(name) => {
            info!(target: ""stdout"", ""Get the chat prompt template type from the chat model named {}."", name)
        }
        None => {
            info!(target: ""stdout"", ""Get the chat prompt template type from the default chat model."")
        }
    }

    let chat_graphs = match CHAT_GRAPHS.get() {
        Some(chat_graphs) => chat_graphs,
        None => {
            let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let chat_graphs = chat_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    match name {
        Some(model_name) => match chat_graphs.contains_key(model_name) {
            true => {
                let graph = chat_graphs.get(model_name).unwrap();
                let prompt_template = graph.prompt_template();

                #[cfg(feature = ""logging"")]
                info!(target: ""stdout"", ""prompt_template: {}"", &prompt_template);

                Ok(prompt_template)
            }
            false => match chat_graphs.iter().next() {
                Some((_, graph)) => {
                    let prompt_template = graph.prompt_template();

                    #[cfg(feature = ""logging"")]
                    info!(target: ""stdout"", ""prompt_template: {}"", &prompt_template);

                    Ok(prompt_template)
                }
                None => {
                    let err_msg = ""There is no model available in the chat graphs."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    Err(LlamaCoreError::Operation(err_msg.into()))
                }
            },
        },
        None => match chat_graphs.iter().next() {
            Some((_, graph)) => {
                let prompt_template = graph.prompt_template();

                #[cfg(feature = ""logging"")]
                info!(target: ""stdout"", ""prompt_template: {}"", &prompt_template);

                Ok(prompt_template)
            }
            None => {
                let err_msg = ""There is no model available in the chat graphs."";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                Err(LlamaCoreError::Operation(err_msg.into()))
            }
        },
    }
}

/// Get output buffer generated by model.
pub(crate) fn get_output_buffer(graph: &Graph, index: usize) -> Result<Vec<u8>, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get the output buffer generated by the model named {}"", graph.name());

    let mut output_buffer: Vec<u8> = Vec::with_capacity(MAX_BUFFER_SIZE);

    let output_size: usize = graph.get_output(index, &mut output_buffer).map_err(|e| {
        let err_msg = format!(""Fail to get the generated output tensor. {msg}"", msg = e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Backend(BackendError::GetOutput(err_msg))
    })?;

    unsafe {
        output_buffer.set_len(output_size);
    }

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Output buffer size: {}"", output_size);

    Ok(output_buffer)
}

/// Get output buffer generated by model in the stream mode.
pub(crate) fn get_output_buffer_single(
    graph: &Graph,
    index: usize,
) -> Result<Vec<u8>, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get output buffer generated by the model named {} in the stream mode."", graph.name());

    let mut output_buffer: Vec<u8> = Vec::with_capacity(MAX_BUFFER_SIZE);

    let output_size: usize = graph
        .get_output_single(index, &mut output_buffer)
        .map_err(|e| {
            let err_msg = format!(""Fail to get plugin metadata. {msg}"", msg = e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Backend(BackendError::GetOutput(err_msg))
        })?;

    unsafe {
        output_buffer.set_len(output_size);
    }

    Ok(output_buffer)
}

pub(crate) fn set_tensor_data_u8(
    graph: &mut Graph,
    idx: usize,
    tensor_data: &[u8],
) -> Result<(), LlamaCoreError> {
    if graph
        .set_input(idx, wasmedge_wasi_nn::TensorType::U8, &[1], tensor_data)
        .is_err()
    {
        let err_msg = format!(""Fail to set input tensor at index {}"", idx);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    };

    Ok(())
}

/// Get the token information from the graph.
pub(crate) fn get_token_info_by_graph(graph: &Graph) -> Result<TokenInfo, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get token info from the model named {}."", graph.name());

    let output_buffer = get_output_buffer(graph, 1)?;
    let token_info: Value = match serde_json::from_slice(&output_buffer[..]) {
        Ok(token_info) => token_info,
        Err(e) => {
            let err_msg = format!(""Fail to deserialize token info: {msg}"", msg = e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg));
        }
    };

    let prompt_tokens = match token_info[""input_tokens""].as_u64() {
        Some(prompt_tokens) => prompt_tokens,
        None => {
            let err_msg = ""Fail to convert `input_tokens` to u64."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };
    let completion_tokens = match token_info[""output_tokens""].as_u64() {
        Some(completion_tokens) => completion_tokens,
        None => {
            let err_msg = ""Fail to convert `output_tokens` to u64."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""prompt tokens: {}, completion tokens: {}"", prompt_tokens, completion_tokens);

    Ok(TokenInfo {
        prompt_tokens,
        completion_tokens,
    })
}

/// Get the token information from the graph by the model name.
pub(crate) fn get_token_info_by_graph_name(
    name: Option<&String>,
) -> Result<TokenInfo, LlamaCoreError> {
    let chat_graphs = match CHAT_GRAPHS.get() {
        Some(chat_graphs) => chat_graphs,
        None => {
            let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let chat_graphs = chat_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    match name {
        Some(model_name) => match chat_graphs.contains_key(model_name) {
            true => {
                let graph = chat_graphs.get(model_name).unwrap();
                get_token_info_by_graph(graph)
            }
            false => match chat_graphs.iter().next() {
                Some((_, graph)) => get_token_info_by_graph(graph),
                None => {
                    let err_msg = ""There is no model available in the chat graphs."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    Err(LlamaCoreError::Operation(err_msg.into()))
                }
            },
        },
        None => match chat_graphs.iter().next() {
            Some((_, graph)) => get_token_info_by_graph(graph),
            None => {
                let err_msg = ""There is no model available in the chat graphs."";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                Err(LlamaCoreError::Operation(err_msg.into()))
            }
        },
    }
}

#[derive(Debug)]
pub(crate) struct TokenInfo {
    pub(crate) prompt_tokens: u64,
    pub(crate) completion_tokens: u64,
}

pub(crate) trait TensorType {
    fn tensor_type() -> wasmedge_wasi_nn::TensorType;
    fn shape(shape: impl AsRef<[usize]>) -> Vec<usize> {
        shape.as_ref().to_vec()
    }
}

impl TensorType for u8 {
    fn tensor_type() -> wasmedge_wasi_nn::TensorType {
        wasmedge_wasi_nn::TensorType::U8
    }
}

impl TensorType for f32 {
    fn tensor_type() -> wasmedge_wasi_nn::TensorType {
        wasmedge_wasi_nn::TensorType::F32
    }
}

pub(crate) fn set_tensor_data<T: TensorType>(
    graph: &mut Graph,
    idx: usize,
    tensor_data: &[T],
    shape: impl AsRef<[usize]>,
) -> Result<(), LlamaCoreError> {
    if graph
        .set_input(idx, T::tensor_type(), &T::shape(shape), tensor_data)
        .is_err()
    {
        let err_msg = format!(""Fail to set input tensor at index {}"", idx);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    };

    Ok(())
}
"
"language: crates/llama-core/src/models.rs
//! Define APIs for querying models.

use crate::{error::LlamaCoreError, CHAT_GRAPHS, EMBEDDING_GRAPHS};
use endpoints::models::{ListModelsResponse, Model};

/// Lists models available
pub async fn models() -> Result<ListModelsResponse, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""List models"");

    let mut models = vec![];

    {
        if let Some(chat_graphs) = CHAT_GRAPHS.get() {
            let chat_graphs = chat_graphs.lock().map_err(|e| {
                let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

            for (name, graph) in chat_graphs.iter() {
                models.push(Model {
                    id: name.clone(),
                    created: graph.created.as_secs(),
                    object: String::from(""model""),
                    owned_by: String::from(""Not specified""),
                });
            }
        }
    }

    {
        if let Some(embedding_graphs) = EMBEDDING_GRAPHS.get() {
            let embedding_graphs = embedding_graphs.lock().map_err(|e| {
                LlamaCoreError::Operation(format!(
                    ""Fail to acquire the lock of `EMBEDDING_GRAPHS`. {}"",
                    e
                ))
            })?;

            if !embedding_graphs.is_empty() {
                for (name, graph) in embedding_graphs.iter() {
                    models.push(Model {
                        id: name.clone(),
                        created: graph.created.as_secs(),
                        object: String::from(""model""),
                        owned_by: String::from(""Not specified""),
                    });
                }
            }
        }
    }

    Ok(ListModelsResponse {
        object: String::from(""list""),
        data: models,
    })
}
"
"language: crates/llama-core/src/error.rs
//! Error types for the Llama Core library.

use thiserror::Error;

/// Error types for the Llama Core library.
#[derive(Error, Debug)]
pub enum LlamaCoreError {
    /// Errors in General operation.
    #[error(""{0}"")]
    Operation(String),
    /// Errors in Context initialization.
    #[error(""Failed to initialize computation context. Reason: {0}"")]
    InitContext(String),
    /// Errors thrown by the wasi-nn-ggml plugin and runtime.
    #[error(""{0}"")]
    Backend(#[from] BackendError),
    /// Errors thrown by the Search Backend
    #[cfg(feature = ""search"")]
    #[error(""{0}"")]
    Search(String),
}

/// Error types for wasi-nn errors.
#[derive(Error, Debug)]
pub enum BackendError {
    /// Errors in setting the input tensor.
    #[error(""{0}"")]
    SetInput(String),
    /// Errors in the model inference.
    #[error(""{0}"")]
    Compute(String),
    /// Errors in the model inference in the stream mode.
    #[error(""{0}"")]
    ComputeSingle(String),
    /// Errors in getting the output tensor.
    #[error(""{0}"")]
    GetOutput(String),
    /// Errors in getting the output tensor in the stream mode.
    #[error(""{0}"")]
    GetOutputSingle(String),
    /// Errors in cleaning up the computation context in the stream mode.
    #[error(""{0}"")]
    FinishSingle(String),
}
"
"language: crates/llama-core/src/audio.rs
//! Define APIs for audio generation, transcription, and translation.

use crate::{
    error::LlamaCoreError, utils::set_tensor_data, AUDIO_GRAPH, MAX_BUFFER_SIZE, PIPER_GRAPH,
};
use endpoints::{
    audio::{
        speech::SpeechRequest,
        transcription::{TranscriptionObject, TranscriptionRequest},
        translation::{TranslationObject, TranslationRequest},
    },
    files::FileObject,
};
use std::{fs, io::Write, path::Path, time::SystemTime};

/// Transcribe audio into the input language.
pub async fn audio_transcriptions(
    request: TranscriptionRequest,
) -> Result<TranscriptionObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""processing audio transcription request"");

    let graph = match AUDIO_GRAPH.get() {
        Some(graph) => graph,
        None => {
            let err_msg = ""The AUDIO_GRAPH is not initialized."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.to_owned()));
        }
    };

    let mut graph = match graph.lock() {
        Ok(graph) => graph,
        Err(e) => {
            let err_msg = format!(""Failed to lock the graph. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg));
        }
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""transcription status: {}"", !graph.metadata.translate);

    // check if translation is disabled so that transcription tasks can be done
    if graph.metadata.translate {
        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""switch to the transcription mode"");

        // enable translation
        graph.metadata.translate = false;

        // set the metadata to the model
        let metadata = graph.metadata.clone();

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""metadata: {:?}"", &metadata);

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""set the metadata to the model."");

        match serde_json::to_string(&metadata) {
            Ok(config) => {
                // update metadata
                set_tensor_data(&mut graph, 1, config.as_bytes(), [1])?;
            }
            Err(e) => {
                let err_msg = format!(""Fail to serialize metadata to a JSON string. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                return Err(LlamaCoreError::Operation(err_msg));
            }
        };

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""enabled transcription mode"");
    }

    let path = Path::new(""archives"")
        .join(&request.file.id)
        .join(&request.file.filename);

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""audio file path: {:?}"", &path);

    // load the audio waveform
    let wav_buf = load_audio_waveform(path)?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""read input tensor, size in bytes: {}"", wav_buf.len());

    // set the input tensor
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Feed the audio data to the model."");
    set_tensor_data(&mut graph, 0, &wav_buf, [1, wav_buf.len()])?;

    // compute the graph
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Transcribe audio to text."");
    if let Err(e) = graph.compute() {
        let err_msg = format!(""Failed to compute the graph. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    // get the output tensor
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""[INFO] Retrieve the transcription data."");

    // Retrieve the output.
    let mut output_buffer = vec![0u8; MAX_BUFFER_SIZE];
    let output_size = graph.get_output(0, &mut output_buffer).map_err(|e| {
        let err_msg = format!(""Failed to get the output tensor. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Output buffer size: {}"", output_size);

    // decode the output buffer
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Decode the transcription data to plain text."");

    let text = std::str::from_utf8(&output_buffer[..output_size]).map_err(|e| {
        let err_msg = format!(
            ""Failed to decode the gerated buffer to a utf-8 string. {}"",
            e
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    let obj = TranscriptionObject {
        text: text.trim().to_owned(),
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""End of the audio transcription."");

    Ok(obj)
}

fn load_audio_waveform(filename: impl AsRef<std::path::Path>) -> Result<Vec<u8>, LlamaCoreError> {
    std::fs::read(filename)
        .map_err(|e| {
            let err_msg = format!(""Failed to read the input tensor. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })
        .map_err(|e| LlamaCoreError::Operation(e.to_string()))
}

/// Generate audio from the input text.
pub async fn create_speech(request: SpeechRequest) -> Result<FileObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""processing audio speech request"");

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get the model instance."");
    let graph = match PIPER_GRAPH.get() {
        Some(graph) => graph,
        None => {
            let err_msg = ""The PIPER_GRAPH is not initialized."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.to_owned()));
        }
    };

    let mut graph = match graph.lock() {
        Ok(graph) => graph,
        Err(e) => {
            let err_msg = format!(""Failed to lock the graph. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg));
        }
    };

    // set the input tensor
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Feed the text to the model."");
    set_tensor_data(&mut graph, 0, request.input.as_bytes(), [1])?;

    // compute the graph
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""create audio."");
    if let Err(e) = graph.compute() {
        let err_msg = format!(""Failed to compute the graph. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    // get the output tensor
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""[INFO] Retrieve the audio."");

    let mut output_buffer = vec![0u8; MAX_BUFFER_SIZE];
    let output_size = graph.get_output(0, &mut output_buffer).map_err(|e| {
        let err_msg = format!(""Failed to get the output tensor. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Output buffer size: {}"", output_size);

    // * save the audio data to a file

    // create a unique file id
    let id = format!(""file_{}"", uuid::Uuid::new_v4());

    // save the file
    let path = Path::new(""archives"");
    if !path.exists() {
        fs::create_dir(path).unwrap();
    }
    let file_path = path.join(&id);
    if !file_path.exists() {
        fs::create_dir(&file_path).unwrap();
    }
    let filename = ""output.wav"";
    let mut audio_file = match fs::File::create(file_path.join(filename)) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to create the output file. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg));
        }
    };
    audio_file.write_all(&output_buffer[..output_size]).unwrap();

    // log
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

    let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
        Ok(n) => n.as_secs(),
        Err(_) => {
            let err_msg = ""Failed to get the current time."";

            // log
            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.to_owned()));
        }
    };

    Ok(FileObject {
        id,
        bytes: output_size as u64,
        created_at,
        filename: filename.to_owned(),
        object: ""file"".to_owned(),
        purpose: ""assistants_output"".to_owned(),
    })
}

/// Translate audio into the target language
pub async fn audio_translations(
    request: TranslationRequest,
) -> Result<TranslationObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""processing audio translation request"");

    let graph = match AUDIO_GRAPH.get() {
        Some(graph) => graph,
        None => {
            let err_msg = ""The AUDIO_GRAPH is not initialized."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.to_owned()));
        }
    };

    let mut graph = match graph.lock() {
        Ok(graph) => graph,
        Err(e) => {
            let err_msg = format!(""Failed to lock the graph. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg));
        }
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""translation status: {}"", graph.metadata.translate);

    // update metadata
    if !graph.metadata.translate {
        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""switch to the translation mode"");

        // update the metadata
        graph.metadata.translate = true;

        // set the metadata to the model
        let metadata = graph.metadata.clone();

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""metadata: {:?}"", &metadata);

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""set the metadata to the model."");

        match serde_json::to_string(&metadata) {
            Ok(config) => {
                // update metadata
                set_tensor_data(&mut graph, 1, config.as_bytes(), [1])?;
            }
            Err(e) => {
                let err_msg = format!(""Fail to serialize metadata to a JSON string. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                return Err(LlamaCoreError::Operation(err_msg));
            }
        };

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""enabled translation mode"");
    }

    let path = Path::new(""archives"")
        .join(&request.file.id)
        .join(&request.file.filename);

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""audio file path: {:?}"", &path);

    // load the audio waveform
    let wav_buf = load_audio_waveform(path)?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""read input tensor, size in bytes: {}"", wav_buf.len());

    // set the input tensor
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""feed the audio data to the model."");
    set_tensor_data(&mut graph, 0, &wav_buf, [1, wav_buf.len()])?;

    // compute the graph
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""translate audio to text."");
    if let Err(e) = graph.compute() {
        let err_msg = format!(""Failed to compute the graph. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    // get the output tensor
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""[INFO] retrieve the translation data."");

    // Retrieve the output.
    let mut output_buffer = vec![0u8; MAX_BUFFER_SIZE];
    let output_size = graph.get_output(0, &mut output_buffer).map_err(|e| {
        let err_msg = format!(""Failed to get the output tensor. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""output buffer size: {}"", output_size);

    // decode the output buffer
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""decode the translation data to plain text."");

    let text = std::str::from_utf8(&output_buffer[..output_size]).map_err(|e| {
        let err_msg = format!(
            ""Failed to decode the gerated buffer to a utf-8 string. {}"",
            e
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    let obj = TranslationObject {
        text: text.trim().to_owned(),
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""End of the audio translation."");

    Ok(obj)
}
"
"language: crates/llama-core/src/lib.rs
//! Llama Core, abbreviated as `llama-core`, defines a set of APIs. Developers can utilize these APIs to build applications based on large models, such as chatbots, RAG, and more.

#[cfg(feature = ""logging"")]
#[macro_use]
extern crate log;

pub mod audio;
pub mod chat;
pub mod completions;
pub mod embeddings;
pub mod error;
pub mod graph;
pub mod images;
pub mod models;
pub mod rag;
#[cfg(feature = ""search"")]
pub mod search;
pub mod utils;

pub use error::LlamaCoreError;
pub use graph::{EngineType, Graph, GraphBuilder};

use chat_prompts::PromptTemplateType;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{
    collections::HashMap,
    path::Path,
    sync::{Mutex, RwLock},
};
use utils::get_output_buffer;
use wasmedge_stable_diffusion::*;

// key: model_name, value: Graph
pub(crate) static CHAT_GRAPHS: OnceCell<Mutex<HashMap<String, Graph>>> = OnceCell::new();
// key: model_name, value: Graph
pub(crate) static EMBEDDING_GRAPHS: OnceCell<Mutex<HashMap<String, Graph>>> = OnceCell::new();
// cache bytes for decoding utf8
pub(crate) static CACHED_UTF8_ENCODINGS: OnceCell<Mutex<Vec<u8>>> = OnceCell::new();
// running mode
pub(crate) static RUNNING_MODE: OnceCell<RwLock<RunningMode>> = OnceCell::new();
// stable diffusion context for the text-to-image task
pub(crate) static SD_TEXT_TO_IMAGE: OnceCell<Mutex<TextToImage>> = OnceCell::new();
// stable diffusion context for the image-to-image task
pub(crate) static SD_IMAGE_TO_IMAGE: OnceCell<Mutex<ImageToImage>> = OnceCell::new();
// context for the audio task
pub(crate) static AUDIO_GRAPH: OnceCell<Mutex<Graph>> = OnceCell::new();
// context for the piper task
pub(crate) static PIPER_GRAPH: OnceCell<Mutex<Graph>> = OnceCell::new();

pub(crate) const MAX_BUFFER_SIZE: usize = 2usize.pow(14) * 15 + 128;
pub(crate) const OUTPUT_TENSOR: usize = 0;
const PLUGIN_VERSION: usize = 1;

/// Model metadata
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // this field not defined for the beckend plugin
    #[serde(skip_serializing)]
    pub model_name: String,
    // this field not defined for the beckend plugin
    #[serde(skip_serializing)]
    pub model_alias: String,
    // this field not defined for the beckend plugin
    #[serde(skip_serializing)]
    pub log_prompts: bool,
    // this field not defined for the beckend plugin
    #[serde(skip_serializing)]
    pub prompt_template: PromptTemplateType,

    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    #[serde(rename = ""enable-debug-log"")]
    pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    /// path to the multimodal projector file for llava
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub mmproj: Option<String>,
    /// Path to the image file for llava
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub image: Option<String>,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    /// The main GPU to use. Defaults to None.
    #[serde(rename = ""main-gpu"")]
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[serde(rename = ""tensor-split"")]
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub tensor_split: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    pub use_mmap: Option<bool>,
    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,
    #[serde(rename = ""threads"")]
    pub threads: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,

    // * grammar parameters
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir). Defaults to empty string.
    pub grammar: String,
    /// JSON schema to constrain generations (<https://json-schema.org/>), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub json_schema: Option<String>,

    // * parameters for whisper
    pub translate: bool,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub language: Option<String>,
    /// Number of processors to use during computation. Defaults to 1.
    pub processors: u32,
    /// Time offset in milliseconds. Defaults to 0.
    pub offset_t: u32,
    /// Duration of audio to process in milliseconds. Defaults to 0.
    pub duration: u32,
    /// Maximum number of text context tokens to store. Defaults to -1.
    pub max_context: i32,
    /// Maximum segment length in characters. Defaults to 0.
    pub max_len: u32,
    /// Split on word rather than on token. Defaults to false.
    pub split_on_word: bool,
    /// Output result in a text file. Defaults to false.
    pub output_txt: bool,
    /// Output result in a vtt file. Defaults to false.
    pub output_vtt: bool,
    /// Output result in a srt file. Defaults to false.
    pub output_srt: bool,
    /// Output result in a lrc file. Defaults to false.
    pub output_lrc: bool,
    /// Output result in a CSV file. Defaults to false.
    pub output_csv: bool,
    /// Output result in a JSON file. Defaults to false.
    pub output_json: bool,
}
impl Default for Metadata {
    fn default() -> Self {
        Self {
            model_name: String::new(),
            model_alias: String::new(),
            log_prompts: false,
            debug_log: false,
            prompt_template: PromptTemplateType::Llama2Chat,
            log_enable: false,
            embeddings: false,
            n_predict: 1024,
            reverse_prompt: None,
            mmproj: None,
            image: None,
            n_gpu_layers: 100,
            main_gpu: None,
            tensor_split: None,
            use_mmap: Some(true),
            ctx_size: 512,
            batch_size: 512,
            threads: 2,
            temperature: 1.0,
            top_p: 1.0,
            repeat_penalty: 1.1,
            presence_penalty: 0.0,
            frequency_penalty: 0.0,
            grammar: String::new(),
            json_schema: None,
            translate: false,
            language: None,
            processors: 1,
            offset_t: 0,
            duration: 0,
            max_context: -1,
            max_len: 0,
            split_on_word: false,
            output_txt: false,
            output_vtt: false,
            output_srt: false,
            output_lrc: false,
            output_csv: false,
            output_json: false,
        }
    }
}

/// Builder for the `Metadata` struct
#[derive(Debug)]
pub struct MetadataBuilder {
    metadata: Metadata,
}
impl MetadataBuilder {
    pub fn new<S: Into<String>>(model_name: S, model_alias: S, pt: PromptTemplateType) -> Self {
        let metadata = Metadata {
            model_name: model_name.into(),
            model_alias: model_alias.into(),
            prompt_template: pt,
            ..Default::default()
        };

        Self { metadata }
    }

    pub fn with_prompt_template(mut self, template: PromptTemplateType) -> Self {
        self.metadata.prompt_template = template;
        self
    }

    pub fn enable_plugin_log(mut self, enable: bool) -> Self {
        self.metadata.log_enable = enable;
        self
    }

    pub fn enable_debug_log(mut self, enable: bool) -> Self {
        self.metadata.debug_log = enable;
        self
    }

    pub fn enable_prompts_log(mut self, enable: bool) -> Self {
        self.metadata.log_prompts = enable;
        self
    }

    pub fn enable_embeddings(mut self, enable: bool) -> Self {
        self.metadata.embeddings = enable;
        self
    }

    pub fn with_n_predict(mut self, n: u64) -> Self {
        self.metadata.n_predict = n;
        self
    }

    pub fn with_main_gpu(mut self, gpu: Option<u64>) -> Self {
        self.metadata.main_gpu = gpu;
        self
    }

    pub fn with_tensor_split(mut self, split: Option<String>) -> Self {
        self.metadata.tensor_split = split;
        self
    }

    pub fn with_threads(mut self, threads: u64) -> Self {
        self.metadata.threads = threads;
        self
    }

    pub fn with_reverse_prompt(mut self, prompt: Option<String>) -> Self {
        self.metadata.reverse_prompt = prompt;
        self
    }

    pub fn with_mmproj(mut self, path: Option<String>) -> Self {
        self.metadata.mmproj = path;
        self
    }

    pub fn with_image(mut self, path: impl Into<String>) -> Self {
        self.metadata.image = Some(path.into());
        self
    }

    pub fn with_n_gpu_layers(mut self, n: u64) -> Self {
        self.metadata.n_gpu_layers = n;
        self
    }

    pub fn disable_mmap(mut self, disable: Option<bool>) -> Self {
        self.metadata.use_mmap = disable.map(|v| !v);
        self
    }

    pub fn with_ctx_size(mut self, size: u64) -> Self {
        self.metadata.ctx_size = size;
        self
    }

    pub fn with_batch_size(mut self, size: u64) -> Self {
        self.metadata.batch_size = size;
        self
    }

    pub fn with_temperature(mut self, temp: f64) -> Self {
        self.metadata.temperature = temp;
        self
    }

    pub fn with_top_p(mut self, top_p: f64) -> Self {
        self.metadata.top_p = top_p;
        self
    }

    pub fn with_repeat_penalty(mut self, penalty: f64) -> Self {
        self.metadata.repeat_penalty = penalty;
        self
    }

    pub fn with_presence_penalty(mut self, penalty: f64) -> Self {
        self.metadata.presence_penalty = penalty;
        self
    }

    pub fn with_frequency_penalty(mut self, penalty: f64) -> Self {
        self.metadata.frequency_penalty = penalty;
        self
    }

    pub fn with_grammar(mut self, grammar: impl Into<String>) -> Self {
        self.metadata.grammar = grammar.into();
        self
    }

    pub fn with_json_schema(mut self, schema: Option<String>) -> Self {
        self.metadata.json_schema = schema;
        self
    }

    pub fn build(self) -> Metadata {
        self.metadata
    }
}

/// Builder for creating an audio metadata
#[derive(Debug)]
pub struct WhisperMetadataBuilder {
    metadata: Metadata,
}
impl WhisperMetadataBuilder {
    pub fn new<S: Into<String>>(model_name: S, model_alias: S) -> Self {
        let metadata = Metadata {
            model_name: model_name.into(),
            model_alias: model_alias.into(),
            prompt_template: PromptTemplateType::Null,
            threads: 4,
            translate: false,
            processors: 1,
            offset_t: 0,
            duration: 0,
            max_context: -1,
            max_len: 0,
            split_on_word: false,
            output_txt: false,
            output_vtt: false,
            output_srt: false,
            output_lrc: false,
            output_csv: false,
            output_json: false,
            ..Default::default()
        };

        Self { metadata }
    }

    pub fn enable_plugin_log(mut self, enable: bool) -> Self {
        self.metadata.log_enable = enable;
        self
    }

    pub fn enable_debug_log(mut self, enable: bool) -> Self {
        self.metadata.debug_log = enable;
        self
    }

    pub fn enable_translate(mut self, enable: bool) -> Self {
        self.metadata.translate = enable;
        self
    }

    pub fn target_language(mut self, language: Option<String>) -> Self {
        self.metadata.language = language;
        self
    }

    pub fn with_processors(mut self, processors: u32) -> Self {
        self.metadata.processors = processors;
        self
    }

    pub fn with_offset_t(mut self, offset_t: u32) -> Self {
        self.metadata.offset_t = offset_t;
        self
    }

    pub fn with_duration(mut self, duration: u32) -> Self {
        self.metadata.duration = duration;
        self
    }

    pub fn with_max_context(mut self, max_context: i32) -> Self {
        self.metadata.max_context = max_context;
        self
    }

    pub fn with_max_len(mut self, max_len: u32) -> Self {
        self.metadata.max_len = max_len;
        self
    }

    pub fn split_on_word(mut self, split_on_word: bool) -> Self {
        self.metadata.split_on_word = split_on_word;
        self
    }

    pub fn output_txt(mut self, output_txt: bool) -> Self {
        self.metadata.output_txt = output_txt;
        self
    }

    pub fn output_vtt(mut self, output_vtt: bool) -> Self {
        self.metadata.output_vtt = output_vtt;
        self
    }

    pub fn output_srt(mut self, output_srt: bool) -> Self {
        self.metadata.output_srt = output_srt;
        self
    }

    pub fn output_lrc(mut self, output_lrc: bool) -> Self {
        self.metadata.output_lrc = output_lrc;
        self
    }

    pub fn output_csv(mut self, output_csv: bool) -> Self {
        self.metadata.output_csv = output_csv;
        self
    }

    pub fn output_json(mut self, output_json: bool) -> Self {
        self.metadata.output_json = output_json;
        self
    }

    pub fn build(self) -> Metadata {
        self.metadata
    }
}

/// Initialize the core context
pub fn init_core_context(
    metadata_for_chats: Option<&[Metadata]>,
    metadata_for_embeddings: Option<&[Metadata]>,
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Initializing the core context"");

    if metadata_for_chats.is_none() && metadata_for_embeddings.is_none() {
        let err_msg = ""Failed to initialize the core context. Please set metadata for chat completions and/or embeddings."";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        return Err(LlamaCoreError::InitContext(err_msg.into()));
    }

    let mut mode = RunningMode::Embeddings;

    if let Some(metadata_chats) = metadata_for_chats {
        let mut chat_graphs = HashMap::new();
        for metadata in metadata_chats {
            let graph = Graph::new(metadata)?;

            chat_graphs.insert(graph.name().to_string(), graph);
        }
        CHAT_GRAPHS.set(Mutex::new(chat_graphs)).map_err(|_| {
            let err_msg = ""Failed to initialize the core context. Reason: The `CHAT_GRAPHS` has already been initialized"";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg.into())
        })?;

        mode = RunningMode::Chat
    }

    if let Some(metadata_embeddings) = metadata_for_embeddings {
        let mut embedding_graphs = HashMap::new();
        for metadata in metadata_embeddings {
            let graph = Graph::new(metadata)?;

            embedding_graphs.insert(graph.name().to_string(), graph);
        }
        EMBEDDING_GRAPHS
            .set(Mutex::new(embedding_graphs))
            .map_err(|_| {
                let err_msg = ""Failed to initialize the core context. Reason: The `EMBEDDING_GRAPHS` has already been initialized"";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                LlamaCoreError::InitContext(err_msg.into())
            })?;

        if mode == RunningMode::Chat {
            mode = RunningMode::ChatEmbedding;
        }
    }

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""running mode: {}"", mode);

    RUNNING_MODE.set(RwLock::new(mode)).map_err(|_| {
        let err_msg = ""Failed to initialize the core context. Reason: The `RUNNING_MODE` has already been initialized"";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        LlamaCoreError::InitContext(err_msg.into())
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""The core context has been initialized"");

    Ok(())
}

/// Initialize the core context for RAG scenarios.
pub fn init_rag_core_context(
    metadata_for_chats: &[Metadata],
    metadata_for_embeddings: &[Metadata],
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Initializing the core context for RAG scenarios"");

    // chat models
    if metadata_for_chats.is_empty() {
        let err_msg = ""The metadata for chat models is empty"";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        return Err(LlamaCoreError::InitContext(err_msg.into()));
    }
    let mut chat_graphs = HashMap::new();
    for metadata in metadata_for_chats {
        let graph = Graph::new(metadata)?;

        chat_graphs.insert(graph.name().to_string(), graph);
    }
    CHAT_GRAPHS.set(Mutex::new(chat_graphs)).map_err(|_| {
        let err_msg = ""Failed to initialize the core context. Reason: The `CHAT_GRAPHS` has already been initialized"";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        LlamaCoreError::InitContext(err_msg.into())
    })?;

    // embedding models
    if metadata_for_embeddings.is_empty() {
        let err_msg = ""The metadata for embeddings is empty"";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        return Err(LlamaCoreError::InitContext(err_msg.into()));
    }
    let mut embedding_graphs = HashMap::new();
    for metadata in metadata_for_embeddings {
        let graph = Graph::new(metadata)?;

        embedding_graphs.insert(graph.name().to_string(), graph);
    }
    EMBEDDING_GRAPHS
        .set(Mutex::new(embedding_graphs))
        .map_err(|_| {
            let err_msg = ""Failed to initialize the core context. Reason: The `EMBEDDING_GRAPHS` has already been initialized"";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg.into())
        })?;

    let running_mode = RunningMode::Rag;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""running mode: {}"", running_mode);

    // set running mode
    RUNNING_MODE.set(RwLock::new(running_mode)).map_err(|_| {
            let err_msg = ""Failed to initialize the core context. Reason: The `RUNNING_MODE` has already been initialized"";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg.into())
        })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""The core context for RAG scenarios has been initialized"");

    Ok(())
}

/// Get the plugin info
///
/// Note that it is required to call `init_core_context` before calling this function.
pub fn get_plugin_info() -> Result<PluginInfo, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Getting the plugin info"");

    match running_mode()? {
        RunningMode::Embeddings => {
            let embedding_graphs = match EMBEDDING_GRAPHS.get() {
                Some(embedding_graphs) => embedding_graphs,
                None => {
                    let err_msg = ""Fail to get the underlying value of `EMBEDDING_GRAPHS`."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", err_msg);

                    return Err(LlamaCoreError::Operation(err_msg.into()));
                }
            };

            let embedding_graphs = embedding_graphs.lock().map_err(|e| {
                let err_msg = format!(""Fail to acquire the lock of `EMBEDDING_GRAPHS`. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

            let graph = match embedding_graphs.values().next() {
                Some(graph) => graph,
                None => {
                    let err_msg = ""Fail to get the underlying value of `EMBEDDING_GRAPHS`."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", err_msg);

                    return Err(LlamaCoreError::Operation(err_msg.into()));
                }
            };

            get_plugin_info_by_graph(graph)
        }
        _ => {
            let chat_graphs = match CHAT_GRAPHS.get() {
                Some(chat_graphs) => chat_graphs,
                None => {
                    let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", err_msg);

                    return Err(LlamaCoreError::Operation(err_msg.into()));
                }
            };

            let chat_graphs = chat_graphs.lock().map_err(|e| {
                let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

            let graph = match chat_graphs.values().next() {
                Some(graph) => graph,
                None => {
                    let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", err_msg);

                    return Err(LlamaCoreError::Operation(err_msg.into()));
                }
            };

            get_plugin_info_by_graph(graph)
        }
    }
}

fn get_plugin_info_by_graph(graph: &Graph) -> Result<PluginInfo, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Getting the plugin info by the graph named {}"", graph.name());

    // get the plugin metadata
    let output_buffer = get_output_buffer(graph, PLUGIN_VERSION)?;
    let metadata: serde_json::Value = serde_json::from_slice(&output_buffer[..]).map_err(|e| {
        let err_msg = format!(""Fail to deserialize the plugin metadata. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    // get build number of the plugin
    let plugin_build_number = match metadata.get(""llama_build_number"") {
        Some(value) => match value.as_u64() {
            Some(number) => number,
            None => {
                let err_msg = ""Failed to convert the build number of the plugin to u64"";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                return Err(LlamaCoreError::Operation(err_msg.into()));
            }
        },
        None => {
            let err_msg = ""Metadata does not have the field `llama_build_number`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    // get commit id of the plugin
    let plugin_commit = match metadata.get(""llama_commit"") {
        Some(value) => match value.as_str() {
            Some(commit) => commit,
            None => {
                let err_msg = ""Failed to convert the commit id of the plugin to string"";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                return Err(LlamaCoreError::Operation(err_msg.into()));
            }
        },
        None => {
            let err_msg = ""Metadata does not have the field `llama_commit`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Plugin info: b{}(commit {})"", plugin_build_number, plugin_commit);

    Ok(PluginInfo {
        build_number: plugin_build_number,
        commit_id: plugin_commit.to_string(),
    })
}

/// Version info of the `wasi-nn_ggml` plugin, including the build number and the commit id.
#[derive(Debug, Clone)]
pub struct PluginInfo {
    pub build_number: u64,
    pub commit_id: String,
}
impl std::fmt::Display for PluginInfo {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            ""wasinn-ggml plugin: b{}(commit {})"",
            self.build_number, self.commit_id
        )
    }
}

/// Running mode
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]
pub enum RunningMode {
    Chat,
    Embeddings,
    ChatEmbedding,
    Rag,
}
impl std::fmt::Display for RunningMode {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            RunningMode::Chat => write!(f, ""chat""),
            RunningMode::Embeddings => write!(f, ""embeddings""),
            RunningMode::ChatEmbedding => write!(f, ""chat-embeddings""),
            RunningMode::Rag => write!(f, ""rag""),
        }
    }
}

/// Return the current running mode.
pub fn running_mode() -> Result<RunningMode, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Get the running mode."");

    let mode = match RUNNING_MODE.get() {
        Some(mode) => match mode.read() {
            Ok(mode) => mode.to_owned(),
            Err(e) => {
                let err_msg = format!(""Fail to get the underlying value of `RUNNING_MODE`. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                return Err(LlamaCoreError::Operation(err_msg));
            }
        },
        None => {
            let err_msg = ""Fail to get the underlying value of `RUNNING_MODE`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""running mode: {}"", &mode);

    Ok(mode.to_owned())
}

/// Initialize the stable diffusion context with the given full diffusion model
///
/// # Arguments
///
/// * `model_file` - Path to the stable diffusion model file.
///
/// * `ctx` - The context type to create.
pub fn init_sd_context_with_full_model(
    model_file: impl AsRef<str>,
    ctx: SDContextType,
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Initializing the stable diffusion context with the full model"");

    // create the stable diffusion context for the text-to-image task
    if ctx == SDContextType::Full || ctx == SDContextType::TextToImage {
        let sd = StableDiffusion::new(Task::TextToImage, model_file.as_ref());

        let ctx = sd.create_context().map_err(|e| {
            let err_msg = format!(""Fail to create the context. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::InitContext(err_msg)
        })?;

        let ctx = match ctx {
            Context::TextToImage(ctx) => ctx,
            _ => {
                let err_msg = ""Fail to get the context for the text-to-image task"";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                return Err(LlamaCoreError::InitContext(err_msg.into()));
            }
        };

        SD_TEXT_TO_IMAGE.set(Mutex::new(ctx)).map_err(|_| {
        let err_msg = ""Failed to initialize the stable diffusion context. Reason: The `SD_TEXT_TO_IMAGE` has already been initialized"";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        LlamaCoreError::InitContext(err_msg.into())
    })?;

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""The stable diffusion text-to-image context has been initialized"");
    }

    // create the stable diffusion context for the image-to-image task
    if ctx == SDContextType::Full || ctx == SDContextType::ImageToImage {
        let sd = StableDiffusion::new(Task::ImageToImage, model_file.as_ref());

        let ctx = sd.create_context().map_err(|e| {
            let err_msg = format!(""Fail to create the context. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::InitContext(err_msg)
        })?;

        let ctx = match ctx {
            Context::ImageToImage(ctx) => ctx,
            _ => {
                let err_msg = ""Fail to get the context for the image-to-image task"";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                return Err(LlamaCoreError::InitContext(err_msg.into()));
            }
        };

        SD_IMAGE_TO_IMAGE.set(Mutex::new(ctx)).map_err(|_| {
            let err_msg = ""Failed to initialize the stable diffusion context. Reason: The `SD_IMAGE_TO_IMAGE` has already been initialized"";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg.into())
        })?;

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""The stable diffusion image-to-image context has been initialized"");
    }

    Ok(())
}

/// Initialize the stable diffusion context with the given standalone diffusion model
///
/// # Arguments
///
/// * `model_file` - Path to the standalone diffusion model file.
///
/// * `vae` - Path to the VAE model file.
///
/// * `clip_l` - Path to the CLIP model file.
///
/// * `t5xxl` - Path to the T5-XXL model file.
///
/// * `lora_model_dir` - Path to the Lora model directory.
///
/// * `n_threads` - Number of threads to use.
///
/// * `ctx` - The context type to create.
pub fn init_sd_context_with_standalone_model(
    model_file: impl AsRef<str>,
    vae: impl AsRef<str>,
    clip_l: impl AsRef<str>,
    t5xxl: impl AsRef<str>,
    lora_model_dir: impl AsRef<str>,
    n_threads: i32,
    ctx: SDContextType,
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Initializing the stable diffusion context with the standalone diffusion model"");

    // create the stable diffusion context for the text-to-image task
    if ctx == SDContextType::Full || ctx == SDContextType::TextToImage {
        let sd = SDBuidler::new_with_standalone_model(Task::TextToImage, model_file.as_ref())
            .map_err(|e| {
                let err_msg = format!(
                    ""Failed to initialize the stable diffusion context. Reason: {}"",
                    e
                );

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                LlamaCoreError::InitContext(err_msg)
            })?
            .with_vae_path(vae.as_ref())
            .map_err(|e| {
                let err_msg = format!(
                    ""Failed to initialize the stable diffusion context. Reason: {}"",
                    e
                );

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                LlamaCoreError::InitContext(err_msg)
            })?
            .with_clip_l_path(clip_l.as_ref())
            .map_err(|e| {
                let err_msg = format!(
                    ""Failed to initialize the stable diffusion context. Reason: {}"",
                    e
                );

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                LlamaCoreError::InitContext(err_msg)
            })?
            .with_t5xxl_path(t5xxl.as_ref())
            .map_err(|e| {
                let err_msg = format!(
                    ""Failed to initialize the stable diffusion context. Reason: {}"",
                    e
                );

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                LlamaCoreError::InitContext(err_msg)
            })?
            .with_lora_model_dir(lora_model_dir.as_ref())
            .map_err(|e| {
                let err_msg = format!(
                    ""Failed to initialize the stable diffusion context. Reason: {}"",
                    e
                );

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                LlamaCoreError::InitContext(err_msg)
            })?
            .with_n_threads(n_threads)
            .build();

        let ctx = sd.create_context().map_err(|e| {
            let err_msg = format!(""Fail to create the context. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::InitContext(err_msg)
        })?;

        let ctx = match ctx {
            Context::TextToImage(ctx) => ctx,
            _ => {
                let err_msg = ""Fail to get the context for the text-to-image task"";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                return Err(LlamaCoreError::InitContext(err_msg.into()));
            }
        };

        SD_TEXT_TO_IMAGE.set(Mutex::new(ctx)).map_err(|_| {
            let err_msg = ""Failed to initialize the stable diffusion context. Reason: The `SD_TEXT_TO_IMAGE` has already been initialized"";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg.into())
        })?;

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""The stable diffusion text-to-image context has been initialized"");
    }

    // create the stable diffusion context for the image-to-image task
    if ctx == SDContextType::Full || ctx == SDContextType::ImageToImage {
        let sd = SDBuidler::new_with_standalone_model(Task::ImageToImage, model_file.as_ref())
            .map_err(|e| {
                let err_msg = format!(
                    ""Failed to initialize the stable diffusion context. Reason: {}"",
                    e
                );

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                LlamaCoreError::InitContext(err_msg)
            })?
            .with_vae_path(vae.as_ref())
            .map_err(|e| {
                let err_msg = format!(
                    ""Failed to initialize the stable diffusion context. Reason: {}"",
                    e
                );

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                LlamaCoreError::InitContext(err_msg)
            })?
            .with_clip_l_path(clip_l.as_ref())
            .map_err(|e| {
                let err_msg = format!(
                    ""Failed to initialize the stable diffusion context. Reason: {}"",
                    e
                );

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                LlamaCoreError::InitContext(err_msg)
            })?
            .with_t5xxl_path(t5xxl.as_ref())
            .map_err(|e| {
                let err_msg = format!(
                    ""Failed to initialize the stable diffusion context. Reason: {}"",
                    e
                );

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                LlamaCoreError::InitContext(err_msg)
            })?
            .with_lora_model_dir(lora_model_dir.as_ref())
            .map_err(|e| {
                let err_msg = format!(
                    ""Failed to initialize the stable diffusion context. Reason: {}"",
                    e
                );

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                LlamaCoreError::InitContext(err_msg)
            })?
            .with_n_threads(n_threads)
            .build();

        let ctx = sd.create_context().map_err(|e| {
            let err_msg = format!(""Fail to create the context. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::InitContext(err_msg)
        })?;

        let ctx = match ctx {
            Context::ImageToImage(ctx) => ctx,
            _ => {
                let err_msg = ""Fail to get the context for the image-to-image task"";

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", err_msg);

                return Err(LlamaCoreError::InitContext(err_msg.into()));
            }
        };

        SD_IMAGE_TO_IMAGE.set(Mutex::new(ctx)).map_err(|_| {
        let err_msg = ""Failed to initialize the stable diffusion context. Reason: The `SD_IMAGE_TO_IMAGE` has already been initialized"";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        LlamaCoreError::InitContext(err_msg.into())
    })?;

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""The stable diffusion image-to-image context has been initialized"");
    }

    Ok(())
}

/// The context to create for the stable diffusion model
#[derive(Clone, Debug, Copy, PartialEq, Eq)]
pub enum SDContextType {
    /// `text_to_image` context
    TextToImage,
    /// `image_to_image` context
    ImageToImage,
    /// Both `text_to_image` and `image_to_image` contexts
    Full,
}

/// Initialize the whisper context
pub fn init_whisper_context(
    whisper_metadata: &Metadata,
    model_file: impl AsRef<Path>,
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Initializing the audio context"");

    // create and initialize the audio context
    let graph = GraphBuilder::new(EngineType::Whisper)?
        .with_config(whisper_metadata)?
        .use_cpu()
        .build_from_files([model_file.as_ref()])?;

    AUDIO_GRAPH.set(Mutex::new(graph)).map_err(|_| {
            let err_msg = ""Failed to initialize the audio context. Reason: The `AUDIO_GRAPH` has already been initialized"";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg.into())
        })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""The audio context has been initialized"");

    Ok(())
}

/// Initialize the piper context
///
/// # Arguments
///
/// * `voice_model` - Path to the voice model file.
///
/// * `voice_config` - Path to the voice config file.
///
/// * `espeak_ng_data` - Path to the espeak-ng data directory.
///
pub fn init_piper_context(
    voice_model: impl AsRef<Path>,
    voice_config: impl AsRef<Path>,
    espeak_ng_data: impl AsRef<Path>,
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Initializing the piper context"");

    let config = serde_json::json!({
        ""model"": voice_model.as_ref().to_owned(),
        ""config"": voice_config.as_ref().to_owned(),
        ""espeak_data"": espeak_ng_data.as_ref().to_owned(),
    });

    // create and initialize the audio context
    let graph = GraphBuilder::new(EngineType::Piper)?
        .use_cpu()
        .build_from_buffer([config.to_string()])?;

    PIPER_GRAPH.set(Mutex::new(graph)).map_err(|_| {
            let err_msg = ""Failed to initialize the piper context. Reason: The `PIPER_GRAPH` has already been initialized"";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            LlamaCoreError::InitContext(err_msg.into())
        })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""The piper context has been initialized"");

    Ok(())
}
"
"language: crates/llama-core/src/images.rs
//! Define APIs for image generation and edit.

use crate::{error::LlamaCoreError, SD_IMAGE_TO_IMAGE, SD_TEXT_TO_IMAGE};
use base64::{engine::general_purpose, Engine as _};
use endpoints::images::{
    ImageCreateRequest, ImageEditRequest, ImageObject, ImageVariationRequest, ListImagesResponse,
    ResponseFormat, SamplingMethod,
};
use std::{
    fs::{self, File},
    io::{self, Read},
    path::Path,
};
use wasmedge_stable_diffusion::{stable_diffusion_interface::ImageType, BaseFunction};

/// Create an image given a prompt.
pub async fn image_generation(
    req: &mut ImageCreateRequest,
) -> Result<ListImagesResponse, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Processing the image generation request."");

    let text_to_image_ctx = match SD_TEXT_TO_IMAGE.get() {
        Some(sd) => sd,
        None => {
            let err_msg = ""Fail to get the underlying value of `SD_TEXT_TO_IMAGE`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let mut context = text_to_image_ctx.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `SD_TEXT_TO_IMAGE`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""sd text_to_image context: {:?}"", &context);

    let ctx = &mut *context;

    // create a unique file id
    let id = format!(""file_{}"", uuid::Uuid::new_v4());

    // save the file
    let path = Path::new(""archives"");
    if !path.exists() {
        fs::create_dir(path).unwrap();
    }
    let file_path = path.join(&id);
    if !file_path.exists() {
        fs::create_dir(&file_path).unwrap();
    }
    let filename = ""output.png"";
    let output_image_file = file_path.join(filename);
    let output_image_file = output_image_file.to_str().unwrap();

    // log
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""prompt: {}"", &req.prompt);

    // negative prompt
    let negative_prompt = req.negative_prompt.clone().unwrap_or_default();

    // log
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""negative prompt: {}"", &negative_prompt);

    // cfg_scale
    let cfg_scale = req.cfg_scale.unwrap_or(7.0);

    // log
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""cfg_scale: {}"", cfg_scale);

    // sampling method
    let sample_method = req.sample_method.unwrap_or(SamplingMethod::EulerA);

    // log
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""sample_method: {}"", sample_method);

    // convert sample method to value of `SampleMethodT` type
    let sample_method = match sample_method {
        SamplingMethod::Euler => {
            wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::EULER
        }
        SamplingMethod::EulerA => {
            wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::EULERA
        }
        SamplingMethod::Heun => {
            wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::HEUN
        }
        SamplingMethod::Dpm2 => {
            wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::DPM2
        }
        SamplingMethod::DpmPlusPlus2sA => {
            wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::DPMPP2SA
        }
        SamplingMethod::DpmPlusPlus2m => {
            wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::DPMPP2M
        }
        SamplingMethod::DpmPlusPlus2mv2 => {
            wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::DPMPP2Mv2
        }
        SamplingMethod::Ipndm => {
            wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::IPNDM
        }
        SamplingMethod::IpndmV => {
            wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::IPNDMV
        }
        SamplingMethod::Lcm => {
            wasmedge_stable_diffusion::stable_diffusion_interface::SampleMethodT::LCM
        }
    };

    // steps
    let steps = req.steps.unwrap_or(20);

    // log
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""steps: {}"", steps);

    // size
    let height = req.height.unwrap_or(512);
    let width = req.width.unwrap_or(512);

    // log
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""height: {}, width: {}"", height, width);

    // log
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""generate image"");

    ctx.set_prompt(&req.prompt)
        .set_negative_prompt(negative_prompt)
        .set_output_path(output_image_file)
        .set_cfg_scale(cfg_scale)
        .set_sample_method(sample_method)
        .set_sample_steps(steps as i32)
        .set_height(height as i32)
        .set_width(width as i32)
        .generate()
        .map_err(|e| {
            let err_msg = format!(""Fail to dump the image. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

    // log
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

    let image = match req.response_format {
        Some(ResponseFormat::B64Json) => {
            // convert the image to base64 string
            let base64_string = match image_to_base64(output_image_file) {
                Ok(base64_string) => base64_string,
                Err(e) => {
                    let err_msg = format!(""Fail to convert the image to base64 string. {}"", e);

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return Err(LlamaCoreError::Operation(err_msg));
                }
            };

            // log
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""base64 string: {}"", &base64_string.chars().take(10).collect::<String>());

            // create an image object
            ImageObject {
                b64_json: Some(base64_string),
                url: None,
                prompt: Some(req.prompt.clone()),
            }
        }
        Some(ResponseFormat::Url) | None => {
            // create an image object
            ImageObject {
                b64_json: None,
                url: Some(format!(""/archives/{}/{}"", &id, &filename)),
                prompt: Some(req.prompt.clone()),
            }
        }
    };

    let created: u64 = match std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
        Ok(n) => n.as_secs(),
        Err(_) => {
            let err_msg = ""Failed to get the current time."";

            // log
            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let res = ListImagesResponse {
        created,
        data: vec![image],
    };

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""End of the image generation."");

    Ok(res)
}

/// Create an edited or extended image given an original image and a prompt.
pub async fn image_edit(req: &mut ImageEditRequest) -> Result<ListImagesResponse, LlamaCoreError> {
    let image_to_image_ctx = match SD_IMAGE_TO_IMAGE.get() {
        Some(sd) => sd,
        None => {
            let err_msg = ""Fail to get the underlying value of `SD_IMAGE_TO_IMAGE`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    let mut context = image_to_image_ctx.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `SD_IMAGE_TO_IMAGE`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Operation(err_msg)
    })?;

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""sd image_to_image context: {:?}"", &context);

    let ctx = &mut *context;

    // create a unique file id
    let id = format!(""file_{}"", uuid::Uuid::new_v4());

    // save the file
    let path = Path::new(""archives"");
    if !path.exists() {
        fs::create_dir(path).unwrap();
    }
    let file_path = path.join(&id);
    if !file_path.exists() {
        fs::create_dir(&file_path).unwrap();
    }
    let filename = ""output.png"";
    let output_image_file = file_path.join(filename);
    let output_image_file = output_image_file.to_str().unwrap();

    // get the path of the original image
    let origin_image_file = Path::new(""archives"")
        .join(&req.image.id)
        .join(&req.image.filename);
    let path_origin_image = origin_image_file.to_str().ok_or(LlamaCoreError::Operation(
        ""Fail to get the path of the original image."".into(),
    ))?;

    // create and dump the generated image
    ctx.set_prompt(&req.prompt)
        .set_image(ImageType::Path(path_origin_image.into()))
        .set_output_path(output_image_file)
        .generate()
        .map_err(|e| {
            let err_msg = format!(""Fail to dump the image. {}"", e);

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

    // log
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

    let image = match req.response_format {
        Some(ResponseFormat::B64Json) => {
            // convert the image to base64 string
            let base64_string = match image_to_base64(output_image_file) {
                Ok(base64_string) => base64_string,
                Err(e) => {
                    let err_msg = format!(""Fail to convert the image to base64 string. {}"", e);

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return Err(LlamaCoreError::Operation(err_msg));
                }
            };

            // log
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""base64 string: {}"", &base64_string.chars().take(10).collect::<String>());

            // create an image object
            ImageObject {
                b64_json: Some(base64_string),
                url: None,
                prompt: Some(req.prompt.clone()),
            }
        }
        Some(ResponseFormat::Url) | None => {
            // create an image object
            ImageObject {
                b64_json: None,
                url: Some(format!(""/archives/{}/{}"", &id, &filename)),
                prompt: Some(req.prompt.clone()),
            }
        }
    };

    let created: u64 = match std::time::SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
        Ok(n) => n.as_secs(),
        Err(_) => {
            let err_msg = ""Failed to get the current time."";

            // log
            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            return Err(LlamaCoreError::Operation(err_msg.into()));
        }
    };

    Ok(ListImagesResponse {
        created,
        data: vec![image],
    })
}

/// Create a variation of a given image.
pub async fn image_variation(
    _req: &mut ImageVariationRequest,
) -> Result<ListImagesResponse, LlamaCoreError> {
    unimplemented!(""image_variation"")
}

// convert an image file to a base64 string
fn image_to_base64(image_path: &str) -> io::Result<String> {
    // Open the file
    let mut image_file = File::open(image_path)?;

    // Read the file into a byte array
    let mut buffer = Vec::new();
    image_file.read_to_end(&mut buffer)?;

    Ok(general_purpose::STANDARD.encode(&buffer))
}
"
"language: crates/llama-core/src/rag.rs
//! Define APIs for RAG operations.

use crate::{embeddings::embeddings, error::LlamaCoreError, running_mode, RunningMode};
use endpoints::{
    embeddings::{EmbeddingObject, EmbeddingsResponse, InputText},
    rag::{RagEmbeddingRequest, RagScoredPoint, RetrieveObject},
};
use qdrant::*;
use text_splitter::{MarkdownSplitter, TextSplitter};
use tiktoken_rs::cl100k_base;

/// Convert document chunks to embeddings.
///
/// # Arguments
///
/// * `embedding_request` - A reference to an `EmbeddingRequest` object.
///
/// * `qdrant_url` - URL of the Qdrant server.
///
/// * `qdrant_collection_name` - Name of the Qdrant collection to be created.
///
/// # Returns
///
/// Name of the Qdrant collection if successful.
pub async fn rag_doc_chunks_to_embeddings(
    rag_embedding_request: &RagEmbeddingRequest,
) -> Result<EmbeddingsResponse, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Convert document chunks to embeddings."");

    let running_mode = running_mode()?;
    if running_mode != RunningMode::Rag {
        let err_msg = format!(
            ""Creating knowledge base is not supported in the {} mode."",
            running_mode
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    let embedding_request = &rag_embedding_request.embedding_request;
    let qdrant_url = rag_embedding_request.qdrant_url.as_str();
    let qdrant_collection_name = rag_embedding_request.qdrant_collection_name.as_str();

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Compute embeddings for document chunks."");

    #[cfg(feature = ""logging"")]
    if let Ok(request_str) = serde_json::to_string(&embedding_request) {
        info!(target: ""stdout"", ""Embedding request: {}"", request_str);
    }

    // compute embeddings for the document
    let response = embeddings(embedding_request).await?;
    let embeddings = response.data.as_slice();
    let dim = embeddings[0].embedding.len();

    // create a Qdrant client
    let qdrant_client = qdrant::Qdrant::new_with_url(qdrant_url.to_string());

    // create a collection
    qdrant_create_collection(&qdrant_client, qdrant_collection_name, dim).await?;

    let chunks = match &embedding_request.input {
        InputText::String(text) => vec![text.clone()],
        InputText::ArrayOfStrings(texts) => texts.clone(),
        InputText::ArrayOfTokens(tokens) => tokens.iter().map(|t| t.to_string()).collect(),
        InputText::ArrayOfTokenArrays(token_arrays) => token_arrays
            .iter()
            .map(|tokens| tokens.iter().map(|t| t.to_string()).collect())
            .collect(),
    };

    // create and upsert points
    qdrant_persist_embeddings(
        &qdrant_client,
        qdrant_collection_name,
        embeddings,
        chunks.as_slice(),
    )
    .await?;

    Ok(response)
}

/// Convert a query to embeddings.
///
/// # Arguments
///
/// * `embedding_request` - A reference to an `EmbeddingRequest` object.
pub async fn rag_query_to_embeddings(
    rag_embedding_request: &RagEmbeddingRequest,
) -> Result<EmbeddingsResponse, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Compute embeddings for the user query."");

    let running_mode = running_mode()?;
    if running_mode != RunningMode::Rag {
        let err_msg = format!(""The RAG query is not supported in the {running_mode} mode."",);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    embeddings(&rag_embedding_request.embedding_request).await
}

/// Retrieve similar points from the Qdrant server using the query embedding
///
/// # Arguments
///
/// * `query_embedding` - A reference to a query embedding.
///
/// * `qdrant_url` - URL of the Qdrant server.
///
/// * `qdrant_collection_name` - Name of the Qdrant collection to be created.
///
/// * `limit` - Max number of retrieved result.
pub async fn rag_retrieve_context(
    query_embedding: &[f32],
    qdrant_url: impl AsRef<str>,
    qdrant_collection_name: impl AsRef<str>,
    limit: usize,
    score_threshold: Option<f32>,
) -> Result<RetrieveObject, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    {
        info!(target: ""stdout"", ""Retrieve context."");

        info!(target: ""stdout"", ""qdrant_url: {}, qdrant_collection_name: {}, limit: {}, score_threshold: {}"", qdrant_url.as_ref(), qdrant_collection_name.as_ref(), limit, score_threshold.unwrap_or_default());
    }

    let running_mode = running_mode()?;
    if running_mode != RunningMode::Rag {
        let err_msg = format!(
            ""The context retrieval is not supported in the {} mode."",
            running_mode
        );

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    // create a Qdrant client
    let qdrant_client = qdrant::Qdrant::new_with_url(qdrant_url.as_ref().to_string());

    // search for similar points
    let scored_points = match qdrant_search_similar_points(
        &qdrant_client,
        qdrant_collection_name.as_ref(),
        query_embedding,
        limit,
        score_threshold,
    )
    .await
    {
        Ok(points) => points,
        Err(e) => {
            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", e.to_string());

            return Err(e);
        }
    };

    let ro = match scored_points.is_empty() {
        true => RetrieveObject {
            points: None,
            limit,
            score_threshold: score_threshold.unwrap_or(0.0),
        },
        false => {
            let mut points: Vec<RagScoredPoint> = vec![];
            for point in scored_points.iter() {
                if let Some(payload) = &point.payload {
                    if let Some(source) = payload.get(""source"") {
                        points.push(RagScoredPoint {
                            source: source.to_string(),
                            score: point.score,
                        })
                    }
                }
            }

            RetrieveObject {
                points: Some(points),
                limit,
                score_threshold: score_threshold.unwrap_or(0.0),
            }
        }
    };

    Ok(ro)
}

async fn qdrant_create_collection(
    qdrant_client: &qdrant::Qdrant,
    collection_name: impl AsRef<str>,
    dim: usize,
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Create a Qdrant collection named {} of {} dimensions."", collection_name.as_ref(), dim);

    if let Err(e) = qdrant_client
        .create_collection(collection_name.as_ref(), dim as u32)
        .await
    {
        let err_msg = e.to_string();

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    Ok(())
}

async fn qdrant_persist_embeddings(
    qdrant_client: &qdrant::Qdrant,
    collection_name: impl AsRef<str>,
    embeddings: &[EmbeddingObject],
    chunks: &[String],
) -> Result<(), LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Persist embeddings to the Qdrant instance."");

    let mut points = Vec::<Point>::new();
    for embedding in embeddings {
        // convert the embedding to a vector
        let vector: Vec<_> = embedding.embedding.iter().map(|x| *x as f32).collect();

        // create a payload
        let payload = serde_json::json!({""source"": chunks[embedding.index as usize]})
            .as_object()
            .map(|m| m.to_owned());

        // create a point
        let p = Point {
            id: PointId::Num(embedding.index),
            vector,
            payload,
        };

        points.push(p);
    }

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Number of points to be upserted: {}"", points.len());

    if let Err(e) = qdrant_client
        .upsert_points(collection_name.as_ref(), points)
        .await
    {
        let err_msg = format!(""Failed to upsert points. Reason: {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        return Err(LlamaCoreError::Operation(err_msg));
    }

    Ok(())
}

async fn qdrant_search_similar_points(
    qdrant_client: &qdrant::Qdrant,
    collection_name: impl AsRef<str>,
    query_vector: &[f32],
    limit: usize,
    score_threshold: Option<f32>,
) -> Result<Vec<ScoredPoint>, LlamaCoreError> {
    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Search similar points from the qdrant instance."");

    match qdrant_client
        .search_points(
            collection_name.as_ref(),
            query_vector.to_vec(),
            limit as u64,
            score_threshold,
        )
        .await
    {
        Ok(search_result) => {
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""Number of similar points found: {}"", search_result.len());

            Ok(search_result)
        }
        Err(e) => {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""Fail to search similar points from the qdrant instance. Reason: {}"", &err_msg);

            Err(LlamaCoreError::Operation(err_msg))
        }
    }
}

/// Generate a list of chunks from a given text. Each chunk will be up to the `chunk_capacity`.
///
/// # Arguments
///
/// * `text` - A reference to a text.
///
/// * `ty` - Type of the text, `txt` for text content or `md` for markdown content.
///
/// * `chunk_capacity` - The max tokens each chunk contains.
///
/// # Returns
///
/// A vector of strings.
///
/// # Errors
///
/// Returns an error if the operation fails.
pub fn chunk_text(
    text: impl AsRef<str>,
    ty: impl AsRef<str>,
    chunk_capacity: usize,
) -> Result<Vec<String>, LlamaCoreError> {
    if ty.as_ref().to_lowercase().as_str() != ""txt"" && ty.as_ref().to_lowercase().as_str() != ""md"" {
        let err_msg = ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported."";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        return Err(LlamaCoreError::Operation(err_msg.into()));
    }

    match ty.as_ref().to_lowercase().as_str() {
        ""txt"" => {
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""Chunk the plain text contents."");

            let tokenizer = cl100k_base().map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

            // create a text splitter
            let splitter = TextSplitter::new(tokenizer).with_trim_chunks(true);

            let chunks = splitter
                .chunks(text.as_ref(), chunk_capacity)
                .map(|s| s.to_string())
                .collect::<Vec<_>>();

            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""Number of chunks: {}"", chunks.len());

            Ok(chunks)
        }
        ""md"" => {
            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""Chunk the markdown contents."");

            let tokenizer = cl100k_base().map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

            // create a markdown splitter
            let splitter = MarkdownSplitter::new(tokenizer).with_trim_chunks(true);

            let chunks = splitter
                .chunks(text.as_ref(), chunk_capacity)
                .map(|s| s.to_string())
                .collect::<Vec<_>>();

            #[cfg(feature = ""logging"")]
            info!(target: ""stdout"", ""Number of chunks: {}"", chunks.len());

            Ok(chunks)
        }
        _ => {
            let err_msg =
                ""Failed to upload the target file. Only text and markdown files are supported."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            Err(LlamaCoreError::Operation(err_msg.into()))
        }
    }
}
"
"language: crates/llama-core/src/graph.rs
//! Define Graph and GraphBuilder APIs for creating a new computation graph.

use crate::{error::LlamaCoreError, utils::set_tensor_data_u8, Metadata};
use chat_prompts::PromptTemplateType;
use wasmedge_wasi_nn::{
    Error as WasiNnError, Graph as WasiNnGraph, GraphExecutionContext, TensorType,
};

/// Builder for creating a new computation graph.
#[derive(Debug)]
pub struct GraphBuilder {
    metadata: Option<Metadata>,
    wasi_nn_graph_builder: wasmedge_wasi_nn::GraphBuilder,
}
impl GraphBuilder {
    /// Create a new computation graph builder.
    pub fn new(ty: EngineType) -> Result<Self, LlamaCoreError> {
        let encoding = match ty {
            EngineType::Ggml => wasmedge_wasi_nn::GraphEncoding::Ggml,
            EngineType::Whisper => wasmedge_wasi_nn::GraphEncoding::Whisper,
            EngineType::Piper => wasmedge_wasi_nn::GraphEncoding::Piper,
        };

        let wasi_nn_graph_builder =
            wasmedge_wasi_nn::GraphBuilder::new(encoding, wasmedge_wasi_nn::ExecutionTarget::AUTO);

        Ok(Self {
            metadata: None,
            wasi_nn_graph_builder,
        })
    }

    pub fn with_config(mut self, metadata: &Metadata) -> Result<Self, LlamaCoreError> {
        let config = serde_json::to_string(&metadata).map_err(|e| {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;
        self.wasi_nn_graph_builder = self.wasi_nn_graph_builder.config(config);
        self.metadata = Some(metadata.clone());

        Ok(self)
    }

    pub fn use_cpu(mut self) -> Self {
        self.wasi_nn_graph_builder = self.wasi_nn_graph_builder.cpu();
        self
    }

    pub fn use_gpu(mut self) -> Self {
        self.wasi_nn_graph_builder = self.wasi_nn_graph_builder.gpu();
        self
    }

    pub fn use_tpu(mut self) -> Self {
        self.wasi_nn_graph_builder = self.wasi_nn_graph_builder.tpu();
        self
    }

    pub fn build_from_buffer<B>(self, bytes_array: impl AsRef<[B]>) -> Result<Graph, LlamaCoreError>
    where
        B: AsRef<[u8]>,
    {
        // load the model
        let graph = self
            .wasi_nn_graph_builder
            .build_from_bytes(bytes_array)
            .map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

        // initialize the execution context
        let context = graph.init_execution_context().map_err(|e| {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

        let created = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

        Ok(Graph {
            created,
            metadata: self.metadata.clone().unwrap_or_default(),
            _graph: graph,
            context,
        })
    }

    pub fn build_from_files<P>(self, files: impl AsRef<[P]>) -> Result<Graph, LlamaCoreError>
    where
        P: AsRef<std::path::Path>,
    {
        // load the model
        let graph = self
            .wasi_nn_graph_builder
            .build_from_files(files)
            .map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

        // initialize the execution context
        let context = graph.init_execution_context().map_err(|e| {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

        let created = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

        Ok(Graph {
            created,
            metadata: self.metadata.clone().unwrap_or_default(),
            _graph: graph,
            context,
        })
    }

    pub fn build_from_cache(self) -> Result<Graph, LlamaCoreError> {
        match &self.metadata {
            Some(metadata) => {
                // load the model
                let graph = self
                    .wasi_nn_graph_builder
                    .build_from_cache(&metadata.model_alias)
                    .map_err(|e| {
                        let err_msg = e.to_string();

                        #[cfg(feature = ""logging"")]
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        LlamaCoreError::Operation(err_msg)
                    })?;

                // initialize the execution context
                let context = graph.init_execution_context().map_err(|e| {
                    let err_msg = e.to_string();

                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    LlamaCoreError::Operation(err_msg)
                })?;

                let created = std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .map_err(|e| {
                        let err_msg = e.to_string();

                        #[cfg(feature = ""logging"")]
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        LlamaCoreError::Operation(err_msg)
                    })?;

                Ok(Graph {
                    created,
                    metadata: metadata.clone(),
                    _graph: graph,
                    context,
                })
            }
            None => {
                let err_msg =
                    ""Failed to create a Graph from cache. Reason: Metadata is not provided.""
                        .to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                Err(LlamaCoreError::Operation(err_msg))
            }
        }
    }
}

/// Wrapper of the `wasmedge_wasi_nn::Graph` struct
#[derive(Debug)]
pub struct Graph {
    pub created: std::time::Duration,
    pub metadata: Metadata,
    _graph: WasiNnGraph,
    context: GraphExecutionContext,
}
impl Graph {
    /// Create a new computation graph from the given metadata.
    pub fn new(metadata: &Metadata) -> Result<Self, LlamaCoreError> {
        let config = serde_json::to_string(&metadata).map_err(|e| {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

        // load the model
        let graph = wasmedge_wasi_nn::GraphBuilder::new(
            wasmedge_wasi_nn::GraphEncoding::Ggml,
            wasmedge_wasi_nn::ExecutionTarget::AUTO,
        )
        .config(config)
        .build_from_cache(&metadata.model_alias)
        .map_err(|e| {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

        // initialize the execution context
        let context = graph.init_execution_context().map_err(|e| {
            let err_msg = e.to_string();

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", &err_msg);

            LlamaCoreError::Operation(err_msg)
        })?;

        let created = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map_err(|e| {
                let err_msg = e.to_string();

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                LlamaCoreError::Operation(err_msg)
            })?;

        Ok(Self {
            created,
            metadata: metadata.clone(),
            _graph: graph,
            context,
        })
    }

    /// Get the name of the model
    pub fn name(&self) -> &str {
        &self.metadata.model_name
    }

    /// Get the alias of the model
    pub fn alias(&self) -> &str {
        &self.metadata.model_alias
    }

    /// Get the prompt template type
    pub fn prompt_template(&self) -> PromptTemplateType {
        self.metadata.prompt_template
    }

    /// Update metadata
    pub fn update_metadata(&mut self) -> Result<(), LlamaCoreError> {
        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""Update metadata for the model named {}"", self.name());

        // update metadata
        let config = match serde_json::to_string(&self.metadata) {
            Ok(config) => config,
            Err(e) => {
                let err_msg = format!(""Failed to update metadta. Reason: Fail to serialize metadata to a JSON string. {}"", e);

                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""{}"", &err_msg);

                return Err(LlamaCoreError::Operation(err_msg));
            }
        };

        let res = set_tensor_data_u8(self, 1, config.as_bytes());

        #[cfg(feature = ""logging"")]
        info!(target: ""stdout"", ""Metadata updated successfully."");

        res
    }

    /// Set input uses the data, not only [u8](https://doc.rust-lang.org/nightly/std/primitive.u8.html), but also [f32](https://doc.rust-lang.org/nightly/std/primitive.f32.html), [i32](https://doc.rust-lang.org/nightly/std/primitive.i32.html), etc.
    pub fn set_input<T: Sized>(
        &mut self,
        index: usize,
        tensor_type: TensorType,
        dimensions: &[usize],
        data: impl AsRef<[T]>,
    ) -> Result<(), WasiNnError> {
        self.context.set_input(index, tensor_type, dimensions, data)
    }

    /// Compute the inference on the given inputs.
    pub fn compute(&mut self) -> Result<(), WasiNnError> {
        self.context.compute()
    }

    /// Compute the inference on the given inputs.
    ///
    /// Note that this method is used for the stream mode. It generates one token at a time.
    pub fn compute_single(&mut self) -> Result<(), WasiNnError> {
        self.context.compute_single()
    }

    /// Copy output tensor to out_buffer, return the output’s **size in bytes**.
    pub fn get_output<T: Sized>(
        &self,
        index: usize,
        out_buffer: &mut [T],
    ) -> Result<usize, WasiNnError> {
        self.context.get_output(index, out_buffer)
    }

    /// Copy output tensor to out_buffer, return the output’s **size in bytes**.
    ///
    /// Note that this method is used for the stream mode. It returns one token at a time.
    pub fn get_output_single<T: Sized>(
        &self,
        index: usize,
        out_buffer: &mut [T],
    ) -> Result<usize, WasiNnError> {
        self.context.get_output_single(index, out_buffer)
    }

    /// Clear the computation context.
    ///
    /// Note that this method is used for the stream mode. It clears the context after the stream mode is finished.
    pub fn finish_single(&mut self) -> Result<(), WasiNnError> {
        self.context.fini_single()
    }
}

/// Engine type
#[derive(Debug, Clone, PartialEq, Eq, PartialOrd, Ord)]
pub enum EngineType {
    Ggml,
    Whisper,
    Piper,
}
"
"language: crates/llama-core/src/search.rs
use crate::{error::LlamaCoreError, CHAT_GRAPHS};
use reqwest::{Client, Url};
use serde::{Deserialize, Serialize};
use serde_json::Value;

/// Possible input/output Content Types. Currently only supports JSON.
#[derive(Debug, Eq, PartialEq)]
pub enum ContentType {
    JSON,
}

impl std::fmt::Display for ContentType {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            ""{}"",
            match &self {
                ContentType::JSON => ""application/json"",
            }
        )
    }
}

/// The base Search Configuration holding all relevant information to access a search api and retrieve results.
#[derive(Debug)]
pub struct SearchConfig {
    /// The search engine we're currently focusing on. Currently only one supported, to ensure stability.
    #[allow(dead_code)]
    pub search_engine: String,
    /// The total number of results.
    pub max_search_results: u8,
    /// The size limit of every search result.
    pub size_limit_per_result: u16,
    /// The endpoint for the search API.
    pub endpoint: String,
    /// The content type of the input.
    pub content_type: ContentType,
    /// The (expected) content type of the output.
    pub output_content_type: ContentType,
    /// Method expected by the api endpoint.
    pub method: String,
    /// Additional headers for any other purpose.
    pub additional_headers: Option<std::collections::HashMap<String, String>>,
    /// Callback function to parse the output of the api-service. Implementation left to the user.
    pub parser: fn(&serde_json::Value) -> Result<SearchOutput, Box<dyn std::error::Error>>,
    /// Prompts for use with summarization functionality. If set to `None`, use hard-coded prompts.
    pub summarization_prompts: Option<(String, String)>,
    /// Context size for summary generation. If `None`, will use the 4 char ~ 1 token metric to generate summary.
    pub summarize_ctx_size: Option<usize>,
}

/// output format for individual results in the final output.
#[derive(Serialize, Deserialize)]
pub struct SearchResult {
    pub url: String,
    pub site_name: String,
    pub text_content: String,
}

/// Final output format for consumption by the LLM.
#[derive(Serialize, Deserialize)]
pub struct SearchOutput {
    pub results: Vec<SearchResult>,
}

impl SearchConfig {
    /// Wrapper for the parser() function.
    pub fn parse_into_results(
        &self,
        raw_results: &serde_json::Value,
    ) -> Result<SearchOutput, Box<dyn std::error::Error>> {
        (self.parser)(raw_results)
    }
    pub fn new(
        search_engine: String,
        max_search_results: u8,
        size_limit_per_result: u16,
        endpoint: String,
        content_type: ContentType,
        output_content_type: ContentType,
        method: String,
        additional_headers: Option<std::collections::HashMap<String, String>>,
        parser: fn(&serde_json::Value) -> Result<SearchOutput, Box<dyn std::error::Error>>,
        summarization_prompts: Option<(String, String)>,
        summarize_ctx_size: Option<usize>,
    ) -> SearchConfig {
        SearchConfig {
            search_engine,
            max_search_results,
            size_limit_per_result,
            endpoint,
            content_type,
            output_content_type,
            method,
            additional_headers,
            parser,
            summarization_prompts,
            summarize_ctx_size,
        }
    }
    /// Perform a web search with a `Serialize`-able input. The `search_input` is used as is to query the search endpoint.
    pub async fn perform_search<T: Serialize>(
        &self,
        search_input: &T,
    ) -> Result<SearchOutput, LlamaCoreError> {
        let client = Client::new();
        let url = match Url::parse(&self.endpoint) {
            Ok(url) => url,
            Err(_) => {
                let msg = ""Malformed endpoint url"";
                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""perform_search: {}"", msg);
                return Err(LlamaCoreError::Search(format!(
                    ""When parsing endpoint url: {}"",
                    msg
                )));
            }
        };

        let method_as_string = match reqwest::Method::from_bytes(self.method.as_bytes()) {
            Ok(method) => method,
            _ => {
                let msg = ""Non Standard or unknown method"";
                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""perform_search: {}"", msg);
                return Err(LlamaCoreError::Search(format!(
                    ""When converting method from bytes: {}"",
                    msg
                )));
            }
        };

        let mut req = client.request(method_as_string.clone(), url);

        // check headers.
        req = req.headers(
            match (&self
                .additional_headers
                .clone()
                .unwrap_or_else(|| std::collections::HashMap::new()))
                .try_into()
            {
                Ok(headers) => headers,
                Err(_) => {
                    let msg = ""Failed to convert headers from HashMaps to HeaderMaps"";
                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""perform_search: {}"", msg);
                    return Err(LlamaCoreError::Search(format!(
                        ""On converting headers: {}"",
                        msg
                    )));
                }
            },
        );

        // For POST requests, search_input goes into the request body. For GET requests, in the
        // params.
        req = match method_as_string {
            reqwest::Method::POST => match self.content_type {
                ContentType::JSON => req.json(search_input),
            },
            reqwest::Method::GET => req.query(search_input),
            _ => {
                let msg = format!(
                    ""Unsupported request method: {}"",
                    method_as_string.to_owned()
                );
                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""perform_search: {}"", msg);
                return Err(LlamaCoreError::Search(msg));
            }
        };

        let res = match req.send().await {
            Ok(r) => r,
            Err(e) => {
                let msg = e.to_string();
                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""perform_search: {}"", msg);
                return Err(LlamaCoreError::Search(format!(
                    ""When recieving response: {}"",
                    msg
                )));
            }
        };

        match res.content_length() {
            Some(length) => {
                if length == 0 {
                    let msg = ""Empty response from server"";
                    #[cfg(feature = ""logging"")]
                    error!(target: ""stdout"", ""perform_search: {}"", msg);
                    return Err(LlamaCoreError::Search(format!(
                        ""Unexpected content length: {}"",
                        msg
                    )));
                }
            }
            None => {
                let msg = ""Content length returned None"";
                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""perform_search: {}"", msg);
                return Err(LlamaCoreError::Search(format!(
                    ""Content length field not found: {}"",
                    msg
                )));
            }
        }

        // start parsing the output.
        //
        // only checking for JSON as the output content type since it's the most common and widely
        // supported.
        let raw_results: Value;
        match self.output_content_type {
            ContentType::JSON => {
                let body_text = match res.text().await {
                    Ok(body) => body,
                    Err(e) => {
                        let msg = e.to_string();
                        #[cfg(feature = ""logging"")]
                        error!(target: ""stdout"", ""perform_search: {}"", msg);
                        return Err(LlamaCoreError::Search(format!(
                            ""When accessing response body: {}"",
                            msg
                        )));
                    }
                };
                println!(""{}"", body_text);
                raw_results = match serde_json::from_str(body_text.as_str()) {
                    Ok(value) => value,
                    Err(e) => {
                        let msg = e.to_string();
                        #[cfg(feature = ""logging"")]
                        error!(target: ""stdout"", ""perform_search: {}"", msg);
                        return Err(LlamaCoreError::Search(format!(
                            ""When converting to a JSON object: {}"",
                            msg
                        )));
                    }
                };
            }
        };

        // start cleaning the output.

        // produce SearchOutput instance with the raw results obtained from the endpoint.
        let mut search_output: SearchOutput = match self.parse_into_results(&raw_results) {
            Ok(search_output) => search_output,
            Err(e) => {
                let msg = e.to_string();
                #[cfg(feature = ""logging"")]
                error!(target: ""stdout"", ""perform_search: {}"", msg);
                return Err(LlamaCoreError::Search(format!(
                    ""When calling parse_into_results: {}"",
                    msg
                )));
            }
        };

        // apply maximum search result limit.
        search_output
            .results
            .truncate(self.max_search_results as usize);

        // apply per result character limit.
        //
        // since the clipping only happens when split_at_checked() returns Some, the results will
        // remain unchanged should split_at_checked() return None.
        for result in search_output.results.iter_mut() {
            if let Some(clipped_content) = result
                .text_content
                .split_at_checked(self.size_limit_per_result as usize)
            {
                result.text_content = clipped_content.0.to_string();
            }
        }

        // Search Output cleaned and finalized.
        Ok(search_output)
    }
    /// Perform a search and summarize the corresponding search results
    pub async fn summarize_search<T: Serialize>(
        &self,
        search_input: &T,
    ) -> Result<String, LlamaCoreError> {
        let search_output = self.perform_search(&search_input).await?;

        let summarization_prompts = self.summarization_prompts.clone().unwrap_or((
            ""The following are search results I found on the internet:\n\n"".to_string(),
            ""\n\nTo sum up them up: "".to_string(),
        ));

        // the fallback context size limit for the search summary to be generated.
        let summarize_ctx_size = self
            .summarize_ctx_size
            .unwrap_or((self.size_limit_per_result * self.max_search_results as u16) as usize);

        summarize(
            search_output,
            summarize_ctx_size,
            summarization_prompts.0,
            summarization_prompts.1,
        )
    }
}

/// Summarize the search output provided
fn summarize(
    search_output: SearchOutput,
    summarize_ctx_size: usize,
    initial_prompt: String,
    final_prompt: String,
) -> Result<String, LlamaCoreError> {
    let mut search_output_string: String = String::new();

    // Add the text content of every result together.
    search_output
        .results
        .iter()
        .for_each(|result| search_output_string.push_str(result.text_content.as_str()));

    // Error on embedding running mode.
    if crate::running_mode()? == crate::RunningMode::Embeddings {
        let err_msg = ""Summarization is not supported in the EMBEDDINGS running mode."";

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", err_msg);

        return Err(LlamaCoreError::Search(err_msg.into()));
    }

    // Get graphs and pick the first graph.
    let chat_graphs = match CHAT_GRAPHS.get() {
        Some(chat_graphs) => chat_graphs,
        None => {
            let err_msg = ""Fail to get the underlying value of `CHAT_GRAPHS`."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Search(err_msg.into()));
        }
    };

    let mut chat_graphs = chat_graphs.lock().map_err(|e| {
        let err_msg = format!(""Fail to acquire the lock of `CHAT_GRAPHS`. {}"", e);

        #[cfg(feature = ""logging"")]
        error!(target: ""stdout"", ""{}"", &err_msg);

        LlamaCoreError::Search(err_msg)
    })?;

    // Prepare input prompt.
    let input = initial_prompt + search_output_string.as_str() + final_prompt.as_str();
    let tensor_data = input.as_bytes().to_vec();

    // Use first available chat graph
    let graph: &mut crate::Graph = match chat_graphs.values_mut().next() {
        Some(graph) => graph,
        None => {
            let err_msg = ""No available chat graph."";

            #[cfg(feature = ""logging"")]
            error!(target: ""stdout"", ""{}"", err_msg);

            return Err(LlamaCoreError::Search(err_msg.into()));
        }
    };

    graph
        .set_input(0, wasmedge_wasi_nn::TensorType::U8, &[1], &tensor_data)
        .expect(""Failed to set prompt as the input tensor"");

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Generating a summary for search results..."");
    // Execute the inference.
    graph.compute().expect(""Failed to complete inference"");

    // Retrieve the output.
    let mut output_buffer = vec![0u8; summarize_ctx_size];
    let mut output_size = graph
        .get_output(0, &mut output_buffer)
        .expect(""Failed to get output tensor"");
    output_size = std::cmp::min(summarize_ctx_size, output_size);

    // Compute lossy UTF-8 output (text only).
    let output = String::from_utf8_lossy(&output_buffer[..output_size]).to_string();

    #[cfg(feature = ""logging"")]
    info!(target: ""stdout"", ""Summary generated."");

    Ok(output)
}
"
