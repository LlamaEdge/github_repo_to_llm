"```bash:run-llm.sh
#!/bin/bash
#
# Helper script for deploying LlamaEdge API Server with a single Bash command
#
# - Works on Linux and macOS
# - Supports: CPU, CUDA, Metal, OpenCL
# - Can run GGUF models from https://huggingface.co/second-state/
#

set -e

# required utils: curl, git, make
if ! command -v curl &> /dev/null; then
    printf ""[-] curl not found\n""
    exit 1
fi
if ! command -v git &> /dev/null; then
    printf ""[-] git not found\n""
    exit 1
fi
if ! command -v make &> /dev/null; then
    printf ""[-] make not found\n""
    exit 1
fi

# parse arguments
port=8080
repo=""""
wtype=""""
backend=""cpu""
ctx_size=512
n_predict=1024
n_gpu_layers=100

# if macOS, use metal backend by default
if [[ ""$OSTYPE"" == ""darwin""* ]]; then
    backend=""metal""
elif command -v nvcc &> /dev/null; then
    backend=""cuda""
fi

gpu_id=0
n_parallel=8
n_kv=4096
verbose=0
log_prompts=0
log_stat=0
# 0: server mode
# 1: local mode
# mode=0
# 0: non-interactive
# 1: interactive
interactive=0
model=""""
# ggml version: latest or bxxxx
ggml_version=""latest""

function print_usage {
    printf ""Usage:\n""
    printf ""  ./run-llm.sh [--port]\n\n""
    printf ""  --model:        model name\n""
    printf ""  --interactive:  run in interactive mode\n""
    printf ""  --port:         port number, default is 8080\n""
    printf ""  --ggml-version: ggml version (for example, b2963). If the option is not used, then install the latest version.\n""
    printf ""Example:\n\n""
    printf '  bash <(curl -sSfL 'https://code.flows.network/webhook/iwYN1SdN3AmPgR5ao5Gt/run-llm.sh')""\n\n'
}

while [[ $# -gt 0 ]]; do
    key=""$1""
    case $key in
        --model)
            model=""$2""
            shift
            shift
            ;;
        --interactive)
            interactive=1
            shift
            ;;
        --port)
            port=""$2""
            shift
            shift
            ;;
        --ggml-version)
            ggml_version=""$2""
            shift
            shift
            ;;
        --help)
            print_usage
            exit 0
            ;;
        *)
            echo ""Unknown argument: $key""
            print_usage
            exit 1
            ;;
    esac
done

# available weights types
wtypes=(""Q2_K"" ""Q3_K_L"" ""Q3_K_M"" ""Q3_K_S"" ""Q4_0"" ""Q4_K_M"" ""Q4_K_S"" ""Q5_0"" ""Q5_K_M"" ""Q5_K_S"" ""Q6_K"" ""Q8_0"")

wfiles=()
for wt in ""${wtypes[@]}""; do
    wfiles+=("""")
done

ss_urls=(
    ""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-1.5-9B-Chat-GGUF/resolve/main/Yi-1.5-9B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/stablelm-2-zephyr-1.6b-GGUF/resolve/main/stablelm-2-zephyr-1_6b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/OpenChat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34B-Chat-GGUF/resolve/main/Yi-34B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34Bx2-MoE-60B-GGUF/resolve/main/Yi-34Bx2-MoE-60B-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-LLM-7B-Chat-GGUF/resolve/main/deepseek-llm-7b-chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-Coder-6.7B-Instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/Mistral-7B-Instruct-v0.2-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Orca-2-13B-GGUF/resolve/main/Orca-2-13b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/SOLAR-10.7B-Instruct-v1.0-GGUF/resolve/main/SOLAR-10.7B-Instruct-v1.0-Q5_K_M.gguf""
)

# sample models
ss_models=(
    ""gemma-2-9b-it""
    ""yi-1.5-9b-chat""
    ""phi-3-mini-4k""
    ""llama-3-8b-instruct""
    ""llama-2-7b-chat""
    ""stablelm-2-zephyr-1.6b""
    ""openchat-3.5-0106""
    ""yi-34b-chat""
    ""yi-34bx2-moe-60b""
    ""deepseek-llm-7b-chat""
    ""deepseek-coder-6.7b-instruct""
    ""mistral-7b-instruct-v0.2""
    ""dolphin-2.6-mistral-7b""
    ""orca-2-13b""
    ""tinyllama-1.1b-chat-v1.0""
    ""solar-10.7b-instruct-v1.0""
)

# prompt types
prompt_types=(
    ""gemma-instruct""
    ""chatml""
    ""phi-3-chat""
    ""llama-3-chat""
    ""llama-2-chat""
    ""chatml""
    ""openchat""
    ""zephyr""
    ""codellama-instruct""
    ""mistral-instruct""
    ""mistrallite""
    ""vicuna-chat""
    ""vicuna-1.1-chat""
    ""wizard-coder""
    ""intel-neural""
    ""deepseek-chat""
    ""deepseek-coder""
    ""solar-instruct""
    ""belle-llama-2-chat""
    ""human-assistant""
)


if [ -n ""$model"" ]; then
    printf ""\n""

    # Check if the model is in the list of supported models
    if [[ ! "" ${ss_models[@]} "" =~ "" ${model} "" ]]; then

        printf ""[+] ${model} is an invalid name or a unsupported model. Please check the model list:\n\n""

        for i in ""${!ss_models[@]}""; do
            printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
        done
        printf ""\n""

        # ask for repo until index of sample repo is provided or an URL
        while [[ -z ""$repo"" ]]; do

            read -p ""[+] Please select a number from the list above: "" repo

            # check if the input is a number
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        done
    else
        # Find the index of the model in the list of supported models
        for i in ""${!ss_models[@]}""; do
            if [[ ""${ss_models[$i]}"" = ""${model}"" ]]; then
                ss_model=""${ss_models[$i]}""
                repo=""${ss_urls[$i]}""

                break
            fi
        done

    fi

    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    ss_url=$repo

    repo=${repo%/resolve/main/*}

    # check file if the model has been downloaded before
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * prompt type and reverse prompt

    readme_url=""$repo/resolve/main/README.md""

    # Download the README.md file
    curl -s $readme_url -o README.md

    # Extract the ""Prompt type: xxxx"" line
    prompt_type_line=$(grep -i ""Prompt type:"" README.md)

    # Extract the xxxx part
    prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

    printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

    # Check if ""Reverse prompt"" exists
    if grep -q ""Reverse prompt:"" README.md; then
        # Extract the ""Reverse prompt: xxxx"" line
        reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

        # Extract the xxxx part
        reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
    else
        printf ""[+] No reverse prompt required\n""
    fi

    # Clean up
    rm README.md

    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Install WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    model_name=${wfile%-Q*}

    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

    # Add reverse prompt if it exists
    if [ -n ""$reverse_prompt"" ]; then
        cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
    fi

    printf ""\n""
    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    printf ""********************* [LOG: MODEL INFO (Load Model & Init Execution Context)] *********************""
    eval $cmd

elif [ ""$interactive"" -eq 0 ]; then

    printf ""\n""
    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Installing WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download gemma-2-9b-it-Q5_K_M.gguf
    ss_url=""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading %s ...\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    # * start llama-api-server
    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:gemma-2-9b-it-Q5_K_M.gguf llama-api-server.wasm -p gemma-instruct -c 4096 --model-name gemma-2-9b-it --socket-addr 0.0.0.0:${port}""

    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    eval $cmd

elif [ ""$interactive"" -eq 1 ]; then

    printf ""\n""
    printf ""[I] This is a helper script for deploying LlamaEdge API Server on this machine.\n\n""
    printf ""    The following tasks will be done:\n""
    printf ""    - Download GGUF model\n""
    printf ""    - Install WasmEdge Runtime and the wasi-nn_ggml plugin\n""
    printf ""    - Download LlamaEdge API Server\n""
    printf ""\n""
    printf ""    Upon the tasks done, an HTTP server will be started and it will serve the selected\n""
    printf ""    model.\n""
    printf ""\n""
    printf ""    Please note:\n""
    printf ""\n""
    printf ""    - All downloaded files will be stored in the current folder\n""
    printf ""    - The server will be listening on all network interfaces\n""
    printf ""    - The server will run with default settings which are not always optimal\n""
    printf ""    - Do not judge the quality of a model based on the results from this script\n""
    printf ""    - This script is only for demonstration purposes\n""
    printf ""\n""
    printf ""    During the whole process, you can press Ctrl-C to abort the current process at any time.\n""
    printf ""\n""
    printf ""    Press Enter to continue ...\n\n""

    read

    # * install WasmEdge + wasi-nn_ggml plugin

    printf ""[+] Installing WasmEdge ...\n\n""

    # Check if WasmEdge has been installed
    reinstall_wasmedge=1
    if command -v wasmedge &> /dev/null
    then
        printf ""    1) Install the latest version of WasmEdge and wasi-nn_ggml plugin (recommended)\n""
        printf ""    2) Keep the current version\n\n""
        read -p ""[+] Select a number from the list above: "" reinstall_wasmedge
    fi

    while [[ ""$reinstall_wasmedge"" -ne 1 && ""$reinstall_wasmedge"" -ne 2 ]]; do
        printf ""    Invalid number. Please enter number 1 or 2\n""
        read reinstall_wasmedge
    done

    if [[ ""$reinstall_wasmedge"" == ""1"" ]]; then
        # install WasmEdge + wasi-nn_ggml plugin
        if [ ""$ggml_version"" = ""latest"" ]; then
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        else
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        fi


    elif [[ ""$reinstall_wasmedge"" == ""2"" ]]; then
        wasmedge_path=$(which wasmedge)
        wasmedge_root_path=${wasmedge_path%""/bin/wasmedge""}

        found=0
        for file in ""$wasmedge_root_path/plugin/libwasmedgePluginWasiNN.""*; do
        if [[ -f $file ]]; then
            found=1
            break
        fi
        done

        if [[ $found -eq 0 ]]; then
            printf ""\n    * Not found wasi-nn_ggml plugin. Please download it from https://github.com/WasmEdge/WasmEdge/releases/ and move it to %s. After that, please rerun the script. \n\n"" ""$wasmedge_root_path/plugin/""

            exit 1
        fi

    fi

    printf ""[+] The most popular models at https://huggingface.co/second-state:\n\n""

    for i in ""${!ss_models[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
    done

    # ask for repo until index of sample repo is provided or an URL
    while [[ -z ""$repo"" ]]; do
        printf ""\n    Or choose one from: https://huggingface.co/models?sort=trending&search=gguf\n\n""

        read -p ""[+] Please select a number from the list above or enter an URL: "" repo

        # check if the input is a number
        if [[ ""$repo"" =~ ^[0-9]+$ ]]; then
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        elif [[ ""$repo"" =~ ^https?:// ]]; then
            repo=""$repo""
        else
            printf ""[-] Invalid repo URL: %s\n"" ""$repo""
            repo=""""
        fi
    done


    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    if [ -n ""$ss_model"" ]; then
        ss_url=$repo
        repo=${repo%/resolve/main/*}

        # check file if the model has been downloaded before
        wfile=$(basename ""$ss_url"")
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$ss_url""
        fi

    else

        printf ""[+] Checking for GGUF model files in %s\n\n"" ""$repo""

        # find GGUF files in the source
        model_tree=""${repo%/}/tree/main""
        model_files=$(curl -s ""$model_tree"" | grep -i ""\\.gguf</span>"" | sed -E 's/.*<span class=""truncate group-hover:underline"">(.*)<\/span><\/a>/\1/g')
        # Convert model_files into an array
        model_files_array=($model_files)

        while IFS= read -r line; do
            sizes+=(""$line"")
        done < <(curl -s ""$model_tree"" | awk -F 'download=true"">' '/download=true"">[0-9\.]+ (GB|MB)/ {print $2}' | awk '{print $1, $2}')

        # list all files in the provided git repo
        length=${#model_files_array[@]}
        for ((i=0; i<$length; i++)); do
            file=${model_files_array[i]}
            size=${sizes[i]}
            iw=-1
            is=0
            for wt in ""${wtypes[@]}""; do
                # uppercase
                ufile=$(echo ""$file"" | tr '[:lower:]' '[:upper:]')
                if [[ ""$ufile"" =~ ""$wt"" ]]; then
                    iw=$is
                    break
                fi
                is=$((is+1))
            done

            if [[ $iw -eq -1 ]]; then
                continue
            fi

            wfiles[$iw]=""$file""

            have="" ""
            if [[ -f ""$file"" ]]; then
                have=""*""
            fi

            printf ""    %2d) %s %7s   %s\n"" $iw ""$have"" ""$size"" ""$file""
        done

        # ask for weights type until provided and available
        while [[ -z ""$wtype"" ]]; do
            printf ""\n""
            read -p ""[+] Please select a number from the list above: "" wtype
            wfile=""${wfiles[$wtype]}""

            if [[ -z ""$wfile"" ]]; then
                printf ""[-] Invalid number: %s\n"" ""$wtype""
                wtype=""""
            fi
        done

        url=""${repo%/}/resolve/main/$wfile""

        # check file if the model has been downloaded before
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$url""
        fi

    fi

    # * prompt type and reverse prompt

    if [[ $repo =~ ^https://huggingface\.co/second-state ]]; then
        readme_url=""$repo/resolve/main/README.md""

        # Download the README.md file
        curl -s $readme_url -o README.md

        # Extract the ""Prompt type: xxxx"" line
        prompt_type_line=$(grep -i ""Prompt type:"" README.md)

        # Extract the xxxx part
        prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

        # Check if ""Reverse prompt"" exists
        if grep -q ""Reverse prompt:"" README.md; then
            # Extract the ""Reverse prompt: xxxx"" line
            reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

            # Extract the xxxx part
            reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

            printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
        else
            printf ""[+] No reverse prompt required\n""
        fi

        # Clean up
        rm README.md
    else
        printf ""[+] Please select a number from the list below:\n""
        printf ""    The definitions of the prompt types below can be found at https://github.com/LlamaEdge/LlamaEdge/raw/main/api-server/chat-prompts/README.md\n\n""

        is=0
        for r in ""${prompt_types[@]}""; do
            printf ""    %2d) %s\n"" $is ""$r""
            is=$((is+1))
        done
        printf ""\n""

        prompt_type_index=-1
        while ((prompt_type_index < 0 || prompt_type_index >= ${#prompt_types[@]})); do
            read -p ""[+] Select prompt type: "" prompt_type_index
            # Check if the input is a number
            if ! [[ ""$prompt_type_index"" =~ ^[0-9]+$ ]]; then
                echo ""Invalid input. Please enter a number.""
                prompt_type_index=-1
            fi
        done
        prompt_type=""${prompt_types[$prompt_type_index]}""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $need_reverse_prompt =~ ^[yYnN]$ ]]; do
            read -p ""[+] Need reverse prompt? (y/n): "" need_reverse_prompt
        done

        # If user answered yes, ask them to input a string
        if [[ ""$need_reverse_prompt"" == ""y"" || ""$need_reverse_prompt"" == ""Y"" ]]; then
            read -p ""    Enter the reverse prompt: "" reverse_prompt
            printf ""\n""
        fi
    fi

    # * running mode

    printf ""[+] Running mode: \n\n""

    running_modes=(""API Server with Chatbot web app"" ""CLI Chat"")

    for i in ""${!running_modes[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${running_modes[$i]}""
    done

    while [[ -z ""$running_mode_index"" ]]; do
        printf ""\n""
        read -p ""[+] Select a number from the list above: "" running_mode_index
        running_mode=""${running_modes[$running_mode_index - 1]}""

        if [[ -z ""$running_mode"" ]]; then
            printf ""[-] Invalid number: %s\n"" ""$running_mode_index""
            running_mode_index=""""
        fi
    done
    printf ""[+] Selected running mode: %s (%s)\n"" ""$running_mode_index"" ""$running_mode""

    # * download llama-api-server.wasm or llama-chat.wasm

    repo=""second-state/LlamaEdge""
    releases=$(curl -s ""https://api.github.com/repos/$repo/releases"")
    if [[ ""$running_mode_index"" == ""1"" ]]; then

        # * Download llama-api-server.wasm

        if [ -f ""llama-api-server.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-api-server.wasm. Download the latest llama-api-server.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-api-server.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

            printf ""\n""
        fi

        # * chatbot-ui

        if [ -d ""chatbot-ui"" ]; then
            printf ""[+] Using cached Chatbot web app\n""
        else
            printf ""[+] Downloading Chatbot web app ...\n""
            files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
            curl -LO $files_tarball
            if [ $? -ne 0 ]; then
                printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
                exit 1
            fi
            tar xzf chatbot-ui.tar.gz
            rm chatbot-ui.tar.gz
            printf ""\n""
        fi

        model_name=${wfile%-Q*}

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start the server:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_server =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start the server? (y/n): "" start_server
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_server"" == ""y"" || ""$start_server"" == ""Y"" ]]; then
            printf ""\n""
            printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
            printf ""*********************************** LlamaEdge API Server ********************************\n\n""
            eval $cmd

        fi

    elif [[ ""$running_mode_index"" == ""2"" ]]; then

        # * Download llama-chat.wasm

        if [ -f ""llama-chat.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-chat.wasm. Download the latest llama-chat.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-chat.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-chat.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-chat.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

            printf ""\n""
        fi

        # * prepare the command

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-chat.wasm --prompt-template $prompt_type""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start CLI Chat:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_chat =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start CLI Chat? (y/n): "" start_chat
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_chat"" == ""y"" || ""$start_chat"" == ""Y"" ]]; then
            printf ""\n""

            # Execute the command
            printf ""********************* LlamaEdge *********************\n\n""
            eval $cmd

        fi

    else
        printf ""[-] Invalid running mode: %s\n"" ""$running_mode_index""
        exit 1
    fi

else
    echo ""Invalid value for interactive""
fi

exit 0

```","Summary:
**Pull Request Review Summary**

The provided script is a Bash helper script designed to deploy the LlamaEdge API Server on Linux and macOS systems. The script performs the following tasks:

1. **Model Selection**: The user can choose from various models available at https://huggingface.co/second-state or select one from their local repository.
2. **Prompt Type Selection**: Based on the selected model, the user is prompted to choose a specific prompt type (e.g., ""Prompt type: XXXX"").
3. **Reverse Prompt Option**: The user can optionally provide a reverse prompt for the chosen model.
4. **Running Mode Selection**: The user selects either API Server with Chatbot web app or CLI Chat as the running mode.
5. **Dependence Downloading**: The script downloads necessary dependencies, such as LlamaEdge's llama-api-server.wasm or llama-chat.wasm.
6. **Command Execution**: For each selected model and prompt type, a command is generated to run the API Server or CLI Chat.

**Code-related Files:**

The following files are used or created during the script execution:

* `llama-api-server.wasm` (or `llama-chat.wasm`): downloaded from LlamaEdge's GitHub repository.
* `chatbot-ui`: a tarball containing the Chatbot web app, which is extracted and stored locally.

**Important Points:**

1. **Interactive Mode**: The script supports an interactive mode where users can choose their preferences, such as model selection, prompt type, reverse prompt, and running mode.
2. **Model Selection**: Users can select models from both https://huggingface.co/second-state and their local repository.
3. **Prompt Type and Reverse Prompt Options**: Based on the selected model, users are presented with a list of available prompt types and an option to provide a reverse prompt.

Overall, this script is well-structured and easy to follow. It covers various aspects of deploying LlamaEdge API Server, such as model selection, prompt type specification, and running mode choice. The use of clear variable names, comments, and sections helps maintain readability."
"```bash:run-llm.sh
#!/bin/bash
#
# Helper script for deploying LlamaEdge API Server with a single Bash command
#
# - Works on Linux and macOS
# - Supports: CPU, CUDA, Metal, OpenCL
# - Can run GGUF models from https://huggingface.co/second-state/
#

set -e

# required utils: curl, git, make
if ! command -v curl &> /dev/null; then
    printf ""[-] curl not found\n""
    exit 1
fi
if ! command -v git &> /dev/null; then
    printf ""[-] git not found\n""
    exit 1
fi
if ! command -v make &> /dev/null; then
    printf ""[-] make not found\n""
    exit 1
fi

# parse arguments
port=8080
repo=""""
wtype=""""
backend=""cpu""
ctx_size=512
n_predict=1024
n_gpu_layers=100

# if macOS, use metal backend by default
if [[ ""$OSTYPE"" == ""darwin""* ]]; then
    backend=""metal""
elif command -v nvcc &> /dev/null; then
    backend=""cuda""
fi

gpu_id=0
n_parallel=8
n_kv=4096
verbose=0
log_prompts=0
log_stat=0
# 0: server mode
# 1: local mode
# mode=0
# 0: non-interactive
# 1: interactive
interactive=0
model=""""
# ggml version: latest or bxxxx
ggml_version=""latest""

function print_usage {
    printf ""Usage:\n""
    printf ""  ./run-llm.sh [--port]\n\n""
    printf ""  --model:        model name\n""
    printf ""  --interactive:  run in interactive mode\n""
    printf ""  --port:         port number, default is 8080\n""
    printf ""  --ggml-version: ggml version (for example, b2963). If the option is not used, then install the latest version.\n""
    printf ""Example:\n\n""
    printf '  bash <(curl -sSfL 'https://code.flows.network/webhook/iwYN1SdN3AmPgR5ao5Gt/run-llm.sh')""\n\n'
}

while [[ $# -gt 0 ]]; do
    key=""$1""
    case $key in
        --model)
            model=""$2""
            shift
            shift
            ;;
        --interactive)
            interactive=1
            shift
            ;;
        --port)
            port=""$2""
            shift
            shift
            ;;
        --ggml-version)
            ggml_version=""$2""
            shift
            shift
            ;;
        --help)
            print_usage
            exit 0
            ;;
        *)
            echo ""Unknown argument: $key""
            print_usage
            exit 1
            ;;
    esac
done

# available weights types
wtypes=(""Q2_K"" ""Q3_K_L"" ""Q3_K_M"" ""Q3_K_S"" ""Q4_0"" ""Q4_K_M"" ""Q4_K_S"" ""Q5_0"" ""Q5_K_M"" ""Q5_K_S"" ""Q6_K"" ""Q8_0"")

wfiles=()
for wt in ""${wtypes[@]}""; do
    wfiles+=("""")
done

ss_urls=(
    ""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-1.5-9B-Chat-GGUF/resolve/main/Yi-1.5-9B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/stablelm-2-zephyr-1.6b-GGUF/resolve/main/stablelm-2-zephyr-1_6b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/OpenChat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34B-Chat-GGUF/resolve/main/Yi-34B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34Bx2-MoE-60B-GGUF/resolve/main/Yi-34Bx2-MoE-60B-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-LLM-7B-Chat-GGUF/resolve/main/deepseek-llm-7b-chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-Coder-6.7B-Instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/Mistral-7B-Instruct-v0.2-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Orca-2-13B-GGUF/resolve/main/Orca-2-13b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/SOLAR-10.7B-Instruct-v1.0-GGUF/resolve/main/SOLAR-10.7B-Instruct-v1.0-Q5_K_M.gguf""
)

# sample models
ss_models=(
    ""gemma-2-9b-it""
    ""yi-1.5-9b-chat""
    ""phi-3-mini-4k""
    ""llama-3-8b-instruct""
    ""llama-2-7b-chat""
    ""stablelm-2-zephyr-1.6b""
    ""openchat-3.5-0106""
    ""yi-34b-chat""
    ""yi-34bx2-moe-60b""
    ""deepseek-llm-7b-chat""
    ""deepseek-coder-6.7b-instruct""
    ""mistral-7b-instruct-v0.2""
    ""dolphin-2.6-mistral-7b""
    ""orca-2-13b""
    ""tinyllama-1.1b-chat-v1.0""
    ""solar-10.7b-instruct-v1.0""
)

# prompt types
prompt_types=(
    ""gemma-instruct""
    ""chatml""
    ""phi-3-chat""
    ""llama-3-chat""
    ""llama-2-chat""
    ""chatml""
    ""openchat""
    ""zephyr""
    ""codellama-instruct""
    ""mistral-instruct""
    ""mistrallite""
    ""vicuna-chat""
    ""vicuna-1.1-chat""
    ""wizard-coder""
    ""intel-neural""
    ""deepseek-chat""
    ""deepseek-coder""
    ""solar-instruct""
    ""belle-llama-2-chat""
    ""human-assistant""
)


if [ -n ""$model"" ]; then
    printf ""\n""

    # Check if the model is in the list of supported models
    if [[ ! "" ${ss_models[@]} "" =~ "" ${model} "" ]]; then

        printf ""[+] ${model} is an invalid name or a unsupported model. Please check the model list:\n\n""

        for i in ""${!ss_models[@]}""; do
            printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
        done
        printf ""\n""

        # ask for repo until index of sample repo is provided or an URL
        while [[ -z ""$repo"" ]]; do

            read -p ""[+] Please select a number from the list above: "" repo

            # check if the input is a number
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        done
    else
        # Find the index of the model in the list of supported models
        for i in ""${!ss_models[@]}""; do
            if [[ ""${ss_models[$i]}"" = ""${model}"" ]]; then
                ss_model=""${ss_models[$i]}""
                repo=""${ss_urls[$i]}""

                break
            fi
        done

    fi

    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    ss_url=$repo

    repo=${repo%/resolve/main/*}

    # check file if the model has been downloaded before
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * prompt type and reverse prompt

    readme_url=""$repo/resolve/main/README.md""

    # Download the README.md file
    curl -s $readme_url -o README.md

    # Extract the ""Prompt type: xxxx"" line
    prompt_type_line=$(grep -i ""Prompt type:"" README.md)

    # Extract the xxxx part
    prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

    printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

    # Check if ""Reverse prompt"" exists
    if grep -q ""Reverse prompt:"" README.md; then
        # Extract the ""Reverse prompt: xxxx"" line
        reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

        # Extract the xxxx part
        reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
    else
        printf ""[+] No reverse prompt required\n""
    fi

    # Clean up
    rm README.md

    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Install WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    model_name=${wfile%-Q*}

    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

    # Add reverse prompt if it exists
    if [ -n ""$reverse_prompt"" ]; then
        cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
    fi

    printf ""\n""
    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    printf ""********************* [LOG: MODEL INFO (Load Model & Init Execution Context)] *********************""
    eval $cmd

elif [ ""$interactive"" -eq 0 ]; then

    printf ""\n""
    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Installing WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download gemma-2-9b-it-Q5_K_M.gguf
    ss_url=""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading %s ...\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    # * start llama-api-server
    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:gemma-2-9b-it-Q5_K_M.gguf llama-api-server.wasm -p gemma-instruct -c 4096 --model-name gemma-2-9b-it --socket-addr 0.0.0.0:${port}""

    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    eval $cmd

elif [ ""$interactive"" -eq 1 ]; then

    printf ""\n""
    printf ""[I] This is a helper script for deploying LlamaEdge API Server on this machine.\n\n""
    printf ""    The following tasks will be done:\n""
    printf ""    - Download GGUF model\n""
    printf ""    - Install WasmEdge Runtime and the wasi-nn_ggml plugin\n""
    printf ""    - Download LlamaEdge API Server\n""
    printf ""\n""
    printf ""    Upon the tasks done, an HTTP server will be started and it will serve the selected\n""
    printf ""    model.\n""
    printf ""\n""
    printf ""    Please note:\n""
    printf ""\n""
    printf ""    - All downloaded files will be stored in the current folder\n""
    printf ""    - The server will be listening on all network interfaces\n""
    printf ""    - The server will run with default settings which are not always optimal\n""
    printf ""    - Do not judge the quality of a model based on the results from this script\n""
    printf ""    - This script is only for demonstration purposes\n""
    printf ""\n""
    printf ""    During the whole process, you can press Ctrl-C to abort the current process at any time.\n""
    printf ""\n""
    printf ""    Press Enter to continue ...\n\n""

    read

    # * install WasmEdge + wasi-nn_ggml plugin

    printf ""[+] Installing WasmEdge ...\n\n""

    # Check if WasmEdge has been installed
    reinstall_wasmedge=1
    if command -v wasmedge &> /dev/null
    then
        printf ""    1) Install the latest version of WasmEdge and wasi-nn_ggml plugin (recommended)\n""
        printf ""    2) Keep the current version\n\n""
        read -p ""[+] Select a number from the list above: "" reinstall_wasmedge
    fi

    while [[ ""$reinstall_wasmedge"" -ne 1 && ""$reinstall_wasmedge"" -ne 2 ]]; do
        printf ""    Invalid number. Please enter number 1 or 2\n""
        read reinstall_wasmedge
    done

    if [[ ""$reinstall_wasmedge"" == ""1"" ]]; then
        # install WasmEdge + wasi-nn_ggml plugin
        if [ ""$ggml_version"" = ""latest"" ]; then
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        else
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        fi


    elif [[ ""$reinstall_wasmedge"" == ""2"" ]]; then
        wasmedge_path=$(which wasmedge)
        wasmedge_root_path=${wasmedge_path%""/bin/wasmedge""}

        found=0
        for file in ""$wasmedge_root_path/plugin/libwasmedgePluginWasiNN.""*; do
        if [[ -f $file ]]; then
            found=1
            break
        fi
        done

        if [[ $found -eq 0 ]]; then
            printf ""\n    * Not found wasi-nn_ggml plugin. Please download it from https://github.com/WasmEdge/WasmEdge/releases/ and move it to %s. After that, please rerun the script. \n\n"" ""$wasmedge_root_path/plugin/""

            exit 1
        fi

    fi

    printf ""[+] The most popular models at https://huggingface.co/second-state:\n\n""

    for i in ""${!ss_models[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
    done

    # ask for repo until index of sample repo is provided or an URL
    while [[ -z ""$repo"" ]]; do
        printf ""\n    Or choose one from: https://huggingface.co/models?sort=trending&search=gguf\n\n""

        read -p ""[+] Please select a number from the list above or enter an URL: "" repo

        # check if the input is a number
        if [[ ""$repo"" =~ ^[0-9]+$ ]]; then
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        elif [[ ""$repo"" =~ ^https?:// ]]; then
            repo=""$repo""
        else
            printf ""[-] Invalid repo URL: %s\n"" ""$repo""
            repo=""""
        fi
    done


    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    if [ -n ""$ss_model"" ]; then
        ss_url=$repo
        repo=${repo%/resolve/main/*}

        # check file if the model has been downloaded before
        wfile=$(basename ""$ss_url"")
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$ss_url""
        fi

    else

        printf ""[+] Checking for GGUF model files in %s\n\n"" ""$repo""

        # find GGUF files in the source
        model_tree=""${repo%/}/tree/main""
        model_files=$(curl -s ""$model_tree"" | grep -i ""\\.gguf</span>"" | sed -E 's/.*<span class=""truncate group-hover:underline"">(.*)<\/span><\/a>/\1/g')
        # Convert model_files into an array
        model_files_array=($model_files)

        while IFS= read -r line; do
            sizes+=(""$line"")
        done < <(curl -s ""$model_tree"" | awk -F 'download=true"">' '/download=true"">[0-9\.]+ (GB|MB)/ {print $2}' | awk '{print $1, $2}')

        # list all files in the provided git repo
        length=${#model_files_array[@]}
        for ((i=0; i<$length; i++)); do
            file=${model_files_array[i]}
            size=${sizes[i]}
            iw=-1
            is=0
            for wt in ""${wtypes[@]}""; do
                # uppercase
                ufile=$(echo ""$file"" | tr '[:lower:]' '[:upper:]')
                if [[ ""$ufile"" =~ ""$wt"" ]]; then
                    iw=$is
                    break
                fi
                is=$((is+1))
            done

            if [[ $iw -eq -1 ]]; then
                continue
            fi

            wfiles[$iw]=""$file""

            have="" ""
            if [[ -f ""$file"" ]]; then
                have=""*""
            fi

            printf ""    %2d) %s %7s   %s\n"" $iw ""$have"" ""$size"" ""$file""
        done

        # ask for weights type until provided and available
        while [[ -z ""$wtype"" ]]; do
            printf ""\n""
            read -p ""[+] Please select a number from the list above: "" wtype
            wfile=""${wfiles[$wtype]}""

            if [[ -z ""$wfile"" ]]; then
                printf ""[-] Invalid number: %s\n"" ""$wtype""
                wtype=""""
            fi
        done

        url=""${repo%/}/resolve/main/$wfile""

        # check file if the model has been downloaded before
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$url""
        fi

    fi

    # * prompt type and reverse prompt

    if [[ $repo =~ ^https://huggingface\.co/second-state ]]; then
        readme_url=""$repo/resolve/main/README.md""

        # Download the README.md file
        curl -s $readme_url -o README.md

        # Extract the ""Prompt type: xxxx"" line
        prompt_type_line=$(grep -i ""Prompt type:"" README.md)

        # Extract the xxxx part
        prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

        # Check if ""Reverse prompt"" exists
        if grep -q ""Reverse prompt:"" README.md; then
            # Extract the ""Reverse prompt: xxxx"" line
            reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

            # Extract the xxxx part
            reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

            printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
        else
            printf ""[+] No reverse prompt required\n""
        fi

        # Clean up
        rm README.md
    else
        printf ""[+] Please select a number from the list below:\n""
        printf ""    The definitions of the prompt types below can be found at https://github.com/LlamaEdge/LlamaEdge/raw/main/api-server/chat-prompts/README.md\n\n""

        is=0
        for r in ""${prompt_types[@]}""; do
            printf ""    %2d) %s\n"" $is ""$r""
            is=$((is+1))
        done
        printf ""\n""

        prompt_type_index=-1
        while ((prompt_type_index < 0 || prompt_type_index >= ${#prompt_types[@]})); do
            read -p ""[+] Select prompt type: "" prompt_type_index
            # Check if the input is a number
            if ! [[ ""$prompt_type_index"" =~ ^[0-9]+$ ]]; then
                echo ""Invalid input. Please enter a number.""
                prompt_type_index=-1
            fi
        done
        prompt_type=""${prompt_types[$prompt_type_index]}""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $need_reverse_prompt =~ ^[yYnN]$ ]]; do
            read -p ""[+] Need reverse prompt? (y/n): "" need_reverse_prompt
        done

        # If user answered yes, ask them to input a string
        if [[ ""$need_reverse_prompt"" == ""y"" || ""$need_reverse_prompt"" == ""Y"" ]]; then
            read -p ""    Enter the reverse prompt: "" reverse_prompt
            printf ""\n""
        fi
    fi

    # * running mode

    printf ""[+] Running mode: \n\n""

    running_modes=(""API Server with Chatbot web app"" ""CLI Chat"")

    for i in ""${!running_modes[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${running_modes[$i]}""
    done

    while [[ -z ""$running_mode_index"" ]]; do
        printf ""\n""
        read -p ""[+] Select a number from the list above: "" running_mode_index
        running_mode=""${running_modes[$running_mode_index - 1]}""

        if [[ -z ""$running_mode"" ]]; then
            printf ""[-] Invalid number: %s\n"" ""$running_mode_index""
            running_mode_index=""""
        fi
    done
    printf ""[+] Selected running mode: %s (%s)\n"" ""$running_mode_index"" ""$running_mode""

    # * download llama-api-server.wasm or llama-chat.wasm

    repo=""second-state/LlamaEdge""
    releases=$(curl -s ""https://api.github.com/repos/$repo/releases"")
    if [[ ""$running_mode_index"" == ""1"" ]]; then

        # * Download llama-api-server.wasm

        if [ -f ""llama-api-server.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-api-server.wasm. Download the latest llama-api-server.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-api-server.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

            printf ""\n""
        fi

        # * chatbot-ui

        if [ -d ""chatbot-ui"" ]; then
            printf ""[+] Using cached Chatbot web app\n""
        else
            printf ""[+] Downloading Chatbot web app ...\n""
            files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
            curl -LO $files_tarball
            if [ $? -ne 0 ]; then
                printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
                exit 1
            fi
            tar xzf chatbot-ui.tar.gz
            rm chatbot-ui.tar.gz
            printf ""\n""
        fi

        model_name=${wfile%-Q*}

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start the server:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_server =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start the server? (y/n): "" start_server
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_server"" == ""y"" || ""$start_server"" == ""Y"" ]]; then
            printf ""\n""
            printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
            printf ""*********************************** LlamaEdge API Server ********************************\n\n""
            eval $cmd

        fi

    elif [[ ""$running_mode_index"" == ""2"" ]]; then

        # * Download llama-chat.wasm

        if [ -f ""llama-chat.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-chat.wasm. Download the latest llama-chat.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-chat.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-chat.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-chat.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

            printf ""\n""
        fi

        # * prepare the command

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-chat.wasm --prompt-template $prompt_type""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start CLI Chat:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_chat =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start CLI Chat? (y/n): "" start_chat
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_chat"" == ""y"" || ""$start_chat"" == ""Y"" ]]; then
            printf ""\n""

            # Execute the command
            printf ""********************* LlamaEdge *********************\n\n""
            eval $cmd

        fi

    else
        printf ""[-] Invalid running mode: %s\n"" ""$running_mode_index""
        exit 1
    fi

else
    echo ""Invalid value for interactive""
fi

exit 0

```","Q: 1. Which Linux distribution is used in the script?
A: The script appears to be written for a Linux distribution that supports the Bash shell and has the `curl` utility installed. However, it does not explicitly specify which Linux distribution is being used.

That being said, based on the shebang line at the top of the script (`#!/bin/bash`), we can infer that the script is likely designed to be run on a Linux system with a Bash-compatible shell (such as Ubuntu, Debian, or Fedora).

In particular, the script uses the `wasmedge` command-line tool, which is part of the WasmEdge runtime. WasmEdge is an open-source webAssembly runtime developed by the China University of Technology, and it appears to be supported on various Linux distributions.

Some examples of Linux distributions that are likely compatible with this script include:

* Ubuntu
* Debian
* Fedora
* CentOS

Please note that this is not an exhaustive list, and the script may run on other Linux distributions as well."
"```bash:run-llm.sh
#!/bin/bash
#
# Helper script for deploying LlamaEdge API Server with a single Bash command
#
# - Works on Linux and macOS
# - Supports: CPU, CUDA, Metal, OpenCL
# - Can run GGUF models from https://huggingface.co/second-state/
#

set -e

# required utils: curl, git, make
if ! command -v curl &> /dev/null; then
    printf ""[-] curl not found\n""
    exit 1
fi
if ! command -v git &> /dev/null; then
    printf ""[-] git not found\n""
    exit 1
fi
if ! command -v make &> /dev/null; then
    printf ""[-] make not found\n""
    exit 1
fi

# parse arguments
port=8080
repo=""""
wtype=""""
backend=""cpu""
ctx_size=512
n_predict=1024
n_gpu_layers=100

# if macOS, use metal backend by default
if [[ ""$OSTYPE"" == ""darwin""* ]]; then
    backend=""metal""
elif command -v nvcc &> /dev/null; then
    backend=""cuda""
fi

gpu_id=0
n_parallel=8
n_kv=4096
verbose=0
log_prompts=0
log_stat=0
# 0: server mode
# 1: local mode
# mode=0
# 0: non-interactive
# 1: interactive
interactive=0
model=""""
# ggml version: latest or bxxxx
ggml_version=""latest""

function print_usage {
    printf ""Usage:\n""
    printf ""  ./run-llm.sh [--port]\n\n""
    printf ""  --model:        model name\n""
    printf ""  --interactive:  run in interactive mode\n""
    printf ""  --port:         port number, default is 8080\n""
    printf ""  --ggml-version: ggml version (for example, b2963). If the option is not used, then install the latest version.\n""
    printf ""Example:\n\n""
    printf '  bash <(curl -sSfL 'https://code.flows.network/webhook/iwYN1SdN3AmPgR5ao5Gt/run-llm.sh')""\n\n'
}

while [[ $# -gt 0 ]]; do
    key=""$1""
    case $key in
        --model)
            model=""$2""
            shift
            shift
            ;;
        --interactive)
            interactive=1
            shift
            ;;
        --port)
            port=""$2""
            shift
            shift
            ;;
        --ggml-version)
            ggml_version=""$2""
            shift
            shift
            ;;
        --help)
            print_usage
            exit 0
            ;;
        *)
            echo ""Unknown argument: $key""
            print_usage
            exit 1
            ;;
    esac
done

# available weights types
wtypes=(""Q2_K"" ""Q3_K_L"" ""Q3_K_M"" ""Q3_K_S"" ""Q4_0"" ""Q4_K_M"" ""Q4_K_S"" ""Q5_0"" ""Q5_K_M"" ""Q5_K_S"" ""Q6_K"" ""Q8_0"")

wfiles=()
for wt in ""${wtypes[@]}""; do
    wfiles+=("""")
done

ss_urls=(
    ""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-1.5-9B-Chat-GGUF/resolve/main/Yi-1.5-9B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/stablelm-2-zephyr-1.6b-GGUF/resolve/main/stablelm-2-zephyr-1_6b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/OpenChat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34B-Chat-GGUF/resolve/main/Yi-34B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34Bx2-MoE-60B-GGUF/resolve/main/Yi-34Bx2-MoE-60B-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-LLM-7B-Chat-GGUF/resolve/main/deepseek-llm-7b-chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-Coder-6.7B-Instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/Mistral-7B-Instruct-v0.2-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Orca-2-13B-GGUF/resolve/main/Orca-2-13b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/SOLAR-10.7B-Instruct-v1.0-GGUF/resolve/main/SOLAR-10.7B-Instruct-v1.0-Q5_K_M.gguf""
)

# sample models
ss_models=(
    ""gemma-2-9b-it""
    ""yi-1.5-9b-chat""
    ""phi-3-mini-4k""
    ""llama-3-8b-instruct""
    ""llama-2-7b-chat""
    ""stablelm-2-zephyr-1.6b""
    ""openchat-3.5-0106""
    ""yi-34b-chat""
    ""yi-34bx2-moe-60b""
    ""deepseek-llm-7b-chat""
    ""deepseek-coder-6.7b-instruct""
    ""mistral-7b-instruct-v0.2""
    ""dolphin-2.6-mistral-7b""
    ""orca-2-13b""
    ""tinyllama-1.1b-chat-v1.0""
    ""solar-10.7b-instruct-v1.0""
)

# prompt types
prompt_types=(
    ""gemma-instruct""
    ""chatml""
    ""phi-3-chat""
    ""llama-3-chat""
    ""llama-2-chat""
    ""chatml""
    ""openchat""
    ""zephyr""
    ""codellama-instruct""
    ""mistral-instruct""
    ""mistrallite""
    ""vicuna-chat""
    ""vicuna-1.1-chat""
    ""wizard-coder""
    ""intel-neural""
    ""deepseek-chat""
    ""deepseek-coder""
    ""solar-instruct""
    ""belle-llama-2-chat""
    ""human-assistant""
)


if [ -n ""$model"" ]; then
    printf ""\n""

    # Check if the model is in the list of supported models
    if [[ ! "" ${ss_models[@]} "" =~ "" ${model} "" ]]; then

        printf ""[+] ${model} is an invalid name or a unsupported model. Please check the model list:\n\n""

        for i in ""${!ss_models[@]}""; do
            printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
        done
        printf ""\n""

        # ask for repo until index of sample repo is provided or an URL
        while [[ -z ""$repo"" ]]; do

            read -p ""[+] Please select a number from the list above: "" repo

            # check if the input is a number
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        done
    else
        # Find the index of the model in the list of supported models
        for i in ""${!ss_models[@]}""; do
            if [[ ""${ss_models[$i]}"" = ""${model}"" ]]; then
                ss_model=""${ss_models[$i]}""
                repo=""${ss_urls[$i]}""

                break
            fi
        done

    fi

    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    ss_url=$repo

    repo=${repo%/resolve/main/*}

    # check file if the model has been downloaded before
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * prompt type and reverse prompt

    readme_url=""$repo/resolve/main/README.md""

    # Download the README.md file
    curl -s $readme_url -o README.md

    # Extract the ""Prompt type: xxxx"" line
    prompt_type_line=$(grep -i ""Prompt type:"" README.md)

    # Extract the xxxx part
    prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

    printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

    # Check if ""Reverse prompt"" exists
    if grep -q ""Reverse prompt:"" README.md; then
        # Extract the ""Reverse prompt: xxxx"" line
        reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

        # Extract the xxxx part
        reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
    else
        printf ""[+] No reverse prompt required\n""
    fi

    # Clean up
    rm README.md

    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Install WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    model_name=${wfile%-Q*}

    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

    # Add reverse prompt if it exists
    if [ -n ""$reverse_prompt"" ]; then
        cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
    fi

    printf ""\n""
    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    printf ""********************* [LOG: MODEL INFO (Load Model & Init Execution Context)] *********************""
    eval $cmd

elif [ ""$interactive"" -eq 0 ]; then

    printf ""\n""
    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Installing WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download gemma-2-9b-it-Q5_K_M.gguf
    ss_url=""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading %s ...\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    # * start llama-api-server
    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:gemma-2-9b-it-Q5_K_M.gguf llama-api-server.wasm -p gemma-instruct -c 4096 --model-name gemma-2-9b-it --socket-addr 0.0.0.0:${port}""

    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    eval $cmd

elif [ ""$interactive"" -eq 1 ]; then

    printf ""\n""
    printf ""[I] This is a helper script for deploying LlamaEdge API Server on this machine.\n\n""
    printf ""    The following tasks will be done:\n""
    printf ""    - Download GGUF model\n""
    printf ""    - Install WasmEdge Runtime and the wasi-nn_ggml plugin\n""
    printf ""    - Download LlamaEdge API Server\n""
    printf ""\n""
    printf ""    Upon the tasks done, an HTTP server will be started and it will serve the selected\n""
    printf ""    model.\n""
    printf ""\n""
    printf ""    Please note:\n""
    printf ""\n""
    printf ""    - All downloaded files will be stored in the current folder\n""
    printf ""    - The server will be listening on all network interfaces\n""
    printf ""    - The server will run with default settings which are not always optimal\n""
    printf ""    - Do not judge the quality of a model based on the results from this script\n""
    printf ""    - This script is only for demonstration purposes\n""
    printf ""\n""
    printf ""    During the whole process, you can press Ctrl-C to abort the current process at any time.\n""
    printf ""\n""
    printf ""    Press Enter to continue ...\n\n""

    read

    # * install WasmEdge + wasi-nn_ggml plugin

    printf ""[+] Installing WasmEdge ...\n\n""

    # Check if WasmEdge has been installed
    reinstall_wasmedge=1
    if command -v wasmedge &> /dev/null
    then
        printf ""    1) Install the latest version of WasmEdge and wasi-nn_ggml plugin (recommended)\n""
        printf ""    2) Keep the current version\n\n""
        read -p ""[+] Select a number from the list above: "" reinstall_wasmedge
    fi

    while [[ ""$reinstall_wasmedge"" -ne 1 && ""$reinstall_wasmedge"" -ne 2 ]]; do
        printf ""    Invalid number. Please enter number 1 or 2\n""
        read reinstall_wasmedge
    done

    if [[ ""$reinstall_wasmedge"" == ""1"" ]]; then
        # install WasmEdge + wasi-nn_ggml plugin
        if [ ""$ggml_version"" = ""latest"" ]; then
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        else
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        fi


    elif [[ ""$reinstall_wasmedge"" == ""2"" ]]; then
        wasmedge_path=$(which wasmedge)
        wasmedge_root_path=${wasmedge_path%""/bin/wasmedge""}

        found=0
        for file in ""$wasmedge_root_path/plugin/libwasmedgePluginWasiNN.""*; do
        if [[ -f $file ]]; then
            found=1
            break
        fi
        done

        if [[ $found -eq 0 ]]; then
            printf ""\n    * Not found wasi-nn_ggml plugin. Please download it from https://github.com/WasmEdge/WasmEdge/releases/ and move it to %s. After that, please rerun the script. \n\n"" ""$wasmedge_root_path/plugin/""

            exit 1
        fi

    fi

    printf ""[+] The most popular models at https://huggingface.co/second-state:\n\n""

    for i in ""${!ss_models[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
    done

    # ask for repo until index of sample repo is provided or an URL
    while [[ -z ""$repo"" ]]; do
        printf ""\n    Or choose one from: https://huggingface.co/models?sort=trending&search=gguf\n\n""

        read -p ""[+] Please select a number from the list above or enter an URL: "" repo

        # check if the input is a number
        if [[ ""$repo"" =~ ^[0-9]+$ ]]; then
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        elif [[ ""$repo"" =~ ^https?:// ]]; then
            repo=""$repo""
        else
            printf ""[-] Invalid repo URL: %s\n"" ""$repo""
            repo=""""
        fi
    done


    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    if [ -n ""$ss_model"" ]; then
        ss_url=$repo
        repo=${repo%/resolve/main/*}

        # check file if the model has been downloaded before
        wfile=$(basename ""$ss_url"")
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$ss_url""
        fi

    else

        printf ""[+] Checking for GGUF model files in %s\n\n"" ""$repo""

        # find GGUF files in the source
        model_tree=""${repo%/}/tree/main""
        model_files=$(curl -s ""$model_tree"" | grep -i ""\\.gguf</span>"" | sed -E 's/.*<span class=""truncate group-hover:underline"">(.*)<\/span><\/a>/\1/g')
        # Convert model_files into an array
        model_files_array=($model_files)

        while IFS= read -r line; do
            sizes+=(""$line"")
        done < <(curl -s ""$model_tree"" | awk -F 'download=true"">' '/download=true"">[0-9\.]+ (GB|MB)/ {print $2}' | awk '{print $1, $2}')

        # list all files in the provided git repo
        length=${#model_files_array[@]}
        for ((i=0; i<$length; i++)); do
            file=${model_files_array[i]}
            size=${sizes[i]}
            iw=-1
            is=0
            for wt in ""${wtypes[@]}""; do
                # uppercase
                ufile=$(echo ""$file"" | tr '[:lower:]' '[:upper:]')
                if [[ ""$ufile"" =~ ""$wt"" ]]; then
                    iw=$is
                    break
                fi
                is=$((is+1))
            done

            if [[ $iw -eq -1 ]]; then
                continue
            fi

            wfiles[$iw]=""$file""

            have="" ""
            if [[ -f ""$file"" ]]; then
                have=""*""
            fi

            printf ""    %2d) %s %7s   %s\n"" $iw ""$have"" ""$size"" ""$file""
        done

        # ask for weights type until provided and available
        while [[ -z ""$wtype"" ]]; do
            printf ""\n""
            read -p ""[+] Please select a number from the list above: "" wtype
            wfile=""${wfiles[$wtype]}""

            if [[ -z ""$wfile"" ]]; then
                printf ""[-] Invalid number: %s\n"" ""$wtype""
                wtype=""""
            fi
        done

        url=""${repo%/}/resolve/main/$wfile""

        # check file if the model has been downloaded before
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$url""
        fi

    fi

    # * prompt type and reverse prompt

    if [[ $repo =~ ^https://huggingface\.co/second-state ]]; then
        readme_url=""$repo/resolve/main/README.md""

        # Download the README.md file
        curl -s $readme_url -o README.md

        # Extract the ""Prompt type: xxxx"" line
        prompt_type_line=$(grep -i ""Prompt type:"" README.md)

        # Extract the xxxx part
        prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

        # Check if ""Reverse prompt"" exists
        if grep -q ""Reverse prompt:"" README.md; then
            # Extract the ""Reverse prompt: xxxx"" line
            reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

            # Extract the xxxx part
            reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

            printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
        else
            printf ""[+] No reverse prompt required\n""
        fi

        # Clean up
        rm README.md
    else
        printf ""[+] Please select a number from the list below:\n""
        printf ""    The definitions of the prompt types below can be found at https://github.com/LlamaEdge/LlamaEdge/raw/main/api-server/chat-prompts/README.md\n\n""

        is=0
        for r in ""${prompt_types[@]}""; do
            printf ""    %2d) %s\n"" $is ""$r""
            is=$((is+1))
        done
        printf ""\n""

        prompt_type_index=-1
        while ((prompt_type_index < 0 || prompt_type_index >= ${#prompt_types[@]})); do
            read -p ""[+] Select prompt type: "" prompt_type_index
            # Check if the input is a number
            if ! [[ ""$prompt_type_index"" =~ ^[0-9]+$ ]]; then
                echo ""Invalid input. Please enter a number.""
                prompt_type_index=-1
            fi
        done
        prompt_type=""${prompt_types[$prompt_type_index]}""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $need_reverse_prompt =~ ^[yYnN]$ ]]; do
            read -p ""[+] Need reverse prompt? (y/n): "" need_reverse_prompt
        done

        # If user answered yes, ask them to input a string
        if [[ ""$need_reverse_prompt"" == ""y"" || ""$need_reverse_prompt"" == ""Y"" ]]; then
            read -p ""    Enter the reverse prompt: "" reverse_prompt
            printf ""\n""
        fi
    fi

    # * running mode

    printf ""[+] Running mode: \n\n""

    running_modes=(""API Server with Chatbot web app"" ""CLI Chat"")

    for i in ""${!running_modes[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${running_modes[$i]}""
    done

    while [[ -z ""$running_mode_index"" ]]; do
        printf ""\n""
        read -p ""[+] Select a number from the list above: "" running_mode_index
        running_mode=""${running_modes[$running_mode_index - 1]}""

        if [[ -z ""$running_mode"" ]]; then
            printf ""[-] Invalid number: %s\n"" ""$running_mode_index""
            running_mode_index=""""
        fi
    done
    printf ""[+] Selected running mode: %s (%s)\n"" ""$running_mode_index"" ""$running_mode""

    # * download llama-api-server.wasm or llama-chat.wasm

    repo=""second-state/LlamaEdge""
    releases=$(curl -s ""https://api.github.com/repos/$repo/releases"")
    if [[ ""$running_mode_index"" == ""1"" ]]; then

        # * Download llama-api-server.wasm

        if [ -f ""llama-api-server.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-api-server.wasm. Download the latest llama-api-server.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-api-server.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

            printf ""\n""
        fi

        # * chatbot-ui

        if [ -d ""chatbot-ui"" ]; then
            printf ""[+] Using cached Chatbot web app\n""
        else
            printf ""[+] Downloading Chatbot web app ...\n""
            files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
            curl -LO $files_tarball
            if [ $? -ne 0 ]; then
                printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
                exit 1
            fi
            tar xzf chatbot-ui.tar.gz
            rm chatbot-ui.tar.gz
            printf ""\n""
        fi

        model_name=${wfile%-Q*}

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start the server:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_server =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start the server? (y/n): "" start_server
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_server"" == ""y"" || ""$start_server"" == ""Y"" ]]; then
            printf ""\n""
            printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
            printf ""*********************************** LlamaEdge API Server ********************************\n\n""
            eval $cmd

        fi

    elif [[ ""$running_mode_index"" == ""2"" ]]; then

        # * Download llama-chat.wasm

        if [ -f ""llama-chat.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-chat.wasm. Download the latest llama-chat.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-chat.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-chat.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-chat.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

            printf ""\n""
        fi

        # * prepare the command

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-chat.wasm --prompt-template $prompt_type""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start CLI Chat:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_chat =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start CLI Chat? (y/n): "" start_chat
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_chat"" == ""y"" || ""$start_chat"" == ""Y"" ]]; then
            printf ""\n""

            # Execute the command
            printf ""********************* LlamaEdge *********************\n\n""
            eval $cmd

        fi

    else
        printf ""[-] Invalid running mode: %s\n"" ""$running_mode_index""
        exit 1
    fi

else
    echo ""Invalid value for interactive""
fi

exit 0

```","Q: 2. What are the available weight types for the model?
A: According to the script, there are 12 available weight types for the model. These are:

1. Q2_K
2. Q3_K_L
3. Q3_K_M
4. Q3_K_S
5. Q4_0
6. Q4_K_M
7. Q4_K_S
8. Q5_0
9. Q5_K_M
10. Q5_K_S
11. Q6_K
12. Q8_0

These weight types are defined in the `wtypes` array at the top of the script.

Bonus Tip: You can select one of these weight types by entering its corresponding number (e.g., ""1"" for Q2_K)."
"```bash:run-llm.sh
#!/bin/bash
#
# Helper script for deploying LlamaEdge API Server with a single Bash command
#
# - Works on Linux and macOS
# - Supports: CPU, CUDA, Metal, OpenCL
# - Can run GGUF models from https://huggingface.co/second-state/
#

set -e

# required utils: curl, git, make
if ! command -v curl &> /dev/null; then
    printf ""[-] curl not found\n""
    exit 1
fi
if ! command -v git &> /dev/null; then
    printf ""[-] git not found\n""
    exit 1
fi
if ! command -v make &> /dev/null; then
    printf ""[-] make not found\n""
    exit 1
fi

# parse arguments
port=8080
repo=""""
wtype=""""
backend=""cpu""
ctx_size=512
n_predict=1024
n_gpu_layers=100

# if macOS, use metal backend by default
if [[ ""$OSTYPE"" == ""darwin""* ]]; then
    backend=""metal""
elif command -v nvcc &> /dev/null; then
    backend=""cuda""
fi

gpu_id=0
n_parallel=8
n_kv=4096
verbose=0
log_prompts=0
log_stat=0
# 0: server mode
# 1: local mode
# mode=0
# 0: non-interactive
# 1: interactive
interactive=0
model=""""
# ggml version: latest or bxxxx
ggml_version=""latest""

function print_usage {
    printf ""Usage:\n""
    printf ""  ./run-llm.sh [--port]\n\n""
    printf ""  --model:        model name\n""
    printf ""  --interactive:  run in interactive mode\n""
    printf ""  --port:         port number, default is 8080\n""
    printf ""  --ggml-version: ggml version (for example, b2963). If the option is not used, then install the latest version.\n""
    printf ""Example:\n\n""
    printf '  bash <(curl -sSfL 'https://code.flows.network/webhook/iwYN1SdN3AmPgR5ao5Gt/run-llm.sh')""\n\n'
}

while [[ $# -gt 0 ]]; do
    key=""$1""
    case $key in
        --model)
            model=""$2""
            shift
            shift
            ;;
        --interactive)
            interactive=1
            shift
            ;;
        --port)
            port=""$2""
            shift
            shift
            ;;
        --ggml-version)
            ggml_version=""$2""
            shift
            shift
            ;;
        --help)
            print_usage
            exit 0
            ;;
        *)
            echo ""Unknown argument: $key""
            print_usage
            exit 1
            ;;
    esac
done

# available weights types
wtypes=(""Q2_K"" ""Q3_K_L"" ""Q3_K_M"" ""Q3_K_S"" ""Q4_0"" ""Q4_K_M"" ""Q4_K_S"" ""Q5_0"" ""Q5_K_M"" ""Q5_K_S"" ""Q6_K"" ""Q8_0"")

wfiles=()
for wt in ""${wtypes[@]}""; do
    wfiles+=("""")
done

ss_urls=(
    ""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-1.5-9B-Chat-GGUF/resolve/main/Yi-1.5-9B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/stablelm-2-zephyr-1.6b-GGUF/resolve/main/stablelm-2-zephyr-1_6b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/OpenChat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34B-Chat-GGUF/resolve/main/Yi-34B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34Bx2-MoE-60B-GGUF/resolve/main/Yi-34Bx2-MoE-60B-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-LLM-7B-Chat-GGUF/resolve/main/deepseek-llm-7b-chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-Coder-6.7B-Instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/Mistral-7B-Instruct-v0.2-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Orca-2-13B-GGUF/resolve/main/Orca-2-13b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/SOLAR-10.7B-Instruct-v1.0-GGUF/resolve/main/SOLAR-10.7B-Instruct-v1.0-Q5_K_M.gguf""
)

# sample models
ss_models=(
    ""gemma-2-9b-it""
    ""yi-1.5-9b-chat""
    ""phi-3-mini-4k""
    ""llama-3-8b-instruct""
    ""llama-2-7b-chat""
    ""stablelm-2-zephyr-1.6b""
    ""openchat-3.5-0106""
    ""yi-34b-chat""
    ""yi-34bx2-moe-60b""
    ""deepseek-llm-7b-chat""
    ""deepseek-coder-6.7b-instruct""
    ""mistral-7b-instruct-v0.2""
    ""dolphin-2.6-mistral-7b""
    ""orca-2-13b""
    ""tinyllama-1.1b-chat-v1.0""
    ""solar-10.7b-instruct-v1.0""
)

# prompt types
prompt_types=(
    ""gemma-instruct""
    ""chatml""
    ""phi-3-chat""
    ""llama-3-chat""
    ""llama-2-chat""
    ""chatml""
    ""openchat""
    ""zephyr""
    ""codellama-instruct""
    ""mistral-instruct""
    ""mistrallite""
    ""vicuna-chat""
    ""vicuna-1.1-chat""
    ""wizard-coder""
    ""intel-neural""
    ""deepseek-chat""
    ""deepseek-coder""
    ""solar-instruct""
    ""belle-llama-2-chat""
    ""human-assistant""
)


if [ -n ""$model"" ]; then
    printf ""\n""

    # Check if the model is in the list of supported models
    if [[ ! "" ${ss_models[@]} "" =~ "" ${model} "" ]]; then

        printf ""[+] ${model} is an invalid name or a unsupported model. Please check the model list:\n\n""

        for i in ""${!ss_models[@]}""; do
            printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
        done
        printf ""\n""

        # ask for repo until index of sample repo is provided or an URL
        while [[ -z ""$repo"" ]]; do

            read -p ""[+] Please select a number from the list above: "" repo

            # check if the input is a number
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        done
    else
        # Find the index of the model in the list of supported models
        for i in ""${!ss_models[@]}""; do
            if [[ ""${ss_models[$i]}"" = ""${model}"" ]]; then
                ss_model=""${ss_models[$i]}""
                repo=""${ss_urls[$i]}""

                break
            fi
        done

    fi

    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    ss_url=$repo

    repo=${repo%/resolve/main/*}

    # check file if the model has been downloaded before
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * prompt type and reverse prompt

    readme_url=""$repo/resolve/main/README.md""

    # Download the README.md file
    curl -s $readme_url -o README.md

    # Extract the ""Prompt type: xxxx"" line
    prompt_type_line=$(grep -i ""Prompt type:"" README.md)

    # Extract the xxxx part
    prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

    printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

    # Check if ""Reverse prompt"" exists
    if grep -q ""Reverse prompt:"" README.md; then
        # Extract the ""Reverse prompt: xxxx"" line
        reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

        # Extract the xxxx part
        reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
    else
        printf ""[+] No reverse prompt required\n""
    fi

    # Clean up
    rm README.md

    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Install WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    model_name=${wfile%-Q*}

    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

    # Add reverse prompt if it exists
    if [ -n ""$reverse_prompt"" ]; then
        cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
    fi

    printf ""\n""
    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    printf ""********************* [LOG: MODEL INFO (Load Model & Init Execution Context)] *********************""
    eval $cmd

elif [ ""$interactive"" -eq 0 ]; then

    printf ""\n""
    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Installing WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download gemma-2-9b-it-Q5_K_M.gguf
    ss_url=""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading %s ...\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    # * start llama-api-server
    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:gemma-2-9b-it-Q5_K_M.gguf llama-api-server.wasm -p gemma-instruct -c 4096 --model-name gemma-2-9b-it --socket-addr 0.0.0.0:${port}""

    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    eval $cmd

elif [ ""$interactive"" -eq 1 ]; then

    printf ""\n""
    printf ""[I] This is a helper script for deploying LlamaEdge API Server on this machine.\n\n""
    printf ""    The following tasks will be done:\n""
    printf ""    - Download GGUF model\n""
    printf ""    - Install WasmEdge Runtime and the wasi-nn_ggml plugin\n""
    printf ""    - Download LlamaEdge API Server\n""
    printf ""\n""
    printf ""    Upon the tasks done, an HTTP server will be started and it will serve the selected\n""
    printf ""    model.\n""
    printf ""\n""
    printf ""    Please note:\n""
    printf ""\n""
    printf ""    - All downloaded files will be stored in the current folder\n""
    printf ""    - The server will be listening on all network interfaces\n""
    printf ""    - The server will run with default settings which are not always optimal\n""
    printf ""    - Do not judge the quality of a model based on the results from this script\n""
    printf ""    - This script is only for demonstration purposes\n""
    printf ""\n""
    printf ""    During the whole process, you can press Ctrl-C to abort the current process at any time.\n""
    printf ""\n""
    printf ""    Press Enter to continue ...\n\n""

    read

    # * install WasmEdge + wasi-nn_ggml plugin

    printf ""[+] Installing WasmEdge ...\n\n""

    # Check if WasmEdge has been installed
    reinstall_wasmedge=1
    if command -v wasmedge &> /dev/null
    then
        printf ""    1) Install the latest version of WasmEdge and wasi-nn_ggml plugin (recommended)\n""
        printf ""    2) Keep the current version\n\n""
        read -p ""[+] Select a number from the list above: "" reinstall_wasmedge
    fi

    while [[ ""$reinstall_wasmedge"" -ne 1 && ""$reinstall_wasmedge"" -ne 2 ]]; do
        printf ""    Invalid number. Please enter number 1 or 2\n""
        read reinstall_wasmedge
    done

    if [[ ""$reinstall_wasmedge"" == ""1"" ]]; then
        # install WasmEdge + wasi-nn_ggml plugin
        if [ ""$ggml_version"" = ""latest"" ]; then
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        else
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        fi


    elif [[ ""$reinstall_wasmedge"" == ""2"" ]]; then
        wasmedge_path=$(which wasmedge)
        wasmedge_root_path=${wasmedge_path%""/bin/wasmedge""}

        found=0
        for file in ""$wasmedge_root_path/plugin/libwasmedgePluginWasiNN.""*; do
        if [[ -f $file ]]; then
            found=1
            break
        fi
        done

        if [[ $found -eq 0 ]]; then
            printf ""\n    * Not found wasi-nn_ggml plugin. Please download it from https://github.com/WasmEdge/WasmEdge/releases/ and move it to %s. After that, please rerun the script. \n\n"" ""$wasmedge_root_path/plugin/""

            exit 1
        fi

    fi

    printf ""[+] The most popular models at https://huggingface.co/second-state:\n\n""

    for i in ""${!ss_models[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
    done

    # ask for repo until index of sample repo is provided or an URL
    while [[ -z ""$repo"" ]]; do
        printf ""\n    Or choose one from: https://huggingface.co/models?sort=trending&search=gguf\n\n""

        read -p ""[+] Please select a number from the list above or enter an URL: "" repo

        # check if the input is a number
        if [[ ""$repo"" =~ ^[0-9]+$ ]]; then
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        elif [[ ""$repo"" =~ ^https?:// ]]; then
            repo=""$repo""
        else
            printf ""[-] Invalid repo URL: %s\n"" ""$repo""
            repo=""""
        fi
    done


    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    if [ -n ""$ss_model"" ]; then
        ss_url=$repo
        repo=${repo%/resolve/main/*}

        # check file if the model has been downloaded before
        wfile=$(basename ""$ss_url"")
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$ss_url""
        fi

    else

        printf ""[+] Checking for GGUF model files in %s\n\n"" ""$repo""

        # find GGUF files in the source
        model_tree=""${repo%/}/tree/main""
        model_files=$(curl -s ""$model_tree"" | grep -i ""\\.gguf</span>"" | sed -E 's/.*<span class=""truncate group-hover:underline"">(.*)<\/span><\/a>/\1/g')
        # Convert model_files into an array
        model_files_array=($model_files)

        while IFS= read -r line; do
            sizes+=(""$line"")
        done < <(curl -s ""$model_tree"" | awk -F 'download=true"">' '/download=true"">[0-9\.]+ (GB|MB)/ {print $2}' | awk '{print $1, $2}')

        # list all files in the provided git repo
        length=${#model_files_array[@]}
        for ((i=0; i<$length; i++)); do
            file=${model_files_array[i]}
            size=${sizes[i]}
            iw=-1
            is=0
            for wt in ""${wtypes[@]}""; do
                # uppercase
                ufile=$(echo ""$file"" | tr '[:lower:]' '[:upper:]')
                if [[ ""$ufile"" =~ ""$wt"" ]]; then
                    iw=$is
                    break
                fi
                is=$((is+1))
            done

            if [[ $iw -eq -1 ]]; then
                continue
            fi

            wfiles[$iw]=""$file""

            have="" ""
            if [[ -f ""$file"" ]]; then
                have=""*""
            fi

            printf ""    %2d) %s %7s   %s\n"" $iw ""$have"" ""$size"" ""$file""
        done

        # ask for weights type until provided and available
        while [[ -z ""$wtype"" ]]; do
            printf ""\n""
            read -p ""[+] Please select a number from the list above: "" wtype
            wfile=""${wfiles[$wtype]}""

            if [[ -z ""$wfile"" ]]; then
                printf ""[-] Invalid number: %s\n"" ""$wtype""
                wtype=""""
            fi
        done

        url=""${repo%/}/resolve/main/$wfile""

        # check file if the model has been downloaded before
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$url""
        fi

    fi

    # * prompt type and reverse prompt

    if [[ $repo =~ ^https://huggingface\.co/second-state ]]; then
        readme_url=""$repo/resolve/main/README.md""

        # Download the README.md file
        curl -s $readme_url -o README.md

        # Extract the ""Prompt type: xxxx"" line
        prompt_type_line=$(grep -i ""Prompt type:"" README.md)

        # Extract the xxxx part
        prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

        # Check if ""Reverse prompt"" exists
        if grep -q ""Reverse prompt:"" README.md; then
            # Extract the ""Reverse prompt: xxxx"" line
            reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

            # Extract the xxxx part
            reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

            printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
        else
            printf ""[+] No reverse prompt required\n""
        fi

        # Clean up
        rm README.md
    else
        printf ""[+] Please select a number from the list below:\n""
        printf ""    The definitions of the prompt types below can be found at https://github.com/LlamaEdge/LlamaEdge/raw/main/api-server/chat-prompts/README.md\n\n""

        is=0
        for r in ""${prompt_types[@]}""; do
            printf ""    %2d) %s\n"" $is ""$r""
            is=$((is+1))
        done
        printf ""\n""

        prompt_type_index=-1
        while ((prompt_type_index < 0 || prompt_type_index >= ${#prompt_types[@]})); do
            read -p ""[+] Select prompt type: "" prompt_type_index
            # Check if the input is a number
            if ! [[ ""$prompt_type_index"" =~ ^[0-9]+$ ]]; then
                echo ""Invalid input. Please enter a number.""
                prompt_type_index=-1
            fi
        done
        prompt_type=""${prompt_types[$prompt_type_index]}""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $need_reverse_prompt =~ ^[yYnN]$ ]]; do
            read -p ""[+] Need reverse prompt? (y/n): "" need_reverse_prompt
        done

        # If user answered yes, ask them to input a string
        if [[ ""$need_reverse_prompt"" == ""y"" || ""$need_reverse_prompt"" == ""Y"" ]]; then
            read -p ""    Enter the reverse prompt: "" reverse_prompt
            printf ""\n""
        fi
    fi

    # * running mode

    printf ""[+] Running mode: \n\n""

    running_modes=(""API Server with Chatbot web app"" ""CLI Chat"")

    for i in ""${!running_modes[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${running_modes[$i]}""
    done

    while [[ -z ""$running_mode_index"" ]]; do
        printf ""\n""
        read -p ""[+] Select a number from the list above: "" running_mode_index
        running_mode=""${running_modes[$running_mode_index - 1]}""

        if [[ -z ""$running_mode"" ]]; then
            printf ""[-] Invalid number: %s\n"" ""$running_mode_index""
            running_mode_index=""""
        fi
    done
    printf ""[+] Selected running mode: %s (%s)\n"" ""$running_mode_index"" ""$running_mode""

    # * download llama-api-server.wasm or llama-chat.wasm

    repo=""second-state/LlamaEdge""
    releases=$(curl -s ""https://api.github.com/repos/$repo/releases"")
    if [[ ""$running_mode_index"" == ""1"" ]]; then

        # * Download llama-api-server.wasm

        if [ -f ""llama-api-server.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-api-server.wasm. Download the latest llama-api-server.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-api-server.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

            printf ""\n""
        fi

        # * chatbot-ui

        if [ -d ""chatbot-ui"" ]; then
            printf ""[+] Using cached Chatbot web app\n""
        else
            printf ""[+] Downloading Chatbot web app ...\n""
            files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
            curl -LO $files_tarball
            if [ $? -ne 0 ]; then
                printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
                exit 1
            fi
            tar xzf chatbot-ui.tar.gz
            rm chatbot-ui.tar.gz
            printf ""\n""
        fi

        model_name=${wfile%-Q*}

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start the server:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_server =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start the server? (y/n): "" start_server
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_server"" == ""y"" || ""$start_server"" == ""Y"" ]]; then
            printf ""\n""
            printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
            printf ""*********************************** LlamaEdge API Server ********************************\n\n""
            eval $cmd

        fi

    elif [[ ""$running_mode_index"" == ""2"" ]]; then

        # * Download llama-chat.wasm

        if [ -f ""llama-chat.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-chat.wasm. Download the latest llama-chat.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-chat.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-chat.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-chat.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

            printf ""\n""
        fi

        # * prepare the command

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-chat.wasm --prompt-template $prompt_type""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start CLI Chat:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_chat =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start CLI Chat? (y/n): "" start_chat
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_chat"" == ""y"" || ""$start_chat"" == ""Y"" ]]; then
            printf ""\n""

            # Execute the command
            printf ""********************* LlamaEdge *********************\n\n""
            eval $cmd

        fi

    else
        printf ""[-] Invalid running mode: %s\n"" ""$running_mode_index""
        exit 1
    fi

else
    echo ""Invalid value for interactive""
fi

exit 0

```","Q: 3. How does the script handle the selection of a model from Hugging Face's Model Hub?
A: The script uses the `curl` command to fetch a list of models from Hugging Face's Model Hub. The URL for fetching the model list is hardcoded in the script as `""https://huggingface.co/models?sort=trending&search=gguf""`, which retrieves a list of trending GGUF (GPT-like) models.

Here are the steps involved in selecting a model:

1.  **Fetching Model List**: The script uses `curl` to fetch the list of models from Hugging Face's Model Hub.
2.  **Parsing Model Information**: The fetched HTML response is parsed using regular expressions (`grep`, `awk`) to extract relevant information about each model, such as its name and size.
3.  **Displaying Model List**: The script displays a numbered list of available models along with their names and sizes.
4.  **Selecting a Model**: The user is prompted to select a model by entering the corresponding number from the displayed list.
5.  **Validating Input**: The input provided by the user is validated to ensure it's a valid number that corresponds to an existing model.

Once a model is selected, the script proceeds with downloading the weights file for the chosen model using its URL (`""https://huggingface.co/second-state/${model_name}/resolve/main/${wfile}""`).

Note: This process assumes that the user has access to the internet and can successfully fetch data from Hugging Face's Model Hub.

**Example**:
Suppose there are two models available in the list:

```
1)  MyModel1 (100M)
2)  MyModel2 (500M)
```

The user would enter `1` to select ""MyModel1"". The script would then proceed with downloading the weights file for this model."
"```bash:run-llm.sh
#!/bin/bash
#
# Helper script for deploying LlamaEdge API Server with a single Bash command
#
# - Works on Linux and macOS
# - Supports: CPU, CUDA, Metal, OpenCL
# - Can run GGUF models from https://huggingface.co/second-state/
#

set -e

# required utils: curl, git, make
if ! command -v curl &> /dev/null; then
    printf ""[-] curl not found\n""
    exit 1
fi
if ! command -v git &> /dev/null; then
    printf ""[-] git not found\n""
    exit 1
fi
if ! command -v make &> /dev/null; then
    printf ""[-] make not found\n""
    exit 1
fi

# parse arguments
port=8080
repo=""""
wtype=""""
backend=""cpu""
ctx_size=512
n_predict=1024
n_gpu_layers=100

# if macOS, use metal backend by default
if [[ ""$OSTYPE"" == ""darwin""* ]]; then
    backend=""metal""
elif command -v nvcc &> /dev/null; then
    backend=""cuda""
fi

gpu_id=0
n_parallel=8
n_kv=4096
verbose=0
log_prompts=0
log_stat=0
# 0: server mode
# 1: local mode
# mode=0
# 0: non-interactive
# 1: interactive
interactive=0
model=""""
# ggml version: latest or bxxxx
ggml_version=""latest""

function print_usage {
    printf ""Usage:\n""
    printf ""  ./run-llm.sh [--port]\n\n""
    printf ""  --model:        model name\n""
    printf ""  --interactive:  run in interactive mode\n""
    printf ""  --port:         port number, default is 8080\n""
    printf ""  --ggml-version: ggml version (for example, b2963). If the option is not used, then install the latest version.\n""
    printf ""Example:\n\n""
    printf '  bash <(curl -sSfL 'https://code.flows.network/webhook/iwYN1SdN3AmPgR5ao5Gt/run-llm.sh')""\n\n'
}

while [[ $# -gt 0 ]]; do
    key=""$1""
    case $key in
        --model)
            model=""$2""
            shift
            shift
            ;;
        --interactive)
            interactive=1
            shift
            ;;
        --port)
            port=""$2""
            shift
            shift
            ;;
        --ggml-version)
            ggml_version=""$2""
            shift
            shift
            ;;
        --help)
            print_usage
            exit 0
            ;;
        *)
            echo ""Unknown argument: $key""
            print_usage
            exit 1
            ;;
    esac
done

# available weights types
wtypes=(""Q2_K"" ""Q3_K_L"" ""Q3_K_M"" ""Q3_K_S"" ""Q4_0"" ""Q4_K_M"" ""Q4_K_S"" ""Q5_0"" ""Q5_K_M"" ""Q5_K_S"" ""Q6_K"" ""Q8_0"")

wfiles=()
for wt in ""${wtypes[@]}""; do
    wfiles+=("""")
done

ss_urls=(
    ""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-1.5-9B-Chat-GGUF/resolve/main/Yi-1.5-9B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/stablelm-2-zephyr-1.6b-GGUF/resolve/main/stablelm-2-zephyr-1_6b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/OpenChat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34B-Chat-GGUF/resolve/main/Yi-34B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34Bx2-MoE-60B-GGUF/resolve/main/Yi-34Bx2-MoE-60B-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-LLM-7B-Chat-GGUF/resolve/main/deepseek-llm-7b-chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-Coder-6.7B-Instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/Mistral-7B-Instruct-v0.2-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Orca-2-13B-GGUF/resolve/main/Orca-2-13b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/SOLAR-10.7B-Instruct-v1.0-GGUF/resolve/main/SOLAR-10.7B-Instruct-v1.0-Q5_K_M.gguf""
)

# sample models
ss_models=(
    ""gemma-2-9b-it""
    ""yi-1.5-9b-chat""
    ""phi-3-mini-4k""
    ""llama-3-8b-instruct""
    ""llama-2-7b-chat""
    ""stablelm-2-zephyr-1.6b""
    ""openchat-3.5-0106""
    ""yi-34b-chat""
    ""yi-34bx2-moe-60b""
    ""deepseek-llm-7b-chat""
    ""deepseek-coder-6.7b-instruct""
    ""mistral-7b-instruct-v0.2""
    ""dolphin-2.6-mistral-7b""
    ""orca-2-13b""
    ""tinyllama-1.1b-chat-v1.0""
    ""solar-10.7b-instruct-v1.0""
)

# prompt types
prompt_types=(
    ""gemma-instruct""
    ""chatml""
    ""phi-3-chat""
    ""llama-3-chat""
    ""llama-2-chat""
    ""chatml""
    ""openchat""
    ""zephyr""
    ""codellama-instruct""
    ""mistral-instruct""
    ""mistrallite""
    ""vicuna-chat""
    ""vicuna-1.1-chat""
    ""wizard-coder""
    ""intel-neural""
    ""deepseek-chat""
    ""deepseek-coder""
    ""solar-instruct""
    ""belle-llama-2-chat""
    ""human-assistant""
)


if [ -n ""$model"" ]; then
    printf ""\n""

    # Check if the model is in the list of supported models
    if [[ ! "" ${ss_models[@]} "" =~ "" ${model} "" ]]; then

        printf ""[+] ${model} is an invalid name or a unsupported model. Please check the model list:\n\n""

        for i in ""${!ss_models[@]}""; do
            printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
        done
        printf ""\n""

        # ask for repo until index of sample repo is provided or an URL
        while [[ -z ""$repo"" ]]; do

            read -p ""[+] Please select a number from the list above: "" repo

            # check if the input is a number
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        done
    else
        # Find the index of the model in the list of supported models
        for i in ""${!ss_models[@]}""; do
            if [[ ""${ss_models[$i]}"" = ""${model}"" ]]; then
                ss_model=""${ss_models[$i]}""
                repo=""${ss_urls[$i]}""

                break
            fi
        done

    fi

    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    ss_url=$repo

    repo=${repo%/resolve/main/*}

    # check file if the model has been downloaded before
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * prompt type and reverse prompt

    readme_url=""$repo/resolve/main/README.md""

    # Download the README.md file
    curl -s $readme_url -o README.md

    # Extract the ""Prompt type: xxxx"" line
    prompt_type_line=$(grep -i ""Prompt type:"" README.md)

    # Extract the xxxx part
    prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

    printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

    # Check if ""Reverse prompt"" exists
    if grep -q ""Reverse prompt:"" README.md; then
        # Extract the ""Reverse prompt: xxxx"" line
        reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

        # Extract the xxxx part
        reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
    else
        printf ""[+] No reverse prompt required\n""
    fi

    # Clean up
    rm README.md

    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Install WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    model_name=${wfile%-Q*}

    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

    # Add reverse prompt if it exists
    if [ -n ""$reverse_prompt"" ]; then
        cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
    fi

    printf ""\n""
    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    printf ""********************* [LOG: MODEL INFO (Load Model & Init Execution Context)] *********************""
    eval $cmd

elif [ ""$interactive"" -eq 0 ]; then

    printf ""\n""
    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Installing WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download gemma-2-9b-it-Q5_K_M.gguf
    ss_url=""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading %s ...\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    # * start llama-api-server
    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:gemma-2-9b-it-Q5_K_M.gguf llama-api-server.wasm -p gemma-instruct -c 4096 --model-name gemma-2-9b-it --socket-addr 0.0.0.0:${port}""

    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    eval $cmd

elif [ ""$interactive"" -eq 1 ]; then

    printf ""\n""
    printf ""[I] This is a helper script for deploying LlamaEdge API Server on this machine.\n\n""
    printf ""    The following tasks will be done:\n""
    printf ""    - Download GGUF model\n""
    printf ""    - Install WasmEdge Runtime and the wasi-nn_ggml plugin\n""
    printf ""    - Download LlamaEdge API Server\n""
    printf ""\n""
    printf ""    Upon the tasks done, an HTTP server will be started and it will serve the selected\n""
    printf ""    model.\n""
    printf ""\n""
    printf ""    Please note:\n""
    printf ""\n""
    printf ""    - All downloaded files will be stored in the current folder\n""
    printf ""    - The server will be listening on all network interfaces\n""
    printf ""    - The server will run with default settings which are not always optimal\n""
    printf ""    - Do not judge the quality of a model based on the results from this script\n""
    printf ""    - This script is only for demonstration purposes\n""
    printf ""\n""
    printf ""    During the whole process, you can press Ctrl-C to abort the current process at any time.\n""
    printf ""\n""
    printf ""    Press Enter to continue ...\n\n""

    read

    # * install WasmEdge + wasi-nn_ggml plugin

    printf ""[+] Installing WasmEdge ...\n\n""

    # Check if WasmEdge has been installed
    reinstall_wasmedge=1
    if command -v wasmedge &> /dev/null
    then
        printf ""    1) Install the latest version of WasmEdge and wasi-nn_ggml plugin (recommended)\n""
        printf ""    2) Keep the current version\n\n""
        read -p ""[+] Select a number from the list above: "" reinstall_wasmedge
    fi

    while [[ ""$reinstall_wasmedge"" -ne 1 && ""$reinstall_wasmedge"" -ne 2 ]]; do
        printf ""    Invalid number. Please enter number 1 or 2\n""
        read reinstall_wasmedge
    done

    if [[ ""$reinstall_wasmedge"" == ""1"" ]]; then
        # install WasmEdge + wasi-nn_ggml plugin
        if [ ""$ggml_version"" = ""latest"" ]; then
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        else
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        fi


    elif [[ ""$reinstall_wasmedge"" == ""2"" ]]; then
        wasmedge_path=$(which wasmedge)
        wasmedge_root_path=${wasmedge_path%""/bin/wasmedge""}

        found=0
        for file in ""$wasmedge_root_path/plugin/libwasmedgePluginWasiNN.""*; do
        if [[ -f $file ]]; then
            found=1
            break
        fi
        done

        if [[ $found -eq 0 ]]; then
            printf ""\n    * Not found wasi-nn_ggml plugin. Please download it from https://github.com/WasmEdge/WasmEdge/releases/ and move it to %s. After that, please rerun the script. \n\n"" ""$wasmedge_root_path/plugin/""

            exit 1
        fi

    fi

    printf ""[+] The most popular models at https://huggingface.co/second-state:\n\n""

    for i in ""${!ss_models[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
    done

    # ask for repo until index of sample repo is provided or an URL
    while [[ -z ""$repo"" ]]; do
        printf ""\n    Or choose one from: https://huggingface.co/models?sort=trending&search=gguf\n\n""

        read -p ""[+] Please select a number from the list above or enter an URL: "" repo

        # check if the input is a number
        if [[ ""$repo"" =~ ^[0-9]+$ ]]; then
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        elif [[ ""$repo"" =~ ^https?:// ]]; then
            repo=""$repo""
        else
            printf ""[-] Invalid repo URL: %s\n"" ""$repo""
            repo=""""
        fi
    done


    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    if [ -n ""$ss_model"" ]; then
        ss_url=$repo
        repo=${repo%/resolve/main/*}

        # check file if the model has been downloaded before
        wfile=$(basename ""$ss_url"")
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$ss_url""
        fi

    else

        printf ""[+] Checking for GGUF model files in %s\n\n"" ""$repo""

        # find GGUF files in the source
        model_tree=""${repo%/}/tree/main""
        model_files=$(curl -s ""$model_tree"" | grep -i ""\\.gguf</span>"" | sed -E 's/.*<span class=""truncate group-hover:underline"">(.*)<\/span><\/a>/\1/g')
        # Convert model_files into an array
        model_files_array=($model_files)

        while IFS= read -r line; do
            sizes+=(""$line"")
        done < <(curl -s ""$model_tree"" | awk -F 'download=true"">' '/download=true"">[0-9\.]+ (GB|MB)/ {print $2}' | awk '{print $1, $2}')

        # list all files in the provided git repo
        length=${#model_files_array[@]}
        for ((i=0; i<$length; i++)); do
            file=${model_files_array[i]}
            size=${sizes[i]}
            iw=-1
            is=0
            for wt in ""${wtypes[@]}""; do
                # uppercase
                ufile=$(echo ""$file"" | tr '[:lower:]' '[:upper:]')
                if [[ ""$ufile"" =~ ""$wt"" ]]; then
                    iw=$is
                    break
                fi
                is=$((is+1))
            done

            if [[ $iw -eq -1 ]]; then
                continue
            fi

            wfiles[$iw]=""$file""

            have="" ""
            if [[ -f ""$file"" ]]; then
                have=""*""
            fi

            printf ""    %2d) %s %7s   %s\n"" $iw ""$have"" ""$size"" ""$file""
        done

        # ask for weights type until provided and available
        while [[ -z ""$wtype"" ]]; do
            printf ""\n""
            read -p ""[+] Please select a number from the list above: "" wtype
            wfile=""${wfiles[$wtype]}""

            if [[ -z ""$wfile"" ]]; then
                printf ""[-] Invalid number: %s\n"" ""$wtype""
                wtype=""""
            fi
        done

        url=""${repo%/}/resolve/main/$wfile""

        # check file if the model has been downloaded before
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$url""
        fi

    fi

    # * prompt type and reverse prompt

    if [[ $repo =~ ^https://huggingface\.co/second-state ]]; then
        readme_url=""$repo/resolve/main/README.md""

        # Download the README.md file
        curl -s $readme_url -o README.md

        # Extract the ""Prompt type: xxxx"" line
        prompt_type_line=$(grep -i ""Prompt type:"" README.md)

        # Extract the xxxx part
        prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

        # Check if ""Reverse prompt"" exists
        if grep -q ""Reverse prompt:"" README.md; then
            # Extract the ""Reverse prompt: xxxx"" line
            reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

            # Extract the xxxx part
            reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

            printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
        else
            printf ""[+] No reverse prompt required\n""
        fi

        # Clean up
        rm README.md
    else
        printf ""[+] Please select a number from the list below:\n""
        printf ""    The definitions of the prompt types below can be found at https://github.com/LlamaEdge/LlamaEdge/raw/main/api-server/chat-prompts/README.md\n\n""

        is=0
        for r in ""${prompt_types[@]}""; do
            printf ""    %2d) %s\n"" $is ""$r""
            is=$((is+1))
        done
        printf ""\n""

        prompt_type_index=-1
        while ((prompt_type_index < 0 || prompt_type_index >= ${#prompt_types[@]})); do
            read -p ""[+] Select prompt type: "" prompt_type_index
            # Check if the input is a number
            if ! [[ ""$prompt_type_index"" =~ ^[0-9]+$ ]]; then
                echo ""Invalid input. Please enter a number.""
                prompt_type_index=-1
            fi
        done
        prompt_type=""${prompt_types[$prompt_type_index]}""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $need_reverse_prompt =~ ^[yYnN]$ ]]; do
            read -p ""[+] Need reverse prompt? (y/n): "" need_reverse_prompt
        done

        # If user answered yes, ask them to input a string
        if [[ ""$need_reverse_prompt"" == ""y"" || ""$need_reverse_prompt"" == ""Y"" ]]; then
            read -p ""    Enter the reverse prompt: "" reverse_prompt
            printf ""\n""
        fi
    fi

    # * running mode

    printf ""[+] Running mode: \n\n""

    running_modes=(""API Server with Chatbot web app"" ""CLI Chat"")

    for i in ""${!running_modes[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${running_modes[$i]}""
    done

    while [[ -z ""$running_mode_index"" ]]; do
        printf ""\n""
        read -p ""[+] Select a number from the list above: "" running_mode_index
        running_mode=""${running_modes[$running_mode_index - 1]}""

        if [[ -z ""$running_mode"" ]]; then
            printf ""[-] Invalid number: %s\n"" ""$running_mode_index""
            running_mode_index=""""
        fi
    done
    printf ""[+] Selected running mode: %s (%s)\n"" ""$running_mode_index"" ""$running_mode""

    # * download llama-api-server.wasm or llama-chat.wasm

    repo=""second-state/LlamaEdge""
    releases=$(curl -s ""https://api.github.com/repos/$repo/releases"")
    if [[ ""$running_mode_index"" == ""1"" ]]; then

        # * Download llama-api-server.wasm

        if [ -f ""llama-api-server.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-api-server.wasm. Download the latest llama-api-server.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-api-server.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

            printf ""\n""
        fi

        # * chatbot-ui

        if [ -d ""chatbot-ui"" ]; then
            printf ""[+] Using cached Chatbot web app\n""
        else
            printf ""[+] Downloading Chatbot web app ...\n""
            files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
            curl -LO $files_tarball
            if [ $? -ne 0 ]; then
                printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
                exit 1
            fi
            tar xzf chatbot-ui.tar.gz
            rm chatbot-ui.tar.gz
            printf ""\n""
        fi

        model_name=${wfile%-Q*}

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start the server:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_server =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start the server? (y/n): "" start_server
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_server"" == ""y"" || ""$start_server"" == ""Y"" ]]; then
            printf ""\n""
            printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
            printf ""*********************************** LlamaEdge API Server ********************************\n\n""
            eval $cmd

        fi

    elif [[ ""$running_mode_index"" == ""2"" ]]; then

        # * Download llama-chat.wasm

        if [ -f ""llama-chat.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-chat.wasm. Download the latest llama-chat.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-chat.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-chat.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-chat.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

            printf ""\n""
        fi

        # * prepare the command

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-chat.wasm --prompt-template $prompt_type""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start CLI Chat:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_chat =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start CLI Chat? (y/n): "" start_chat
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_chat"" == ""y"" || ""$start_chat"" == ""Y"" ]]; then
            printf ""\n""

            # Execute the command
            printf ""********************* LlamaEdge *********************\n\n""
            eval $cmd

        fi

    else
        printf ""[-] Invalid running mode: %s\n"" ""$running_mode_index""
        exit 1
    fi

else
    echo ""Invalid value for interactive""
fi

exit 0

```","Q: 4. Can you explain how the prompt type and reverse prompt are handled by the script?
A: The script handles the prompt type and reverse prompt in a dynamic way. Here's how it works:

1. **Determine if a README.md file exists**: If the script is downloading models from Hugging Face, it checks if a `README.md` file exists for the model repository.
2. **Extract prompt type and reverse prompt information**: If the `README.md` file exists, the script extracts the ""Prompt type"" and ""Reverse prompt"" sections using regular expressions.
3. **Get user input for custom prompt types**: If the prompt type is not automatically extracted, the script asks the user to select a number from a list of predefined prompt types (e.g., `gemma-instruct`, `chatml`, etc.). The user's selection determines which prompt type will be used.
4. **Ask user if they need reverse prompts**: After selecting or extracting the prompt type, the script asks the user if they need to set ""reverse prompts."" If yes, it asks for the reverse prompt text.
5. **Use the extracted/selected prompt type and reverse prompt (if applicable)**: The script uses the chosen prompt type and reverse prompt (if any) when running the llama-api-server or llama-chat command.

Here's an example of how this works in practice:

Let's say you're downloading the ""gemma-2-9b-it"" model from Hugging Face. The `README.md` file for that model has a section titled ""Prompt type: gemma-instruct"". However, you want to use the ""chatml"" prompt type instead. In this case, you'll be prompted to select a number from the list of predefined prompt types, and you'll choose option 2 (chatml). The script will then use the ""chatml"" prompt type when running the llama-api-server or llama-chat command.

If you need to set reverse prompts, the script will ask for the reverse prompt text after selecting/extracting the prompt type. You can enter a string as the reverse prompt, and it will be used accordingly."
"```bash:run-llm.sh
#!/bin/bash
#
# Helper script for deploying LlamaEdge API Server with a single Bash command
#
# - Works on Linux and macOS
# - Supports: CPU, CUDA, Metal, OpenCL
# - Can run GGUF models from https://huggingface.co/second-state/
#

set -e

# required utils: curl, git, make
if ! command -v curl &> /dev/null; then
    printf ""[-] curl not found\n""
    exit 1
fi
if ! command -v git &> /dev/null; then
    printf ""[-] git not found\n""
    exit 1
fi
if ! command -v make &> /dev/null; then
    printf ""[-] make not found\n""
    exit 1
fi

# parse arguments
port=8080
repo=""""
wtype=""""
backend=""cpu""
ctx_size=512
n_predict=1024
n_gpu_layers=100

# if macOS, use metal backend by default
if [[ ""$OSTYPE"" == ""darwin""* ]]; then
    backend=""metal""
elif command -v nvcc &> /dev/null; then
    backend=""cuda""
fi

gpu_id=0
n_parallel=8
n_kv=4096
verbose=0
log_prompts=0
log_stat=0
# 0: server mode
# 1: local mode
# mode=0
# 0: non-interactive
# 1: interactive
interactive=0
model=""""
# ggml version: latest or bxxxx
ggml_version=""latest""

function print_usage {
    printf ""Usage:\n""
    printf ""  ./run-llm.sh [--port]\n\n""
    printf ""  --model:        model name\n""
    printf ""  --interactive:  run in interactive mode\n""
    printf ""  --port:         port number, default is 8080\n""
    printf ""  --ggml-version: ggml version (for example, b2963). If the option is not used, then install the latest version.\n""
    printf ""Example:\n\n""
    printf '  bash <(curl -sSfL 'https://code.flows.network/webhook/iwYN1SdN3AmPgR5ao5Gt/run-llm.sh')""\n\n'
}

while [[ $# -gt 0 ]]; do
    key=""$1""
    case $key in
        --model)
            model=""$2""
            shift
            shift
            ;;
        --interactive)
            interactive=1
            shift
            ;;
        --port)
            port=""$2""
            shift
            shift
            ;;
        --ggml-version)
            ggml_version=""$2""
            shift
            shift
            ;;
        --help)
            print_usage
            exit 0
            ;;
        *)
            echo ""Unknown argument: $key""
            print_usage
            exit 1
            ;;
    esac
done

# available weights types
wtypes=(""Q2_K"" ""Q3_K_L"" ""Q3_K_M"" ""Q3_K_S"" ""Q4_0"" ""Q4_K_M"" ""Q4_K_S"" ""Q5_0"" ""Q5_K_M"" ""Q5_K_S"" ""Q6_K"" ""Q8_0"")

wfiles=()
for wt in ""${wtypes[@]}""; do
    wfiles+=("""")
done

ss_urls=(
    ""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-1.5-9B-Chat-GGUF/resolve/main/Yi-1.5-9B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/stablelm-2-zephyr-1.6b-GGUF/resolve/main/stablelm-2-zephyr-1_6b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/OpenChat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34B-Chat-GGUF/resolve/main/Yi-34B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34Bx2-MoE-60B-GGUF/resolve/main/Yi-34Bx2-MoE-60B-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-LLM-7B-Chat-GGUF/resolve/main/deepseek-llm-7b-chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-Coder-6.7B-Instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/Mistral-7B-Instruct-v0.2-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Orca-2-13B-GGUF/resolve/main/Orca-2-13b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/SOLAR-10.7B-Instruct-v1.0-GGUF/resolve/main/SOLAR-10.7B-Instruct-v1.0-Q5_K_M.gguf""
)

# sample models
ss_models=(
    ""gemma-2-9b-it""
    ""yi-1.5-9b-chat""
    ""phi-3-mini-4k""
    ""llama-3-8b-instruct""
    ""llama-2-7b-chat""
    ""stablelm-2-zephyr-1.6b""
    ""openchat-3.5-0106""
    ""yi-34b-chat""
    ""yi-34bx2-moe-60b""
    ""deepseek-llm-7b-chat""
    ""deepseek-coder-6.7b-instruct""
    ""mistral-7b-instruct-v0.2""
    ""dolphin-2.6-mistral-7b""
    ""orca-2-13b""
    ""tinyllama-1.1b-chat-v1.0""
    ""solar-10.7b-instruct-v1.0""
)

# prompt types
prompt_types=(
    ""gemma-instruct""
    ""chatml""
    ""phi-3-chat""
    ""llama-3-chat""
    ""llama-2-chat""
    ""chatml""
    ""openchat""
    ""zephyr""
    ""codellama-instruct""
    ""mistral-instruct""
    ""mistrallite""
    ""vicuna-chat""
    ""vicuna-1.1-chat""
    ""wizard-coder""
    ""intel-neural""
    ""deepseek-chat""
    ""deepseek-coder""
    ""solar-instruct""
    ""belle-llama-2-chat""
    ""human-assistant""
)


if [ -n ""$model"" ]; then
    printf ""\n""

    # Check if the model is in the list of supported models
    if [[ ! "" ${ss_models[@]} "" =~ "" ${model} "" ]]; then

        printf ""[+] ${model} is an invalid name or a unsupported model. Please check the model list:\n\n""

        for i in ""${!ss_models[@]}""; do
            printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
        done
        printf ""\n""

        # ask for repo until index of sample repo is provided or an URL
        while [[ -z ""$repo"" ]]; do

            read -p ""[+] Please select a number from the list above: "" repo

            # check if the input is a number
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        done
    else
        # Find the index of the model in the list of supported models
        for i in ""${!ss_models[@]}""; do
            if [[ ""${ss_models[$i]}"" = ""${model}"" ]]; then
                ss_model=""${ss_models[$i]}""
                repo=""${ss_urls[$i]}""

                break
            fi
        done

    fi

    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    ss_url=$repo

    repo=${repo%/resolve/main/*}

    # check file if the model has been downloaded before
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * prompt type and reverse prompt

    readme_url=""$repo/resolve/main/README.md""

    # Download the README.md file
    curl -s $readme_url -o README.md

    # Extract the ""Prompt type: xxxx"" line
    prompt_type_line=$(grep -i ""Prompt type:"" README.md)

    # Extract the xxxx part
    prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

    printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

    # Check if ""Reverse prompt"" exists
    if grep -q ""Reverse prompt:"" README.md; then
        # Extract the ""Reverse prompt: xxxx"" line
        reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

        # Extract the xxxx part
        reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
    else
        printf ""[+] No reverse prompt required\n""
    fi

    # Clean up
    rm README.md

    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Install WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    model_name=${wfile%-Q*}

    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

    # Add reverse prompt if it exists
    if [ -n ""$reverse_prompt"" ]; then
        cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
    fi

    printf ""\n""
    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    printf ""********************* [LOG: MODEL INFO (Load Model & Init Execution Context)] *********************""
    eval $cmd

elif [ ""$interactive"" -eq 0 ]; then

    printf ""\n""
    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Installing WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download gemma-2-9b-it-Q5_K_M.gguf
    ss_url=""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading %s ...\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    # * start llama-api-server
    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:gemma-2-9b-it-Q5_K_M.gguf llama-api-server.wasm -p gemma-instruct -c 4096 --model-name gemma-2-9b-it --socket-addr 0.0.0.0:${port}""

    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    eval $cmd

elif [ ""$interactive"" -eq 1 ]; then

    printf ""\n""
    printf ""[I] This is a helper script for deploying LlamaEdge API Server on this machine.\n\n""
    printf ""    The following tasks will be done:\n""
    printf ""    - Download GGUF model\n""
    printf ""    - Install WasmEdge Runtime and the wasi-nn_ggml plugin\n""
    printf ""    - Download LlamaEdge API Server\n""
    printf ""\n""
    printf ""    Upon the tasks done, an HTTP server will be started and it will serve the selected\n""
    printf ""    model.\n""
    printf ""\n""
    printf ""    Please note:\n""
    printf ""\n""
    printf ""    - All downloaded files will be stored in the current folder\n""
    printf ""    - The server will be listening on all network interfaces\n""
    printf ""    - The server will run with default settings which are not always optimal\n""
    printf ""    - Do not judge the quality of a model based on the results from this script\n""
    printf ""    - This script is only for demonstration purposes\n""
    printf ""\n""
    printf ""    During the whole process, you can press Ctrl-C to abort the current process at any time.\n""
    printf ""\n""
    printf ""    Press Enter to continue ...\n\n""

    read

    # * install WasmEdge + wasi-nn_ggml plugin

    printf ""[+] Installing WasmEdge ...\n\n""

    # Check if WasmEdge has been installed
    reinstall_wasmedge=1
    if command -v wasmedge &> /dev/null
    then
        printf ""    1) Install the latest version of WasmEdge and wasi-nn_ggml plugin (recommended)\n""
        printf ""    2) Keep the current version\n\n""
        read -p ""[+] Select a number from the list above: "" reinstall_wasmedge
    fi

    while [[ ""$reinstall_wasmedge"" -ne 1 && ""$reinstall_wasmedge"" -ne 2 ]]; do
        printf ""    Invalid number. Please enter number 1 or 2\n""
        read reinstall_wasmedge
    done

    if [[ ""$reinstall_wasmedge"" == ""1"" ]]; then
        # install WasmEdge + wasi-nn_ggml plugin
        if [ ""$ggml_version"" = ""latest"" ]; then
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        else
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        fi


    elif [[ ""$reinstall_wasmedge"" == ""2"" ]]; then
        wasmedge_path=$(which wasmedge)
        wasmedge_root_path=${wasmedge_path%""/bin/wasmedge""}

        found=0
        for file in ""$wasmedge_root_path/plugin/libwasmedgePluginWasiNN.""*; do
        if [[ -f $file ]]; then
            found=1
            break
        fi
        done

        if [[ $found -eq 0 ]]; then
            printf ""\n    * Not found wasi-nn_ggml plugin. Please download it from https://github.com/WasmEdge/WasmEdge/releases/ and move it to %s. After that, please rerun the script. \n\n"" ""$wasmedge_root_path/plugin/""

            exit 1
        fi

    fi

    printf ""[+] The most popular models at https://huggingface.co/second-state:\n\n""

    for i in ""${!ss_models[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
    done

    # ask for repo until index of sample repo is provided or an URL
    while [[ -z ""$repo"" ]]; do
        printf ""\n    Or choose one from: https://huggingface.co/models?sort=trending&search=gguf\n\n""

        read -p ""[+] Please select a number from the list above or enter an URL: "" repo

        # check if the input is a number
        if [[ ""$repo"" =~ ^[0-9]+$ ]]; then
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        elif [[ ""$repo"" =~ ^https?:// ]]; then
            repo=""$repo""
        else
            printf ""[-] Invalid repo URL: %s\n"" ""$repo""
            repo=""""
        fi
    done


    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    if [ -n ""$ss_model"" ]; then
        ss_url=$repo
        repo=${repo%/resolve/main/*}

        # check file if the model has been downloaded before
        wfile=$(basename ""$ss_url"")
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$ss_url""
        fi

    else

        printf ""[+] Checking for GGUF model files in %s\n\n"" ""$repo""

        # find GGUF files in the source
        model_tree=""${repo%/}/tree/main""
        model_files=$(curl -s ""$model_tree"" | grep -i ""\\.gguf</span>"" | sed -E 's/.*<span class=""truncate group-hover:underline"">(.*)<\/span><\/a>/\1/g')
        # Convert model_files into an array
        model_files_array=($model_files)

        while IFS= read -r line; do
            sizes+=(""$line"")
        done < <(curl -s ""$model_tree"" | awk -F 'download=true"">' '/download=true"">[0-9\.]+ (GB|MB)/ {print $2}' | awk '{print $1, $2}')

        # list all files in the provided git repo
        length=${#model_files_array[@]}
        for ((i=0; i<$length; i++)); do
            file=${model_files_array[i]}
            size=${sizes[i]}
            iw=-1
            is=0
            for wt in ""${wtypes[@]}""; do
                # uppercase
                ufile=$(echo ""$file"" | tr '[:lower:]' '[:upper:]')
                if [[ ""$ufile"" =~ ""$wt"" ]]; then
                    iw=$is
                    break
                fi
                is=$((is+1))
            done

            if [[ $iw -eq -1 ]]; then
                continue
            fi

            wfiles[$iw]=""$file""

            have="" ""
            if [[ -f ""$file"" ]]; then
                have=""*""
            fi

            printf ""    %2d) %s %7s   %s\n"" $iw ""$have"" ""$size"" ""$file""
        done

        # ask for weights type until provided and available
        while [[ -z ""$wtype"" ]]; do
            printf ""\n""
            read -p ""[+] Please select a number from the list above: "" wtype
            wfile=""${wfiles[$wtype]}""

            if [[ -z ""$wfile"" ]]; then
                printf ""[-] Invalid number: %s\n"" ""$wtype""
                wtype=""""
            fi
        done

        url=""${repo%/}/resolve/main/$wfile""

        # check file if the model has been downloaded before
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$url""
        fi

    fi

    # * prompt type and reverse prompt

    if [[ $repo =~ ^https://huggingface\.co/second-state ]]; then
        readme_url=""$repo/resolve/main/README.md""

        # Download the README.md file
        curl -s $readme_url -o README.md

        # Extract the ""Prompt type: xxxx"" line
        prompt_type_line=$(grep -i ""Prompt type:"" README.md)

        # Extract the xxxx part
        prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

        # Check if ""Reverse prompt"" exists
        if grep -q ""Reverse prompt:"" README.md; then
            # Extract the ""Reverse prompt: xxxx"" line
            reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

            # Extract the xxxx part
            reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

            printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
        else
            printf ""[+] No reverse prompt required\n""
        fi

        # Clean up
        rm README.md
    else
        printf ""[+] Please select a number from the list below:\n""
        printf ""    The definitions of the prompt types below can be found at https://github.com/LlamaEdge/LlamaEdge/raw/main/api-server/chat-prompts/README.md\n\n""

        is=0
        for r in ""${prompt_types[@]}""; do
            printf ""    %2d) %s\n"" $is ""$r""
            is=$((is+1))
        done
        printf ""\n""

        prompt_type_index=-1
        while ((prompt_type_index < 0 || prompt_type_index >= ${#prompt_types[@]})); do
            read -p ""[+] Select prompt type: "" prompt_type_index
            # Check if the input is a number
            if ! [[ ""$prompt_type_index"" =~ ^[0-9]+$ ]]; then
                echo ""Invalid input. Please enter a number.""
                prompt_type_index=-1
            fi
        done
        prompt_type=""${prompt_types[$prompt_type_index]}""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $need_reverse_prompt =~ ^[yYnN]$ ]]; do
            read -p ""[+] Need reverse prompt? (y/n): "" need_reverse_prompt
        done

        # If user answered yes, ask them to input a string
        if [[ ""$need_reverse_prompt"" == ""y"" || ""$need_reverse_prompt"" == ""Y"" ]]; then
            read -p ""    Enter the reverse prompt: "" reverse_prompt
            printf ""\n""
        fi
    fi

    # * running mode

    printf ""[+] Running mode: \n\n""

    running_modes=(""API Server with Chatbot web app"" ""CLI Chat"")

    for i in ""${!running_modes[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${running_modes[$i]}""
    done

    while [[ -z ""$running_mode_index"" ]]; do
        printf ""\n""
        read -p ""[+] Select a number from the list above: "" running_mode_index
        running_mode=""${running_modes[$running_mode_index - 1]}""

        if [[ -z ""$running_mode"" ]]; then
            printf ""[-] Invalid number: %s\n"" ""$running_mode_index""
            running_mode_index=""""
        fi
    done
    printf ""[+] Selected running mode: %s (%s)\n"" ""$running_mode_index"" ""$running_mode""

    # * download llama-api-server.wasm or llama-chat.wasm

    repo=""second-state/LlamaEdge""
    releases=$(curl -s ""https://api.github.com/repos/$repo/releases"")
    if [[ ""$running_mode_index"" == ""1"" ]]; then

        # * Download llama-api-server.wasm

        if [ -f ""llama-api-server.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-api-server.wasm. Download the latest llama-api-server.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-api-server.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

            printf ""\n""
        fi

        # * chatbot-ui

        if [ -d ""chatbot-ui"" ]; then
            printf ""[+] Using cached Chatbot web app\n""
        else
            printf ""[+] Downloading Chatbot web app ...\n""
            files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
            curl -LO $files_tarball
            if [ $? -ne 0 ]; then
                printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
                exit 1
            fi
            tar xzf chatbot-ui.tar.gz
            rm chatbot-ui.tar.gz
            printf ""\n""
        fi

        model_name=${wfile%-Q*}

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start the server:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_server =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start the server? (y/n): "" start_server
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_server"" == ""y"" || ""$start_server"" == ""Y"" ]]; then
            printf ""\n""
            printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
            printf ""*********************************** LlamaEdge API Server ********************************\n\n""
            eval $cmd

        fi

    elif [[ ""$running_mode_index"" == ""2"" ]]; then

        # * Download llama-chat.wasm

        if [ -f ""llama-chat.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-chat.wasm. Download the latest llama-chat.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-chat.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-chat.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-chat.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

            printf ""\n""
        fi

        # * prepare the command

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-chat.wasm --prompt-template $prompt_type""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start CLI Chat:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_chat =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start CLI Chat? (y/n): "" start_chat
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_chat"" == ""y"" || ""$start_chat"" == ""Y"" ]]; then
            printf ""\n""

            # Execute the command
            printf ""********************* LlamaEdge *********************\n\n""
            eval $cmd

        fi

    else
        printf ""[-] Invalid running mode: %s\n"" ""$running_mode_index""
        exit 1
    fi

else
    echo ""Invalid value for interactive""
fi

exit 0

```","Q: 5. What is the default running mode, and how can it be changed?
A: The default running mode is not explicitly specified in the script, but based on the context, it seems that the default running mode is ""API Server with Chatbot web app"".

To change the running mode, you can modify the value of `running_mode_index` variable before starting the script. You can set it to either 1 (for API Server with Chatbot web app) or 2 (for CLI Chat).

Here's an example:

```bash
#!/bin/bash

# ...
running_mode_index=2  # Change this line to switch to CLI Chat mode
# ...
```

Alternatively, you can prompt the user to select a running mode from the list of available options:

```bash
printf ""[+] Select a running mode:\n""
printf ""    %s\n"" ""${running_modes[@]}""

read -p ""[+] Please enter a number (1 for API Server with Chatbot web app or 2 for CLI Chat): "" running_mode_index

# ...
```

This will allow the user to choose their preferred running mode at runtime."
"```bash:run-llm.sh
#!/bin/bash
#
# Helper script for deploying LlamaEdge API Server with a single Bash command
#
# - Works on Linux and macOS
# - Supports: CPU, CUDA, Metal, OpenCL
# - Can run GGUF models from https://huggingface.co/second-state/
#

set -e

# required utils: curl, git, make
if ! command -v curl &> /dev/null; then
    printf ""[-] curl not found\n""
    exit 1
fi
if ! command -v git &> /dev/null; then
    printf ""[-] git not found\n""
    exit 1
fi
if ! command -v make &> /dev/null; then
    printf ""[-] make not found\n""
    exit 1
fi

# parse arguments
port=8080
repo=""""
wtype=""""
backend=""cpu""
ctx_size=512
n_predict=1024
n_gpu_layers=100

# if macOS, use metal backend by default
if [[ ""$OSTYPE"" == ""darwin""* ]]; then
    backend=""metal""
elif command -v nvcc &> /dev/null; then
    backend=""cuda""
fi

gpu_id=0
n_parallel=8
n_kv=4096
verbose=0
log_prompts=0
log_stat=0
# 0: server mode
# 1: local mode
# mode=0
# 0: non-interactive
# 1: interactive
interactive=0
model=""""
# ggml version: latest or bxxxx
ggml_version=""latest""

function print_usage {
    printf ""Usage:\n""
    printf ""  ./run-llm.sh [--port]\n\n""
    printf ""  --model:        model name\n""
    printf ""  --interactive:  run in interactive mode\n""
    printf ""  --port:         port number, default is 8080\n""
    printf ""  --ggml-version: ggml version (for example, b2963). If the option is not used, then install the latest version.\n""
    printf ""Example:\n\n""
    printf '  bash <(curl -sSfL 'https://code.flows.network/webhook/iwYN1SdN3AmPgR5ao5Gt/run-llm.sh')""\n\n'
}

while [[ $# -gt 0 ]]; do
    key=""$1""
    case $key in
        --model)
            model=""$2""
            shift
            shift
            ;;
        --interactive)
            interactive=1
            shift
            ;;
        --port)
            port=""$2""
            shift
            shift
            ;;
        --ggml-version)
            ggml_version=""$2""
            shift
            shift
            ;;
        --help)
            print_usage
            exit 0
            ;;
        *)
            echo ""Unknown argument: $key""
            print_usage
            exit 1
            ;;
    esac
done

# available weights types
wtypes=(""Q2_K"" ""Q3_K_L"" ""Q3_K_M"" ""Q3_K_S"" ""Q4_0"" ""Q4_K_M"" ""Q4_K_S"" ""Q5_0"" ""Q5_K_M"" ""Q5_K_S"" ""Q6_K"" ""Q8_0"")

wfiles=()
for wt in ""${wtypes[@]}""; do
    wfiles+=("""")
done

ss_urls=(
    ""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-1.5-9B-Chat-GGUF/resolve/main/Yi-1.5-9B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/stablelm-2-zephyr-1.6b-GGUF/resolve/main/stablelm-2-zephyr-1_6b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/OpenChat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34B-Chat-GGUF/resolve/main/Yi-34B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34Bx2-MoE-60B-GGUF/resolve/main/Yi-34Bx2-MoE-60B-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-LLM-7B-Chat-GGUF/resolve/main/deepseek-llm-7b-chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-Coder-6.7B-Instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/Mistral-7B-Instruct-v0.2-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Orca-2-13B-GGUF/resolve/main/Orca-2-13b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/SOLAR-10.7B-Instruct-v1.0-GGUF/resolve/main/SOLAR-10.7B-Instruct-v1.0-Q5_K_M.gguf""
)

# sample models
ss_models=(
    ""gemma-2-9b-it""
    ""yi-1.5-9b-chat""
    ""phi-3-mini-4k""
    ""llama-3-8b-instruct""
    ""llama-2-7b-chat""
    ""stablelm-2-zephyr-1.6b""
    ""openchat-3.5-0106""
    ""yi-34b-chat""
    ""yi-34bx2-moe-60b""
    ""deepseek-llm-7b-chat""
    ""deepseek-coder-6.7b-instruct""
    ""mistral-7b-instruct-v0.2""
    ""dolphin-2.6-mistral-7b""
    ""orca-2-13b""
    ""tinyllama-1.1b-chat-v1.0""
    ""solar-10.7b-instruct-v1.0""
)

# prompt types
prompt_types=(
    ""gemma-instruct""
    ""chatml""
    ""phi-3-chat""
    ""llama-3-chat""
    ""llama-2-chat""
    ""chatml""
    ""openchat""
    ""zephyr""
    ""codellama-instruct""
    ""mistral-instruct""
    ""mistrallite""
    ""vicuna-chat""
    ""vicuna-1.1-chat""
    ""wizard-coder""
    ""intel-neural""
    ""deepseek-chat""
    ""deepseek-coder""
    ""solar-instruct""
    ""belle-llama-2-chat""
    ""human-assistant""
)


if [ -n ""$model"" ]; then
    printf ""\n""

    # Check if the model is in the list of supported models
    if [[ ! "" ${ss_models[@]} "" =~ "" ${model} "" ]]; then

        printf ""[+] ${model} is an invalid name or a unsupported model. Please check the model list:\n\n""

        for i in ""${!ss_models[@]}""; do
            printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
        done
        printf ""\n""

        # ask for repo until index of sample repo is provided or an URL
        while [[ -z ""$repo"" ]]; do

            read -p ""[+] Please select a number from the list above: "" repo

            # check if the input is a number
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        done
    else
        # Find the index of the model in the list of supported models
        for i in ""${!ss_models[@]}""; do
            if [[ ""${ss_models[$i]}"" = ""${model}"" ]]; then
                ss_model=""${ss_models[$i]}""
                repo=""${ss_urls[$i]}""

                break
            fi
        done

    fi

    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    ss_url=$repo

    repo=${repo%/resolve/main/*}

    # check file if the model has been downloaded before
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * prompt type and reverse prompt

    readme_url=""$repo/resolve/main/README.md""

    # Download the README.md file
    curl -s $readme_url -o README.md

    # Extract the ""Prompt type: xxxx"" line
    prompt_type_line=$(grep -i ""Prompt type:"" README.md)

    # Extract the xxxx part
    prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

    printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

    # Check if ""Reverse prompt"" exists
    if grep -q ""Reverse prompt:"" README.md; then
        # Extract the ""Reverse prompt: xxxx"" line
        reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

        # Extract the xxxx part
        reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
    else
        printf ""[+] No reverse prompt required\n""
    fi

    # Clean up
    rm README.md

    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Install WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    model_name=${wfile%-Q*}

    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

    # Add reverse prompt if it exists
    if [ -n ""$reverse_prompt"" ]; then
        cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
    fi

    printf ""\n""
    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    printf ""********************* [LOG: MODEL INFO (Load Model & Init Execution Context)] *********************""
    eval $cmd

elif [ ""$interactive"" -eq 0 ]; then

    printf ""\n""
    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Installing WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download gemma-2-9b-it-Q5_K_M.gguf
    ss_url=""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading %s ...\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    # * start llama-api-server
    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:gemma-2-9b-it-Q5_K_M.gguf llama-api-server.wasm -p gemma-instruct -c 4096 --model-name gemma-2-9b-it --socket-addr 0.0.0.0:${port}""

    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    eval $cmd

elif [ ""$interactive"" -eq 1 ]; then

    printf ""\n""
    printf ""[I] This is a helper script for deploying LlamaEdge API Server on this machine.\n\n""
    printf ""    The following tasks will be done:\n""
    printf ""    - Download GGUF model\n""
    printf ""    - Install WasmEdge Runtime and the wasi-nn_ggml plugin\n""
    printf ""    - Download LlamaEdge API Server\n""
    printf ""\n""
    printf ""    Upon the tasks done, an HTTP server will be started and it will serve the selected\n""
    printf ""    model.\n""
    printf ""\n""
    printf ""    Please note:\n""
    printf ""\n""
    printf ""    - All downloaded files will be stored in the current folder\n""
    printf ""    - The server will be listening on all network interfaces\n""
    printf ""    - The server will run with default settings which are not always optimal\n""
    printf ""    - Do not judge the quality of a model based on the results from this script\n""
    printf ""    - This script is only for demonstration purposes\n""
    printf ""\n""
    printf ""    During the whole process, you can press Ctrl-C to abort the current process at any time.\n""
    printf ""\n""
    printf ""    Press Enter to continue ...\n\n""

    read

    # * install WasmEdge + wasi-nn_ggml plugin

    printf ""[+] Installing WasmEdge ...\n\n""

    # Check if WasmEdge has been installed
    reinstall_wasmedge=1
    if command -v wasmedge &> /dev/null
    then
        printf ""    1) Install the latest version of WasmEdge and wasi-nn_ggml plugin (recommended)\n""
        printf ""    2) Keep the current version\n\n""
        read -p ""[+] Select a number from the list above: "" reinstall_wasmedge
    fi

    while [[ ""$reinstall_wasmedge"" -ne 1 && ""$reinstall_wasmedge"" -ne 2 ]]; do
        printf ""    Invalid number. Please enter number 1 or 2\n""
        read reinstall_wasmedge
    done

    if [[ ""$reinstall_wasmedge"" == ""1"" ]]; then
        # install WasmEdge + wasi-nn_ggml plugin
        if [ ""$ggml_version"" = ""latest"" ]; then
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        else
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        fi


    elif [[ ""$reinstall_wasmedge"" == ""2"" ]]; then
        wasmedge_path=$(which wasmedge)
        wasmedge_root_path=${wasmedge_path%""/bin/wasmedge""}

        found=0
        for file in ""$wasmedge_root_path/plugin/libwasmedgePluginWasiNN.""*; do
        if [[ -f $file ]]; then
            found=1
            break
        fi
        done

        if [[ $found -eq 0 ]]; then
            printf ""\n    * Not found wasi-nn_ggml plugin. Please download it from https://github.com/WasmEdge/WasmEdge/releases/ and move it to %s. After that, please rerun the script. \n\n"" ""$wasmedge_root_path/plugin/""

            exit 1
        fi

    fi

    printf ""[+] The most popular models at https://huggingface.co/second-state:\n\n""

    for i in ""${!ss_models[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
    done

    # ask for repo until index of sample repo is provided or an URL
    while [[ -z ""$repo"" ]]; do
        printf ""\n    Or choose one from: https://huggingface.co/models?sort=trending&search=gguf\n\n""

        read -p ""[+] Please select a number from the list above or enter an URL: "" repo

        # check if the input is a number
        if [[ ""$repo"" =~ ^[0-9]+$ ]]; then
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        elif [[ ""$repo"" =~ ^https?:// ]]; then
            repo=""$repo""
        else
            printf ""[-] Invalid repo URL: %s\n"" ""$repo""
            repo=""""
        fi
    done


    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    if [ -n ""$ss_model"" ]; then
        ss_url=$repo
        repo=${repo%/resolve/main/*}

        # check file if the model has been downloaded before
        wfile=$(basename ""$ss_url"")
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$ss_url""
        fi

    else

        printf ""[+] Checking for GGUF model files in %s\n\n"" ""$repo""

        # find GGUF files in the source
        model_tree=""${repo%/}/tree/main""
        model_files=$(curl -s ""$model_tree"" | grep -i ""\\.gguf</span>"" | sed -E 's/.*<span class=""truncate group-hover:underline"">(.*)<\/span><\/a>/\1/g')
        # Convert model_files into an array
        model_files_array=($model_files)

        while IFS= read -r line; do
            sizes+=(""$line"")
        done < <(curl -s ""$model_tree"" | awk -F 'download=true"">' '/download=true"">[0-9\.]+ (GB|MB)/ {print $2}' | awk '{print $1, $2}')

        # list all files in the provided git repo
        length=${#model_files_array[@]}
        for ((i=0; i<$length; i++)); do
            file=${model_files_array[i]}
            size=${sizes[i]}
            iw=-1
            is=0
            for wt in ""${wtypes[@]}""; do
                # uppercase
                ufile=$(echo ""$file"" | tr '[:lower:]' '[:upper:]')
                if [[ ""$ufile"" =~ ""$wt"" ]]; then
                    iw=$is
                    break
                fi
                is=$((is+1))
            done

            if [[ $iw -eq -1 ]]; then
                continue
            fi

            wfiles[$iw]=""$file""

            have="" ""
            if [[ -f ""$file"" ]]; then
                have=""*""
            fi

            printf ""    %2d) %s %7s   %s\n"" $iw ""$have"" ""$size"" ""$file""
        done

        # ask for weights type until provided and available
        while [[ -z ""$wtype"" ]]; do
            printf ""\n""
            read -p ""[+] Please select a number from the list above: "" wtype
            wfile=""${wfiles[$wtype]}""

            if [[ -z ""$wfile"" ]]; then
                printf ""[-] Invalid number: %s\n"" ""$wtype""
                wtype=""""
            fi
        done

        url=""${repo%/}/resolve/main/$wfile""

        # check file if the model has been downloaded before
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$url""
        fi

    fi

    # * prompt type and reverse prompt

    if [[ $repo =~ ^https://huggingface\.co/second-state ]]; then
        readme_url=""$repo/resolve/main/README.md""

        # Download the README.md file
        curl -s $readme_url -o README.md

        # Extract the ""Prompt type: xxxx"" line
        prompt_type_line=$(grep -i ""Prompt type:"" README.md)

        # Extract the xxxx part
        prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

        # Check if ""Reverse prompt"" exists
        if grep -q ""Reverse prompt:"" README.md; then
            # Extract the ""Reverse prompt: xxxx"" line
            reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

            # Extract the xxxx part
            reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

            printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
        else
            printf ""[+] No reverse prompt required\n""
        fi

        # Clean up
        rm README.md
    else
        printf ""[+] Please select a number from the list below:\n""
        printf ""    The definitions of the prompt types below can be found at https://github.com/LlamaEdge/LlamaEdge/raw/main/api-server/chat-prompts/README.md\n\n""

        is=0
        for r in ""${prompt_types[@]}""; do
            printf ""    %2d) %s\n"" $is ""$r""
            is=$((is+1))
        done
        printf ""\n""

        prompt_type_index=-1
        while ((prompt_type_index < 0 || prompt_type_index >= ${#prompt_types[@]})); do
            read -p ""[+] Select prompt type: "" prompt_type_index
            # Check if the input is a number
            if ! [[ ""$prompt_type_index"" =~ ^[0-9]+$ ]]; then
                echo ""Invalid input. Please enter a number.""
                prompt_type_index=-1
            fi
        done
        prompt_type=""${prompt_types[$prompt_type_index]}""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $need_reverse_prompt =~ ^[yYnN]$ ]]; do
            read -p ""[+] Need reverse prompt? (y/n): "" need_reverse_prompt
        done

        # If user answered yes, ask them to input a string
        if [[ ""$need_reverse_prompt"" == ""y"" || ""$need_reverse_prompt"" == ""Y"" ]]; then
            read -p ""    Enter the reverse prompt: "" reverse_prompt
            printf ""\n""
        fi
    fi

    # * running mode

    printf ""[+] Running mode: \n\n""

    running_modes=(""API Server with Chatbot web app"" ""CLI Chat"")

    for i in ""${!running_modes[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${running_modes[$i]}""
    done

    while [[ -z ""$running_mode_index"" ]]; do
        printf ""\n""
        read -p ""[+] Select a number from the list above: "" running_mode_index
        running_mode=""${running_modes[$running_mode_index - 1]}""

        if [[ -z ""$running_mode"" ]]; then
            printf ""[-] Invalid number: %s\n"" ""$running_mode_index""
            running_mode_index=""""
        fi
    done
    printf ""[+] Selected running mode: %s (%s)\n"" ""$running_mode_index"" ""$running_mode""

    # * download llama-api-server.wasm or llama-chat.wasm

    repo=""second-state/LlamaEdge""
    releases=$(curl -s ""https://api.github.com/repos/$repo/releases"")
    if [[ ""$running_mode_index"" == ""1"" ]]; then

        # * Download llama-api-server.wasm

        if [ -f ""llama-api-server.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-api-server.wasm. Download the latest llama-api-server.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-api-server.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

            printf ""\n""
        fi

        # * chatbot-ui

        if [ -d ""chatbot-ui"" ]; then
            printf ""[+] Using cached Chatbot web app\n""
        else
            printf ""[+] Downloading Chatbot web app ...\n""
            files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
            curl -LO $files_tarball
            if [ $? -ne 0 ]; then
                printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
                exit 1
            fi
            tar xzf chatbot-ui.tar.gz
            rm chatbot-ui.tar.gz
            printf ""\n""
        fi

        model_name=${wfile%-Q*}

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start the server:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_server =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start the server? (y/n): "" start_server
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_server"" == ""y"" || ""$start_server"" == ""Y"" ]]; then
            printf ""\n""
            printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
            printf ""*********************************** LlamaEdge API Server ********************************\n\n""
            eval $cmd

        fi

    elif [[ ""$running_mode_index"" == ""2"" ]]; then

        # * Download llama-chat.wasm

        if [ -f ""llama-chat.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-chat.wasm. Download the latest llama-chat.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-chat.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-chat.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-chat.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

            printf ""\n""
        fi

        # * prepare the command

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-chat.wasm --prompt-template $prompt_type""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start CLI Chat:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_chat =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start CLI Chat? (y/n): "" start_chat
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_chat"" == ""y"" || ""$start_chat"" == ""Y"" ]]; then
            printf ""\n""

            # Execute the command
            printf ""********************* LlamaEdge *********************\n\n""
            eval $cmd

        fi

    else
        printf ""[-] Invalid running mode: %s\n"" ""$running_mode_index""
        exit 1
    fi

else
    echo ""Invalid value for interactive""
fi

exit 0

```","Q: 6. Where does the script download llama-api-server.wasm or llama-chat.wasm?
A: The script downloads `llama-api-server.wasm` or `llama-chat.wasm` from the following URLs:

* For `llama-api-server.wasm`:
  * It checks if the file already exists in the current directory. If it does, it prompts the user to download the latest version.
  * If not, it downloads the latest version of `llama-api-server.wasm` from `https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm`.

* For `llama-chat.wasm`:
  * It checks if the file already exists in the current directory. If it does, it prompts the user to download the latest version.
  * If not, it downloads the latest version of `llama-chat.wasm` from `https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm`.

These downloads are done using the `curl` command."
"```bash:run-llm.sh
#!/bin/bash
#
# Helper script for deploying LlamaEdge API Server with a single Bash command
#
# - Works on Linux and macOS
# - Supports: CPU, CUDA, Metal, OpenCL
# - Can run GGUF models from https://huggingface.co/second-state/
#

set -e

# required utils: curl, git, make
if ! command -v curl &> /dev/null; then
    printf ""[-] curl not found\n""
    exit 1
fi
if ! command -v git &> /dev/null; then
    printf ""[-] git not found\n""
    exit 1
fi
if ! command -v make &> /dev/null; then
    printf ""[-] make not found\n""
    exit 1
fi

# parse arguments
port=8080
repo=""""
wtype=""""
backend=""cpu""
ctx_size=512
n_predict=1024
n_gpu_layers=100

# if macOS, use metal backend by default
if [[ ""$OSTYPE"" == ""darwin""* ]]; then
    backend=""metal""
elif command -v nvcc &> /dev/null; then
    backend=""cuda""
fi

gpu_id=0
n_parallel=8
n_kv=4096
verbose=0
log_prompts=0
log_stat=0
# 0: server mode
# 1: local mode
# mode=0
# 0: non-interactive
# 1: interactive
interactive=0
model=""""
# ggml version: latest or bxxxx
ggml_version=""latest""

function print_usage {
    printf ""Usage:\n""
    printf ""  ./run-llm.sh [--port]\n\n""
    printf ""  --model:        model name\n""
    printf ""  --interactive:  run in interactive mode\n""
    printf ""  --port:         port number, default is 8080\n""
    printf ""  --ggml-version: ggml version (for example, b2963). If the option is not used, then install the latest version.\n""
    printf ""Example:\n\n""
    printf '  bash <(curl -sSfL 'https://code.flows.network/webhook/iwYN1SdN3AmPgR5ao5Gt/run-llm.sh')""\n\n'
}

while [[ $# -gt 0 ]]; do
    key=""$1""
    case $key in
        --model)
            model=""$2""
            shift
            shift
            ;;
        --interactive)
            interactive=1
            shift
            ;;
        --port)
            port=""$2""
            shift
            shift
            ;;
        --ggml-version)
            ggml_version=""$2""
            shift
            shift
            ;;
        --help)
            print_usage
            exit 0
            ;;
        *)
            echo ""Unknown argument: $key""
            print_usage
            exit 1
            ;;
    esac
done

# available weights types
wtypes=(""Q2_K"" ""Q3_K_L"" ""Q3_K_M"" ""Q3_K_S"" ""Q4_0"" ""Q4_K_M"" ""Q4_K_S"" ""Q5_0"" ""Q5_K_M"" ""Q5_K_S"" ""Q6_K"" ""Q8_0"")

wfiles=()
for wt in ""${wtypes[@]}""; do
    wfiles+=("""")
done

ss_urls=(
    ""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-1.5-9B-Chat-GGUF/resolve/main/Yi-1.5-9B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/stablelm-2-zephyr-1.6b-GGUF/resolve/main/stablelm-2-zephyr-1_6b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/OpenChat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34B-Chat-GGUF/resolve/main/Yi-34B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34Bx2-MoE-60B-GGUF/resolve/main/Yi-34Bx2-MoE-60B-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-LLM-7B-Chat-GGUF/resolve/main/deepseek-llm-7b-chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-Coder-6.7B-Instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/Mistral-7B-Instruct-v0.2-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Orca-2-13B-GGUF/resolve/main/Orca-2-13b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/SOLAR-10.7B-Instruct-v1.0-GGUF/resolve/main/SOLAR-10.7B-Instruct-v1.0-Q5_K_M.gguf""
)

# sample models
ss_models=(
    ""gemma-2-9b-it""
    ""yi-1.5-9b-chat""
    ""phi-3-mini-4k""
    ""llama-3-8b-instruct""
    ""llama-2-7b-chat""
    ""stablelm-2-zephyr-1.6b""
    ""openchat-3.5-0106""
    ""yi-34b-chat""
    ""yi-34bx2-moe-60b""
    ""deepseek-llm-7b-chat""
    ""deepseek-coder-6.7b-instruct""
    ""mistral-7b-instruct-v0.2""
    ""dolphin-2.6-mistral-7b""
    ""orca-2-13b""
    ""tinyllama-1.1b-chat-v1.0""
    ""solar-10.7b-instruct-v1.0""
)

# prompt types
prompt_types=(
    ""gemma-instruct""
    ""chatml""
    ""phi-3-chat""
    ""llama-3-chat""
    ""llama-2-chat""
    ""chatml""
    ""openchat""
    ""zephyr""
    ""codellama-instruct""
    ""mistral-instruct""
    ""mistrallite""
    ""vicuna-chat""
    ""vicuna-1.1-chat""
    ""wizard-coder""
    ""intel-neural""
    ""deepseek-chat""
    ""deepseek-coder""
    ""solar-instruct""
    ""belle-llama-2-chat""
    ""human-assistant""
)


if [ -n ""$model"" ]; then
    printf ""\n""

    # Check if the model is in the list of supported models
    if [[ ! "" ${ss_models[@]} "" =~ "" ${model} "" ]]; then

        printf ""[+] ${model} is an invalid name or a unsupported model. Please check the model list:\n\n""

        for i in ""${!ss_models[@]}""; do
            printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
        done
        printf ""\n""

        # ask for repo until index of sample repo is provided or an URL
        while [[ -z ""$repo"" ]]; do

            read -p ""[+] Please select a number from the list above: "" repo

            # check if the input is a number
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        done
    else
        # Find the index of the model in the list of supported models
        for i in ""${!ss_models[@]}""; do
            if [[ ""${ss_models[$i]}"" = ""${model}"" ]]; then
                ss_model=""${ss_models[$i]}""
                repo=""${ss_urls[$i]}""

                break
            fi
        done

    fi

    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    ss_url=$repo

    repo=${repo%/resolve/main/*}

    # check file if the model has been downloaded before
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * prompt type and reverse prompt

    readme_url=""$repo/resolve/main/README.md""

    # Download the README.md file
    curl -s $readme_url -o README.md

    # Extract the ""Prompt type: xxxx"" line
    prompt_type_line=$(grep -i ""Prompt type:"" README.md)

    # Extract the xxxx part
    prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

    printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

    # Check if ""Reverse prompt"" exists
    if grep -q ""Reverse prompt:"" README.md; then
        # Extract the ""Reverse prompt: xxxx"" line
        reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

        # Extract the xxxx part
        reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
    else
        printf ""[+] No reverse prompt required\n""
    fi

    # Clean up
    rm README.md

    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Install WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    model_name=${wfile%-Q*}

    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

    # Add reverse prompt if it exists
    if [ -n ""$reverse_prompt"" ]; then
        cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
    fi

    printf ""\n""
    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    printf ""********************* [LOG: MODEL INFO (Load Model & Init Execution Context)] *********************""
    eval $cmd

elif [ ""$interactive"" -eq 0 ]; then

    printf ""\n""
    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Installing WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download gemma-2-9b-it-Q5_K_M.gguf
    ss_url=""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading %s ...\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    # * start llama-api-server
    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:gemma-2-9b-it-Q5_K_M.gguf llama-api-server.wasm -p gemma-instruct -c 4096 --model-name gemma-2-9b-it --socket-addr 0.0.0.0:${port}""

    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    eval $cmd

elif [ ""$interactive"" -eq 1 ]; then

    printf ""\n""
    printf ""[I] This is a helper script for deploying LlamaEdge API Server on this machine.\n\n""
    printf ""    The following tasks will be done:\n""
    printf ""    - Download GGUF model\n""
    printf ""    - Install WasmEdge Runtime and the wasi-nn_ggml plugin\n""
    printf ""    - Download LlamaEdge API Server\n""
    printf ""\n""
    printf ""    Upon the tasks done, an HTTP server will be started and it will serve the selected\n""
    printf ""    model.\n""
    printf ""\n""
    printf ""    Please note:\n""
    printf ""\n""
    printf ""    - All downloaded files will be stored in the current folder\n""
    printf ""    - The server will be listening on all network interfaces\n""
    printf ""    - The server will run with default settings which are not always optimal\n""
    printf ""    - Do not judge the quality of a model based on the results from this script\n""
    printf ""    - This script is only for demonstration purposes\n""
    printf ""\n""
    printf ""    During the whole process, you can press Ctrl-C to abort the current process at any time.\n""
    printf ""\n""
    printf ""    Press Enter to continue ...\n\n""

    read

    # * install WasmEdge + wasi-nn_ggml plugin

    printf ""[+] Installing WasmEdge ...\n\n""

    # Check if WasmEdge has been installed
    reinstall_wasmedge=1
    if command -v wasmedge &> /dev/null
    then
        printf ""    1) Install the latest version of WasmEdge and wasi-nn_ggml plugin (recommended)\n""
        printf ""    2) Keep the current version\n\n""
        read -p ""[+] Select a number from the list above: "" reinstall_wasmedge
    fi

    while [[ ""$reinstall_wasmedge"" -ne 1 && ""$reinstall_wasmedge"" -ne 2 ]]; do
        printf ""    Invalid number. Please enter number 1 or 2\n""
        read reinstall_wasmedge
    done

    if [[ ""$reinstall_wasmedge"" == ""1"" ]]; then
        # install WasmEdge + wasi-nn_ggml plugin
        if [ ""$ggml_version"" = ""latest"" ]; then
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        else
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        fi


    elif [[ ""$reinstall_wasmedge"" == ""2"" ]]; then
        wasmedge_path=$(which wasmedge)
        wasmedge_root_path=${wasmedge_path%""/bin/wasmedge""}

        found=0
        for file in ""$wasmedge_root_path/plugin/libwasmedgePluginWasiNN.""*; do
        if [[ -f $file ]]; then
            found=1
            break
        fi
        done

        if [[ $found -eq 0 ]]; then
            printf ""\n    * Not found wasi-nn_ggml plugin. Please download it from https://github.com/WasmEdge/WasmEdge/releases/ and move it to %s. After that, please rerun the script. \n\n"" ""$wasmedge_root_path/plugin/""

            exit 1
        fi

    fi

    printf ""[+] The most popular models at https://huggingface.co/second-state:\n\n""

    for i in ""${!ss_models[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
    done

    # ask for repo until index of sample repo is provided or an URL
    while [[ -z ""$repo"" ]]; do
        printf ""\n    Or choose one from: https://huggingface.co/models?sort=trending&search=gguf\n\n""

        read -p ""[+] Please select a number from the list above or enter an URL: "" repo

        # check if the input is a number
        if [[ ""$repo"" =~ ^[0-9]+$ ]]; then
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        elif [[ ""$repo"" =~ ^https?:// ]]; then
            repo=""$repo""
        else
            printf ""[-] Invalid repo URL: %s\n"" ""$repo""
            repo=""""
        fi
    done


    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    if [ -n ""$ss_model"" ]; then
        ss_url=$repo
        repo=${repo%/resolve/main/*}

        # check file if the model has been downloaded before
        wfile=$(basename ""$ss_url"")
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$ss_url""
        fi

    else

        printf ""[+] Checking for GGUF model files in %s\n\n"" ""$repo""

        # find GGUF files in the source
        model_tree=""${repo%/}/tree/main""
        model_files=$(curl -s ""$model_tree"" | grep -i ""\\.gguf</span>"" | sed -E 's/.*<span class=""truncate group-hover:underline"">(.*)<\/span><\/a>/\1/g')
        # Convert model_files into an array
        model_files_array=($model_files)

        while IFS= read -r line; do
            sizes+=(""$line"")
        done < <(curl -s ""$model_tree"" | awk -F 'download=true"">' '/download=true"">[0-9\.]+ (GB|MB)/ {print $2}' | awk '{print $1, $2}')

        # list all files in the provided git repo
        length=${#model_files_array[@]}
        for ((i=0; i<$length; i++)); do
            file=${model_files_array[i]}
            size=${sizes[i]}
            iw=-1
            is=0
            for wt in ""${wtypes[@]}""; do
                # uppercase
                ufile=$(echo ""$file"" | tr '[:lower:]' '[:upper:]')
                if [[ ""$ufile"" =~ ""$wt"" ]]; then
                    iw=$is
                    break
                fi
                is=$((is+1))
            done

            if [[ $iw -eq -1 ]]; then
                continue
            fi

            wfiles[$iw]=""$file""

            have="" ""
            if [[ -f ""$file"" ]]; then
                have=""*""
            fi

            printf ""    %2d) %s %7s   %s\n"" $iw ""$have"" ""$size"" ""$file""
        done

        # ask for weights type until provided and available
        while [[ -z ""$wtype"" ]]; do
            printf ""\n""
            read -p ""[+] Please select a number from the list above: "" wtype
            wfile=""${wfiles[$wtype]}""

            if [[ -z ""$wfile"" ]]; then
                printf ""[-] Invalid number: %s\n"" ""$wtype""
                wtype=""""
            fi
        done

        url=""${repo%/}/resolve/main/$wfile""

        # check file if the model has been downloaded before
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$url""
        fi

    fi

    # * prompt type and reverse prompt

    if [[ $repo =~ ^https://huggingface\.co/second-state ]]; then
        readme_url=""$repo/resolve/main/README.md""

        # Download the README.md file
        curl -s $readme_url -o README.md

        # Extract the ""Prompt type: xxxx"" line
        prompt_type_line=$(grep -i ""Prompt type:"" README.md)

        # Extract the xxxx part
        prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

        # Check if ""Reverse prompt"" exists
        if grep -q ""Reverse prompt:"" README.md; then
            # Extract the ""Reverse prompt: xxxx"" line
            reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

            # Extract the xxxx part
            reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

            printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
        else
            printf ""[+] No reverse prompt required\n""
        fi

        # Clean up
        rm README.md
    else
        printf ""[+] Please select a number from the list below:\n""
        printf ""    The definitions of the prompt types below can be found at https://github.com/LlamaEdge/LlamaEdge/raw/main/api-server/chat-prompts/README.md\n\n""

        is=0
        for r in ""${prompt_types[@]}""; do
            printf ""    %2d) %s\n"" $is ""$r""
            is=$((is+1))
        done
        printf ""\n""

        prompt_type_index=-1
        while ((prompt_type_index < 0 || prompt_type_index >= ${#prompt_types[@]})); do
            read -p ""[+] Select prompt type: "" prompt_type_index
            # Check if the input is a number
            if ! [[ ""$prompt_type_index"" =~ ^[0-9]+$ ]]; then
                echo ""Invalid input. Please enter a number.""
                prompt_type_index=-1
            fi
        done
        prompt_type=""${prompt_types[$prompt_type_index]}""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $need_reverse_prompt =~ ^[yYnN]$ ]]; do
            read -p ""[+] Need reverse prompt? (y/n): "" need_reverse_prompt
        done

        # If user answered yes, ask them to input a string
        if [[ ""$need_reverse_prompt"" == ""y"" || ""$need_reverse_prompt"" == ""Y"" ]]; then
            read -p ""    Enter the reverse prompt: "" reverse_prompt
            printf ""\n""
        fi
    fi

    # * running mode

    printf ""[+] Running mode: \n\n""

    running_modes=(""API Server with Chatbot web app"" ""CLI Chat"")

    for i in ""${!running_modes[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${running_modes[$i]}""
    done

    while [[ -z ""$running_mode_index"" ]]; do
        printf ""\n""
        read -p ""[+] Select a number from the list above: "" running_mode_index
        running_mode=""${running_modes[$running_mode_index - 1]}""

        if [[ -z ""$running_mode"" ]]; then
            printf ""[-] Invalid number: %s\n"" ""$running_mode_index""
            running_mode_index=""""
        fi
    done
    printf ""[+] Selected running mode: %s (%s)\n"" ""$running_mode_index"" ""$running_mode""

    # * download llama-api-server.wasm or llama-chat.wasm

    repo=""second-state/LlamaEdge""
    releases=$(curl -s ""https://api.github.com/repos/$repo/releases"")
    if [[ ""$running_mode_index"" == ""1"" ]]; then

        # * Download llama-api-server.wasm

        if [ -f ""llama-api-server.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-api-server.wasm. Download the latest llama-api-server.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-api-server.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

            printf ""\n""
        fi

        # * chatbot-ui

        if [ -d ""chatbot-ui"" ]; then
            printf ""[+] Using cached Chatbot web app\n""
        else
            printf ""[+] Downloading Chatbot web app ...\n""
            files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
            curl -LO $files_tarball
            if [ $? -ne 0 ]; then
                printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
                exit 1
            fi
            tar xzf chatbot-ui.tar.gz
            rm chatbot-ui.tar.gz
            printf ""\n""
        fi

        model_name=${wfile%-Q*}

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start the server:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_server =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start the server? (y/n): "" start_server
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_server"" == ""y"" || ""$start_server"" == ""Y"" ]]; then
            printf ""\n""
            printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
            printf ""*********************************** LlamaEdge API Server ********************************\n\n""
            eval $cmd

        fi

    elif [[ ""$running_mode_index"" == ""2"" ]]; then

        # * Download llama-chat.wasm

        if [ -f ""llama-chat.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-chat.wasm. Download the latest llama-chat.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-chat.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-chat.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-chat.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

            printf ""\n""
        fi

        # * prepare the command

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-chat.wasm --prompt-template $prompt_type""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start CLI Chat:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_chat =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start CLI Chat? (y/n): "" start_chat
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_chat"" == ""y"" || ""$start_chat"" == ""Y"" ]]; then
            printf ""\n""

            # Execute the command
            printf ""********************* LlamaEdge *********************\n\n""
            eval $cmd

        fi

    else
        printf ""[-] Invalid running mode: %s\n"" ""$running_mode_index""
        exit 1
    fi

else
    echo ""Invalid value for interactive""
fi

exit 0

```","Q: 7. How does the script prepare the command to start the server or CLI Chat?
A: The script prepares the command to start the server or CLI Chat by building a string based on the user's input and settings.

Here are the relevant parts of the script:

**Starting the Server:**

```bash
cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

# Add reverse prompt if it exists
if [ -n ""$reverse_prompt"" ]; then
    cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
fi

printf ""[+] Will run the following command to start the server:\n\n""
printf ""    %s\n\n"" ""$cmd""

eval $cmd
```

**Starting CLI Chat:**

```bash
cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-chat.wasm --prompt-template $prompt_type""

# Add reverse prompt if it exists
if [ -n ""$reverse_prompt"" ]; then
    cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
fi

printf ""[+] Will run the following command to start CLI Chat:\n\n""
printf ""    %s\n\n"" ""$cmd""

eval $cmd
```

In both cases, the script builds a string by concatenating the necessary options and arguments. The `wasmedge` command is used with the following options:

*   `--dir .:.`: specifies the working directory for the WasmEdge runtime.
*   `--nn-preload default:GGML:AUTO:$wfile`: loads the GGUF model from the file specified by `$wfile`.
*   `llama-api-server.wasm` or `llama-chat.wasm`: the name of the executable file to run (either the server or CLI Chat).
*   `--prompt-template ${prompt_type}`: specifies the prompt type for the LLaMA chatbot.
*   `--model-name ${model_name}`: specifies the model name for the LLaMA chatbot.
*   `--socket-addr 0.0.0.0:${port}`: specifies the IP address and port number for the server to listen on (only when starting the server).

The script then adds a reverse prompt if one is specified, and finally executes the command using `eval`."
"```bash:run-llm.sh
#!/bin/bash
#
# Helper script for deploying LlamaEdge API Server with a single Bash command
#
# - Works on Linux and macOS
# - Supports: CPU, CUDA, Metal, OpenCL
# - Can run GGUF models from https://huggingface.co/second-state/
#

set -e

# required utils: curl, git, make
if ! command -v curl &> /dev/null; then
    printf ""[-] curl not found\n""
    exit 1
fi
if ! command -v git &> /dev/null; then
    printf ""[-] git not found\n""
    exit 1
fi
if ! command -v make &> /dev/null; then
    printf ""[-] make not found\n""
    exit 1
fi

# parse arguments
port=8080
repo=""""
wtype=""""
backend=""cpu""
ctx_size=512
n_predict=1024
n_gpu_layers=100

# if macOS, use metal backend by default
if [[ ""$OSTYPE"" == ""darwin""* ]]; then
    backend=""metal""
elif command -v nvcc &> /dev/null; then
    backend=""cuda""
fi

gpu_id=0
n_parallel=8
n_kv=4096
verbose=0
log_prompts=0
log_stat=0
# 0: server mode
# 1: local mode
# mode=0
# 0: non-interactive
# 1: interactive
interactive=0
model=""""
# ggml version: latest or bxxxx
ggml_version=""latest""

function print_usage {
    printf ""Usage:\n""
    printf ""  ./run-llm.sh [--port]\n\n""
    printf ""  --model:        model name\n""
    printf ""  --interactive:  run in interactive mode\n""
    printf ""  --port:         port number, default is 8080\n""
    printf ""  --ggml-version: ggml version (for example, b2963). If the option is not used, then install the latest version.\n""
    printf ""Example:\n\n""
    printf '  bash <(curl -sSfL 'https://code.flows.network/webhook/iwYN1SdN3AmPgR5ao5Gt/run-llm.sh')""\n\n'
}

while [[ $# -gt 0 ]]; do
    key=""$1""
    case $key in
        --model)
            model=""$2""
            shift
            shift
            ;;
        --interactive)
            interactive=1
            shift
            ;;
        --port)
            port=""$2""
            shift
            shift
            ;;
        --ggml-version)
            ggml_version=""$2""
            shift
            shift
            ;;
        --help)
            print_usage
            exit 0
            ;;
        *)
            echo ""Unknown argument: $key""
            print_usage
            exit 1
            ;;
    esac
done

# available weights types
wtypes=(""Q2_K"" ""Q3_K_L"" ""Q3_K_M"" ""Q3_K_S"" ""Q4_0"" ""Q4_K_M"" ""Q4_K_S"" ""Q5_0"" ""Q5_K_M"" ""Q5_K_S"" ""Q6_K"" ""Q8_0"")

wfiles=()
for wt in ""${wtypes[@]}""; do
    wfiles+=("""")
done

ss_urls=(
    ""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-1.5-9B-Chat-GGUF/resolve/main/Yi-1.5-9B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Llama-2-7B-Chat-GGUF/resolve/main/Llama-2-7b-chat-hf-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/stablelm-2-zephyr-1.6b-GGUF/resolve/main/stablelm-2-zephyr-1_6b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/OpenChat-3.5-0106-GGUF/resolve/main/openchat-3.5-0106-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34B-Chat-GGUF/resolve/main/Yi-34B-Chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Yi-34Bx2-MoE-60B-GGUF/resolve/main/Yi-34Bx2-MoE-60B-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-LLM-7B-Chat-GGUF/resolve/main/deepseek-llm-7b-chat-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Deepseek-Coder-6.7B-Instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/Mistral-7B-Instruct-v0.2-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/dolphin-2.6-mistral-7B-GGUF/resolve/main/dolphin-2.6-mistral-7b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/Orca-2-13B-GGUF/resolve/main/Orca-2-13b-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0-Q5_K_M.gguf""
    ""https://huggingface.co/second-state/SOLAR-10.7B-Instruct-v1.0-GGUF/resolve/main/SOLAR-10.7B-Instruct-v1.0-Q5_K_M.gguf""
)

# sample models
ss_models=(
    ""gemma-2-9b-it""
    ""yi-1.5-9b-chat""
    ""phi-3-mini-4k""
    ""llama-3-8b-instruct""
    ""llama-2-7b-chat""
    ""stablelm-2-zephyr-1.6b""
    ""openchat-3.5-0106""
    ""yi-34b-chat""
    ""yi-34bx2-moe-60b""
    ""deepseek-llm-7b-chat""
    ""deepseek-coder-6.7b-instruct""
    ""mistral-7b-instruct-v0.2""
    ""dolphin-2.6-mistral-7b""
    ""orca-2-13b""
    ""tinyllama-1.1b-chat-v1.0""
    ""solar-10.7b-instruct-v1.0""
)

# prompt types
prompt_types=(
    ""gemma-instruct""
    ""chatml""
    ""phi-3-chat""
    ""llama-3-chat""
    ""llama-2-chat""
    ""chatml""
    ""openchat""
    ""zephyr""
    ""codellama-instruct""
    ""mistral-instruct""
    ""mistrallite""
    ""vicuna-chat""
    ""vicuna-1.1-chat""
    ""wizard-coder""
    ""intel-neural""
    ""deepseek-chat""
    ""deepseek-coder""
    ""solar-instruct""
    ""belle-llama-2-chat""
    ""human-assistant""
)


if [ -n ""$model"" ]; then
    printf ""\n""

    # Check if the model is in the list of supported models
    if [[ ! "" ${ss_models[@]} "" =~ "" ${model} "" ]]; then

        printf ""[+] ${model} is an invalid name or a unsupported model. Please check the model list:\n\n""

        for i in ""${!ss_models[@]}""; do
            printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
        done
        printf ""\n""

        # ask for repo until index of sample repo is provided or an URL
        while [[ -z ""$repo"" ]]; do

            read -p ""[+] Please select a number from the list above: "" repo

            # check if the input is a number
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        done
    else
        # Find the index of the model in the list of supported models
        for i in ""${!ss_models[@]}""; do
            if [[ ""${ss_models[$i]}"" = ""${model}"" ]]; then
                ss_model=""${ss_models[$i]}""
                repo=""${ss_urls[$i]}""

                break
            fi
        done

    fi

    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    ss_url=$repo

    repo=${repo%/resolve/main/*}

    # check file if the model has been downloaded before
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * prompt type and reverse prompt

    readme_url=""$repo/resolve/main/README.md""

    # Download the README.md file
    curl -s $readme_url -o README.md

    # Extract the ""Prompt type: xxxx"" line
    prompt_type_line=$(grep -i ""Prompt type:"" README.md)

    # Extract the xxxx part
    prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

    printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

    # Check if ""Reverse prompt"" exists
    if grep -q ""Reverse prompt:"" README.md; then
        # Extract the ""Reverse prompt: xxxx"" line
        reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

        # Extract the xxxx part
        reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
    else
        printf ""[+] No reverse prompt required\n""
    fi

    # Clean up
    rm README.md

    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Install WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    model_name=${wfile%-Q*}

    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

    # Add reverse prompt if it exists
    if [ -n ""$reverse_prompt"" ]; then
        cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
    fi

    printf ""\n""
    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    printf ""********************* [LOG: MODEL INFO (Load Model & Init Execution Context)] *********************""
    eval $cmd

elif [ ""$interactive"" -eq 0 ]; then

    printf ""\n""
    # * install WasmEdge + wasi-nn_ggml plugin
    printf ""[+] Installing WasmEdge with wasi-nn_ggml plugin ...\n\n""

    if [ ""$ggml_version"" = ""latest"" ]; then
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    else
        if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
            source $HOME/.wasmedge/env
            wasmedge_path=$(which wasmedge)
            printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
        else
            echo ""Failed to install WasmEdge""
            exit 1
        fi
    fi

    printf ""\n""

    # * download gemma-2-9b-it-Q5_K_M.gguf
    ss_url=""https://huggingface.co/second-state/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q5_K_M.gguf""
    wfile=$(basename ""$ss_url"")
    if [ -f ""$wfile"" ]; then
        printf ""[+] Using cached model %s \n"" ""$wfile""
    else
        printf ""[+] Downloading %s ...\n"" ""$ss_url""

        # download the weights file
        curl -o ""$wfile"" -# -L ""$ss_url""
    fi

    # * download llama-api-server.wasm
    printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
    curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm
    printf ""\n""

    # * download chatbot-ui
    printf ""[+] Downloading Chatbot web app ...\n""
    files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
    curl -LO $files_tarball
    if [ $? -ne 0 ]; then
        printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
        exit 1
    fi
    tar xzf chatbot-ui.tar.gz
    rm chatbot-ui.tar.gz
    printf ""\n""

    # * start llama-api-server
    cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:gemma-2-9b-it-Q5_K_M.gguf llama-api-server.wasm -p gemma-instruct -c 4096 --model-name gemma-2-9b-it --socket-addr 0.0.0.0:${port}""

    printf ""[+] Will run the following command to start the server:\n\n""
    printf ""    %s\n\n"" ""$cmd""
    printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
    printf ""*********************************** LlamaEdge API Server ********************************\n\n""
    eval $cmd

elif [ ""$interactive"" -eq 1 ]; then

    printf ""\n""
    printf ""[I] This is a helper script for deploying LlamaEdge API Server on this machine.\n\n""
    printf ""    The following tasks will be done:\n""
    printf ""    - Download GGUF model\n""
    printf ""    - Install WasmEdge Runtime and the wasi-nn_ggml plugin\n""
    printf ""    - Download LlamaEdge API Server\n""
    printf ""\n""
    printf ""    Upon the tasks done, an HTTP server will be started and it will serve the selected\n""
    printf ""    model.\n""
    printf ""\n""
    printf ""    Please note:\n""
    printf ""\n""
    printf ""    - All downloaded files will be stored in the current folder\n""
    printf ""    - The server will be listening on all network interfaces\n""
    printf ""    - The server will run with default settings which are not always optimal\n""
    printf ""    - Do not judge the quality of a model based on the results from this script\n""
    printf ""    - This script is only for demonstration purposes\n""
    printf ""\n""
    printf ""    During the whole process, you can press Ctrl-C to abort the current process at any time.\n""
    printf ""\n""
    printf ""    Press Enter to continue ...\n\n""

    read

    # * install WasmEdge + wasi-nn_ggml plugin

    printf ""[+] Installing WasmEdge ...\n\n""

    # Check if WasmEdge has been installed
    reinstall_wasmedge=1
    if command -v wasmedge &> /dev/null
    then
        printf ""    1) Install the latest version of WasmEdge and wasi-nn_ggml plugin (recommended)\n""
        printf ""    2) Keep the current version\n\n""
        read -p ""[+] Select a number from the list above: "" reinstall_wasmedge
    fi

    while [[ ""$reinstall_wasmedge"" -ne 1 && ""$reinstall_wasmedge"" -ne 2 ]]; do
        printf ""    Invalid number. Please enter number 1 or 2\n""
        read reinstall_wasmedge
    done

    if [[ ""$reinstall_wasmedge"" == ""1"" ]]; then
        # install WasmEdge + wasi-nn_ggml plugin
        if [ ""$ggml_version"" = ""latest"" ]; then
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        else
            if curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.0 --ggmlbn=$ggml_version; then
                source $HOME/.wasmedge/env
                wasmedge_path=$(which wasmedge)
                printf ""\n    The WasmEdge Runtime is installed in %s.\n\n"" ""$wasmedge_path""
            else
                echo ""Failed to install WasmEdge""
                exit 1
            fi
        fi


    elif [[ ""$reinstall_wasmedge"" == ""2"" ]]; then
        wasmedge_path=$(which wasmedge)
        wasmedge_root_path=${wasmedge_path%""/bin/wasmedge""}

        found=0
        for file in ""$wasmedge_root_path/plugin/libwasmedgePluginWasiNN.""*; do
        if [[ -f $file ]]; then
            found=1
            break
        fi
        done

        if [[ $found -eq 0 ]]; then
            printf ""\n    * Not found wasi-nn_ggml plugin. Please download it from https://github.com/WasmEdge/WasmEdge/releases/ and move it to %s. After that, please rerun the script. \n\n"" ""$wasmedge_root_path/plugin/""

            exit 1
        fi

    fi

    printf ""[+] The most popular models at https://huggingface.co/second-state:\n\n""

    for i in ""${!ss_models[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${ss_models[$i]}""
    done

    # ask for repo until index of sample repo is provided or an URL
    while [[ -z ""$repo"" ]]; do
        printf ""\n    Or choose one from: https://huggingface.co/models?sort=trending&search=gguf\n\n""

        read -p ""[+] Please select a number from the list above or enter an URL: "" repo

        # check if the input is a number
        if [[ ""$repo"" =~ ^[0-9]+$ ]]; then
            if [[ ""$repo"" -ge 1 && ""$repo"" -le ${#ss_models[@]} ]]; then
                ss_model=""${ss_models[$repo-1]}""
                repo=""${ss_urls[$repo-1]}""
            else
                printf ""[-] Invalid repo index: %s\n"" ""$repo""
                repo=""""
            fi
        elif [[ ""$repo"" =~ ^https?:// ]]; then
            repo=""$repo""
        else
            printf ""[-] Invalid repo URL: %s\n"" ""$repo""
            repo=""""
        fi
    done


    # remove suffix
    repo=$(echo ""$repo"" | sed -E 's/\/tree\/main$//g')

    if [ -n ""$ss_model"" ]; then
        ss_url=$repo
        repo=${repo%/resolve/main/*}

        # check file if the model has been downloaded before
        wfile=$(basename ""$ss_url"")
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$ss_url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$ss_url""
        fi

    else

        printf ""[+] Checking for GGUF model files in %s\n\n"" ""$repo""

        # find GGUF files in the source
        model_tree=""${repo%/}/tree/main""
        model_files=$(curl -s ""$model_tree"" | grep -i ""\\.gguf</span>"" | sed -E 's/.*<span class=""truncate group-hover:underline"">(.*)<\/span><\/a>/\1/g')
        # Convert model_files into an array
        model_files_array=($model_files)

        while IFS= read -r line; do
            sizes+=(""$line"")
        done < <(curl -s ""$model_tree"" | awk -F 'download=true"">' '/download=true"">[0-9\.]+ (GB|MB)/ {print $2}' | awk '{print $1, $2}')

        # list all files in the provided git repo
        length=${#model_files_array[@]}
        for ((i=0; i<$length; i++)); do
            file=${model_files_array[i]}
            size=${sizes[i]}
            iw=-1
            is=0
            for wt in ""${wtypes[@]}""; do
                # uppercase
                ufile=$(echo ""$file"" | tr '[:lower:]' '[:upper:]')
                if [[ ""$ufile"" =~ ""$wt"" ]]; then
                    iw=$is
                    break
                fi
                is=$((is+1))
            done

            if [[ $iw -eq -1 ]]; then
                continue
            fi

            wfiles[$iw]=""$file""

            have="" ""
            if [[ -f ""$file"" ]]; then
                have=""*""
            fi

            printf ""    %2d) %s %7s   %s\n"" $iw ""$have"" ""$size"" ""$file""
        done

        # ask for weights type until provided and available
        while [[ -z ""$wtype"" ]]; do
            printf ""\n""
            read -p ""[+] Please select a number from the list above: "" wtype
            wfile=""${wfiles[$wtype]}""

            if [[ -z ""$wfile"" ]]; then
                printf ""[-] Invalid number: %s\n"" ""$wtype""
                wtype=""""
            fi
        done

        url=""${repo%/}/resolve/main/$wfile""

        # check file if the model has been downloaded before
        if [ -f ""$wfile"" ]; then
            printf ""[+] Using cached model %s \n"" ""$wfile""
        else
            printf ""[+] Downloading the selected model from %s\n"" ""$url""

            # download the weights file
            curl -o ""$wfile"" -# -L ""$url""
        fi

    fi

    # * prompt type and reverse prompt

    if [[ $repo =~ ^https://huggingface\.co/second-state ]]; then
        readme_url=""$repo/resolve/main/README.md""

        # Download the README.md file
        curl -s $readme_url -o README.md

        # Extract the ""Prompt type: xxxx"" line
        prompt_type_line=$(grep -i ""Prompt type:"" README.md)

        # Extract the xxxx part
        prompt_type=$(echo $prompt_type_line | cut -d'`' -f2 | xargs)

        printf ""[+] Extracting prompt type: %s \n"" ""$prompt_type""

        # Check if ""Reverse prompt"" exists
        if grep -q ""Reverse prompt:"" README.md; then
            # Extract the ""Reverse prompt: xxxx"" line
            reverse_prompt_line=$(grep -i ""Reverse prompt:"" README.md)

            # Extract the xxxx part
            reverse_prompt=$(echo $reverse_prompt_line | cut -d'`' -f2 | xargs)

            printf ""[+] Extracting reverse prompt: %s \n"" ""$reverse_prompt""
        else
            printf ""[+] No reverse prompt required\n""
        fi

        # Clean up
        rm README.md
    else
        printf ""[+] Please select a number from the list below:\n""
        printf ""    The definitions of the prompt types below can be found at https://github.com/LlamaEdge/LlamaEdge/raw/main/api-server/chat-prompts/README.md\n\n""

        is=0
        for r in ""${prompt_types[@]}""; do
            printf ""    %2d) %s\n"" $is ""$r""
            is=$((is+1))
        done
        printf ""\n""

        prompt_type_index=-1
        while ((prompt_type_index < 0 || prompt_type_index >= ${#prompt_types[@]})); do
            read -p ""[+] Select prompt type: "" prompt_type_index
            # Check if the input is a number
            if ! [[ ""$prompt_type_index"" =~ ^[0-9]+$ ]]; then
                echo ""Invalid input. Please enter a number.""
                prompt_type_index=-1
            fi
        done
        prompt_type=""${prompt_types[$prompt_type_index]}""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $need_reverse_prompt =~ ^[yYnN]$ ]]; do
            read -p ""[+] Need reverse prompt? (y/n): "" need_reverse_prompt
        done

        # If user answered yes, ask them to input a string
        if [[ ""$need_reverse_prompt"" == ""y"" || ""$need_reverse_prompt"" == ""Y"" ]]; then
            read -p ""    Enter the reverse prompt: "" reverse_prompt
            printf ""\n""
        fi
    fi

    # * running mode

    printf ""[+] Running mode: \n\n""

    running_modes=(""API Server with Chatbot web app"" ""CLI Chat"")

    for i in ""${!running_modes[@]}""; do
        printf ""    %2d) %s\n"" ""$((i+1))"" ""${running_modes[$i]}""
    done

    while [[ -z ""$running_mode_index"" ]]; do
        printf ""\n""
        read -p ""[+] Select a number from the list above: "" running_mode_index
        running_mode=""${running_modes[$running_mode_index - 1]}""

        if [[ -z ""$running_mode"" ]]; then
            printf ""[-] Invalid number: %s\n"" ""$running_mode_index""
            running_mode_index=""""
        fi
    done
    printf ""[+] Selected running mode: %s (%s)\n"" ""$running_mode_index"" ""$running_mode""

    # * download llama-api-server.wasm or llama-chat.wasm

    repo=""second-state/LlamaEdge""
    releases=$(curl -s ""https://api.github.com/repos/$repo/releases"")
    if [[ ""$running_mode_index"" == ""1"" ]]; then

        # * Download llama-api-server.wasm

        if [ -f ""llama-api-server.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-api-server.wasm. Download the latest llama-api-server.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-api-server.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-api-server.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-api-server.wasm

            printf ""\n""
        fi

        # * chatbot-ui

        if [ -d ""chatbot-ui"" ]; then
            printf ""[+] Using cached Chatbot web app\n""
        else
            printf ""[+] Downloading Chatbot web app ...\n""
            files_tarball=""https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz""
            curl -LO $files_tarball
            if [ $? -ne 0 ]; then
                printf ""    \nFailed to download ui tarball. Please manually download from https://github.com/second-state/chatbot-ui/releases/latest/download/chatbot-ui.tar.gz and unzip the ""chatbot-ui.tar.gz"" to the current directory.\n""
                exit 1
            fi
            tar xzf chatbot-ui.tar.gz
            rm chatbot-ui.tar.gz
            printf ""\n""
        fi

        model_name=${wfile%-Q*}

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-api-server.wasm --prompt-template ${prompt_type} --model-name ${model_name} --socket-addr 0.0.0.0:${port}""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start the server:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_server =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start the server? (y/n): "" start_server
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_server"" == ""y"" || ""$start_server"" == ""Y"" ]]; then
            printf ""\n""
            printf ""    Chatbot web app can be accessed at http://0.0.0.0:%s after the server is started\n\n\n"" ""$port""
            printf ""*********************************** LlamaEdge API Server ********************************\n\n""
            eval $cmd

        fi

    elif [[ ""$running_mode_index"" == ""2"" ]]; then

        # * Download llama-chat.wasm

        if [ -f ""llama-chat.wasm"" ]; then
            # Ask user if they need to set ""reverse prompt""
            while [[ ! $use_latest_version =~ ^[yYnN]$ ]]; do
                read -p ""[+] You already have llama-chat.wasm. Download the latest llama-chat.wasm? (y/n): "" use_latest_version
            done

            # If user answered yes, ask them to input a string
            if [[ ""$use_latest_version"" == ""y"" || ""$use_latest_version"" == ""Y"" ]]; then
                printf ""[+] Downloading the latest llama-chat.wasm ...\n""
                curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

                printf ""\n""

            else
                printf ""[+] Using cached llama-chat.wasm\n""
            fi

        else
            printf ""[+] Downloading the latest llama-chat.wasm ...\n""
            curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/latest/download/llama-chat.wasm

            printf ""\n""
        fi

        # * prepare the command

        cmd=""wasmedge --dir .:. --nn-preload default:GGML:AUTO:$wfile llama-chat.wasm --prompt-template $prompt_type""

        # Add reverse prompt if it exists
        if [ -n ""$reverse_prompt"" ]; then
            cmd=""$cmd --reverse-prompt \""${reverse_prompt}\""""
        fi

        printf ""[+] Will run the following command to start CLI Chat:\n\n""
        printf ""    %s\n\n"" ""$cmd""

        # Ask user if they need to set ""reverse prompt""
        while [[ ! $start_chat =~ ^[yYnN]$ ]]; do
            read -p ""[+] Confirm to start CLI Chat? (y/n): "" start_chat
        done

        # If user answered yes, ask them to input a string
        if [[ ""$start_chat"" == ""y"" || ""$start_chat"" == ""Y"" ]]; then
            printf ""\n""

            # Execute the command
            printf ""********************* LlamaEdge *********************\n\n""
            eval $cmd

        fi

    else
        printf ""[-] Invalid running mode: %s\n"" ""$running_mode_index""
        exit 1
    fi

else
    echo ""Invalid value for interactive""
fi

exit 0

```","Q: 8. Can you explain how the script handles errors and exceptions?
A: The script uses a variety of techniques to handle errors and exceptions:

1. **Command checking**: The script checks if certain commands are available before executing them, for example `curl` and `git`. This helps prevent errors when trying to run scripts that rely on these tools.
2. **Error handling with `if` statements**: Many sections of the script use `if` statements to check for specific error conditions. For example, after downloading a file using `curl`, the script checks if the download was successful by checking the exit code (`$?`) of the `curl` command.
3. **Using `set -e`**: The script uses the `set -e` command at the beginning to enable ""exit on error"" mode. This means that if any command fails, the shell will automatically exit with a non-zero status code.
4. **`while` loops for user input**: Some sections of the script use `while` loops to repeatedly ask users for input until they provide valid responses.
5. **Error handling for file operations**: The script handles errors when working with files by checking if files exist or can be written to, and providing informative error messages if something goes wrong.

However, there are some areas where the script could improve its error handling:

1. **Lack of specific error handling for network-related issues**: While the script checks for exit codes from `curl`, it does not specifically handle other types of network-related errors that might occur.
2. **Insufficient logging and debugging information**: In some cases, when an error occurs, the script may provide minimal or unclear error messages that make it difficult to diagnose the issue.

Overall, while the script provides a good foundation for handling errors and exceptions, there are opportunities for improvement in terms of robustness and clarity."
"```rust:llama-chat/src/main.rs
use anyhow::bail;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use either::{Left, Right};
use endpoints::chat::{
    ChatCompletionChunk, ChatCompletionRequestBuilder, ChatCompletionRequestMessage,
    ChatCompletionRequestSampling, ChatCompletionUserMessageContent,
};
use futures::TryStreamExt;
use llama_core::{init_core_context, MetadataBuilder};
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Debug, Parser)]
#[command(author, about, version, long_about=None)]
struct Cli {
    /// Model name
    #[arg(short, long, default_value = ""default"")]
    model_name: String,
    /// Model alias
    #[arg(short = 'a', long, default_value = ""default"")]
    model_alias: String,
    /// Size of the prompt context
    #[arg(short, long, default_value = ""512"")]
    ctx_size: u64,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Batch size for prompt processing
    #[arg(short, long, default_value = ""512"")]
    batch_size: u64,
    /// Temperature for sampling
    #[arg(long, conflicts_with = ""top_p"")]
    temp: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, conflicts_with = ""temp"")]
    top_p: Option<f64>,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Sets the prompt template.
    #[arg(short, long, value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: PromptTemplateType,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// System prompt message string.
    #[arg(short, long)]
    system_prompt: Option<String>,
    /// Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Print all log information to stdout
    #[arg(long)]
    log_all: bool,
    /// enable streaming stdout
    #[arg(long, default_value = ""false"")]
    disable_stream: bool,
}

#[allow(clippy::needless_return)]
#[allow(unreachable_code)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> anyhow::Result<()> {
    // get the environment variable `PLUGIN_DEBUG`
    let plugin_debug = std::env::var(""PLUGIN_DEBUG"").unwrap_or_default();
    let plugin_debug = match plugin_debug.is_empty() {
        true => false,
        false => plugin_debug.to_lowercase().parse::<bool>().unwrap_or(false),
    };

    // parse the command line arguments
    let cli = Cli::parse();

    // log version
    log(format!(
        ""\n[INFO] llama-chat version: {}"",
        env!(""CARGO_PKG_VERSION"")
    ));

    // log the cli options
    log(format!(""[INFO] Model name: {}"", &cli.model_name));
    log(format!(""[INFO] Model alias: {}"", &cli.model_alias));
    log(format!(""[INFO] Prompt template: {}"", &cli.prompt_template));
    // ctx size
    log(format!(""[INFO] Context size: {}"", &cli.ctx_size));
    // reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        log(format!(""[INFO] reverse prompt: {}"", reverse_prompt));
    }
    // system prompt
    if let Some(system_prompt) = &cli.system_prompt {
        log(format!(""[INFO] system prompt: {}"", system_prompt));
    }
    // n_predict
    log(format!(
        ""[INFO] Number of tokens to predict: {}"",
        &cli.n_predict
    ));
    // n_gpu_layers
    log(format!(
        ""[INFO] Number of layers to run on the GPU: {}"",
        &cli.n_gpu_layers
    ));
    // main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        log(format!(""[INFO] Main GPU to use: {}"", main_gpu));
    }
    // tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        log(format!(""[INFO] Tensor split: {}"", tensor_split));
    }
    log(format!(""[INFO] Threads: {}"", &cli.threads));
    // no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        log(format!(
            ""[INFO] Disable memory mapping for file access of chat models : {}"",
            &no_mmap
        ));
    }
    // batch size
    log(format!(
        ""[INFO] Batch size for prompt processing: {}"",
        &cli.batch_size
    ));
    // temp and top_p
    if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
    }
    // repeat penalty
    log(format!(
        ""[INFO] Penalize repeat sequence of tokens: {}"",
        &cli.repeat_penalty
    ));
    // presence penalty
    log(format!(
        ""[INFO] Presence penalty (0.0 = disabled): {}"",
        &cli.presence_penalty
    ));
    // frequency penalty
    log(format!(
        ""[INFO] Frequency penalty (0.0 = disabled): {}"",
        &cli.frequency_penalty
    ));
    // grammar
    log(format!(""[INFO] BNF-like grammar: {}"", &cli.grammar));
    // json schema
    if let Some(json_schema) = &cli.json_schema {
        log(format!(""[INFO] JSON schema: {}"", json_schema));
    }
    // log prompts
    log(format!(""[INFO] Enable prompt log: {}"", &cli.log_prompts));
    // log statistics
    log(format!(""[INFO] Enable plugin log: {}"", &cli.log_stat));

    // create a MetadataBuilder instance
    let builder = MetadataBuilder::new(&cli.model_name, &cli.model_alias, cli.prompt_template)
        .with_ctx_size(cli.ctx_size)
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_batch_size(cli.batch_size)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .enable_prompts_log(cli.log_prompts || cli.log_all)
        .enable_plugin_log(cli.log_stat || cli.log_all)
        .enable_debug_log(plugin_debug);
    // temp and top_p
    let builder = if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
        builder.with_top_p(top_p)
    } else {
        let temp = cli.temp.unwrap();
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    };
    // create a Metadata instance
    let metadata = builder.build();

    // initialize the core context
    init_core_context(Some(&[metadata]), None)?;

    // get the plugin version info
    let plugin_info = llama_core::get_plugin_info()?;
    log(format!(
        ""[INFO] Wasi-nn-ggml plugin: b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    ));

    // create a ChatCompletionRequestSampling instance
    let sampling = if cli.temp.is_none() && cli.top_p.is_none() {
        ChatCompletionRequestSampling::Temperature(1.0)
    } else if let Some(temp) = cli.temp {
        ChatCompletionRequestSampling::Temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        ChatCompletionRequestSampling::TopP(top_p)
    } else {
        let temp = cli.temp.unwrap();
        ChatCompletionRequestSampling::Temperature(temp)
    };

    // create a chat request
    let mut chat_request = ChatCompletionRequestBuilder::new(&cli.model_name, vec![])
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_sampling(sampling)
        .enable_stream(!cli.disable_stream)
        .build();

    // add system message if provided
    if let Some(system_prompt) = &cli.system_prompt {
        let system_message = ChatCompletionRequestMessage::new_system_message(system_prompt, None);

        chat_request.messages.push(system_message);
    }

    let readme = ""
================================== Running in interactive mode. ===================================\n
    - Press [Ctrl+C] to interject at any time.
    - Press [Return] to end the input.
    - For multi-line inputs, end each line with '\\' and press [Return] to get another line.\n"";
    log(readme);

    loop {
        println!(""\n[You]: "");
        let user_input = read_input();

        // put the user message into the messages sequence of chat_request
        let user_message = ChatCompletionRequestMessage::new_user_message(
            ChatCompletionUserMessageContent::Text(user_input),
            None,
        );

        chat_request.messages.push(user_message);

        if cli.log_stat || cli.log_all {
            print_log_begin_separator(""STATISTICS (Set Input)"", Some(""*""), None);
        }

        if cli.log_stat || cli.log_all {
            print_log_end_separator(Some(""*""), None);
        }

        println!(""\n[Bot]:"");
        let mut assistant_answer = String::new();
        match llama_core::chat::chat(&mut chat_request).await {
            Ok(res) => match res {
                Left(mut stream) => {
                    while let Some(data) = stream.try_next().await? {
                        if let Some(chunk) = parse_sse_event(&data) {
                            if let Some(content) = &chunk.choices[0].delta.content {
                                if content.is_empty() {
                                    continue;
                                }
                                if assistant_answer.is_empty() {
                                    let content = content.trim_start();
                                    print!(""{}"", content);
                                    assistant_answer.push_str(content);
                                } else {
                                    print!(""{content}"");
                                    assistant_answer.push_str(content);
                                }
                                io::stdout().flush().unwrap();
                            }
                        }
                    }
                    println!();
                }
                Right(completion) => {
                    let chat_completion = completion.choices[0]
                        .message
                        .content
                        .to_owned()
                        .unwrap_or_default();
                    println!(""{chat_completion}"");
                    assistant_answer = chat_completion;
                }
            },
            Err(e) => {
                bail!(""Fail to generate chat completion. Reason: {msg}"", msg = e)
            }
        };

        let assistant_message = ChatCompletionRequestMessage::new_assistant_message(
            Some(assistant_answer.trim().to_string()),
            None,
            None,
        );
        chat_request.messages.push(assistant_message);
    }

    Ok(())
}

// For single line input, just press [Return] to end the input.
// For multi-line input, end your input with '\\' and press [Return].
//
// For example:
//  [You]:
//  what is the capital of France?[Return]
//
//  [You]:
//  Count the words in the following sentence: \[Return]
//  \[Return]
//  You can use Git to save new files and any changes to already existing files as a bundle of changes called a commit, which can be thought of as a revision to your project.[Return]
//
fn read_input() -> String {
    let mut answer = String::new();
    loop {
        let mut temp = String::new();
        std::io::stdin()
            .read_line(&mut temp)
            .expect(""The read bytes are not valid UTF-8"");

        if temp.ends_with(""\\\n"") {
            temp.pop();
            temp.pop();
            temp.push('\n');
            answer.push_str(&temp);
            continue;
        } else if temp.ends_with('\n') {
            answer.push_str(&temp);
            return answer;
        } else {
            return answer;
        }
    }
}

fn print_log_begin_separator(
    title: impl AsRef<str>,
    ch: Option<&str>,
    len: Option<usize>,
) -> usize {
    let title = format!("" [LOG: {}] "", title.as_ref());

    let total_len: usize = len.unwrap_or(100);
    let separator_len: usize = (total_len - title.len()) / 2;

    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push_str(&title);
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push('\n');
    println!(""{}"", separator);
    total_len
}

fn print_log_end_separator(ch: Option<&str>, len: Option<usize>) {
    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(len.unwrap_or(100)).as_str());
    separator.push('\n');
    println!(""{}"", separator);
}

#[derive(Debug, Default, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    // #[serde(rename = ""enable-debug-log"")]
    // pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    // pub mmproj: String,
    // pub image: String,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    // #[serde(rename = ""main-gpu"")]
    // pub main_gpu: u64,
    // #[serde(rename = ""tensor-split"")]
    // pub tensor_split: String,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,

    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,
}

fn log(msg: impl std::fmt::Display) {
    println!(""{}"", msg);
}

fn parse_sse_event(s: &str) -> Option<ChatCompletionChunk> {
    let lines: Vec<&str> = s.split('\n').collect();
    // let mutevent = None;
    let mut data = None;

    for line in lines {
        if line.starts_with(""data:"") {
            data = Some(line.trim_start_matches(""data:"").trim());
        }
    }

    match data {
        Some(s) => {
            if s.trim() == ""[DONE]"" {
                return None;
            }

            match serde_json::from_str(s) {
                Ok(chunk) => Some(chunk),
                Err(e) => {
                    log(format!(
                        ""[ERROR] Fail to parse SSE data. Reason: {msg}. Data: {data}"",
                        msg = e,
                        data = s
                    ));
                    None
                }
            }
        }
        _ => None,
    }
}

```","Summary:
**Summary of Code-related Files**

This code is part of the `llama-chat` project, which appears to be a conversational AI model. The main file is `main.rs`, where we'll focus our summary.

### Key Changes and Impact

* **Added support for user input**: The code includes a `read_input` function that reads user input from standard input, allowing the chatbot to respond to questions or statements.
* **Improved logging**: The code has improved logging mechanisms, including the ability to print log messages with custom separators and titles.
* **Chat completion functionality**: The code uses the `llama_core::chat::chat` function to generate chat completions based on user input. This involves parsing SSE events from the API response.

### Code Organization

The code is organized into several modules, including:

1. **Main module**: This is the entry point of the program, containing functions such as `main`, `log`, and `parse_sse_event`.
2. **Utils module**: This module contains utility functions like `read_input` and `print_log_begin_separator`.

### Dependencies

The code depends on several external crates, including:

1. `anyhow`: For error handling.
2. `clap`: For parsing command-line arguments.
3. `serde`: For serialization and deserialization of data.
4. `tokio`: For asynchronous I/O operations.

### Performance Implications

The use of async/await syntax and TokIO for I/O operations may improve performance by allowing the program to run concurrently with other tasks.

### Dependencies and Error Handling

The code uses a combination of error handling mechanisms, including:

1. **Try-unwrap**: Used in places like `parse_sse_event` to handle potential errors.
2. **Error messages**: Provided for informative purposes when encountering issues.
3. **Custom logging**: Allows the program to print log messages with custom separators and titles.

### API Documentation

While not explicitly documented, the code follows Rust conventions for naming, organization, and commenting.

---

**Summary of Markdown Files**

There are no separate Markdown files in this repository. However, we can summarize the information presented in the code:

* **README**: Although not provided, a README file would likely contain an overview of the project, its features, and usage instructions.
* **Documentation**: The code includes some internal documentation comments explaining the purpose of functions like `read_input` and `print_log_begin_separator`.

**Key Points**

* User input is read from standard input using the `read_input` function.
* Log messages are printed with custom separators and titles.
* Chat completions are generated based on user input using the `llama_core::chat::chat` function.

---

I hope this summary provides a comprehensive overview of the code-related files in the repository."
"```rust:llama-chat/src/main.rs
use anyhow::bail;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use either::{Left, Right};
use endpoints::chat::{
    ChatCompletionChunk, ChatCompletionRequestBuilder, ChatCompletionRequestMessage,
    ChatCompletionRequestSampling, ChatCompletionUserMessageContent,
};
use futures::TryStreamExt;
use llama_core::{init_core_context, MetadataBuilder};
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Debug, Parser)]
#[command(author, about, version, long_about=None)]
struct Cli {
    /// Model name
    #[arg(short, long, default_value = ""default"")]
    model_name: String,
    /// Model alias
    #[arg(short = 'a', long, default_value = ""default"")]
    model_alias: String,
    /// Size of the prompt context
    #[arg(short, long, default_value = ""512"")]
    ctx_size: u64,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Batch size for prompt processing
    #[arg(short, long, default_value = ""512"")]
    batch_size: u64,
    /// Temperature for sampling
    #[arg(long, conflicts_with = ""top_p"")]
    temp: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, conflicts_with = ""temp"")]
    top_p: Option<f64>,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Sets the prompt template.
    #[arg(short, long, value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: PromptTemplateType,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// System prompt message string.
    #[arg(short, long)]
    system_prompt: Option<String>,
    /// Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Print all log information to stdout
    #[arg(long)]
    log_all: bool,
    /// enable streaming stdout
    #[arg(long, default_value = ""false"")]
    disable_stream: bool,
}

#[allow(clippy::needless_return)]
#[allow(unreachable_code)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> anyhow::Result<()> {
    // get the environment variable `PLUGIN_DEBUG`
    let plugin_debug = std::env::var(""PLUGIN_DEBUG"").unwrap_or_default();
    let plugin_debug = match plugin_debug.is_empty() {
        true => false,
        false => plugin_debug.to_lowercase().parse::<bool>().unwrap_or(false),
    };

    // parse the command line arguments
    let cli = Cli::parse();

    // log version
    log(format!(
        ""\n[INFO] llama-chat version: {}"",
        env!(""CARGO_PKG_VERSION"")
    ));

    // log the cli options
    log(format!(""[INFO] Model name: {}"", &cli.model_name));
    log(format!(""[INFO] Model alias: {}"", &cli.model_alias));
    log(format!(""[INFO] Prompt template: {}"", &cli.prompt_template));
    // ctx size
    log(format!(""[INFO] Context size: {}"", &cli.ctx_size));
    // reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        log(format!(""[INFO] reverse prompt: {}"", reverse_prompt));
    }
    // system prompt
    if let Some(system_prompt) = &cli.system_prompt {
        log(format!(""[INFO] system prompt: {}"", system_prompt));
    }
    // n_predict
    log(format!(
        ""[INFO] Number of tokens to predict: {}"",
        &cli.n_predict
    ));
    // n_gpu_layers
    log(format!(
        ""[INFO] Number of layers to run on the GPU: {}"",
        &cli.n_gpu_layers
    ));
    // main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        log(format!(""[INFO] Main GPU to use: {}"", main_gpu));
    }
    // tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        log(format!(""[INFO] Tensor split: {}"", tensor_split));
    }
    log(format!(""[INFO] Threads: {}"", &cli.threads));
    // no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        log(format!(
            ""[INFO] Disable memory mapping for file access of chat models : {}"",
            &no_mmap
        ));
    }
    // batch size
    log(format!(
        ""[INFO] Batch size for prompt processing: {}"",
        &cli.batch_size
    ));
    // temp and top_p
    if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
    }
    // repeat penalty
    log(format!(
        ""[INFO] Penalize repeat sequence of tokens: {}"",
        &cli.repeat_penalty
    ));
    // presence penalty
    log(format!(
        ""[INFO] Presence penalty (0.0 = disabled): {}"",
        &cli.presence_penalty
    ));
    // frequency penalty
    log(format!(
        ""[INFO] Frequency penalty (0.0 = disabled): {}"",
        &cli.frequency_penalty
    ));
    // grammar
    log(format!(""[INFO] BNF-like grammar: {}"", &cli.grammar));
    // json schema
    if let Some(json_schema) = &cli.json_schema {
        log(format!(""[INFO] JSON schema: {}"", json_schema));
    }
    // log prompts
    log(format!(""[INFO] Enable prompt log: {}"", &cli.log_prompts));
    // log statistics
    log(format!(""[INFO] Enable plugin log: {}"", &cli.log_stat));

    // create a MetadataBuilder instance
    let builder = MetadataBuilder::new(&cli.model_name, &cli.model_alias, cli.prompt_template)
        .with_ctx_size(cli.ctx_size)
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_batch_size(cli.batch_size)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .enable_prompts_log(cli.log_prompts || cli.log_all)
        .enable_plugin_log(cli.log_stat || cli.log_all)
        .enable_debug_log(plugin_debug);
    // temp and top_p
    let builder = if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
        builder.with_top_p(top_p)
    } else {
        let temp = cli.temp.unwrap();
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    };
    // create a Metadata instance
    let metadata = builder.build();

    // initialize the core context
    init_core_context(Some(&[metadata]), None)?;

    // get the plugin version info
    let plugin_info = llama_core::get_plugin_info()?;
    log(format!(
        ""[INFO] Wasi-nn-ggml plugin: b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    ));

    // create a ChatCompletionRequestSampling instance
    let sampling = if cli.temp.is_none() && cli.top_p.is_none() {
        ChatCompletionRequestSampling::Temperature(1.0)
    } else if let Some(temp) = cli.temp {
        ChatCompletionRequestSampling::Temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        ChatCompletionRequestSampling::TopP(top_p)
    } else {
        let temp = cli.temp.unwrap();
        ChatCompletionRequestSampling::Temperature(temp)
    };

    // create a chat request
    let mut chat_request = ChatCompletionRequestBuilder::new(&cli.model_name, vec![])
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_sampling(sampling)
        .enable_stream(!cli.disable_stream)
        .build();

    // add system message if provided
    if let Some(system_prompt) = &cli.system_prompt {
        let system_message = ChatCompletionRequestMessage::new_system_message(system_prompt, None);

        chat_request.messages.push(system_message);
    }

    let readme = ""
================================== Running in interactive mode. ===================================\n
    - Press [Ctrl+C] to interject at any time.
    - Press [Return] to end the input.
    - For multi-line inputs, end each line with '\\' and press [Return] to get another line.\n"";
    log(readme);

    loop {
        println!(""\n[You]: "");
        let user_input = read_input();

        // put the user message into the messages sequence of chat_request
        let user_message = ChatCompletionRequestMessage::new_user_message(
            ChatCompletionUserMessageContent::Text(user_input),
            None,
        );

        chat_request.messages.push(user_message);

        if cli.log_stat || cli.log_all {
            print_log_begin_separator(""STATISTICS (Set Input)"", Some(""*""), None);
        }

        if cli.log_stat || cli.log_all {
            print_log_end_separator(Some(""*""), None);
        }

        println!(""\n[Bot]:"");
        let mut assistant_answer = String::new();
        match llama_core::chat::chat(&mut chat_request).await {
            Ok(res) => match res {
                Left(mut stream) => {
                    while let Some(data) = stream.try_next().await? {
                        if let Some(chunk) = parse_sse_event(&data) {
                            if let Some(content) = &chunk.choices[0].delta.content {
                                if content.is_empty() {
                                    continue;
                                }
                                if assistant_answer.is_empty() {
                                    let content = content.trim_start();
                                    print!(""{}"", content);
                                    assistant_answer.push_str(content);
                                } else {
                                    print!(""{content}"");
                                    assistant_answer.push_str(content);
                                }
                                io::stdout().flush().unwrap();
                            }
                        }
                    }
                    println!();
                }
                Right(completion) => {
                    let chat_completion = completion.choices[0]
                        .message
                        .content
                        .to_owned()
                        .unwrap_or_default();
                    println!(""{chat_completion}"");
                    assistant_answer = chat_completion;
                }
            },
            Err(e) => {
                bail!(""Fail to generate chat completion. Reason: {msg}"", msg = e)
            }
        };

        let assistant_message = ChatCompletionRequestMessage::new_assistant_message(
            Some(assistant_answer.trim().to_string()),
            None,
            None,
        );
        chat_request.messages.push(assistant_message);
    }

    Ok(())
}

// For single line input, just press [Return] to end the input.
// For multi-line input, end your input with '\\' and press [Return].
//
// For example:
//  [You]:
//  what is the capital of France?[Return]
//
//  [You]:
//  Count the words in the following sentence: \[Return]
//  \[Return]
//  You can use Git to save new files and any changes to already existing files as a bundle of changes called a commit, which can be thought of as a revision to your project.[Return]
//
fn read_input() -> String {
    let mut answer = String::new();
    loop {
        let mut temp = String::new();
        std::io::stdin()
            .read_line(&mut temp)
            .expect(""The read bytes are not valid UTF-8"");

        if temp.ends_with(""\\\n"") {
            temp.pop();
            temp.pop();
            temp.push('\n');
            answer.push_str(&temp);
            continue;
        } else if temp.ends_with('\n') {
            answer.push_str(&temp);
            return answer;
        } else {
            return answer;
        }
    }
}

fn print_log_begin_separator(
    title: impl AsRef<str>,
    ch: Option<&str>,
    len: Option<usize>,
) -> usize {
    let title = format!("" [LOG: {}] "", title.as_ref());

    let total_len: usize = len.unwrap_or(100);
    let separator_len: usize = (total_len - title.len()) / 2;

    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push_str(&title);
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push('\n');
    println!(""{}"", separator);
    total_len
}

fn print_log_end_separator(ch: Option<&str>, len: Option<usize>) {
    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(len.unwrap_or(100)).as_str());
    separator.push('\n');
    println!(""{}"", separator);
}

#[derive(Debug, Default, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    // #[serde(rename = ""enable-debug-log"")]
    // pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    // pub mmproj: String,
    // pub image: String,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    // #[serde(rename = ""main-gpu"")]
    // pub main_gpu: u64,
    // #[serde(rename = ""tensor-split"")]
    // pub tensor_split: String,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,

    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,
}

fn log(msg: impl std::fmt::Display) {
    println!(""{}"", msg);
}

fn parse_sse_event(s: &str) -> Option<ChatCompletionChunk> {
    let lines: Vec<&str> = s.split('\n').collect();
    // let mutevent = None;
    let mut data = None;

    for line in lines {
        if line.starts_with(""data:"") {
            data = Some(line.trim_start_matches(""data:"").trim());
        }
    }

    match data {
        Some(s) => {
            if s.trim() == ""[DONE]"" {
                return None;
            }

            match serde_json::from_str(s) {
                Ok(chunk) => Some(chunk),
                Err(e) => {
                    log(format!(
                        ""[ERROR] Fail to parse SSE data. Reason: {msg}. Data: {data}"",
                        msg = e,
                        data = s
                    ));
                    None
                }
            }
        }
        _ => None,
    }
}

```","Q: 1. What is the name of the model used in this code?
A: The name of the model used in this code is not explicitly mentioned as a fixed string, but rather it's provided as an argument to the `Cli` parser. The model name can be specified by the user when running the program using the command-line flag `--model_name`. 

So, the actual model name used in the execution of the code would depend on how the user invokes the program."
"```rust:llama-chat/src/main.rs
use anyhow::bail;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use either::{Left, Right};
use endpoints::chat::{
    ChatCompletionChunk, ChatCompletionRequestBuilder, ChatCompletionRequestMessage,
    ChatCompletionRequestSampling, ChatCompletionUserMessageContent,
};
use futures::TryStreamExt;
use llama_core::{init_core_context, MetadataBuilder};
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Debug, Parser)]
#[command(author, about, version, long_about=None)]
struct Cli {
    /// Model name
    #[arg(short, long, default_value = ""default"")]
    model_name: String,
    /// Model alias
    #[arg(short = 'a', long, default_value = ""default"")]
    model_alias: String,
    /// Size of the prompt context
    #[arg(short, long, default_value = ""512"")]
    ctx_size: u64,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Batch size for prompt processing
    #[arg(short, long, default_value = ""512"")]
    batch_size: u64,
    /// Temperature for sampling
    #[arg(long, conflicts_with = ""top_p"")]
    temp: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, conflicts_with = ""temp"")]
    top_p: Option<f64>,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Sets the prompt template.
    #[arg(short, long, value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: PromptTemplateType,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// System prompt message string.
    #[arg(short, long)]
    system_prompt: Option<String>,
    /// Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Print all log information to stdout
    #[arg(long)]
    log_all: bool,
    /// enable streaming stdout
    #[arg(long, default_value = ""false"")]
    disable_stream: bool,
}

#[allow(clippy::needless_return)]
#[allow(unreachable_code)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> anyhow::Result<()> {
    // get the environment variable `PLUGIN_DEBUG`
    let plugin_debug = std::env::var(""PLUGIN_DEBUG"").unwrap_or_default();
    let plugin_debug = match plugin_debug.is_empty() {
        true => false,
        false => plugin_debug.to_lowercase().parse::<bool>().unwrap_or(false),
    };

    // parse the command line arguments
    let cli = Cli::parse();

    // log version
    log(format!(
        ""\n[INFO] llama-chat version: {}"",
        env!(""CARGO_PKG_VERSION"")
    ));

    // log the cli options
    log(format!(""[INFO] Model name: {}"", &cli.model_name));
    log(format!(""[INFO] Model alias: {}"", &cli.model_alias));
    log(format!(""[INFO] Prompt template: {}"", &cli.prompt_template));
    // ctx size
    log(format!(""[INFO] Context size: {}"", &cli.ctx_size));
    // reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        log(format!(""[INFO] reverse prompt: {}"", reverse_prompt));
    }
    // system prompt
    if let Some(system_prompt) = &cli.system_prompt {
        log(format!(""[INFO] system prompt: {}"", system_prompt));
    }
    // n_predict
    log(format!(
        ""[INFO] Number of tokens to predict: {}"",
        &cli.n_predict
    ));
    // n_gpu_layers
    log(format!(
        ""[INFO] Number of layers to run on the GPU: {}"",
        &cli.n_gpu_layers
    ));
    // main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        log(format!(""[INFO] Main GPU to use: {}"", main_gpu));
    }
    // tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        log(format!(""[INFO] Tensor split: {}"", tensor_split));
    }
    log(format!(""[INFO] Threads: {}"", &cli.threads));
    // no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        log(format!(
            ""[INFO] Disable memory mapping for file access of chat models : {}"",
            &no_mmap
        ));
    }
    // batch size
    log(format!(
        ""[INFO] Batch size for prompt processing: {}"",
        &cli.batch_size
    ));
    // temp and top_p
    if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
    }
    // repeat penalty
    log(format!(
        ""[INFO] Penalize repeat sequence of tokens: {}"",
        &cli.repeat_penalty
    ));
    // presence penalty
    log(format!(
        ""[INFO] Presence penalty (0.0 = disabled): {}"",
        &cli.presence_penalty
    ));
    // frequency penalty
    log(format!(
        ""[INFO] Frequency penalty (0.0 = disabled): {}"",
        &cli.frequency_penalty
    ));
    // grammar
    log(format!(""[INFO] BNF-like grammar: {}"", &cli.grammar));
    // json schema
    if let Some(json_schema) = &cli.json_schema {
        log(format!(""[INFO] JSON schema: {}"", json_schema));
    }
    // log prompts
    log(format!(""[INFO] Enable prompt log: {}"", &cli.log_prompts));
    // log statistics
    log(format!(""[INFO] Enable plugin log: {}"", &cli.log_stat));

    // create a MetadataBuilder instance
    let builder = MetadataBuilder::new(&cli.model_name, &cli.model_alias, cli.prompt_template)
        .with_ctx_size(cli.ctx_size)
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_batch_size(cli.batch_size)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .enable_prompts_log(cli.log_prompts || cli.log_all)
        .enable_plugin_log(cli.log_stat || cli.log_all)
        .enable_debug_log(plugin_debug);
    // temp and top_p
    let builder = if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
        builder.with_top_p(top_p)
    } else {
        let temp = cli.temp.unwrap();
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    };
    // create a Metadata instance
    let metadata = builder.build();

    // initialize the core context
    init_core_context(Some(&[metadata]), None)?;

    // get the plugin version info
    let plugin_info = llama_core::get_plugin_info()?;
    log(format!(
        ""[INFO] Wasi-nn-ggml plugin: b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    ));

    // create a ChatCompletionRequestSampling instance
    let sampling = if cli.temp.is_none() && cli.top_p.is_none() {
        ChatCompletionRequestSampling::Temperature(1.0)
    } else if let Some(temp) = cli.temp {
        ChatCompletionRequestSampling::Temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        ChatCompletionRequestSampling::TopP(top_p)
    } else {
        let temp = cli.temp.unwrap();
        ChatCompletionRequestSampling::Temperature(temp)
    };

    // create a chat request
    let mut chat_request = ChatCompletionRequestBuilder::new(&cli.model_name, vec![])
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_sampling(sampling)
        .enable_stream(!cli.disable_stream)
        .build();

    // add system message if provided
    if let Some(system_prompt) = &cli.system_prompt {
        let system_message = ChatCompletionRequestMessage::new_system_message(system_prompt, None);

        chat_request.messages.push(system_message);
    }

    let readme = ""
================================== Running in interactive mode. ===================================\n
    - Press [Ctrl+C] to interject at any time.
    - Press [Return] to end the input.
    - For multi-line inputs, end each line with '\\' and press [Return] to get another line.\n"";
    log(readme);

    loop {
        println!(""\n[You]: "");
        let user_input = read_input();

        // put the user message into the messages sequence of chat_request
        let user_message = ChatCompletionRequestMessage::new_user_message(
            ChatCompletionUserMessageContent::Text(user_input),
            None,
        );

        chat_request.messages.push(user_message);

        if cli.log_stat || cli.log_all {
            print_log_begin_separator(""STATISTICS (Set Input)"", Some(""*""), None);
        }

        if cli.log_stat || cli.log_all {
            print_log_end_separator(Some(""*""), None);
        }

        println!(""\n[Bot]:"");
        let mut assistant_answer = String::new();
        match llama_core::chat::chat(&mut chat_request).await {
            Ok(res) => match res {
                Left(mut stream) => {
                    while let Some(data) = stream.try_next().await? {
                        if let Some(chunk) = parse_sse_event(&data) {
                            if let Some(content) = &chunk.choices[0].delta.content {
                                if content.is_empty() {
                                    continue;
                                }
                                if assistant_answer.is_empty() {
                                    let content = content.trim_start();
                                    print!(""{}"", content);
                                    assistant_answer.push_str(content);
                                } else {
                                    print!(""{content}"");
                                    assistant_answer.push_str(content);
                                }
                                io::stdout().flush().unwrap();
                            }
                        }
                    }
                    println!();
                }
                Right(completion) => {
                    let chat_completion = completion.choices[0]
                        .message
                        .content
                        .to_owned()
                        .unwrap_or_default();
                    println!(""{chat_completion}"");
                    assistant_answer = chat_completion;
                }
            },
            Err(e) => {
                bail!(""Fail to generate chat completion. Reason: {msg}"", msg = e)
            }
        };

        let assistant_message = ChatCompletionRequestMessage::new_assistant_message(
            Some(assistant_answer.trim().to_string()),
            None,
            None,
        );
        chat_request.messages.push(assistant_message);
    }

    Ok(())
}

// For single line input, just press [Return] to end the input.
// For multi-line input, end your input with '\\' and press [Return].
//
// For example:
//  [You]:
//  what is the capital of France?[Return]
//
//  [You]:
//  Count the words in the following sentence: \[Return]
//  \[Return]
//  You can use Git to save new files and any changes to already existing files as a bundle of changes called a commit, which can be thought of as a revision to your project.[Return]
//
fn read_input() -> String {
    let mut answer = String::new();
    loop {
        let mut temp = String::new();
        std::io::stdin()
            .read_line(&mut temp)
            .expect(""The read bytes are not valid UTF-8"");

        if temp.ends_with(""\\\n"") {
            temp.pop();
            temp.pop();
            temp.push('\n');
            answer.push_str(&temp);
            continue;
        } else if temp.ends_with('\n') {
            answer.push_str(&temp);
            return answer;
        } else {
            return answer;
        }
    }
}

fn print_log_begin_separator(
    title: impl AsRef<str>,
    ch: Option<&str>,
    len: Option<usize>,
) -> usize {
    let title = format!("" [LOG: {}] "", title.as_ref());

    let total_len: usize = len.unwrap_or(100);
    let separator_len: usize = (total_len - title.len()) / 2;

    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push_str(&title);
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push('\n');
    println!(""{}"", separator);
    total_len
}

fn print_log_end_separator(ch: Option<&str>, len: Option<usize>) {
    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(len.unwrap_or(100)).as_str());
    separator.push('\n');
    println!(""{}"", separator);
}

#[derive(Debug, Default, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    // #[serde(rename = ""enable-debug-log"")]
    // pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    // pub mmproj: String,
    // pub image: String,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    // #[serde(rename = ""main-gpu"")]
    // pub main_gpu: u64,
    // #[serde(rename = ""tensor-split"")]
    // pub tensor_split: String,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,

    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,
}

fn log(msg: impl std::fmt::Display) {
    println!(""{}"", msg);
}

fn parse_sse_event(s: &str) -> Option<ChatCompletionChunk> {
    let lines: Vec<&str> = s.split('\n').collect();
    // let mutevent = None;
    let mut data = None;

    for line in lines {
        if line.starts_with(""data:"") {
            data = Some(line.trim_start_matches(""data:"").trim());
        }
    }

    match data {
        Some(s) => {
            if s.trim() == ""[DONE]"" {
                return None;
            }

            match serde_json::from_str(s) {
                Ok(chunk) => Some(chunk),
                Err(e) => {
                    log(format!(
                        ""[ERROR] Fail to parse SSE data. Reason: {msg}. Data: {data}"",
                        msg = e,
                        data = s
                    ));
                    None
                }
            }
        }
        _ => None,
    }
}

```","Q: 2. How does the plugin version info get retrieved in the code?
A: The plugin version info is retrieved in the code by calling the `get_plugin_info` function from the `llama_core` module.

Here's a snippet of the relevant code:

```rust
// get the plugin version info
let plugin_info = llama_core::get_plugin_info()?;
```

This line calls the `get_plugin_info` function and assigns the result to a variable named `plugin_info`. The `?` operator is used here, which will return an error if the function call fails.

Looking at the implementation of `get_plugin_info`, it appears that this function uses the `wasi-nn-ggml` plugin's internal API to retrieve information about itself. This might involve reading a file or using some other form of communication with the plugin's code.

Here is an excerpt of the definition for the `get_plugin_info()` method:

```rust
pub fn get_plugin_info() -> Result<PluginInfo, Error> {
    // Implementation-specific code here to fetch plugin info from within llama_core module.
}
```

This function returns a `Result` type, which can be either `Ok(PluginInfo)` or `Err(Error)`. If an error occurs while fetching the plugin info, the `?` operator will propagate this error up the call stack.

Without seeing more of the implementation details for `get_plugin_info()`, we can't provide further information. However, in general terms, the retrieval of plugin version info involves calling a function that uses some form of internal API to access data stored within the plugin itself or in another part of the codebase.

**Bonus Tip:** In Rust, when using the `?` operator to propagate errors from one function call to another, it's essential to handle these potential failures by adding error handling mechanisms throughout your code. This way, you can provide meaningful error messages and maintain robustness in case things don't go as planned.

To do this effectively:

1.  Ensure that all functions have a clear understanding of what kinds of errors might occur during their execution.
2.  Use `Result` or `Option` types to handle potential failures and propagate them up the call stack using the `?` operator.
3.  In your main entry point (or other high-level logic), add explicit error handling mechanisms, such as matching on specific error variants, to provide meaningful feedback in case something goes wrong.

This way, you can write more robust code that's better equipped to handle unexpected situations while still maintaining a clear and concise design."
"```rust:llama-chat/src/main.rs
use anyhow::bail;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use either::{Left, Right};
use endpoints::chat::{
    ChatCompletionChunk, ChatCompletionRequestBuilder, ChatCompletionRequestMessage,
    ChatCompletionRequestSampling, ChatCompletionUserMessageContent,
};
use futures::TryStreamExt;
use llama_core::{init_core_context, MetadataBuilder};
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Debug, Parser)]
#[command(author, about, version, long_about=None)]
struct Cli {
    /// Model name
    #[arg(short, long, default_value = ""default"")]
    model_name: String,
    /// Model alias
    #[arg(short = 'a', long, default_value = ""default"")]
    model_alias: String,
    /// Size of the prompt context
    #[arg(short, long, default_value = ""512"")]
    ctx_size: u64,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Batch size for prompt processing
    #[arg(short, long, default_value = ""512"")]
    batch_size: u64,
    /// Temperature for sampling
    #[arg(long, conflicts_with = ""top_p"")]
    temp: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, conflicts_with = ""temp"")]
    top_p: Option<f64>,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Sets the prompt template.
    #[arg(short, long, value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: PromptTemplateType,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// System prompt message string.
    #[arg(short, long)]
    system_prompt: Option<String>,
    /// Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Print all log information to stdout
    #[arg(long)]
    log_all: bool,
    /// enable streaming stdout
    #[arg(long, default_value = ""false"")]
    disable_stream: bool,
}

#[allow(clippy::needless_return)]
#[allow(unreachable_code)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> anyhow::Result<()> {
    // get the environment variable `PLUGIN_DEBUG`
    let plugin_debug = std::env::var(""PLUGIN_DEBUG"").unwrap_or_default();
    let plugin_debug = match plugin_debug.is_empty() {
        true => false,
        false => plugin_debug.to_lowercase().parse::<bool>().unwrap_or(false),
    };

    // parse the command line arguments
    let cli = Cli::parse();

    // log version
    log(format!(
        ""\n[INFO] llama-chat version: {}"",
        env!(""CARGO_PKG_VERSION"")
    ));

    // log the cli options
    log(format!(""[INFO] Model name: {}"", &cli.model_name));
    log(format!(""[INFO] Model alias: {}"", &cli.model_alias));
    log(format!(""[INFO] Prompt template: {}"", &cli.prompt_template));
    // ctx size
    log(format!(""[INFO] Context size: {}"", &cli.ctx_size));
    // reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        log(format!(""[INFO] reverse prompt: {}"", reverse_prompt));
    }
    // system prompt
    if let Some(system_prompt) = &cli.system_prompt {
        log(format!(""[INFO] system prompt: {}"", system_prompt));
    }
    // n_predict
    log(format!(
        ""[INFO] Number of tokens to predict: {}"",
        &cli.n_predict
    ));
    // n_gpu_layers
    log(format!(
        ""[INFO] Number of layers to run on the GPU: {}"",
        &cli.n_gpu_layers
    ));
    // main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        log(format!(""[INFO] Main GPU to use: {}"", main_gpu));
    }
    // tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        log(format!(""[INFO] Tensor split: {}"", tensor_split));
    }
    log(format!(""[INFO] Threads: {}"", &cli.threads));
    // no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        log(format!(
            ""[INFO] Disable memory mapping for file access of chat models : {}"",
            &no_mmap
        ));
    }
    // batch size
    log(format!(
        ""[INFO] Batch size for prompt processing: {}"",
        &cli.batch_size
    ));
    // temp and top_p
    if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
    }
    // repeat penalty
    log(format!(
        ""[INFO] Penalize repeat sequence of tokens: {}"",
        &cli.repeat_penalty
    ));
    // presence penalty
    log(format!(
        ""[INFO] Presence penalty (0.0 = disabled): {}"",
        &cli.presence_penalty
    ));
    // frequency penalty
    log(format!(
        ""[INFO] Frequency penalty (0.0 = disabled): {}"",
        &cli.frequency_penalty
    ));
    // grammar
    log(format!(""[INFO] BNF-like grammar: {}"", &cli.grammar));
    // json schema
    if let Some(json_schema) = &cli.json_schema {
        log(format!(""[INFO] JSON schema: {}"", json_schema));
    }
    // log prompts
    log(format!(""[INFO] Enable prompt log: {}"", &cli.log_prompts));
    // log statistics
    log(format!(""[INFO] Enable plugin log: {}"", &cli.log_stat));

    // create a MetadataBuilder instance
    let builder = MetadataBuilder::new(&cli.model_name, &cli.model_alias, cli.prompt_template)
        .with_ctx_size(cli.ctx_size)
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_batch_size(cli.batch_size)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .enable_prompts_log(cli.log_prompts || cli.log_all)
        .enable_plugin_log(cli.log_stat || cli.log_all)
        .enable_debug_log(plugin_debug);
    // temp and top_p
    let builder = if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
        builder.with_top_p(top_p)
    } else {
        let temp = cli.temp.unwrap();
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    };
    // create a Metadata instance
    let metadata = builder.build();

    // initialize the core context
    init_core_context(Some(&[metadata]), None)?;

    // get the plugin version info
    let plugin_info = llama_core::get_plugin_info()?;
    log(format!(
        ""[INFO] Wasi-nn-ggml plugin: b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    ));

    // create a ChatCompletionRequestSampling instance
    let sampling = if cli.temp.is_none() && cli.top_p.is_none() {
        ChatCompletionRequestSampling::Temperature(1.0)
    } else if let Some(temp) = cli.temp {
        ChatCompletionRequestSampling::Temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        ChatCompletionRequestSampling::TopP(top_p)
    } else {
        let temp = cli.temp.unwrap();
        ChatCompletionRequestSampling::Temperature(temp)
    };

    // create a chat request
    let mut chat_request = ChatCompletionRequestBuilder::new(&cli.model_name, vec![])
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_sampling(sampling)
        .enable_stream(!cli.disable_stream)
        .build();

    // add system message if provided
    if let Some(system_prompt) = &cli.system_prompt {
        let system_message = ChatCompletionRequestMessage::new_system_message(system_prompt, None);

        chat_request.messages.push(system_message);
    }

    let readme = ""
================================== Running in interactive mode. ===================================\n
    - Press [Ctrl+C] to interject at any time.
    - Press [Return] to end the input.
    - For multi-line inputs, end each line with '\\' and press [Return] to get another line.\n"";
    log(readme);

    loop {
        println!(""\n[You]: "");
        let user_input = read_input();

        // put the user message into the messages sequence of chat_request
        let user_message = ChatCompletionRequestMessage::new_user_message(
            ChatCompletionUserMessageContent::Text(user_input),
            None,
        );

        chat_request.messages.push(user_message);

        if cli.log_stat || cli.log_all {
            print_log_begin_separator(""STATISTICS (Set Input)"", Some(""*""), None);
        }

        if cli.log_stat || cli.log_all {
            print_log_end_separator(Some(""*""), None);
        }

        println!(""\n[Bot]:"");
        let mut assistant_answer = String::new();
        match llama_core::chat::chat(&mut chat_request).await {
            Ok(res) => match res {
                Left(mut stream) => {
                    while let Some(data) = stream.try_next().await? {
                        if let Some(chunk) = parse_sse_event(&data) {
                            if let Some(content) = &chunk.choices[0].delta.content {
                                if content.is_empty() {
                                    continue;
                                }
                                if assistant_answer.is_empty() {
                                    let content = content.trim_start();
                                    print!(""{}"", content);
                                    assistant_answer.push_str(content);
                                } else {
                                    print!(""{content}"");
                                    assistant_answer.push_str(content);
                                }
                                io::stdout().flush().unwrap();
                            }
                        }
                    }
                    println!();
                }
                Right(completion) => {
                    let chat_completion = completion.choices[0]
                        .message
                        .content
                        .to_owned()
                        .unwrap_or_default();
                    println!(""{chat_completion}"");
                    assistant_answer = chat_completion;
                }
            },
            Err(e) => {
                bail!(""Fail to generate chat completion. Reason: {msg}"", msg = e)
            }
        };

        let assistant_message = ChatCompletionRequestMessage::new_assistant_message(
            Some(assistant_answer.trim().to_string()),
            None,
            None,
        );
        chat_request.messages.push(assistant_message);
    }

    Ok(())
}

// For single line input, just press [Return] to end the input.
// For multi-line input, end your input with '\\' and press [Return].
//
// For example:
//  [You]:
//  what is the capital of France?[Return]
//
//  [You]:
//  Count the words in the following sentence: \[Return]
//  \[Return]
//  You can use Git to save new files and any changes to already existing files as a bundle of changes called a commit, which can be thought of as a revision to your project.[Return]
//
fn read_input() -> String {
    let mut answer = String::new();
    loop {
        let mut temp = String::new();
        std::io::stdin()
            .read_line(&mut temp)
            .expect(""The read bytes are not valid UTF-8"");

        if temp.ends_with(""\\\n"") {
            temp.pop();
            temp.pop();
            temp.push('\n');
            answer.push_str(&temp);
            continue;
        } else if temp.ends_with('\n') {
            answer.push_str(&temp);
            return answer;
        } else {
            return answer;
        }
    }
}

fn print_log_begin_separator(
    title: impl AsRef<str>,
    ch: Option<&str>,
    len: Option<usize>,
) -> usize {
    let title = format!("" [LOG: {}] "", title.as_ref());

    let total_len: usize = len.unwrap_or(100);
    let separator_len: usize = (total_len - title.len()) / 2;

    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push_str(&title);
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push('\n');
    println!(""{}"", separator);
    total_len
}

fn print_log_end_separator(ch: Option<&str>, len: Option<usize>) {
    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(len.unwrap_or(100)).as_str());
    separator.push('\n');
    println!(""{}"", separator);
}

#[derive(Debug, Default, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    // #[serde(rename = ""enable-debug-log"")]
    // pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    // pub mmproj: String,
    // pub image: String,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    // #[serde(rename = ""main-gpu"")]
    // pub main_gpu: u64,
    // #[serde(rename = ""tensor-split"")]
    // pub tensor_split: String,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,

    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,
}

fn log(msg: impl std::fmt::Display) {
    println!(""{}"", msg);
}

fn parse_sse_event(s: &str) -> Option<ChatCompletionChunk> {
    let lines: Vec<&str> = s.split('\n').collect();
    // let mutevent = None;
    let mut data = None;

    for line in lines {
        if line.starts_with(""data:"") {
            data = Some(line.trim_start_matches(""data:"").trim());
        }
    }

    match data {
        Some(s) => {
            if s.trim() == ""[DONE]"" {
                return None;
            }

            match serde_json::from_str(s) {
                Ok(chunk) => Some(chunk),
                Err(e) => {
                    log(format!(
                        ""[ERROR] Fail to parse SSE data. Reason: {msg}. Data: {data}"",
                        msg = e,
                        data = s
                    ));
                    None
                }
            }
        }
        _ => None,
    }
}

```","Q: 3. Where do the user messages and assistant answers get printed to?
A: The user messages and assistant answers get printed to the console. 

In Rust, when you use `println!`, it automatically prints to the standard output (stdout), which is usually the console where your program was run from.

So, in this case:

- The user's input (`[You]:`) gets printed by this line: `print!(""\n[You]: "");`
- The assistant's answer (`[Bot]:`) gets printed by these lines:
  ```rust
println!(""\n[Bot]:"");
let mut assistant_answer = String::new();
match llama_core::chat::chat(&mut chat_request).await {
    // ...
}
```

And when we want to print the assistant's answer inside a loop, we do this:

```rust
while let Some(data) = stream.try_next().await? {
    if let Some(chunk) = parse_sse_event(&data) {
        if let Some(content) = &chunk.choices[0].delta.content {
            if content.is_empty() {
                continue;
            }
            if assistant_answer.is_empty() {
                let content = content.trim_start();
                print!(""{}"", content);
                assistant_answer.push_str(content);
            } else {
                print!(""{content}"");
                assistant_answer.push_str(content);
            }
            io::stdout().flush().unwrap(); // This is what prints to the console
        }
    }
}
```

This way, we can see the chat conversation unfolding in real-time in the console.

**Bonus Tip**: If you want to save this conversation for later or analyze it programmatically, consider storing it in a file instead of printing it to the console. You could use `io::BufWriter` and write each line or message to the file as it's printed, like so:

```rust
let mut writer = BufWriter::new(File::create(""conversation.txt"").unwrap());
// ...
writer.write_all(line.as_bytes()).unwrap();
writer.flush().unwrap();
```

This way, you can keep a record of your conversation and reuse it later in any way you need."
"```rust:llama-chat/src/main.rs
use anyhow::bail;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use either::{Left, Right};
use endpoints::chat::{
    ChatCompletionChunk, ChatCompletionRequestBuilder, ChatCompletionRequestMessage,
    ChatCompletionRequestSampling, ChatCompletionUserMessageContent,
};
use futures::TryStreamExt;
use llama_core::{init_core_context, MetadataBuilder};
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Debug, Parser)]
#[command(author, about, version, long_about=None)]
struct Cli {
    /// Model name
    #[arg(short, long, default_value = ""default"")]
    model_name: String,
    /// Model alias
    #[arg(short = 'a', long, default_value = ""default"")]
    model_alias: String,
    /// Size of the prompt context
    #[arg(short, long, default_value = ""512"")]
    ctx_size: u64,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Batch size for prompt processing
    #[arg(short, long, default_value = ""512"")]
    batch_size: u64,
    /// Temperature for sampling
    #[arg(long, conflicts_with = ""top_p"")]
    temp: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, conflicts_with = ""temp"")]
    top_p: Option<f64>,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Sets the prompt template.
    #[arg(short, long, value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: PromptTemplateType,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// System prompt message string.
    #[arg(short, long)]
    system_prompt: Option<String>,
    /// Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Print all log information to stdout
    #[arg(long)]
    log_all: bool,
    /// enable streaming stdout
    #[arg(long, default_value = ""false"")]
    disable_stream: bool,
}

#[allow(clippy::needless_return)]
#[allow(unreachable_code)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> anyhow::Result<()> {
    // get the environment variable `PLUGIN_DEBUG`
    let plugin_debug = std::env::var(""PLUGIN_DEBUG"").unwrap_or_default();
    let plugin_debug = match plugin_debug.is_empty() {
        true => false,
        false => plugin_debug.to_lowercase().parse::<bool>().unwrap_or(false),
    };

    // parse the command line arguments
    let cli = Cli::parse();

    // log version
    log(format!(
        ""\n[INFO] llama-chat version: {}"",
        env!(""CARGO_PKG_VERSION"")
    ));

    // log the cli options
    log(format!(""[INFO] Model name: {}"", &cli.model_name));
    log(format!(""[INFO] Model alias: {}"", &cli.model_alias));
    log(format!(""[INFO] Prompt template: {}"", &cli.prompt_template));
    // ctx size
    log(format!(""[INFO] Context size: {}"", &cli.ctx_size));
    // reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        log(format!(""[INFO] reverse prompt: {}"", reverse_prompt));
    }
    // system prompt
    if let Some(system_prompt) = &cli.system_prompt {
        log(format!(""[INFO] system prompt: {}"", system_prompt));
    }
    // n_predict
    log(format!(
        ""[INFO] Number of tokens to predict: {}"",
        &cli.n_predict
    ));
    // n_gpu_layers
    log(format!(
        ""[INFO] Number of layers to run on the GPU: {}"",
        &cli.n_gpu_layers
    ));
    // main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        log(format!(""[INFO] Main GPU to use: {}"", main_gpu));
    }
    // tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        log(format!(""[INFO] Tensor split: {}"", tensor_split));
    }
    log(format!(""[INFO] Threads: {}"", &cli.threads));
    // no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        log(format!(
            ""[INFO] Disable memory mapping for file access of chat models : {}"",
            &no_mmap
        ));
    }
    // batch size
    log(format!(
        ""[INFO] Batch size for prompt processing: {}"",
        &cli.batch_size
    ));
    // temp and top_p
    if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
    }
    // repeat penalty
    log(format!(
        ""[INFO] Penalize repeat sequence of tokens: {}"",
        &cli.repeat_penalty
    ));
    // presence penalty
    log(format!(
        ""[INFO] Presence penalty (0.0 = disabled): {}"",
        &cli.presence_penalty
    ));
    // frequency penalty
    log(format!(
        ""[INFO] Frequency penalty (0.0 = disabled): {}"",
        &cli.frequency_penalty
    ));
    // grammar
    log(format!(""[INFO] BNF-like grammar: {}"", &cli.grammar));
    // json schema
    if let Some(json_schema) = &cli.json_schema {
        log(format!(""[INFO] JSON schema: {}"", json_schema));
    }
    // log prompts
    log(format!(""[INFO] Enable prompt log: {}"", &cli.log_prompts));
    // log statistics
    log(format!(""[INFO] Enable plugin log: {}"", &cli.log_stat));

    // create a MetadataBuilder instance
    let builder = MetadataBuilder::new(&cli.model_name, &cli.model_alias, cli.prompt_template)
        .with_ctx_size(cli.ctx_size)
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_batch_size(cli.batch_size)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .enable_prompts_log(cli.log_prompts || cli.log_all)
        .enable_plugin_log(cli.log_stat || cli.log_all)
        .enable_debug_log(plugin_debug);
    // temp and top_p
    let builder = if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
        builder.with_top_p(top_p)
    } else {
        let temp = cli.temp.unwrap();
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    };
    // create a Metadata instance
    let metadata = builder.build();

    // initialize the core context
    init_core_context(Some(&[metadata]), None)?;

    // get the plugin version info
    let plugin_info = llama_core::get_plugin_info()?;
    log(format!(
        ""[INFO] Wasi-nn-ggml plugin: b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    ));

    // create a ChatCompletionRequestSampling instance
    let sampling = if cli.temp.is_none() && cli.top_p.is_none() {
        ChatCompletionRequestSampling::Temperature(1.0)
    } else if let Some(temp) = cli.temp {
        ChatCompletionRequestSampling::Temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        ChatCompletionRequestSampling::TopP(top_p)
    } else {
        let temp = cli.temp.unwrap();
        ChatCompletionRequestSampling::Temperature(temp)
    };

    // create a chat request
    let mut chat_request = ChatCompletionRequestBuilder::new(&cli.model_name, vec![])
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_sampling(sampling)
        .enable_stream(!cli.disable_stream)
        .build();

    // add system message if provided
    if let Some(system_prompt) = &cli.system_prompt {
        let system_message = ChatCompletionRequestMessage::new_system_message(system_prompt, None);

        chat_request.messages.push(system_message);
    }

    let readme = ""
================================== Running in interactive mode. ===================================\n
    - Press [Ctrl+C] to interject at any time.
    - Press [Return] to end the input.
    - For multi-line inputs, end each line with '\\' and press [Return] to get another line.\n"";
    log(readme);

    loop {
        println!(""\n[You]: "");
        let user_input = read_input();

        // put the user message into the messages sequence of chat_request
        let user_message = ChatCompletionRequestMessage::new_user_message(
            ChatCompletionUserMessageContent::Text(user_input),
            None,
        );

        chat_request.messages.push(user_message);

        if cli.log_stat || cli.log_all {
            print_log_begin_separator(""STATISTICS (Set Input)"", Some(""*""), None);
        }

        if cli.log_stat || cli.log_all {
            print_log_end_separator(Some(""*""), None);
        }

        println!(""\n[Bot]:"");
        let mut assistant_answer = String::new();
        match llama_core::chat::chat(&mut chat_request).await {
            Ok(res) => match res {
                Left(mut stream) => {
                    while let Some(data) = stream.try_next().await? {
                        if let Some(chunk) = parse_sse_event(&data) {
                            if let Some(content) = &chunk.choices[0].delta.content {
                                if content.is_empty() {
                                    continue;
                                }
                                if assistant_answer.is_empty() {
                                    let content = content.trim_start();
                                    print!(""{}"", content);
                                    assistant_answer.push_str(content);
                                } else {
                                    print!(""{content}"");
                                    assistant_answer.push_str(content);
                                }
                                io::stdout().flush().unwrap();
                            }
                        }
                    }
                    println!();
                }
                Right(completion) => {
                    let chat_completion = completion.choices[0]
                        .message
                        .content
                        .to_owned()
                        .unwrap_or_default();
                    println!(""{chat_completion}"");
                    assistant_answer = chat_completion;
                }
            },
            Err(e) => {
                bail!(""Fail to generate chat completion. Reason: {msg}"", msg = e)
            }
        };

        let assistant_message = ChatCompletionRequestMessage::new_assistant_message(
            Some(assistant_answer.trim().to_string()),
            None,
            None,
        );
        chat_request.messages.push(assistant_message);
    }

    Ok(())
}

// For single line input, just press [Return] to end the input.
// For multi-line input, end your input with '\\' and press [Return].
//
// For example:
//  [You]:
//  what is the capital of France?[Return]
//
//  [You]:
//  Count the words in the following sentence: \[Return]
//  \[Return]
//  You can use Git to save new files and any changes to already existing files as a bundle of changes called a commit, which can be thought of as a revision to your project.[Return]
//
fn read_input() -> String {
    let mut answer = String::new();
    loop {
        let mut temp = String::new();
        std::io::stdin()
            .read_line(&mut temp)
            .expect(""The read bytes are not valid UTF-8"");

        if temp.ends_with(""\\\n"") {
            temp.pop();
            temp.pop();
            temp.push('\n');
            answer.push_str(&temp);
            continue;
        } else if temp.ends_with('\n') {
            answer.push_str(&temp);
            return answer;
        } else {
            return answer;
        }
    }
}

fn print_log_begin_separator(
    title: impl AsRef<str>,
    ch: Option<&str>,
    len: Option<usize>,
) -> usize {
    let title = format!("" [LOG: {}] "", title.as_ref());

    let total_len: usize = len.unwrap_or(100);
    let separator_len: usize = (total_len - title.len()) / 2;

    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push_str(&title);
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push('\n');
    println!(""{}"", separator);
    total_len
}

fn print_log_end_separator(ch: Option<&str>, len: Option<usize>) {
    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(len.unwrap_or(100)).as_str());
    separator.push('\n');
    println!(""{}"", separator);
}

#[derive(Debug, Default, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    // #[serde(rename = ""enable-debug-log"")]
    // pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    // pub mmproj: String,
    // pub image: String,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    // #[serde(rename = ""main-gpu"")]
    // pub main_gpu: u64,
    // #[serde(rename = ""tensor-split"")]
    // pub tensor_split: String,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,

    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,
}

fn log(msg: impl std::fmt::Display) {
    println!(""{}"", msg);
}

fn parse_sse_event(s: &str) -> Option<ChatCompletionChunk> {
    let lines: Vec<&str> = s.split('\n').collect();
    // let mutevent = None;
    let mut data = None;

    for line in lines {
        if line.starts_with(""data:"") {
            data = Some(line.trim_start_matches(""data:"").trim());
        }
    }

    match data {
        Some(s) => {
            if s.trim() == ""[DONE]"" {
                return None;
            }

            match serde_json::from_str(s) {
                Ok(chunk) => Some(chunk),
                Err(e) => {
                    log(format!(
                        ""[ERROR] Fail to parse SSE data. Reason: {msg}. Data: {data}"",
                        msg = e,
                        data = s
                    ));
                    None
                }
            }
        }
        _ => None,
    }
}

```","Q: 4. What are the possible values for the ""n_gpu_layers"" parameter in the metadata?
A: Based on the provided code, the ""n_gpu_layers"" parameter in the metadata is a `u64` type. This means that it can take any non-negative integer value.

In other words, the possible values for ""n_gpu_layers"" are:

* 0
* 1
* 2
* ...
* Any positive integer up to the maximum value that can be represented by a `u64` (which is typically 18,446,744,073,709,551,616).

The actual valid range of values for ""n_gpu_layers"" might depend on the specific use case and requirements of your application. However, as a general rule, it's always good practice to specify a reasonable maximum value that is likely to be needed in most cases.

**Bonus Tip:** If you need to restrict the possible values for ""n_gpu_layers"" to a smaller range (e.g., 0-10), you can add some error handling code to enforce this constraint when parsing the metadata. For example:

```rust
match metadata.n_gpu_layers {
    0..=10 => {}
    _ => log(format!(""Invalid value for n_gpu_layers: {}"", metadata.n_gpu_layers))
}
```"
"```rust:llama-chat/src/main.rs
use anyhow::bail;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use either::{Left, Right};
use endpoints::chat::{
    ChatCompletionChunk, ChatCompletionRequestBuilder, ChatCompletionRequestMessage,
    ChatCompletionRequestSampling, ChatCompletionUserMessageContent,
};
use futures::TryStreamExt;
use llama_core::{init_core_context, MetadataBuilder};
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Debug, Parser)]
#[command(author, about, version, long_about=None)]
struct Cli {
    /// Model name
    #[arg(short, long, default_value = ""default"")]
    model_name: String,
    /// Model alias
    #[arg(short = 'a', long, default_value = ""default"")]
    model_alias: String,
    /// Size of the prompt context
    #[arg(short, long, default_value = ""512"")]
    ctx_size: u64,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Batch size for prompt processing
    #[arg(short, long, default_value = ""512"")]
    batch_size: u64,
    /// Temperature for sampling
    #[arg(long, conflicts_with = ""top_p"")]
    temp: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, conflicts_with = ""temp"")]
    top_p: Option<f64>,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Sets the prompt template.
    #[arg(short, long, value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: PromptTemplateType,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// System prompt message string.
    #[arg(short, long)]
    system_prompt: Option<String>,
    /// Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Print all log information to stdout
    #[arg(long)]
    log_all: bool,
    /// enable streaming stdout
    #[arg(long, default_value = ""false"")]
    disable_stream: bool,
}

#[allow(clippy::needless_return)]
#[allow(unreachable_code)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> anyhow::Result<()> {
    // get the environment variable `PLUGIN_DEBUG`
    let plugin_debug = std::env::var(""PLUGIN_DEBUG"").unwrap_or_default();
    let plugin_debug = match plugin_debug.is_empty() {
        true => false,
        false => plugin_debug.to_lowercase().parse::<bool>().unwrap_or(false),
    };

    // parse the command line arguments
    let cli = Cli::parse();

    // log version
    log(format!(
        ""\n[INFO] llama-chat version: {}"",
        env!(""CARGO_PKG_VERSION"")
    ));

    // log the cli options
    log(format!(""[INFO] Model name: {}"", &cli.model_name));
    log(format!(""[INFO] Model alias: {}"", &cli.model_alias));
    log(format!(""[INFO] Prompt template: {}"", &cli.prompt_template));
    // ctx size
    log(format!(""[INFO] Context size: {}"", &cli.ctx_size));
    // reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        log(format!(""[INFO] reverse prompt: {}"", reverse_prompt));
    }
    // system prompt
    if let Some(system_prompt) = &cli.system_prompt {
        log(format!(""[INFO] system prompt: {}"", system_prompt));
    }
    // n_predict
    log(format!(
        ""[INFO] Number of tokens to predict: {}"",
        &cli.n_predict
    ));
    // n_gpu_layers
    log(format!(
        ""[INFO] Number of layers to run on the GPU: {}"",
        &cli.n_gpu_layers
    ));
    // main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        log(format!(""[INFO] Main GPU to use: {}"", main_gpu));
    }
    // tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        log(format!(""[INFO] Tensor split: {}"", tensor_split));
    }
    log(format!(""[INFO] Threads: {}"", &cli.threads));
    // no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        log(format!(
            ""[INFO] Disable memory mapping for file access of chat models : {}"",
            &no_mmap
        ));
    }
    // batch size
    log(format!(
        ""[INFO] Batch size for prompt processing: {}"",
        &cli.batch_size
    ));
    // temp and top_p
    if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
    }
    // repeat penalty
    log(format!(
        ""[INFO] Penalize repeat sequence of tokens: {}"",
        &cli.repeat_penalty
    ));
    // presence penalty
    log(format!(
        ""[INFO] Presence penalty (0.0 = disabled): {}"",
        &cli.presence_penalty
    ));
    // frequency penalty
    log(format!(
        ""[INFO] Frequency penalty (0.0 = disabled): {}"",
        &cli.frequency_penalty
    ));
    // grammar
    log(format!(""[INFO] BNF-like grammar: {}"", &cli.grammar));
    // json schema
    if let Some(json_schema) = &cli.json_schema {
        log(format!(""[INFO] JSON schema: {}"", json_schema));
    }
    // log prompts
    log(format!(""[INFO] Enable prompt log: {}"", &cli.log_prompts));
    // log statistics
    log(format!(""[INFO] Enable plugin log: {}"", &cli.log_stat));

    // create a MetadataBuilder instance
    let builder = MetadataBuilder::new(&cli.model_name, &cli.model_alias, cli.prompt_template)
        .with_ctx_size(cli.ctx_size)
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_batch_size(cli.batch_size)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .enable_prompts_log(cli.log_prompts || cli.log_all)
        .enable_plugin_log(cli.log_stat || cli.log_all)
        .enable_debug_log(plugin_debug);
    // temp and top_p
    let builder = if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
        builder.with_top_p(top_p)
    } else {
        let temp = cli.temp.unwrap();
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    };
    // create a Metadata instance
    let metadata = builder.build();

    // initialize the core context
    init_core_context(Some(&[metadata]), None)?;

    // get the plugin version info
    let plugin_info = llama_core::get_plugin_info()?;
    log(format!(
        ""[INFO] Wasi-nn-ggml plugin: b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    ));

    // create a ChatCompletionRequestSampling instance
    let sampling = if cli.temp.is_none() && cli.top_p.is_none() {
        ChatCompletionRequestSampling::Temperature(1.0)
    } else if let Some(temp) = cli.temp {
        ChatCompletionRequestSampling::Temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        ChatCompletionRequestSampling::TopP(top_p)
    } else {
        let temp = cli.temp.unwrap();
        ChatCompletionRequestSampling::Temperature(temp)
    };

    // create a chat request
    let mut chat_request = ChatCompletionRequestBuilder::new(&cli.model_name, vec![])
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_sampling(sampling)
        .enable_stream(!cli.disable_stream)
        .build();

    // add system message if provided
    if let Some(system_prompt) = &cli.system_prompt {
        let system_message = ChatCompletionRequestMessage::new_system_message(system_prompt, None);

        chat_request.messages.push(system_message);
    }

    let readme = ""
================================== Running in interactive mode. ===================================\n
    - Press [Ctrl+C] to interject at any time.
    - Press [Return] to end the input.
    - For multi-line inputs, end each line with '\\' and press [Return] to get another line.\n"";
    log(readme);

    loop {
        println!(""\n[You]: "");
        let user_input = read_input();

        // put the user message into the messages sequence of chat_request
        let user_message = ChatCompletionRequestMessage::new_user_message(
            ChatCompletionUserMessageContent::Text(user_input),
            None,
        );

        chat_request.messages.push(user_message);

        if cli.log_stat || cli.log_all {
            print_log_begin_separator(""STATISTICS (Set Input)"", Some(""*""), None);
        }

        if cli.log_stat || cli.log_all {
            print_log_end_separator(Some(""*""), None);
        }

        println!(""\n[Bot]:"");
        let mut assistant_answer = String::new();
        match llama_core::chat::chat(&mut chat_request).await {
            Ok(res) => match res {
                Left(mut stream) => {
                    while let Some(data) = stream.try_next().await? {
                        if let Some(chunk) = parse_sse_event(&data) {
                            if let Some(content) = &chunk.choices[0].delta.content {
                                if content.is_empty() {
                                    continue;
                                }
                                if assistant_answer.is_empty() {
                                    let content = content.trim_start();
                                    print!(""{}"", content);
                                    assistant_answer.push_str(content);
                                } else {
                                    print!(""{content}"");
                                    assistant_answer.push_str(content);
                                }
                                io::stdout().flush().unwrap();
                            }
                        }
                    }
                    println!();
                }
                Right(completion) => {
                    let chat_completion = completion.choices[0]
                        .message
                        .content
                        .to_owned()
                        .unwrap_or_default();
                    println!(""{chat_completion}"");
                    assistant_answer = chat_completion;
                }
            },
            Err(e) => {
                bail!(""Fail to generate chat completion. Reason: {msg}"", msg = e)
            }
        };

        let assistant_message = ChatCompletionRequestMessage::new_assistant_message(
            Some(assistant_answer.trim().to_string()),
            None,
            None,
        );
        chat_request.messages.push(assistant_message);
    }

    Ok(())
}

// For single line input, just press [Return] to end the input.
// For multi-line input, end your input with '\\' and press [Return].
//
// For example:
//  [You]:
//  what is the capital of France?[Return]
//
//  [You]:
//  Count the words in the following sentence: \[Return]
//  \[Return]
//  You can use Git to save new files and any changes to already existing files as a bundle of changes called a commit, which can be thought of as a revision to your project.[Return]
//
fn read_input() -> String {
    let mut answer = String::new();
    loop {
        let mut temp = String::new();
        std::io::stdin()
            .read_line(&mut temp)
            .expect(""The read bytes are not valid UTF-8"");

        if temp.ends_with(""\\\n"") {
            temp.pop();
            temp.pop();
            temp.push('\n');
            answer.push_str(&temp);
            continue;
        } else if temp.ends_with('\n') {
            answer.push_str(&temp);
            return answer;
        } else {
            return answer;
        }
    }
}

fn print_log_begin_separator(
    title: impl AsRef<str>,
    ch: Option<&str>,
    len: Option<usize>,
) -> usize {
    let title = format!("" [LOG: {}] "", title.as_ref());

    let total_len: usize = len.unwrap_or(100);
    let separator_len: usize = (total_len - title.len()) / 2;

    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push_str(&title);
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push('\n');
    println!(""{}"", separator);
    total_len
}

fn print_log_end_separator(ch: Option<&str>, len: Option<usize>) {
    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(len.unwrap_or(100)).as_str());
    separator.push('\n');
    println!(""{}"", separator);
}

#[derive(Debug, Default, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    // #[serde(rename = ""enable-debug-log"")]
    // pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    // pub mmproj: String,
    // pub image: String,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    // #[serde(rename = ""main-gpu"")]
    // pub main_gpu: u64,
    // #[serde(rename = ""tensor-split"")]
    // pub tensor_split: String,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,

    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,
}

fn log(msg: impl std::fmt::Display) {
    println!(""{}"", msg);
}

fn parse_sse_event(s: &str) -> Option<ChatCompletionChunk> {
    let lines: Vec<&str> = s.split('\n').collect();
    // let mutevent = None;
    let mut data = None;

    for line in lines {
        if line.starts_with(""data:"") {
            data = Some(line.trim_start_matches(""data:"").trim());
        }
    }

    match data {
        Some(s) => {
            if s.trim() == ""[DONE]"" {
                return None;
            }

            match serde_json::from_str(s) {
                Ok(chunk) => Some(chunk),
                Err(e) => {
                    log(format!(
                        ""[ERROR] Fail to parse SSE data. Reason: {msg}. Data: {data}"",
                        msg = e,
                        data = s
                    ));
                    None
                }
            }
        }
        _ => None,
    }
}

```","Q: 5. What are the possible values for the ""temperature"" parameter in the metadata?
A: The ""temperature"" parameter in the metadata is used for temperature-based sampling, which helps to control the diversity of the generated text.

The possible values for the ""temperature"" parameter are:

* A floating-point number greater than 0. This will determine the level of randomness or uncertainty introduced during the sampling process. A higher value means more random and less constrained generation, while a lower value results in more predictable and repetitive output.
* When no ""temperature"" is specified, it defaults to 1.0.

So, in summary:

* The valid range for the ""temperature"" parameter is (0, infinity).
* No temperature specified or zero: equivalent to setting it to 1.0
* Any positive number can be used"
"```rust:llama-chat/src/main.rs
use anyhow::bail;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use either::{Left, Right};
use endpoints::chat::{
    ChatCompletionChunk, ChatCompletionRequestBuilder, ChatCompletionRequestMessage,
    ChatCompletionRequestSampling, ChatCompletionUserMessageContent,
};
use futures::TryStreamExt;
use llama_core::{init_core_context, MetadataBuilder};
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Debug, Parser)]
#[command(author, about, version, long_about=None)]
struct Cli {
    /// Model name
    #[arg(short, long, default_value = ""default"")]
    model_name: String,
    /// Model alias
    #[arg(short = 'a', long, default_value = ""default"")]
    model_alias: String,
    /// Size of the prompt context
    #[arg(short, long, default_value = ""512"")]
    ctx_size: u64,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Batch size for prompt processing
    #[arg(short, long, default_value = ""512"")]
    batch_size: u64,
    /// Temperature for sampling
    #[arg(long, conflicts_with = ""top_p"")]
    temp: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, conflicts_with = ""temp"")]
    top_p: Option<f64>,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Sets the prompt template.
    #[arg(short, long, value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: PromptTemplateType,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// System prompt message string.
    #[arg(short, long)]
    system_prompt: Option<String>,
    /// Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Print all log information to stdout
    #[arg(long)]
    log_all: bool,
    /// enable streaming stdout
    #[arg(long, default_value = ""false"")]
    disable_stream: bool,
}

#[allow(clippy::needless_return)]
#[allow(unreachable_code)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> anyhow::Result<()> {
    // get the environment variable `PLUGIN_DEBUG`
    let plugin_debug = std::env::var(""PLUGIN_DEBUG"").unwrap_or_default();
    let plugin_debug = match plugin_debug.is_empty() {
        true => false,
        false => plugin_debug.to_lowercase().parse::<bool>().unwrap_or(false),
    };

    // parse the command line arguments
    let cli = Cli::parse();

    // log version
    log(format!(
        ""\n[INFO] llama-chat version: {}"",
        env!(""CARGO_PKG_VERSION"")
    ));

    // log the cli options
    log(format!(""[INFO] Model name: {}"", &cli.model_name));
    log(format!(""[INFO] Model alias: {}"", &cli.model_alias));
    log(format!(""[INFO] Prompt template: {}"", &cli.prompt_template));
    // ctx size
    log(format!(""[INFO] Context size: {}"", &cli.ctx_size));
    // reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        log(format!(""[INFO] reverse prompt: {}"", reverse_prompt));
    }
    // system prompt
    if let Some(system_prompt) = &cli.system_prompt {
        log(format!(""[INFO] system prompt: {}"", system_prompt));
    }
    // n_predict
    log(format!(
        ""[INFO] Number of tokens to predict: {}"",
        &cli.n_predict
    ));
    // n_gpu_layers
    log(format!(
        ""[INFO] Number of layers to run on the GPU: {}"",
        &cli.n_gpu_layers
    ));
    // main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        log(format!(""[INFO] Main GPU to use: {}"", main_gpu));
    }
    // tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        log(format!(""[INFO] Tensor split: {}"", tensor_split));
    }
    log(format!(""[INFO] Threads: {}"", &cli.threads));
    // no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        log(format!(
            ""[INFO] Disable memory mapping for file access of chat models : {}"",
            &no_mmap
        ));
    }
    // batch size
    log(format!(
        ""[INFO] Batch size for prompt processing: {}"",
        &cli.batch_size
    ));
    // temp and top_p
    if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
    }
    // repeat penalty
    log(format!(
        ""[INFO] Penalize repeat sequence of tokens: {}"",
        &cli.repeat_penalty
    ));
    // presence penalty
    log(format!(
        ""[INFO] Presence penalty (0.0 = disabled): {}"",
        &cli.presence_penalty
    ));
    // frequency penalty
    log(format!(
        ""[INFO] Frequency penalty (0.0 = disabled): {}"",
        &cli.frequency_penalty
    ));
    // grammar
    log(format!(""[INFO] BNF-like grammar: {}"", &cli.grammar));
    // json schema
    if let Some(json_schema) = &cli.json_schema {
        log(format!(""[INFO] JSON schema: {}"", json_schema));
    }
    // log prompts
    log(format!(""[INFO] Enable prompt log: {}"", &cli.log_prompts));
    // log statistics
    log(format!(""[INFO] Enable plugin log: {}"", &cli.log_stat));

    // create a MetadataBuilder instance
    let builder = MetadataBuilder::new(&cli.model_name, &cli.model_alias, cli.prompt_template)
        .with_ctx_size(cli.ctx_size)
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_batch_size(cli.batch_size)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .enable_prompts_log(cli.log_prompts || cli.log_all)
        .enable_plugin_log(cli.log_stat || cli.log_all)
        .enable_debug_log(plugin_debug);
    // temp and top_p
    let builder = if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
        builder.with_top_p(top_p)
    } else {
        let temp = cli.temp.unwrap();
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    };
    // create a Metadata instance
    let metadata = builder.build();

    // initialize the core context
    init_core_context(Some(&[metadata]), None)?;

    // get the plugin version info
    let plugin_info = llama_core::get_plugin_info()?;
    log(format!(
        ""[INFO] Wasi-nn-ggml plugin: b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    ));

    // create a ChatCompletionRequestSampling instance
    let sampling = if cli.temp.is_none() && cli.top_p.is_none() {
        ChatCompletionRequestSampling::Temperature(1.0)
    } else if let Some(temp) = cli.temp {
        ChatCompletionRequestSampling::Temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        ChatCompletionRequestSampling::TopP(top_p)
    } else {
        let temp = cli.temp.unwrap();
        ChatCompletionRequestSampling::Temperature(temp)
    };

    // create a chat request
    let mut chat_request = ChatCompletionRequestBuilder::new(&cli.model_name, vec![])
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_sampling(sampling)
        .enable_stream(!cli.disable_stream)
        .build();

    // add system message if provided
    if let Some(system_prompt) = &cli.system_prompt {
        let system_message = ChatCompletionRequestMessage::new_system_message(system_prompt, None);

        chat_request.messages.push(system_message);
    }

    let readme = ""
================================== Running in interactive mode. ===================================\n
    - Press [Ctrl+C] to interject at any time.
    - Press [Return] to end the input.
    - For multi-line inputs, end each line with '\\' and press [Return] to get another line.\n"";
    log(readme);

    loop {
        println!(""\n[You]: "");
        let user_input = read_input();

        // put the user message into the messages sequence of chat_request
        let user_message = ChatCompletionRequestMessage::new_user_message(
            ChatCompletionUserMessageContent::Text(user_input),
            None,
        );

        chat_request.messages.push(user_message);

        if cli.log_stat || cli.log_all {
            print_log_begin_separator(""STATISTICS (Set Input)"", Some(""*""), None);
        }

        if cli.log_stat || cli.log_all {
            print_log_end_separator(Some(""*""), None);
        }

        println!(""\n[Bot]:"");
        let mut assistant_answer = String::new();
        match llama_core::chat::chat(&mut chat_request).await {
            Ok(res) => match res {
                Left(mut stream) => {
                    while let Some(data) = stream.try_next().await? {
                        if let Some(chunk) = parse_sse_event(&data) {
                            if let Some(content) = &chunk.choices[0].delta.content {
                                if content.is_empty() {
                                    continue;
                                }
                                if assistant_answer.is_empty() {
                                    let content = content.trim_start();
                                    print!(""{}"", content);
                                    assistant_answer.push_str(content);
                                } else {
                                    print!(""{content}"");
                                    assistant_answer.push_str(content);
                                }
                                io::stdout().flush().unwrap();
                            }
                        }
                    }
                    println!();
                }
                Right(completion) => {
                    let chat_completion = completion.choices[0]
                        .message
                        .content
                        .to_owned()
                        .unwrap_or_default();
                    println!(""{chat_completion}"");
                    assistant_answer = chat_completion;
                }
            },
            Err(e) => {
                bail!(""Fail to generate chat completion. Reason: {msg}"", msg = e)
            }
        };

        let assistant_message = ChatCompletionRequestMessage::new_assistant_message(
            Some(assistant_answer.trim().to_string()),
            None,
            None,
        );
        chat_request.messages.push(assistant_message);
    }

    Ok(())
}

// For single line input, just press [Return] to end the input.
// For multi-line input, end your input with '\\' and press [Return].
//
// For example:
//  [You]:
//  what is the capital of France?[Return]
//
//  [You]:
//  Count the words in the following sentence: \[Return]
//  \[Return]
//  You can use Git to save new files and any changes to already existing files as a bundle of changes called a commit, which can be thought of as a revision to your project.[Return]
//
fn read_input() -> String {
    let mut answer = String::new();
    loop {
        let mut temp = String::new();
        std::io::stdin()
            .read_line(&mut temp)
            .expect(""The read bytes are not valid UTF-8"");

        if temp.ends_with(""\\\n"") {
            temp.pop();
            temp.pop();
            temp.push('\n');
            answer.push_str(&temp);
            continue;
        } else if temp.ends_with('\n') {
            answer.push_str(&temp);
            return answer;
        } else {
            return answer;
        }
    }
}

fn print_log_begin_separator(
    title: impl AsRef<str>,
    ch: Option<&str>,
    len: Option<usize>,
) -> usize {
    let title = format!("" [LOG: {}] "", title.as_ref());

    let total_len: usize = len.unwrap_or(100);
    let separator_len: usize = (total_len - title.len()) / 2;

    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push_str(&title);
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push('\n');
    println!(""{}"", separator);
    total_len
}

fn print_log_end_separator(ch: Option<&str>, len: Option<usize>) {
    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(len.unwrap_or(100)).as_str());
    separator.push('\n');
    println!(""{}"", separator);
}

#[derive(Debug, Default, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    // #[serde(rename = ""enable-debug-log"")]
    // pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    // pub mmproj: String,
    // pub image: String,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    // #[serde(rename = ""main-gpu"")]
    // pub main_gpu: u64,
    // #[serde(rename = ""tensor-split"")]
    // pub tensor_split: String,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,

    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,
}

fn log(msg: impl std::fmt::Display) {
    println!(""{}"", msg);
}

fn parse_sse_event(s: &str) -> Option<ChatCompletionChunk> {
    let lines: Vec<&str> = s.split('\n').collect();
    // let mutevent = None;
    let mut data = None;

    for line in lines {
        if line.starts_with(""data:"") {
            data = Some(line.trim_start_matches(""data:"").trim());
        }
    }

    match data {
        Some(s) => {
            if s.trim() == ""[DONE]"" {
                return None;
            }

            match serde_json::from_str(s) {
                Ok(chunk) => Some(chunk),
                Err(e) => {
                    log(format!(
                        ""[ERROR] Fail to parse SSE data. Reason: {msg}. Data: {data}"",
                        msg = e,
                        data = s
                    ));
                    None
                }
            }
        }
        _ => None,
    }
}

```","Q: 6. In which method is the core context initialized?
A: The core context is initialized in the `main` method, specifically at this line:

```rust
init_core_context(Some(&[metadata]), None)?;
```

This line calls the `init_core_context` function from the `llama_core` module and passes a reference to the created metadata instance (`metadata`) as an argument."
"```rust:llama-chat/src/main.rs
use anyhow::bail;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use either::{Left, Right};
use endpoints::chat::{
    ChatCompletionChunk, ChatCompletionRequestBuilder, ChatCompletionRequestMessage,
    ChatCompletionRequestSampling, ChatCompletionUserMessageContent,
};
use futures::TryStreamExt;
use llama_core::{init_core_context, MetadataBuilder};
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Debug, Parser)]
#[command(author, about, version, long_about=None)]
struct Cli {
    /// Model name
    #[arg(short, long, default_value = ""default"")]
    model_name: String,
    /// Model alias
    #[arg(short = 'a', long, default_value = ""default"")]
    model_alias: String,
    /// Size of the prompt context
    #[arg(short, long, default_value = ""512"")]
    ctx_size: u64,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Batch size for prompt processing
    #[arg(short, long, default_value = ""512"")]
    batch_size: u64,
    /// Temperature for sampling
    #[arg(long, conflicts_with = ""top_p"")]
    temp: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, conflicts_with = ""temp"")]
    top_p: Option<f64>,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Sets the prompt template.
    #[arg(short, long, value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: PromptTemplateType,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// System prompt message string.
    #[arg(short, long)]
    system_prompt: Option<String>,
    /// Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Print all log information to stdout
    #[arg(long)]
    log_all: bool,
    /// enable streaming stdout
    #[arg(long, default_value = ""false"")]
    disable_stream: bool,
}

#[allow(clippy::needless_return)]
#[allow(unreachable_code)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> anyhow::Result<()> {
    // get the environment variable `PLUGIN_DEBUG`
    let plugin_debug = std::env::var(""PLUGIN_DEBUG"").unwrap_or_default();
    let plugin_debug = match plugin_debug.is_empty() {
        true => false,
        false => plugin_debug.to_lowercase().parse::<bool>().unwrap_or(false),
    };

    // parse the command line arguments
    let cli = Cli::parse();

    // log version
    log(format!(
        ""\n[INFO] llama-chat version: {}"",
        env!(""CARGO_PKG_VERSION"")
    ));

    // log the cli options
    log(format!(""[INFO] Model name: {}"", &cli.model_name));
    log(format!(""[INFO] Model alias: {}"", &cli.model_alias));
    log(format!(""[INFO] Prompt template: {}"", &cli.prompt_template));
    // ctx size
    log(format!(""[INFO] Context size: {}"", &cli.ctx_size));
    // reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        log(format!(""[INFO] reverse prompt: {}"", reverse_prompt));
    }
    // system prompt
    if let Some(system_prompt) = &cli.system_prompt {
        log(format!(""[INFO] system prompt: {}"", system_prompt));
    }
    // n_predict
    log(format!(
        ""[INFO] Number of tokens to predict: {}"",
        &cli.n_predict
    ));
    // n_gpu_layers
    log(format!(
        ""[INFO] Number of layers to run on the GPU: {}"",
        &cli.n_gpu_layers
    ));
    // main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        log(format!(""[INFO] Main GPU to use: {}"", main_gpu));
    }
    // tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        log(format!(""[INFO] Tensor split: {}"", tensor_split));
    }
    log(format!(""[INFO] Threads: {}"", &cli.threads));
    // no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        log(format!(
            ""[INFO] Disable memory mapping for file access of chat models : {}"",
            &no_mmap
        ));
    }
    // batch size
    log(format!(
        ""[INFO] Batch size for prompt processing: {}"",
        &cli.batch_size
    ));
    // temp and top_p
    if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
    }
    // repeat penalty
    log(format!(
        ""[INFO] Penalize repeat sequence of tokens: {}"",
        &cli.repeat_penalty
    ));
    // presence penalty
    log(format!(
        ""[INFO] Presence penalty (0.0 = disabled): {}"",
        &cli.presence_penalty
    ));
    // frequency penalty
    log(format!(
        ""[INFO] Frequency penalty (0.0 = disabled): {}"",
        &cli.frequency_penalty
    ));
    // grammar
    log(format!(""[INFO] BNF-like grammar: {}"", &cli.grammar));
    // json schema
    if let Some(json_schema) = &cli.json_schema {
        log(format!(""[INFO] JSON schema: {}"", json_schema));
    }
    // log prompts
    log(format!(""[INFO] Enable prompt log: {}"", &cli.log_prompts));
    // log statistics
    log(format!(""[INFO] Enable plugin log: {}"", &cli.log_stat));

    // create a MetadataBuilder instance
    let builder = MetadataBuilder::new(&cli.model_name, &cli.model_alias, cli.prompt_template)
        .with_ctx_size(cli.ctx_size)
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_batch_size(cli.batch_size)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .enable_prompts_log(cli.log_prompts || cli.log_all)
        .enable_plugin_log(cli.log_stat || cli.log_all)
        .enable_debug_log(plugin_debug);
    // temp and top_p
    let builder = if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
        builder.with_top_p(top_p)
    } else {
        let temp = cli.temp.unwrap();
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    };
    // create a Metadata instance
    let metadata = builder.build();

    // initialize the core context
    init_core_context(Some(&[metadata]), None)?;

    // get the plugin version info
    let plugin_info = llama_core::get_plugin_info()?;
    log(format!(
        ""[INFO] Wasi-nn-ggml plugin: b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    ));

    // create a ChatCompletionRequestSampling instance
    let sampling = if cli.temp.is_none() && cli.top_p.is_none() {
        ChatCompletionRequestSampling::Temperature(1.0)
    } else if let Some(temp) = cli.temp {
        ChatCompletionRequestSampling::Temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        ChatCompletionRequestSampling::TopP(top_p)
    } else {
        let temp = cli.temp.unwrap();
        ChatCompletionRequestSampling::Temperature(temp)
    };

    // create a chat request
    let mut chat_request = ChatCompletionRequestBuilder::new(&cli.model_name, vec![])
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_sampling(sampling)
        .enable_stream(!cli.disable_stream)
        .build();

    // add system message if provided
    if let Some(system_prompt) = &cli.system_prompt {
        let system_message = ChatCompletionRequestMessage::new_system_message(system_prompt, None);

        chat_request.messages.push(system_message);
    }

    let readme = ""
================================== Running in interactive mode. ===================================\n
    - Press [Ctrl+C] to interject at any time.
    - Press [Return] to end the input.
    - For multi-line inputs, end each line with '\\' and press [Return] to get another line.\n"";
    log(readme);

    loop {
        println!(""\n[You]: "");
        let user_input = read_input();

        // put the user message into the messages sequence of chat_request
        let user_message = ChatCompletionRequestMessage::new_user_message(
            ChatCompletionUserMessageContent::Text(user_input),
            None,
        );

        chat_request.messages.push(user_message);

        if cli.log_stat || cli.log_all {
            print_log_begin_separator(""STATISTICS (Set Input)"", Some(""*""), None);
        }

        if cli.log_stat || cli.log_all {
            print_log_end_separator(Some(""*""), None);
        }

        println!(""\n[Bot]:"");
        let mut assistant_answer = String::new();
        match llama_core::chat::chat(&mut chat_request).await {
            Ok(res) => match res {
                Left(mut stream) => {
                    while let Some(data) = stream.try_next().await? {
                        if let Some(chunk) = parse_sse_event(&data) {
                            if let Some(content) = &chunk.choices[0].delta.content {
                                if content.is_empty() {
                                    continue;
                                }
                                if assistant_answer.is_empty() {
                                    let content = content.trim_start();
                                    print!(""{}"", content);
                                    assistant_answer.push_str(content);
                                } else {
                                    print!(""{content}"");
                                    assistant_answer.push_str(content);
                                }
                                io::stdout().flush().unwrap();
                            }
                        }
                    }
                    println!();
                }
                Right(completion) => {
                    let chat_completion = completion.choices[0]
                        .message
                        .content
                        .to_owned()
                        .unwrap_or_default();
                    println!(""{chat_completion}"");
                    assistant_answer = chat_completion;
                }
            },
            Err(e) => {
                bail!(""Fail to generate chat completion. Reason: {msg}"", msg = e)
            }
        };

        let assistant_message = ChatCompletionRequestMessage::new_assistant_message(
            Some(assistant_answer.trim().to_string()),
            None,
            None,
        );
        chat_request.messages.push(assistant_message);
    }

    Ok(())
}

// For single line input, just press [Return] to end the input.
// For multi-line input, end your input with '\\' and press [Return].
//
// For example:
//  [You]:
//  what is the capital of France?[Return]
//
//  [You]:
//  Count the words in the following sentence: \[Return]
//  \[Return]
//  You can use Git to save new files and any changes to already existing files as a bundle of changes called a commit, which can be thought of as a revision to your project.[Return]
//
fn read_input() -> String {
    let mut answer = String::new();
    loop {
        let mut temp = String::new();
        std::io::stdin()
            .read_line(&mut temp)
            .expect(""The read bytes are not valid UTF-8"");

        if temp.ends_with(""\\\n"") {
            temp.pop();
            temp.pop();
            temp.push('\n');
            answer.push_str(&temp);
            continue;
        } else if temp.ends_with('\n') {
            answer.push_str(&temp);
            return answer;
        } else {
            return answer;
        }
    }
}

fn print_log_begin_separator(
    title: impl AsRef<str>,
    ch: Option<&str>,
    len: Option<usize>,
) -> usize {
    let title = format!("" [LOG: {}] "", title.as_ref());

    let total_len: usize = len.unwrap_or(100);
    let separator_len: usize = (total_len - title.len()) / 2;

    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push_str(&title);
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push('\n');
    println!(""{}"", separator);
    total_len
}

fn print_log_end_separator(ch: Option<&str>, len: Option<usize>) {
    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(len.unwrap_or(100)).as_str());
    separator.push('\n');
    println!(""{}"", separator);
}

#[derive(Debug, Default, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    // #[serde(rename = ""enable-debug-log"")]
    // pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    // pub mmproj: String,
    // pub image: String,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    // #[serde(rename = ""main-gpu"")]
    // pub main_gpu: u64,
    // #[serde(rename = ""tensor-split"")]
    // pub tensor_split: String,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,

    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,
}

fn log(msg: impl std::fmt::Display) {
    println!(""{}"", msg);
}

fn parse_sse_event(s: &str) -> Option<ChatCompletionChunk> {
    let lines: Vec<&str> = s.split('\n').collect();
    // let mutevent = None;
    let mut data = None;

    for line in lines {
        if line.starts_with(""data:"") {
            data = Some(line.trim_start_matches(""data:"").trim());
        }
    }

    match data {
        Some(s) => {
            if s.trim() == ""[DONE]"" {
                return None;
            }

            match serde_json::from_str(s) {
                Ok(chunk) => Some(chunk),
                Err(e) => {
                    log(format!(
                        ""[ERROR] Fail to parse SSE data. Reason: {msg}. Data: {data}"",
                        msg = e,
                        data = s
                    ));
                    None
                }
            }
        }
        _ => None,
    }
}

```","Q: 7. What is the name of the file that contains the plugin information?
A: Based on the provided code, the plugin information is stored in a file that contains the build number and commit ID of the Wasi-nn-ggml plugin. The name of this file is not explicitly mentioned, but we can infer its existence from the following line:

```rust
let plugin_info = llama_core::get_plugin_info()?;
```

In Rust, `llama_core::get_plugin_info()` likely returns a `PluginInfo` struct that contains the build number and commit ID. This struct probably has a method to access or return this information.

As for the actual file name, we don't have enough information to determine it directly from the provided code snippet. However, based on common practices in software development, this information is usually stored in a JSON or YAML configuration file. If I had to make an educated guess, I would say that the plugin information might be stored in a file named `plugin-info.json` or `llama-plugin-info.yml`, but without further context or code, it's impossible to know for certain.

**Bonus Tip**: When developing a Rust library, consider using the `serde_json` crate to serialize and deserialize JSON data. This will make it easier to work with JSON files and enable you to store complex data structures like plugin information in a human-readable format."
"```rust:llama-chat/src/main.rs
use anyhow::bail;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use either::{Left, Right};
use endpoints::chat::{
    ChatCompletionChunk, ChatCompletionRequestBuilder, ChatCompletionRequestMessage,
    ChatCompletionRequestSampling, ChatCompletionUserMessageContent,
};
use futures::TryStreamExt;
use llama_core::{init_core_context, MetadataBuilder};
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Debug, Parser)]
#[command(author, about, version, long_about=None)]
struct Cli {
    /// Model name
    #[arg(short, long, default_value = ""default"")]
    model_name: String,
    /// Model alias
    #[arg(short = 'a', long, default_value = ""default"")]
    model_alias: String,
    /// Size of the prompt context
    #[arg(short, long, default_value = ""512"")]
    ctx_size: u64,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Batch size for prompt processing
    #[arg(short, long, default_value = ""512"")]
    batch_size: u64,
    /// Temperature for sampling
    #[arg(long, conflicts_with = ""top_p"")]
    temp: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, conflicts_with = ""temp"")]
    top_p: Option<f64>,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Sets the prompt template.
    #[arg(short, long, value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: PromptTemplateType,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// System prompt message string.
    #[arg(short, long)]
    system_prompt: Option<String>,
    /// Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Print all log information to stdout
    #[arg(long)]
    log_all: bool,
    /// enable streaming stdout
    #[arg(long, default_value = ""false"")]
    disable_stream: bool,
}

#[allow(clippy::needless_return)]
#[allow(unreachable_code)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> anyhow::Result<()> {
    // get the environment variable `PLUGIN_DEBUG`
    let plugin_debug = std::env::var(""PLUGIN_DEBUG"").unwrap_or_default();
    let plugin_debug = match plugin_debug.is_empty() {
        true => false,
        false => plugin_debug.to_lowercase().parse::<bool>().unwrap_or(false),
    };

    // parse the command line arguments
    let cli = Cli::parse();

    // log version
    log(format!(
        ""\n[INFO] llama-chat version: {}"",
        env!(""CARGO_PKG_VERSION"")
    ));

    // log the cli options
    log(format!(""[INFO] Model name: {}"", &cli.model_name));
    log(format!(""[INFO] Model alias: {}"", &cli.model_alias));
    log(format!(""[INFO] Prompt template: {}"", &cli.prompt_template));
    // ctx size
    log(format!(""[INFO] Context size: {}"", &cli.ctx_size));
    // reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        log(format!(""[INFO] reverse prompt: {}"", reverse_prompt));
    }
    // system prompt
    if let Some(system_prompt) = &cli.system_prompt {
        log(format!(""[INFO] system prompt: {}"", system_prompt));
    }
    // n_predict
    log(format!(
        ""[INFO] Number of tokens to predict: {}"",
        &cli.n_predict
    ));
    // n_gpu_layers
    log(format!(
        ""[INFO] Number of layers to run on the GPU: {}"",
        &cli.n_gpu_layers
    ));
    // main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        log(format!(""[INFO] Main GPU to use: {}"", main_gpu));
    }
    // tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        log(format!(""[INFO] Tensor split: {}"", tensor_split));
    }
    log(format!(""[INFO] Threads: {}"", &cli.threads));
    // no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        log(format!(
            ""[INFO] Disable memory mapping for file access of chat models : {}"",
            &no_mmap
        ));
    }
    // batch size
    log(format!(
        ""[INFO] Batch size for prompt processing: {}"",
        &cli.batch_size
    ));
    // temp and top_p
    if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
    }
    // repeat penalty
    log(format!(
        ""[INFO] Penalize repeat sequence of tokens: {}"",
        &cli.repeat_penalty
    ));
    // presence penalty
    log(format!(
        ""[INFO] Presence penalty (0.0 = disabled): {}"",
        &cli.presence_penalty
    ));
    // frequency penalty
    log(format!(
        ""[INFO] Frequency penalty (0.0 = disabled): {}"",
        &cli.frequency_penalty
    ));
    // grammar
    log(format!(""[INFO] BNF-like grammar: {}"", &cli.grammar));
    // json schema
    if let Some(json_schema) = &cli.json_schema {
        log(format!(""[INFO] JSON schema: {}"", json_schema));
    }
    // log prompts
    log(format!(""[INFO] Enable prompt log: {}"", &cli.log_prompts));
    // log statistics
    log(format!(""[INFO] Enable plugin log: {}"", &cli.log_stat));

    // create a MetadataBuilder instance
    let builder = MetadataBuilder::new(&cli.model_name, &cli.model_alias, cli.prompt_template)
        .with_ctx_size(cli.ctx_size)
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_batch_size(cli.batch_size)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .enable_prompts_log(cli.log_prompts || cli.log_all)
        .enable_plugin_log(cli.log_stat || cli.log_all)
        .enable_debug_log(plugin_debug);
    // temp and top_p
    let builder = if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
        builder.with_top_p(top_p)
    } else {
        let temp = cli.temp.unwrap();
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    };
    // create a Metadata instance
    let metadata = builder.build();

    // initialize the core context
    init_core_context(Some(&[metadata]), None)?;

    // get the plugin version info
    let plugin_info = llama_core::get_plugin_info()?;
    log(format!(
        ""[INFO] Wasi-nn-ggml plugin: b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    ));

    // create a ChatCompletionRequestSampling instance
    let sampling = if cli.temp.is_none() && cli.top_p.is_none() {
        ChatCompletionRequestSampling::Temperature(1.0)
    } else if let Some(temp) = cli.temp {
        ChatCompletionRequestSampling::Temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        ChatCompletionRequestSampling::TopP(top_p)
    } else {
        let temp = cli.temp.unwrap();
        ChatCompletionRequestSampling::Temperature(temp)
    };

    // create a chat request
    let mut chat_request = ChatCompletionRequestBuilder::new(&cli.model_name, vec![])
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_sampling(sampling)
        .enable_stream(!cli.disable_stream)
        .build();

    // add system message if provided
    if let Some(system_prompt) = &cli.system_prompt {
        let system_message = ChatCompletionRequestMessage::new_system_message(system_prompt, None);

        chat_request.messages.push(system_message);
    }

    let readme = ""
================================== Running in interactive mode. ===================================\n
    - Press [Ctrl+C] to interject at any time.
    - Press [Return] to end the input.
    - For multi-line inputs, end each line with '\\' and press [Return] to get another line.\n"";
    log(readme);

    loop {
        println!(""\n[You]: "");
        let user_input = read_input();

        // put the user message into the messages sequence of chat_request
        let user_message = ChatCompletionRequestMessage::new_user_message(
            ChatCompletionUserMessageContent::Text(user_input),
            None,
        );

        chat_request.messages.push(user_message);

        if cli.log_stat || cli.log_all {
            print_log_begin_separator(""STATISTICS (Set Input)"", Some(""*""), None);
        }

        if cli.log_stat || cli.log_all {
            print_log_end_separator(Some(""*""), None);
        }

        println!(""\n[Bot]:"");
        let mut assistant_answer = String::new();
        match llama_core::chat::chat(&mut chat_request).await {
            Ok(res) => match res {
                Left(mut stream) => {
                    while let Some(data) = stream.try_next().await? {
                        if let Some(chunk) = parse_sse_event(&data) {
                            if let Some(content) = &chunk.choices[0].delta.content {
                                if content.is_empty() {
                                    continue;
                                }
                                if assistant_answer.is_empty() {
                                    let content = content.trim_start();
                                    print!(""{}"", content);
                                    assistant_answer.push_str(content);
                                } else {
                                    print!(""{content}"");
                                    assistant_answer.push_str(content);
                                }
                                io::stdout().flush().unwrap();
                            }
                        }
                    }
                    println!();
                }
                Right(completion) => {
                    let chat_completion = completion.choices[0]
                        .message
                        .content
                        .to_owned()
                        .unwrap_or_default();
                    println!(""{chat_completion}"");
                    assistant_answer = chat_completion;
                }
            },
            Err(e) => {
                bail!(""Fail to generate chat completion. Reason: {msg}"", msg = e)
            }
        };

        let assistant_message = ChatCompletionRequestMessage::new_assistant_message(
            Some(assistant_answer.trim().to_string()),
            None,
            None,
        );
        chat_request.messages.push(assistant_message);
    }

    Ok(())
}

// For single line input, just press [Return] to end the input.
// For multi-line input, end your input with '\\' and press [Return].
//
// For example:
//  [You]:
//  what is the capital of France?[Return]
//
//  [You]:
//  Count the words in the following sentence: \[Return]
//  \[Return]
//  You can use Git to save new files and any changes to already existing files as a bundle of changes called a commit, which can be thought of as a revision to your project.[Return]
//
fn read_input() -> String {
    let mut answer = String::new();
    loop {
        let mut temp = String::new();
        std::io::stdin()
            .read_line(&mut temp)
            .expect(""The read bytes are not valid UTF-8"");

        if temp.ends_with(""\\\n"") {
            temp.pop();
            temp.pop();
            temp.push('\n');
            answer.push_str(&temp);
            continue;
        } else if temp.ends_with('\n') {
            answer.push_str(&temp);
            return answer;
        } else {
            return answer;
        }
    }
}

fn print_log_begin_separator(
    title: impl AsRef<str>,
    ch: Option<&str>,
    len: Option<usize>,
) -> usize {
    let title = format!("" [LOG: {}] "", title.as_ref());

    let total_len: usize = len.unwrap_or(100);
    let separator_len: usize = (total_len - title.len()) / 2;

    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push_str(&title);
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push('\n');
    println!(""{}"", separator);
    total_len
}

fn print_log_end_separator(ch: Option<&str>, len: Option<usize>) {
    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(len.unwrap_or(100)).as_str());
    separator.push('\n');
    println!(""{}"", separator);
}

#[derive(Debug, Default, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    // #[serde(rename = ""enable-debug-log"")]
    // pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    // pub mmproj: String,
    // pub image: String,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    // #[serde(rename = ""main-gpu"")]
    // pub main_gpu: u64,
    // #[serde(rename = ""tensor-split"")]
    // pub tensor_split: String,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,

    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,
}

fn log(msg: impl std::fmt::Display) {
    println!(""{}"", msg);
}

fn parse_sse_event(s: &str) -> Option<ChatCompletionChunk> {
    let lines: Vec<&str> = s.split('\n').collect();
    // let mutevent = None;
    let mut data = None;

    for line in lines {
        if line.starts_with(""data:"") {
            data = Some(line.trim_start_matches(""data:"").trim());
        }
    }

    match data {
        Some(s) => {
            if s.trim() == ""[DONE]"" {
                return None;
            }

            match serde_json::from_str(s) {
                Ok(chunk) => Some(chunk),
                Err(e) => {
                    log(format!(
                        ""[ERROR] Fail to parse SSE data. Reason: {msg}. Data: {data}"",
                        msg = e,
                        data = s
                    ));
                    None
                }
            }
        }
        _ => None,
    }
}

```","Q: 8. How does the code handle multi-line inputs from users?
A: The code handles multi-line inputs from users by using the following approach:

1. **Single Line Input**: When a user enters a single line, they can press `[Return]` to end the input.

2. **Multi-Line Input**: If a user wants to enter multiple lines of text, they need to use a special syntax. The user should:
   - Type their input.
   - Press `\\` (backslash) at the end of the line.
   - Press `[Return]`.
   - Continue typing on the next line.
   - Repeat steps 3-4 until done with the entire input.

When the user finishes entering their multi-line input, they can simply press `[Return]`, and the code will recognize it as a complete input. 

For example:

```
[You]:
What is your name? [Return]
John\\
I love coding and llama-chat! [Return]

// The input has been parsed correctly.
```"
"```rust:llama-chat/src/main.rs
use anyhow::bail;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use either::{Left, Right};
use endpoints::chat::{
    ChatCompletionChunk, ChatCompletionRequestBuilder, ChatCompletionRequestMessage,
    ChatCompletionRequestSampling, ChatCompletionUserMessageContent,
};
use futures::TryStreamExt;
use llama_core::{init_core_context, MetadataBuilder};
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Debug, Parser)]
#[command(author, about, version, long_about=None)]
struct Cli {
    /// Model name
    #[arg(short, long, default_value = ""default"")]
    model_name: String,
    /// Model alias
    #[arg(short = 'a', long, default_value = ""default"")]
    model_alias: String,
    /// Size of the prompt context
    #[arg(short, long, default_value = ""512"")]
    ctx_size: u64,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Batch size for prompt processing
    #[arg(short, long, default_value = ""512"")]
    batch_size: u64,
    /// Temperature for sampling
    #[arg(long, conflicts_with = ""top_p"")]
    temp: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, conflicts_with = ""temp"")]
    top_p: Option<f64>,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Sets the prompt template.
    #[arg(short, long, value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: PromptTemplateType,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// System prompt message string.
    #[arg(short, long)]
    system_prompt: Option<String>,
    /// Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Print all log information to stdout
    #[arg(long)]
    log_all: bool,
    /// enable streaming stdout
    #[arg(long, default_value = ""false"")]
    disable_stream: bool,
}

#[allow(clippy::needless_return)]
#[allow(unreachable_code)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> anyhow::Result<()> {
    // get the environment variable `PLUGIN_DEBUG`
    let plugin_debug = std::env::var(""PLUGIN_DEBUG"").unwrap_or_default();
    let plugin_debug = match plugin_debug.is_empty() {
        true => false,
        false => plugin_debug.to_lowercase().parse::<bool>().unwrap_or(false),
    };

    // parse the command line arguments
    let cli = Cli::parse();

    // log version
    log(format!(
        ""\n[INFO] llama-chat version: {}"",
        env!(""CARGO_PKG_VERSION"")
    ));

    // log the cli options
    log(format!(""[INFO] Model name: {}"", &cli.model_name));
    log(format!(""[INFO] Model alias: {}"", &cli.model_alias));
    log(format!(""[INFO] Prompt template: {}"", &cli.prompt_template));
    // ctx size
    log(format!(""[INFO] Context size: {}"", &cli.ctx_size));
    // reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        log(format!(""[INFO] reverse prompt: {}"", reverse_prompt));
    }
    // system prompt
    if let Some(system_prompt) = &cli.system_prompt {
        log(format!(""[INFO] system prompt: {}"", system_prompt));
    }
    // n_predict
    log(format!(
        ""[INFO] Number of tokens to predict: {}"",
        &cli.n_predict
    ));
    // n_gpu_layers
    log(format!(
        ""[INFO] Number of layers to run on the GPU: {}"",
        &cli.n_gpu_layers
    ));
    // main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        log(format!(""[INFO] Main GPU to use: {}"", main_gpu));
    }
    // tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        log(format!(""[INFO] Tensor split: {}"", tensor_split));
    }
    log(format!(""[INFO] Threads: {}"", &cli.threads));
    // no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        log(format!(
            ""[INFO] Disable memory mapping for file access of chat models : {}"",
            &no_mmap
        ));
    }
    // batch size
    log(format!(
        ""[INFO] Batch size for prompt processing: {}"",
        &cli.batch_size
    ));
    // temp and top_p
    if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
    }
    // repeat penalty
    log(format!(
        ""[INFO] Penalize repeat sequence of tokens: {}"",
        &cli.repeat_penalty
    ));
    // presence penalty
    log(format!(
        ""[INFO] Presence penalty (0.0 = disabled): {}"",
        &cli.presence_penalty
    ));
    // frequency penalty
    log(format!(
        ""[INFO] Frequency penalty (0.0 = disabled): {}"",
        &cli.frequency_penalty
    ));
    // grammar
    log(format!(""[INFO] BNF-like grammar: {}"", &cli.grammar));
    // json schema
    if let Some(json_schema) = &cli.json_schema {
        log(format!(""[INFO] JSON schema: {}"", json_schema));
    }
    // log prompts
    log(format!(""[INFO] Enable prompt log: {}"", &cli.log_prompts));
    // log statistics
    log(format!(""[INFO] Enable plugin log: {}"", &cli.log_stat));

    // create a MetadataBuilder instance
    let builder = MetadataBuilder::new(&cli.model_name, &cli.model_alias, cli.prompt_template)
        .with_ctx_size(cli.ctx_size)
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_batch_size(cli.batch_size)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .enable_prompts_log(cli.log_prompts || cli.log_all)
        .enable_plugin_log(cli.log_stat || cli.log_all)
        .enable_debug_log(plugin_debug);
    // temp and top_p
    let builder = if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
        builder.with_top_p(top_p)
    } else {
        let temp = cli.temp.unwrap();
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    };
    // create a Metadata instance
    let metadata = builder.build();

    // initialize the core context
    init_core_context(Some(&[metadata]), None)?;

    // get the plugin version info
    let plugin_info = llama_core::get_plugin_info()?;
    log(format!(
        ""[INFO] Wasi-nn-ggml plugin: b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    ));

    // create a ChatCompletionRequestSampling instance
    let sampling = if cli.temp.is_none() && cli.top_p.is_none() {
        ChatCompletionRequestSampling::Temperature(1.0)
    } else if let Some(temp) = cli.temp {
        ChatCompletionRequestSampling::Temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        ChatCompletionRequestSampling::TopP(top_p)
    } else {
        let temp = cli.temp.unwrap();
        ChatCompletionRequestSampling::Temperature(temp)
    };

    // create a chat request
    let mut chat_request = ChatCompletionRequestBuilder::new(&cli.model_name, vec![])
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_sampling(sampling)
        .enable_stream(!cli.disable_stream)
        .build();

    // add system message if provided
    if let Some(system_prompt) = &cli.system_prompt {
        let system_message = ChatCompletionRequestMessage::new_system_message(system_prompt, None);

        chat_request.messages.push(system_message);
    }

    let readme = ""
================================== Running in interactive mode. ===================================\n
    - Press [Ctrl+C] to interject at any time.
    - Press [Return] to end the input.
    - For multi-line inputs, end each line with '\\' and press [Return] to get another line.\n"";
    log(readme);

    loop {
        println!(""\n[You]: "");
        let user_input = read_input();

        // put the user message into the messages sequence of chat_request
        let user_message = ChatCompletionRequestMessage::new_user_message(
            ChatCompletionUserMessageContent::Text(user_input),
            None,
        );

        chat_request.messages.push(user_message);

        if cli.log_stat || cli.log_all {
            print_log_begin_separator(""STATISTICS (Set Input)"", Some(""*""), None);
        }

        if cli.log_stat || cli.log_all {
            print_log_end_separator(Some(""*""), None);
        }

        println!(""\n[Bot]:"");
        let mut assistant_answer = String::new();
        match llama_core::chat::chat(&mut chat_request).await {
            Ok(res) => match res {
                Left(mut stream) => {
                    while let Some(data) = stream.try_next().await? {
                        if let Some(chunk) = parse_sse_event(&data) {
                            if let Some(content) = &chunk.choices[0].delta.content {
                                if content.is_empty() {
                                    continue;
                                }
                                if assistant_answer.is_empty() {
                                    let content = content.trim_start();
                                    print!(""{}"", content);
                                    assistant_answer.push_str(content);
                                } else {
                                    print!(""{content}"");
                                    assistant_answer.push_str(content);
                                }
                                io::stdout().flush().unwrap();
                            }
                        }
                    }
                    println!();
                }
                Right(completion) => {
                    let chat_completion = completion.choices[0]
                        .message
                        .content
                        .to_owned()
                        .unwrap_or_default();
                    println!(""{chat_completion}"");
                    assistant_answer = chat_completion;
                }
            },
            Err(e) => {
                bail!(""Fail to generate chat completion. Reason: {msg}"", msg = e)
            }
        };

        let assistant_message = ChatCompletionRequestMessage::new_assistant_message(
            Some(assistant_answer.trim().to_string()),
            None,
            None,
        );
        chat_request.messages.push(assistant_message);
    }

    Ok(())
}

// For single line input, just press [Return] to end the input.
// For multi-line input, end your input with '\\' and press [Return].
//
// For example:
//  [You]:
//  what is the capital of France?[Return]
//
//  [You]:
//  Count the words in the following sentence: \[Return]
//  \[Return]
//  You can use Git to save new files and any changes to already existing files as a bundle of changes called a commit, which can be thought of as a revision to your project.[Return]
//
fn read_input() -> String {
    let mut answer = String::new();
    loop {
        let mut temp = String::new();
        std::io::stdin()
            .read_line(&mut temp)
            .expect(""The read bytes are not valid UTF-8"");

        if temp.ends_with(""\\\n"") {
            temp.pop();
            temp.pop();
            temp.push('\n');
            answer.push_str(&temp);
            continue;
        } else if temp.ends_with('\n') {
            answer.push_str(&temp);
            return answer;
        } else {
            return answer;
        }
    }
}

fn print_log_begin_separator(
    title: impl AsRef<str>,
    ch: Option<&str>,
    len: Option<usize>,
) -> usize {
    let title = format!("" [LOG: {}] "", title.as_ref());

    let total_len: usize = len.unwrap_or(100);
    let separator_len: usize = (total_len - title.len()) / 2;

    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push_str(&title);
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push('\n');
    println!(""{}"", separator);
    total_len
}

fn print_log_end_separator(ch: Option<&str>, len: Option<usize>) {
    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(len.unwrap_or(100)).as_str());
    separator.push('\n');
    println!(""{}"", separator);
}

#[derive(Debug, Default, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    // #[serde(rename = ""enable-debug-log"")]
    // pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    // pub mmproj: String,
    // pub image: String,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    // #[serde(rename = ""main-gpu"")]
    // pub main_gpu: u64,
    // #[serde(rename = ""tensor-split"")]
    // pub tensor_split: String,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,

    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,
}

fn log(msg: impl std::fmt::Display) {
    println!(""{}"", msg);
}

fn parse_sse_event(s: &str) -> Option<ChatCompletionChunk> {
    let lines: Vec<&str> = s.split('\n').collect();
    // let mutevent = None;
    let mut data = None;

    for line in lines {
        if line.starts_with(""data:"") {
            data = Some(line.trim_start_matches(""data:"").trim());
        }
    }

    match data {
        Some(s) => {
            if s.trim() == ""[DONE]"" {
                return None;
            }

            match serde_json::from_str(s) {
                Ok(chunk) => Some(chunk),
                Err(e) => {
                    log(format!(
                        ""[ERROR] Fail to parse SSE data. Reason: {msg}. Data: {data}"",
                        msg = e,
                        data = s
                    ));
                    None
                }
            }
        }
        _ => None,
    }
}

```","Q: 9. Which function is responsible for reading input from the user?
A: The function responsible for reading input from the user is `read_input()`. This function reads a line of input from the standard input and returns it as a string. If the input contains multiple lines, they are concatenated together.

Here's how you can call this function to get user input:

```rust
fn main() {
    let user_input = read_input();
    println!(""You said: {}"", user_input);
}
```

This will prompt the user for input and then print out what the user typed."
"```rust:llama-chat/src/main.rs
use anyhow::bail;
use chat_prompts::PromptTemplateType;
use clap::Parser;
use either::{Left, Right};
use endpoints::chat::{
    ChatCompletionChunk, ChatCompletionRequestBuilder, ChatCompletionRequestMessage,
    ChatCompletionRequestSampling, ChatCompletionUserMessageContent,
};
use futures::TryStreamExt;
use llama_core::{init_core_context, MetadataBuilder};
use serde::{Deserialize, Serialize};
use std::io::{self, Write};

#[derive(Debug, Parser)]
#[command(author, about, version, long_about=None)]
struct Cli {
    /// Model name
    #[arg(short, long, default_value = ""default"")]
    model_name: String,
    /// Model alias
    #[arg(short = 'a', long, default_value = ""default"")]
    model_alias: String,
    /// Size of the prompt context
    #[arg(short, long, default_value = ""512"")]
    ctx_size: u64,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Batch size for prompt processing
    #[arg(short, long, default_value = ""512"")]
    batch_size: u64,
    /// Temperature for sampling
    #[arg(long, conflicts_with = ""top_p"")]
    temp: Option<f64>,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, conflicts_with = ""temp"")]
    top_p: Option<f64>,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Sets the prompt template.
    #[arg(short, long, value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: PromptTemplateType,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// System prompt message string.
    #[arg(short, long)]
    system_prompt: Option<String>,
    /// Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Print all log information to stdout
    #[arg(long)]
    log_all: bool,
    /// enable streaming stdout
    #[arg(long, default_value = ""false"")]
    disable_stream: bool,
}

#[allow(clippy::needless_return)]
#[allow(unreachable_code)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> anyhow::Result<()> {
    // get the environment variable `PLUGIN_DEBUG`
    let plugin_debug = std::env::var(""PLUGIN_DEBUG"").unwrap_or_default();
    let plugin_debug = match plugin_debug.is_empty() {
        true => false,
        false => plugin_debug.to_lowercase().parse::<bool>().unwrap_or(false),
    };

    // parse the command line arguments
    let cli = Cli::parse();

    // log version
    log(format!(
        ""\n[INFO] llama-chat version: {}"",
        env!(""CARGO_PKG_VERSION"")
    ));

    // log the cli options
    log(format!(""[INFO] Model name: {}"", &cli.model_name));
    log(format!(""[INFO] Model alias: {}"", &cli.model_alias));
    log(format!(""[INFO] Prompt template: {}"", &cli.prompt_template));
    // ctx size
    log(format!(""[INFO] Context size: {}"", &cli.ctx_size));
    // reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        log(format!(""[INFO] reverse prompt: {}"", reverse_prompt));
    }
    // system prompt
    if let Some(system_prompt) = &cli.system_prompt {
        log(format!(""[INFO] system prompt: {}"", system_prompt));
    }
    // n_predict
    log(format!(
        ""[INFO] Number of tokens to predict: {}"",
        &cli.n_predict
    ));
    // n_gpu_layers
    log(format!(
        ""[INFO] Number of layers to run on the GPU: {}"",
        &cli.n_gpu_layers
    ));
    // main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        log(format!(""[INFO] Main GPU to use: {}"", main_gpu));
    }
    // tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        log(format!(""[INFO] Tensor split: {}"", tensor_split));
    }
    log(format!(""[INFO] Threads: {}"", &cli.threads));
    // no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        log(format!(
            ""[INFO] Disable memory mapping for file access of chat models : {}"",
            &no_mmap
        ));
    }
    // batch size
    log(format!(
        ""[INFO] Batch size for prompt processing: {}"",
        &cli.batch_size
    ));
    // temp and top_p
    if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
    }
    // repeat penalty
    log(format!(
        ""[INFO] Penalize repeat sequence of tokens: {}"",
        &cli.repeat_penalty
    ));
    // presence penalty
    log(format!(
        ""[INFO] Presence penalty (0.0 = disabled): {}"",
        &cli.presence_penalty
    ));
    // frequency penalty
    log(format!(
        ""[INFO] Frequency penalty (0.0 = disabled): {}"",
        &cli.frequency_penalty
    ));
    // grammar
    log(format!(""[INFO] BNF-like grammar: {}"", &cli.grammar));
    // json schema
    if let Some(json_schema) = &cli.json_schema {
        log(format!(""[INFO] JSON schema: {}"", json_schema));
    }
    // log prompts
    log(format!(""[INFO] Enable prompt log: {}"", &cli.log_prompts));
    // log statistics
    log(format!(""[INFO] Enable plugin log: {}"", &cli.log_stat));

    // create a MetadataBuilder instance
    let builder = MetadataBuilder::new(&cli.model_name, &cli.model_alias, cli.prompt_template)
        .with_ctx_size(cli.ctx_size)
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_batch_size(cli.batch_size)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .enable_prompts_log(cli.log_prompts || cli.log_all)
        .enable_plugin_log(cli.log_stat || cli.log_all)
        .enable_debug_log(plugin_debug);
    // temp and top_p
    let builder = if cli.temp.is_none() && cli.top_p.is_none() {
        let temp = 1.0;
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(temp) = cli.temp {
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        log(format!(""[INFO] Top-p sampling (1.0 = disabled): {}"", top_p));
        builder.with_top_p(top_p)
    } else {
        let temp = cli.temp.unwrap();
        log(format!(""[INFO] Temperature for sampling: {}"", temp));
        builder.with_temperature(temp)
    };
    // create a Metadata instance
    let metadata = builder.build();

    // initialize the core context
    init_core_context(Some(&[metadata]), None)?;

    // get the plugin version info
    let plugin_info = llama_core::get_plugin_info()?;
    log(format!(
        ""[INFO] Wasi-nn-ggml plugin: b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    ));

    // create a ChatCompletionRequestSampling instance
    let sampling = if cli.temp.is_none() && cli.top_p.is_none() {
        ChatCompletionRequestSampling::Temperature(1.0)
    } else if let Some(temp) = cli.temp {
        ChatCompletionRequestSampling::Temperature(temp)
    } else if let Some(top_p) = cli.top_p {
        ChatCompletionRequestSampling::TopP(top_p)
    } else {
        let temp = cli.temp.unwrap();
        ChatCompletionRequestSampling::Temperature(temp)
    };

    // create a chat request
    let mut chat_request = ChatCompletionRequestBuilder::new(&cli.model_name, vec![])
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_sampling(sampling)
        .enable_stream(!cli.disable_stream)
        .build();

    // add system message if provided
    if let Some(system_prompt) = &cli.system_prompt {
        let system_message = ChatCompletionRequestMessage::new_system_message(system_prompt, None);

        chat_request.messages.push(system_message);
    }

    let readme = ""
================================== Running in interactive mode. ===================================\n
    - Press [Ctrl+C] to interject at any time.
    - Press [Return] to end the input.
    - For multi-line inputs, end each line with '\\' and press [Return] to get another line.\n"";
    log(readme);

    loop {
        println!(""\n[You]: "");
        let user_input = read_input();

        // put the user message into the messages sequence of chat_request
        let user_message = ChatCompletionRequestMessage::new_user_message(
            ChatCompletionUserMessageContent::Text(user_input),
            None,
        );

        chat_request.messages.push(user_message);

        if cli.log_stat || cli.log_all {
            print_log_begin_separator(""STATISTICS (Set Input)"", Some(""*""), None);
        }

        if cli.log_stat || cli.log_all {
            print_log_end_separator(Some(""*""), None);
        }

        println!(""\n[Bot]:"");
        let mut assistant_answer = String::new();
        match llama_core::chat::chat(&mut chat_request).await {
            Ok(res) => match res {
                Left(mut stream) => {
                    while let Some(data) = stream.try_next().await? {
                        if let Some(chunk) = parse_sse_event(&data) {
                            if let Some(content) = &chunk.choices[0].delta.content {
                                if content.is_empty() {
                                    continue;
                                }
                                if assistant_answer.is_empty() {
                                    let content = content.trim_start();
                                    print!(""{}"", content);
                                    assistant_answer.push_str(content);
                                } else {
                                    print!(""{content}"");
                                    assistant_answer.push_str(content);
                                }
                                io::stdout().flush().unwrap();
                            }
                        }
                    }
                    println!();
                }
                Right(completion) => {
                    let chat_completion = completion.choices[0]
                        .message
                        .content
                        .to_owned()
                        .unwrap_or_default();
                    println!(""{chat_completion}"");
                    assistant_answer = chat_completion;
                }
            },
            Err(e) => {
                bail!(""Fail to generate chat completion. Reason: {msg}"", msg = e)
            }
        };

        let assistant_message = ChatCompletionRequestMessage::new_assistant_message(
            Some(assistant_answer.trim().to_string()),
            None,
            None,
        );
        chat_request.messages.push(assistant_message);
    }

    Ok(())
}

// For single line input, just press [Return] to end the input.
// For multi-line input, end your input with '\\' and press [Return].
//
// For example:
//  [You]:
//  what is the capital of France?[Return]
//
//  [You]:
//  Count the words in the following sentence: \[Return]
//  \[Return]
//  You can use Git to save new files and any changes to already existing files as a bundle of changes called a commit, which can be thought of as a revision to your project.[Return]
//
fn read_input() -> String {
    let mut answer = String::new();
    loop {
        let mut temp = String::new();
        std::io::stdin()
            .read_line(&mut temp)
            .expect(""The read bytes are not valid UTF-8"");

        if temp.ends_with(""\\\n"") {
            temp.pop();
            temp.pop();
            temp.push('\n');
            answer.push_str(&temp);
            continue;
        } else if temp.ends_with('\n') {
            answer.push_str(&temp);
            return answer;
        } else {
            return answer;
        }
    }
}

fn print_log_begin_separator(
    title: impl AsRef<str>,
    ch: Option<&str>,
    len: Option<usize>,
) -> usize {
    let title = format!("" [LOG: {}] "", title.as_ref());

    let total_len: usize = len.unwrap_or(100);
    let separator_len: usize = (total_len - title.len()) / 2;

    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push_str(&title);
    separator.push_str(ch.repeat(separator_len).as_str());
    separator.push('\n');
    println!(""{}"", separator);
    total_len
}

fn print_log_end_separator(ch: Option<&str>, len: Option<usize>) {
    let ch = ch.unwrap_or(""-"");
    let mut separator = ""\n\n"".to_string();
    separator.push_str(ch.repeat(len.unwrap_or(100)).as_str());
    separator.push('\n');
    println!(""{}"", separator);
}

#[derive(Debug, Default, Clone, Deserialize, Serialize)]
pub struct Metadata {
    // * Plugin parameters (used by this plugin):
    #[serde(rename = ""enable-log"")]
    pub log_enable: bool,
    // #[serde(rename = ""enable-debug-log"")]
    // pub debug_log: bool,
    // #[serde(rename = ""stream-stdout"")]
    // pub stream_stdout: bool,
    #[serde(rename = ""embedding"")]
    pub embeddings: bool,
    #[serde(rename = ""n-predict"")]
    pub n_predict: u64,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""reverse-prompt"")]
    pub reverse_prompt: Option<String>,
    // pub mmproj: String,
    // pub image: String,

    // * Model parameters (need to reload the model if updated):
    #[serde(rename = ""n-gpu-layers"")]
    pub n_gpu_layers: u64,
    // #[serde(rename = ""main-gpu"")]
    // pub main_gpu: u64,
    // #[serde(rename = ""tensor-split"")]
    // pub tensor_split: String,
    #[serde(skip_serializing_if = ""Option::is_none"", rename = ""use-mmap"")]
    use_mmap: Option<bool>,

    // * Context parameters (used by the llama context):
    #[serde(rename = ""ctx-size"")]
    pub ctx_size: u64,
    #[serde(rename = ""batch-size"")]
    pub batch_size: u64,

    // * Sampling parameters (used by the llama sampling context).
    #[serde(rename = ""temp"")]
    pub temperature: f64,
    #[serde(rename = ""top-p"")]
    pub top_p: f64,
    #[serde(rename = ""repeat-penalty"")]
    pub repeat_penalty: f64,
    #[serde(rename = ""presence-penalty"")]
    pub presence_penalty: f64,
    #[serde(rename = ""frequency-penalty"")]
    pub frequency_penalty: f64,
}

fn log(msg: impl std::fmt::Display) {
    println!(""{}"", msg);
}

fn parse_sse_event(s: &str) -> Option<ChatCompletionChunk> {
    let lines: Vec<&str> = s.split('\n').collect();
    // let mutevent = None;
    let mut data = None;

    for line in lines {
        if line.starts_with(""data:"") {
            data = Some(line.trim_start_matches(""data:"").trim());
        }
    }

    match data {
        Some(s) => {
            if s.trim() == ""[DONE]"" {
                return None;
            }

            match serde_json::from_str(s) {
                Ok(chunk) => Some(chunk),
                Err(e) => {
                    log(format!(
                        ""[ERROR] Fail to parse SSE data. Reason: {msg}. Data: {data}"",
                        msg = e,
                        data = s
                    ));
                    None
                }
            }
        }
        _ => None,
    }
}

```","Q: 10. Where are log messages printed to?
A: According to the code, log messages are printed to the standard output (stdout) using `println!()`.

However, there is an option to customize where log messages are printed by modifying the `print_log_begin_separator()` and `print_log_end_separator()` functions. For example, if you want to print log messages to a file instead of stdout, you can modify these functions to write to a file descriptor instead of using `println!()`. 

Also, there is an option for printing logs in ""debug"" mode with the flag `--plugin-debug` which enables debug-level logging. If this flag is set, the plugin will print more verbose log messages.

Here's a bonus tip: You can use environment variables to customize where log messages are printed. For example, you can set an environment variable `LOG_FILE` to specify a file path for log output. In the code, you would need to add logic to read this environment variable and use it to determine how to print log messages.

Here is some example code:
```rust
use std::env;

fn main() {
    let log_file = env::var(""LOG_FILE"").unwrap_or_else(|_| ""/dev/null"".to_string());

    // ...
}
```
This code checks for the presence of an environment variable `LOG_FILE` and uses it as the default path for log output. If the variable is not set, it defaults to `/dev/null`, which means all log messages will be discarded."
"```rust:llama-api-server/src/main.rs
#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::{ArgGroup, Parser};
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default port
const DEFAULT_PORT: &str = ""8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
#[command(group = ArgGroup::new(""socket_address_group"").multiple(false).args(&[""socket_addr"", ""port""]))]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance. For example, `0.0.0.0:8080`.
    #[arg(long, default_value = None, value_parser = clap::value_parser!(SocketAddr), group = ""socket_address_group"")]
    socket_addr: Option<SocketAddr>,
    /// Port number
    #[arg(long, default_value = DEFAULT_PORT, value_parser = clap::value_parser!(u16), group = ""socket_address_group"")]
    port: u16,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[allow(clippy::needless_return)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = match cli.socket_addr {
        Some(addr) => addr,
        None => SocketAddr::from(([0, 0, 0, 0], cli.port)),
    };
    let port = addr.port().to_string();

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    info!(target: ""stdout"", ""Listening on {}"", addr);

    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}

```","Summary:
Based on the provided code, here's a summary of the changes in key areas:

1. **Server Configuration**:
   - The `Cli` struct is used to parse command-line arguments.
   - The server listens on a specified port (default 8080) and handles requests based on the provided path.

2. **Model Configurations**:
   - Two types of models can be configured: chat and embedding. Each model has its own set of parameters, such as context size, batch size, prompt template, number of predictions, and temperature.
   - The `ServerInfo` struct holds information about the server, including the node version (if available), API server details (type, version, plugin version, and port), chat model configuration, embedding model configuration, and other extra details.

3. **Error Handling**:
   - The code uses a custom error type (`ServerError`) to handle various types of errors that may occur during execution.
   - Each method has its own error handling mechanism in place to catch any potential exceptions and convert them into the custom `ServerError` type.

4. **Logging**:
   - The code uses a logging framework (wasi_logger) to log important events, such as server startup, incoming requests, and outgoing responses.
   - Log messages are categorized by their severity level (INFO, WARNING, ERROR), making it easier to track issues during execution.

5. **API Endpoints**:
   - The `/echo` endpoint simply returns a static ""echo test"" message.
   - The `/v1` endpoint is used to handle requests for the llama model, which involves calling the `handle_llama_request` function in the `backend` module.
   - All other endpoints are handled by serving static files from the specified path.

Overall, this code provides a robust and well-structured implementation of an API server that handles different types of requests, configures models with various parameters, and logs important events for debugging purposes."
"```rust:llama-api-server/src/main.rs
#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::{ArgGroup, Parser};
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default port
const DEFAULT_PORT: &str = ""8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
#[command(group = ArgGroup::new(""socket_address_group"").multiple(false).args(&[""socket_addr"", ""port""]))]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance. For example, `0.0.0.0:8080`.
    #[arg(long, default_value = None, value_parser = clap::value_parser!(SocketAddr), group = ""socket_address_group"")]
    socket_addr: Option<SocketAddr>,
    /// Port number
    #[arg(long, default_value = DEFAULT_PORT, value_parser = clap::value_parser!(u16), group = ""socket_address_group"")]
    port: u16,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[allow(clippy::needless_return)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = match cli.socket_addr {
        Some(addr) => addr,
        None => SocketAddr::from(([0, 0, 0, 0], cli.port)),
    };
    let port = addr.port().to_string();

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    info!(target: ""stdout"", ""Listening on {}"", addr);

    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}

```","Q: Here are 10 questions based on the given code:
A: I'm ready to help. What are the 10 questions based on the code?"
"```rust:llama-api-server/src/main.rs
#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::{ArgGroup, Parser};
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default port
const DEFAULT_PORT: &str = ""8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
#[command(group = ArgGroup::new(""socket_address_group"").multiple(false).args(&[""socket_addr"", ""port""]))]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance. For example, `0.0.0.0:8080`.
    #[arg(long, default_value = None, value_parser = clap::value_parser!(SocketAddr), group = ""socket_address_group"")]
    socket_addr: Option<SocketAddr>,
    /// Port number
    #[arg(long, default_value = DEFAULT_PORT, value_parser = clap::value_parser!(u16), group = ""socket_address_group"")]
    port: u16,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[allow(clippy::needless_return)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = match cli.socket_addr {
        Some(addr) => addr,
        None => SocketAddr::from(([0, 0, 0, 0], cli.port)),
    };
    let port = addr.port().to_string();

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    info!(target: ""stdout"", ""Listening on {}"", addr);

    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}

```","Q: 1. What is the default port number used by the LlamaEdge API Server?
A: The default port number used by the LlamaEdge API Server is 8080. This can be seen in the `DEFAULT_PORT` constant defined at the top of the code, which is set to the string ""8080""."
"```rust:llama-api-server/src/main.rs
#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::{ArgGroup, Parser};
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default port
const DEFAULT_PORT: &str = ""8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
#[command(group = ArgGroup::new(""socket_address_group"").multiple(false).args(&[""socket_addr"", ""port""]))]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance. For example, `0.0.0.0:8080`.
    #[arg(long, default_value = None, value_parser = clap::value_parser!(SocketAddr), group = ""socket_address_group"")]
    socket_addr: Option<SocketAddr>,
    /// Port number
    #[arg(long, default_value = DEFAULT_PORT, value_parser = clap::value_parser!(u16), group = ""socket_address_group"")]
    port: u16,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[allow(clippy::needless_return)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = match cli.socket_addr {
        Some(addr) => addr,
        None => SocketAddr::from(([0, 0, 0, 0], cli.port)),
    };
    let port = addr.port().to_string();

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    info!(target: ""stdout"", ""Listening on {}"", addr);

    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}

```","Q: 2. How can you specify multiple model names for chat and/or embedding models when running the server?
A: To specify multiple model names for chat and/or embedding models, you can use commas to separate the names in the following command-line arguments:

* `--model-name`: comma-separated list of model names for the chat model (default: ""default"")
* `--model-alias`: comma-separated list of model aliases for the chat model (default: ""default,embedding"")
* `--prompt-template`: comma-separated list of prompt templates for the chat and/or embedding models

For example:
```bash
llama-edge-server --model-name llama-2-7b,default-chat,all-minilm --model-alias llama-2-7b,chat,embedding --prompt-template llama-2-chat,embedding --ctx-size 4096,384,1024 --batch-size 128,64,512 --port 8080
```
In this example:

* The chat model will use three different names: ""llama-2-7b"", ""default-chat"", and ""all-minilm"".
* The embedding model will use a single name: ""embedding"".
* The prompt template for the chat model is set to ""llama-2-chat"" (although this will override the value of `--prompt-template`).
* The context size, batch size, and port are specified separately for each model.

Note that when specifying multiple model names or prompt templates, you must separate them with commas without spaces. Also, be aware that if you specify a different number of models for chat and embedding, some configurations may not work as expected (e.g., if you specify two chat models but only one embedding model).

Make sure to refer to the [official documentation](https://github.com/mljar/llama-edge-server) for more details on these arguments."
"```rust:llama-api-server/src/main.rs
#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::{ArgGroup, Parser};
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default port
const DEFAULT_PORT: &str = ""8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
#[command(group = ArgGroup::new(""socket_address_group"").multiple(false).args(&[""socket_addr"", ""port""]))]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance. For example, `0.0.0.0:8080`.
    #[arg(long, default_value = None, value_parser = clap::value_parser!(SocketAddr), group = ""socket_address_group"")]
    socket_addr: Option<SocketAddr>,
    /// Port number
    #[arg(long, default_value = DEFAULT_PORT, value_parser = clap::value_parser!(u16), group = ""socket_address_group"")]
    port: u16,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[allow(clippy::needless_return)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = match cli.socket_addr {
        Some(addr) => addr,
        None => SocketAddr::from(([0, 0, 0, 0], cli.port)),
    };
    let port = addr.port().to_string();

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    info!(target: ""stdout"", ""Listening on {}"", addr);

    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}

```","Q: 3. What is the purpose of the `log_prompts`, `log_stat`, and `log_all` arguments in the `Cli` struct?
A: The `log_prompts`, `log_stat`, and `log_all` arguments in the `Cli` struct are deprecated flags that were likely used to control logging behavior in a previous version of the LlamaEdge API Server.

* `log_prompts`: This flag would print prompt strings to stdout. However, this is now handled by the `reverse_prompt` argument, which allows users to specify a value for the `reverse_prompt` field.
* `log_stat`: This flag would print statistics to stdout. However, the server already logs various metrics and information to stdout, including request information, response status codes, and other data.
* `log_all`: This flag would enable logging of all log information to stdout. Again, this is not necessary as the server already logs relevant information to stdout by default.

In the current implementation, these flags are not used or validated, and their presence does not affect the behavior of the server. It's likely that they will be removed in a future version of the codebase.

**Bonus Tip:** To avoid using deprecated arguments in your own code, you can use tools like `rustfmt` and `clippy` to detect and report unused or deprecated features. Additionally, consider updating your dependencies regularly to ensure you're using the latest versions of libraries and frameworks."
"```rust:llama-api-server/src/main.rs
#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::{ArgGroup, Parser};
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default port
const DEFAULT_PORT: &str = ""8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
#[command(group = ArgGroup::new(""socket_address_group"").multiple(false).args(&[""socket_addr"", ""port""]))]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance. For example, `0.0.0.0:8080`.
    #[arg(long, default_value = None, value_parser = clap::value_parser!(SocketAddr), group = ""socket_address_group"")]
    socket_addr: Option<SocketAddr>,
    /// Port number
    #[arg(long, default_value = DEFAULT_PORT, value_parser = clap::value_parser!(u16), group = ""socket_address_group"")]
    port: u16,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[allow(clippy::needless_return)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = match cli.socket_addr {
        Some(addr) => addr,
        None => SocketAddr::from(([0, 0, 0, 0], cli.port)),
    };
    let port = addr.port().to_string();

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    info!(target: ""stdout"", ""Listening on {}"", addr);

    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}

```","Q: 4. How do you handle requests that match the `/echo` or `/v1` endpoints?
A: In the code, requests that match the `/echo` or `/v1` endpoints are handled in the following ways:

**For the `/echo` endpoint:**

```rust
let response = Response::new(Body::from(""echo test""));
```

Here, a simple `Response` is created with a `Body` containing the string ""echo test"". This means that any request to the `/echo` endpoint will return this exact response.

**For the `/v1` endpoint:**

```rust
let response = backend::handle_llama_request(req).await;
```

Here, the `backend::handle_llama_request` function is called with the current request (`req`) as an argument. This function is not defined in the provided code snippet, but it's likely responsible for handling requests to the `/v1` endpoint according to some specific logic.

**For any other endpoint:**

```rust
let response = static_response(path_str, web_ui);
```

Here, the `static_response` function is called with the current request path (`path_str`) and the root of the Web UI files (`web_ui`). This function returns a `Response` that serves static files from the Web UI directory.

So, to summarize:

* The `/echo` endpoint always returns a simple response.
* The `/v1` endpoint delegates its handling to some other function (`backend::handle_llama_request`) for specific logic.
* All other endpoints serve static files from the Web UI directory."
"```rust:llama-api-server/src/main.rs
#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::{ArgGroup, Parser};
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default port
const DEFAULT_PORT: &str = ""8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
#[command(group = ArgGroup::new(""socket_address_group"").multiple(false).args(&[""socket_addr"", ""port""]))]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance. For example, `0.0.0.0:8080`.
    #[arg(long, default_value = None, value_parser = clap::value_parser!(SocketAddr), group = ""socket_address_group"")]
    socket_addr: Option<SocketAddr>,
    /// Port number
    #[arg(long, default_value = DEFAULT_PORT, value_parser = clap::value_parser!(u16), group = ""socket_address_group"")]
    port: u16,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[allow(clippy::needless_return)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = match cli.socket_addr {
        Some(addr) => addr,
        None => SocketAddr::from(([0, 0, 0, 0], cli.port)),
    };
    let port = addr.port().to_string();

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    info!(target: ""stdout"", ""Listening on {}"", addr);

    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}

```","Q: 5. What is the `static_response` function used for?
A: The `static_response` function in the provided code is used to serve static responses from a file. It takes two arguments: `path_str`, which is the path of the file to be served, and `root`, which is the root directory of the files.

Here's how it works:

1. It checks if the requested path is the root directory (`""/""`) or not. If it is, it redirects the request to `""/index.html""` instead.
2. It uses the `mime_guess` crate to guess the MIME type of the file based on its extension.
3. It attempts to read the content of the file from disk using the provided `root` directory.
4. If the file exists and can be read successfully, it returns a HTTP response with the file's content as the body, using the guessed MIME type in the `Content-Type` header.
5. If the file does not exist or cannot be read for some reason (e.g., permissions issue), it returns a 404 Not Found response with a default HTML content.

The purpose of this function is to serve static files such as HTML pages, CSS stylesheets, JavaScript files, images, and other assets without requiring them to be processed by the server's routing or business logic. This can improve performance and reduce the load on the server.

For example, if you request a URL like `http://localhost:8080/index.html`, this function would serve the content of the `index.html` file located in the root directory (`root = ""/chatbot-ui""`).

Note that this implementation assumes that the files are stored in a directory structure that is accessible by the server. If you need to serve files from a different location or with different permissions, you may need to modify this function accordingly."
"```rust:llama-api-server/src/main.rs
#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::{ArgGroup, Parser};
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default port
const DEFAULT_PORT: &str = ""8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
#[command(group = ArgGroup::new(""socket_address_group"").multiple(false).args(&[""socket_addr"", ""port""]))]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance. For example, `0.0.0.0:8080`.
    #[arg(long, default_value = None, value_parser = clap::value_parser!(SocketAddr), group = ""socket_address_group"")]
    socket_addr: Option<SocketAddr>,
    /// Port number
    #[arg(long, default_value = DEFAULT_PORT, value_parser = clap::value_parser!(u16), group = ""socket_address_group"")]
    port: u16,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[allow(clippy::needless_return)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = match cli.socket_addr {
        Some(addr) => addr,
        None => SocketAddr::from(([0, 0, 0, 0], cli.port)),
    };
    let port = addr.port().to_string();

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    info!(target: ""stdout"", ""Listening on {}"", addr);

    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}

```","Q: 6. In what context would you use the `web_ui` argument when creating a new request?
A: The `web_ui` argument is used in the context of handling requests for a Web UI. 

When creating a new request, if it's a request to serve a static web page, you would use the `web_ui` argument. This argument is passed to the `static_response` function which generates a response with the contents of the requested web page.

Here are some examples when you would typically use the `web_ui` argument:

1. **Serving static web pages**: When handling requests for static HTML files, CSS files, or JavaScript files, you would use the `web_ui` argument to generate the correct response.
2. **Handling index.html requests**: If a user requests the root URL of your server (`/`), you may want to serve an `index.html` file. In this case, you would use the `web_ui` argument with the path to the `index.html` file.
3. **Serving 404 pages**: When a user requests a non-existent resource on your server, you can use the `web_ui` argument to serve a custom 404 page.

Here's an example of how you might use the `web_ui` argument when creating a new request:
```rust
let root = ""/path/to/web/ui"";
match path_str {
    ""/"" => ""/index.html"",
    _ => path_str,
}
let response = static_response(path_str, root);
```
In this example, the `static_response` function is called with the requested path (`path_str`) and the root path to the web UI (`root`). The function generates a response based on the requested resource."
"```rust:llama-api-server/src/main.rs
#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::{ArgGroup, Parser};
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default port
const DEFAULT_PORT: &str = ""8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
#[command(group = ArgGroup::new(""socket_address_group"").multiple(false).args(&[""socket_addr"", ""port""]))]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance. For example, `0.0.0.0:8080`.
    #[arg(long, default_value = None, value_parser = clap::value_parser!(SocketAddr), group = ""socket_address_group"")]
    socket_addr: Option<SocketAddr>,
    /// Port number
    #[arg(long, default_value = DEFAULT_PORT, value_parser = clap::value_parser!(u16), group = ""socket_address_group"")]
    port: u16,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[allow(clippy::needless_return)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = match cli.socket_addr {
        Some(addr) => addr,
        None => SocketAddr::from(([0, 0, 0, 0], cli.port)),
    };
    let port = addr.port().to_string();

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    info!(target: ""stdout"", ""Listening on {}"", addr);

    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}

```","Q: 7. How does the server determine which model configuration to use based on the request path?
A: The server determines which model configuration to use based on the request path in the following way:

1.  The `handle_request` function takes two arguments: `req` (a hyper Request object) and `web_ui` (the root path of the Web UI).
2.  It logs the request by printing the method, HTTP version, content length (if any), endpoint (request path), response status code, response body size, and whether the response is successful or not.
3.  The function checks if the request path matches certain patterns using an `if-else` statement:

    *   If the request path starts with ""/echo"", it returns a simple echo test response.
    *   If the request path is ""/v1"", it calls the `backend::handle_llama_request` function to handle the LLAMA API request, which likely uses the model configuration stored in the server's state.
    *   For any other request path, it calls the `static_response` function to return a static response from the Web UI.

This approach allows the server to select the appropriate model configuration based on the type of request being made. However, without more context about the specific models and their configurations, it is difficult to provide further details on how the server determines which model configuration to use.

**Bonus Tip:** 

You can add a custom route handling function by modifying the `handle_request` function to check for specific path prefixes or patterns and call different functions accordingly. This allows you to extend the server's functionality without modifying its core logic. For example:

```rust
match req.uri().path() {
    ""/custom-route"" => handle_custom_route(req, web_ui),
    _ => backend::handle_llama_request(req).await,
}
```

This way, when a request is made to the ""/custom-route"" path, it will be handled by the `handle_custom_route` function instead of the default LLAMA API handling."
"```rust:llama-api-server/src/main.rs
#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::{ArgGroup, Parser};
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default port
const DEFAULT_PORT: &str = ""8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
#[command(group = ArgGroup::new(""socket_address_group"").multiple(false).args(&[""socket_addr"", ""port""]))]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance. For example, `0.0.0.0:8080`.
    #[arg(long, default_value = None, value_parser = clap::value_parser!(SocketAddr), group = ""socket_address_group"")]
    socket_addr: Option<SocketAddr>,
    /// Port number
    #[arg(long, default_value = DEFAULT_PORT, value_parser = clap::value_parser!(u16), group = ""socket_address_group"")]
    port: u16,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[allow(clippy::needless_return)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = match cli.socket_addr {
        Some(addr) => addr,
        None => SocketAddr::from(([0, 0, 0, 0], cli.port)),
    };
    let port = addr.port().to_string();

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    info!(target: ""stdout"", ""Listening on {}"", addr);

    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}

```","Q: 8. What type of error handling does the server implement for requests that return 400-level status codes?
A: The server implements error handling for requests that return 400-level status codes by logging the error at the ""error"" level and including details such as the response body size, status code, and whether it's a client or server error. 

Here is an example from the `handle_request` function:

```rust
// log response
{
    let status_code = response.status();
    if status_code.as_u16() < 400 {
        // log response
        let response_version = format!(""{:?}"", response.version());
        info!(target: ""stdout"", ""response_version: {}"", response_version);
        let response_body_size: u64 = response.body().size_hint().lower();
        info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
        let response_status = status_code.as_u16();
        info!(target: ""stdout"", ""response_status: {}"", response_status);
        let response_is_success = status_code.is_success();
        info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
    } else {
        let response_version = format!(""{:?}"", response.version());
        error!(target: ""stdout"", ""response_version: {}"", response_version);
        let response_body_size: u64 = response.body().size_hint().lower();
        error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
        let response_status = status_code.as_u16();
        error!(target: ""stdout"", ""response_status: {}"", response_status);
        let response_is_success = status_code.is_success();
        error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        let response_is_client_error = status_code.is_client_error();
        error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
        let response_is_server_error = status_code.is_server_error();
        error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
    }
}
```

However, it does not handle these errors in a specific way that would allow the server to continue running or recover from the error. The server simply logs the error and continues with the next request. 

If you want to implement more sophisticated error handling, such as retrying the request after a certain amount of time, you will need to add additional logic to your `handle_request` function."
"```rust:llama-api-server/src/main.rs
#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::{ArgGroup, Parser};
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default port
const DEFAULT_PORT: &str = ""8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
#[command(group = ArgGroup::new(""socket_address_group"").multiple(false).args(&[""socket_addr"", ""port""]))]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance. For example, `0.0.0.0:8080`.
    #[arg(long, default_value = None, value_parser = clap::value_parser!(SocketAddr), group = ""socket_address_group"")]
    socket_addr: Option<SocketAddr>,
    /// Port number
    #[arg(long, default_value = DEFAULT_PORT, value_parser = clap::value_parser!(u16), group = ""socket_address_group"")]
    port: u16,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[allow(clippy::needless_return)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = match cli.socket_addr {
        Some(addr) => addr,
        None => SocketAddr::from(([0, 0, 0, 0], cli.port)),
    };
    let port = addr.port().to_string();

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    info!(target: ""stdout"", ""Listening on {}"", addr);

    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}

```","Q: 9. Can you explain how the server handles requests with missing or empty bodies?
A: The server handles requests with missing or empty bodies in the following way:

1. In the `handle_request` function, the `req` object is checked to see if it has a body:
   ```rust
let response = match root_path.as_str() {
    ""/echo"" => Response::new(Body::from(""echo test"")),
    ""/v1"" => backend::handle_llama_request(req).await,
    _ => static_response(path_str, web_ui),
};
```
   In this case, the `req` object is not checked for a body.

2. However, in the `backend::handle_llama_request` function (which is called when the URL `/v1` is requested), the request body is used:
   ```rust
let mut response = match req.method() {
    Method::POST => handle_post(req).await,
    _ => Ok(()) // Note: this will be unreachable if the code above ever passes, as it's a match statement with only one arm
};
```
   In the `handle_post` function (not shown here), the request body is used to read the POST data.

3. If the request method is GET or HEAD, the request body is not used:
   ```rust
let mut response = match req.method() {
    Method::GET => handle_get(req).await,
    Method::HEAD => handle_head(req).await,
    // ...
};
```
4. The `handle_request` function uses a pattern matching to determine which handler to call based on the URL path:
   ```rust
let response = match root_path.as_str() {
    ""/"" | ""/v1"" | ""/v2"" => backend::handle_llama_request(req).await,
    // ...
};
```
5. If none of these patterns match, a static response is returned using the `static_response` function:
   ```rust
let response = match root_path.as_str() {
    ""/"" => ""/index.html"",
    _ => path_str,
};

// ...

match std::fs::read(format!(""{root}/{path}"")) {
    Ok(content) => Response::builder()
        .status(StatusCode::OK)
        .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
        .body(Body::from(content))
        .unwrap(),
    Err(_) => {
        let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
        Response::builder()
            .status(StatusCode::NOT_FOUND)
            .header(header::CONTENT_TYPE, ""text/html"")
            .body(body)
            .unwrap()
    }
};
```
   This function reads the content of a file based on the URL path and returns a response with that content. If the file does not exist, it returns a 404 error.

In summary, the server handles requests with missing or empty bodies by:

* Checking if the request body is present in the `req` object.
* Using the request body to read POST data in the `handle_post` function.
* Ignoring the request body for GET and HEAD requests.
* Returning a static response using the `static_response` function if none of these patterns match.

Note that this explanation assumes that the code is correct and does not contain any errors. If there are any issues with the code, it may behave differently than expected."
"```rust:llama-api-server/src/main.rs
#[macro_use]
extern crate log;

mod backend;
mod error;
mod utils;

use anyhow::Result;
use chat_prompts::PromptTemplateType;
use clap::{ArgGroup, Parser};
use error::ServerError;
use hyper::{
    body::HttpBody,
    header,
    server::conn::AddrStream,
    service::{make_service_fn, service_fn},
    Body, Request, Response, Server, StatusCode,
};
use llama_core::MetadataBuilder;
use once_cell::sync::OnceCell;
use serde::{Deserialize, Serialize};
use std::{collections::HashMap, net::SocketAddr, path::PathBuf};
use tokio::net::TcpListener;
use utils::LogLevel;

type Error = Box<dyn std::error::Error + Send + Sync + 'static>;

// server info
pub(crate) static SERVER_INFO: OnceCell<ServerInfo> = OnceCell::new();

// default port
const DEFAULT_PORT: &str = ""8080"";

#[derive(Debug, Parser)]
#[command(name = ""LlamaEdge API Server"", version = env!(""CARGO_PKG_VERSION""), author = env!(""CARGO_PKG_AUTHORS""), about = ""LlamaEdge API Server"")]
#[command(group = ArgGroup::new(""socket_address_group"").multiple(false).args(&[""socket_addr"", ""port""]))]
struct Cli {
    /// Sets names for chat and/or embedding models. To run both chat and embedding models, the names should be separated by comma without space, for example, '--model-name Llama-2-7b,all-minilm'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""default"")]
    model_name: Vec<String>,
    /// Model aliases for chat and embedding models
    #[arg(
        short = 'a',
        long,
        value_delimiter = ',',
        default_value = ""default,embedding""
    )]
    model_alias: Vec<String>,
    /// Sets context sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--ctx-size 4096,384'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(
        short = 'c',
        long,
        value_delimiter = ',',
        default_value = ""4096,384"",
        value_parser = clap::value_parser!(u64)
    )]
    ctx_size: Vec<u64>,
    /// Sets batch sizes for chat and/or embedding models. To run both chat and embedding models, the sizes should be separated by comma without space, for example, '--batch-size 128,64'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', default_value = ""512,512"", value_parser = clap::value_parser!(u64))]
    batch_size: Vec<u64>,
    /// Sets prompt templates for chat and/or embedding models, respectively. To run both chat and embedding models, the prompt templates should be separated by comma without space, for example, '--prompt-template llama-2-chat,embedding'. The first value is for the chat model, and the second is for the embedding model.
    #[arg(short, long, value_delimiter = ',', value_parser = clap::value_parser!(PromptTemplateType), required = true)]
    prompt_template: Vec<PromptTemplateType>,
    /// Halt generation at PROMPT, return control.
    #[arg(short, long)]
    reverse_prompt: Option<String>,
    /// Number of tokens to predict
    #[arg(short, long, default_value = ""1024"")]
    n_predict: u64,
    /// Number of layers to run on the GPU
    #[arg(short = 'g', long, default_value = ""100"")]
    n_gpu_layers: u64,
    /// The main GPU to use.
    #[arg(long)]
    main_gpu: Option<u64>,
    /// How split tensors should be distributed accross GPUs. If None the model is not split; otherwise, a comma-separated list of non-negative values, e.g., ""3,2"" presents 60% of the data to GPU 0 and 40% to GPU 1.
    #[arg(long)]
    tensor_split: Option<String>,
    /// Number of threads to use during computation
    #[arg(long, default_value = ""2"")]
    threads: u64,
    /// Disable memory mapping for file access of chat models
    #[arg(long)]
    no_mmap: Option<bool>,
    /// Temperature for sampling
    #[arg(long, default_value = ""1.0"")]
    temp: f64,
    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. 1.0 = disabled
    #[arg(long, default_value = ""1.0"")]
    top_p: f64,
    /// Penalize repeat sequence of tokens
    #[arg(long, default_value = ""1.1"")]
    repeat_penalty: f64,
    /// Repeat alpha presence penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    presence_penalty: f64,
    /// Repeat alpha frequency penalty. 0.0 = disabled
    #[arg(long, default_value = ""0.0"")]
    frequency_penalty: f64,
    /// BNF-like grammar to constrain generations (see samples in grammars/ dir).
    #[arg(long, default_value = """")]
    pub grammar: String,
    /// JSON schema to constrain generations (https://json-schema.org/), e.g. `{}` for any JSON object. For schemas w/ external $refs, use --grammar + example/json_schema_to_grammar.py instead.
    #[arg(long)]
    pub json_schema: Option<String>,
    /// Path to the multimodal projector file
    #[arg(long)]
    llava_mmproj: Option<String>,
    /// Socket address of LlamaEdge API Server instance. For example, `0.0.0.0:8080`.
    #[arg(long, default_value = None, value_parser = clap::value_parser!(SocketAddr), group = ""socket_address_group"")]
    socket_addr: Option<SocketAddr>,
    /// Port number
    #[arg(long, default_value = DEFAULT_PORT, value_parser = clap::value_parser!(u16), group = ""socket_address_group"")]
    port: u16,
    /// Root path for the Web UI files
    #[arg(long, default_value = ""chatbot-ui"")]
    web_ui: PathBuf,
    /// Deprecated. Print prompt strings to stdout
    #[arg(long)]
    log_prompts: bool,
    /// Deprecated. Print statistics to stdout
    #[arg(long)]
    log_stat: bool,
    /// Deprecated. Print all log information to stdout
    #[arg(long)]
    log_all: bool,
}

#[allow(clippy::needless_return)]
#[tokio::main(flavor = ""current_thread"")]
async fn main() -> Result<(), ServerError> {
    let mut plugin_debug = false;

    // get the environment variable `RUST_LOG`
    let rust_log = std::env::var(""RUST_LOG"").unwrap_or_default().to_lowercase();
    let (_, log_level) = match rust_log.is_empty() {
        true => (""stdout"", LogLevel::Info),
        false => match rust_log.split_once(""="") {
            Some((target, level)) => (target, level.parse().unwrap_or(LogLevel::Info)),
            None => (""stdout"", rust_log.parse().unwrap_or(LogLevel::Info)),
        },
    };

    if log_level == LogLevel::Debug || log_level == LogLevel::Trace {
        plugin_debug = true;
    }

    // set global logger
    wasi_logger::Logger::install().expect(""failed to install wasi_logger::Logger"");
    log::set_max_level(log_level.into());

    // parse the command line arguments
    let cli = Cli::parse();

    // log the version of the server
    info!(target: ""stdout"", ""server version: {}"", env!(""CARGO_PKG_VERSION""));

    // log model names
    if cli.model_name.is_empty() && cli.model_name.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for model name. For running chat or embedding model, please specify a single model name. For running both chat and embedding models, please specify two model names: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    info!(target: ""stdout"", ""model_name: {}"", cli.model_name.join("","").to_string());

    // log model alias
    let mut model_alias = String::new();
    if cli.model_name.len() == 1 {
        model_alias.clone_from(&cli.model_alias[0]);
    } else if cli.model_alias.len() == 2 {
        model_alias = cli.model_alias.join("","").to_string();
    }
    info!(target: ""stdout"", ""model_alias: {}"", model_alias);

    // log context size
    if cli.ctx_size.is_empty() && cli.ctx_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for context size. For running chat or embedding model, please specify a single context size. For running both chat and embedding models, please specify two context sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut ctx_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        ctx_sizes_str = cli.ctx_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        ctx_sizes_str = cli
            .ctx_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""ctx_size: {}"", ctx_sizes_str);

    // log batch size
    if cli.batch_size.is_empty() && cli.batch_size.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""Invalid setting for batch size. For running chat or embedding model, please specify a single batch size. For running both chat and embedding models, please specify two batch sizes: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let mut batch_sizes_str = String::new();
    if cli.model_name.len() == 1 {
        batch_sizes_str = cli.batch_size[0].to_string();
    } else if cli.model_name.len() == 2 {
        batch_sizes_str = cli
            .batch_size
            .iter()
            .map(|n| n.to_string())
            .collect::<Vec<String>>()
            .join("","");
    }
    info!(target: ""stdout"", ""batch_size: {}"", batch_sizes_str);

    // log prompt template
    if cli.prompt_template.is_empty() && cli.prompt_template.len() > 2 {
        return Err(ServerError::ArgumentError(
            ""LlamaEdge API server requires prompt templates. For running chat or embedding model, please specify a single prompt template. For running both chat and embedding models, please specify two prompt templates: the first one for chat model, the other for embedding model."".to_owned(),
        ));
    }
    let prompt_template_str: String = cli
        .prompt_template
        .iter()
        .map(|n| n.to_string())
        .collect::<Vec<String>>()
        .join("","");
    info!(target: ""stdout"", ""prompt_template: {}"", prompt_template_str);
    if cli.model_name.len() != cli.prompt_template.len() {
        return Err(ServerError::ArgumentError(
            ""The number of model names and prompt templates must be the same."".to_owned(),
        ));
    }

    // log reverse prompt
    if let Some(reverse_prompt) = &cli.reverse_prompt {
        info!(target: ""stdout"", ""reverse_prompt: {}"", reverse_prompt);
    }

    // log n_predict
    info!(target: ""stdout"", ""n_predict: {}"", cli.n_predict);

    // log n_gpu_layers
    info!(target: ""stdout"", ""n_gpu_layers: {}"", cli.n_gpu_layers);

    // log main_gpu
    if let Some(main_gpu) = &cli.main_gpu {
        info!(target: ""stdout"", ""main_gpu: {}"", main_gpu);
    }

    // log tensor_split
    if let Some(tensor_split) = &cli.tensor_split {
        info!(target: ""stdout"", ""tensor_split: {}"", tensor_split);
    }

    // log threads
    info!(target: ""stdout"", ""threads: {}"", cli.threads);

    // log no_mmap
    if let Some(no_mmap) = &cli.no_mmap {
        info!(target: ""stdout"", ""no_mmap: {}"", no_mmap);
    }

    // log temperature
    info!(target: ""stdout"", ""temp: {}"", cli.temp);

    // log top-p sampling
    info!(target: ""stdout"", ""top_p: {}"", cli.top_p);

    // repeat penalty
    info!(target: ""stdout"", ""repeat_penalty: {}"", cli.repeat_penalty);

    // log presence penalty
    info!(target: ""stdout"", ""presence_penalty: {}"", cli.presence_penalty);

    // log frequency penalty
    info!(target: ""stdout"", ""frequency_penalty: {}"", cli.frequency_penalty);

    // log grammar
    if !cli.grammar.is_empty() {
        info!(target: ""stdout"", ""grammar: {}"", &cli.grammar);
    }

    // log json schema
    if let Some(json_schema) = &cli.json_schema {
        info!(target: ""stdout"", ""json_schema: {}"", json_schema);
    }

    // log multimodal projector
    if let Some(llava_mmproj) = &cli.llava_mmproj {
        info!(target: ""stdout"", ""llava_mmproj: {}"", llava_mmproj);
    }

    // initialize the core context
    let mut chat_model_config = None;
    let mut embedding_model_config = None;
    if cli.prompt_template.len() == 1 {
        match cli.prompt_template[0] {
            PromptTemplateType::Embedding => {
                // create a Metadata instance
                let metadata_embedding = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the embedding model config
                embedding_model_config = Some(ModelConfig {
                    name: metadata_embedding.model_name.clone(),
                    ty: ""embedding"".to_string(),
                    ctx_size: metadata_embedding.ctx_size,
                    batch_size: metadata_embedding.batch_size,
                    ..Default::default()
                });

                // initialize the core context
                llama_core::init_core_context(None, Some(&[metadata_embedding]))
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
            _ => {
                // create a Metadata instance
                let metadata_chat = MetadataBuilder::new(
                    cli.model_name[0].clone(),
                    cli.model_alias[0].clone(),
                    cli.prompt_template[0],
                )
                .with_ctx_size(cli.ctx_size[0])
                .with_batch_size(cli.batch_size[0])
                .with_n_predict(cli.n_predict)
                .with_n_gpu_layers(cli.n_gpu_layers)
                .with_main_gpu(cli.main_gpu)
                .with_tensor_split(cli.tensor_split)
                .with_threads(cli.threads)
                .disable_mmap(cli.no_mmap)
                .with_temperature(cli.temp)
                .with_top_p(cli.top_p)
                .with_repeat_penalty(cli.repeat_penalty)
                .with_presence_penalty(cli.presence_penalty)
                .with_frequency_penalty(cli.frequency_penalty)
                .with_grammar(cli.grammar)
                .with_json_schema(cli.json_schema)
                .with_reverse_prompt(cli.reverse_prompt)
                .with_mmproj(cli.llava_mmproj.clone())
                .enable_plugin_log(true)
                .enable_debug_log(plugin_debug)
                .build();

                // set the chat model config
                chat_model_config = Some(ModelConfig {
                    name: metadata_chat.model_name.clone(),
                    ty: ""chat"".to_string(),
                    ctx_size: metadata_chat.ctx_size,
                    batch_size: metadata_chat.batch_size,
                    prompt_template: Some(metadata_chat.prompt_template),
                    n_predict: Some(metadata_chat.n_predict),
                    reverse_prompt: metadata_chat.reverse_prompt.clone(),
                    n_gpu_layers: Some(metadata_chat.n_gpu_layers),
                    use_mmap: metadata_chat.use_mmap,
                    temperature: Some(metadata_chat.temperature),
                    top_p: Some(metadata_chat.top_p),
                    repeat_penalty: Some(metadata_chat.repeat_penalty),
                    presence_penalty: Some(metadata_chat.presence_penalty),
                    frequency_penalty: Some(metadata_chat.frequency_penalty),
                });

                // initialize the core context
                llama_core::init_core_context(Some(&[metadata_chat]), None)
                    .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
            }
        }
    } else if cli.prompt_template.len() == 2 {
        // create a Metadata instance
        let metadata_chat = MetadataBuilder::new(
            cli.model_name[0].clone(),
            cli.model_alias[0].clone(),
            cli.prompt_template[0],
        )
        .with_ctx_size(cli.ctx_size[0])
        .with_batch_size(cli.batch_size[0])
        .with_n_predict(cli.n_predict)
        .with_n_gpu_layers(cli.n_gpu_layers)
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split.clone())
        .with_threads(cli.threads)
        .disable_mmap(cli.no_mmap)
        .with_temperature(cli.temp)
        .with_top_p(cli.top_p)
        .with_repeat_penalty(cli.repeat_penalty)
        .with_presence_penalty(cli.presence_penalty)
        .with_frequency_penalty(cli.frequency_penalty)
        .with_grammar(cli.grammar)
        .with_json_schema(cli.json_schema)
        .with_reverse_prompt(cli.reverse_prompt)
        .with_mmproj(cli.llava_mmproj.clone())
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the chat model config
        chat_model_config = Some(ModelConfig {
            name: metadata_chat.model_name.clone(),
            ty: ""chat"".to_string(),
            ctx_size: metadata_chat.ctx_size,
            batch_size: metadata_chat.batch_size,
            prompt_template: Some(metadata_chat.prompt_template),
            n_predict: Some(metadata_chat.n_predict),
            reverse_prompt: metadata_chat.reverse_prompt.clone(),
            n_gpu_layers: Some(metadata_chat.n_gpu_layers),
            use_mmap: metadata_chat.use_mmap,
            temperature: Some(metadata_chat.temperature),
            top_p: Some(metadata_chat.top_p),
            repeat_penalty: Some(metadata_chat.repeat_penalty),
            presence_penalty: Some(metadata_chat.presence_penalty),
            frequency_penalty: Some(metadata_chat.frequency_penalty),
        });

        // create a Metadata instance
        let metadata_embedding = MetadataBuilder::new(
            cli.model_name[1].clone(),
            cli.model_alias[1].clone(),
            cli.prompt_template[1],
        )
        .with_ctx_size(cli.ctx_size[1])
        .with_batch_size(cli.batch_size[1])
        .with_main_gpu(cli.main_gpu)
        .with_tensor_split(cli.tensor_split)
        .with_threads(cli.threads)
        .enable_plugin_log(true)
        .enable_debug_log(plugin_debug)
        .build();

        // set the embedding model config
        embedding_model_config = Some(ModelConfig {
            name: metadata_embedding.model_name.clone(),
            ty: ""embedding"".to_string(),
            ctx_size: metadata_embedding.ctx_size,
            batch_size: metadata_embedding.batch_size,
            ..Default::default()
        });

        // initialize the core context
        llama_core::init_core_context(Some(&[metadata_chat]), Some(&[metadata_embedding]))
            .map_err(|e| ServerError::Operation(format!(""{}"", e)))?;
    }

    // log plugin version
    let plugin_info =
        llama_core::get_plugin_info().map_err(|e| ServerError::Operation(e.to_string()))?;
    let plugin_version = format!(
        ""b{build_number} (commit {commit_id})"",
        build_number = plugin_info.build_number,
        commit_id = plugin_info.commit_id,
    );
    info!(target: ""stdout"", ""plugin_ggml_version: {}"", plugin_version);

    // socket address
    let addr = match cli.socket_addr {
        Some(addr) => addr,
        None => SocketAddr::from(([0, 0, 0, 0], cli.port)),
    };
    let port = addr.port().to_string();

    // get the environment variable `NODE_VERSION`
    // Note that this is for satisfying the requirement of `gaianet-node` project.
    let node = std::env::var(""NODE_VERSION"").ok();
    if node.is_some() {
        // log node version
        info!(target: ""stdout"", ""gaianet_node_version: {}"", node.as_ref().unwrap());
    }

    // create server info
    let server_info = ServerInfo {
        node,
        server: ApiServer {
            ty: ""llama"".to_string(),
            version: env!(""CARGO_PKG_VERSION"").to_string(),
            plugin_version,
            port,
        },
        chat_model: chat_model_config,
        embedding_model: embedding_model_config,
        extras: HashMap::new(),
    };
    SERVER_INFO
        .set(server_info)
        .map_err(|_| ServerError::Operation(""Failed to set `SERVER_INFO`."".to_string()))?;

    let new_service = make_service_fn(move |conn: &AddrStream| {
        // log socket address
        info!(target: ""stdout"", ""remote_addr: {}, local_addr: {}"", conn.remote_addr().to_string(), conn.local_addr().to_string());

        // web ui
        let web_ui = cli.web_ui.to_string_lossy().to_string();

        async move { Ok::<_, Error>(service_fn(move |req| handle_request(req, web_ui.clone()))) }
    });

    let tcp_listener = TcpListener::bind(addr).await.unwrap();
    info!(target: ""stdout"", ""Listening on {}"", addr);

    let server = Server::from_tcp(tcp_listener.into_std().unwrap())
        .unwrap()
        .serve(new_service);

    match server.await {
        Ok(_) => Ok(()),
        Err(e) => Err(ServerError::Operation(e.to_string())),
    }
}

async fn handle_request(
    req: Request<Body>,
    web_ui: String,
) -> Result<Response<Body>, hyper::Error> {
    let path_str = req.uri().path();
    let path_buf = PathBuf::from(path_str);
    let mut path_iter = path_buf.iter();
    path_iter.next(); // Must be Some(OsStr::new(&path::MAIN_SEPARATOR.to_string()))
    let root_path = path_iter.next().unwrap_or_default();
    let root_path = ""/"".to_owned() + root_path.to_str().unwrap_or_default();

    // log request
    {
        let method = hyper::http::Method::as_str(req.method()).to_string();
        let path = req.uri().path().to_string();
        let version = format!(""{:?}"", req.version());
        if req.method() == hyper::http::Method::POST {
            let size: u64 = match req.headers().get(""content-length"") {
                Some(content_length) => content_length.to_str().unwrap().parse().unwrap(),
                None => 0,
            };

            info!(target: ""stdout"", ""method: {}, http_version: {}, content-length: {}"", method, version, size);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        } else {
            info!(target: ""stdout"", ""method: {}, http_version: {}"", method, version);
            info!(target: ""stdout"", ""endpoint: {}"", path);
        }
    }

    let response = match root_path.as_str() {
        ""/echo"" => Response::new(Body::from(""echo test"")),
        ""/v1"" => backend::handle_llama_request(req).await,
        _ => static_response(path_str, web_ui),
    };

    // log response
    {
        let status_code = response.status();
        if status_code.as_u16() < 400 {
            // log response
            let response_version = format!(""{:?}"", response.version());
            info!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            info!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            info!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            info!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
        } else {
            let response_version = format!(""{:?}"", response.version());
            error!(target: ""stdout"", ""response_version: {}"", response_version);
            let response_body_size: u64 = response.body().size_hint().lower();
            error!(target: ""stdout"", ""response_body_size: {}"", response_body_size);
            let response_status = status_code.as_u16();
            error!(target: ""stdout"", ""response_status: {}"", response_status);
            let response_is_success = status_code.is_success();
            error!(target: ""stdout"", ""response_is_success: {}"", response_is_success);
            let response_is_client_error = status_code.is_client_error();
            error!(target: ""stdout"", ""response_is_client_error: {}"", response_is_client_error);
            let response_is_server_error = status_code.is_server_error();
            error!(target: ""stdout"", ""response_is_server_error: {}"", response_is_server_error);
        }
    }

    Ok(response)
}

fn static_response(path_str: &str, root: String) -> Response<Body> {
    let path = match path_str {
        ""/"" => ""/index.html"",
        _ => path_str,
    };

    let mime = mime_guess::from_path(path);

    match std::fs::read(format!(""{root}/{path}"")) {
        Ok(content) => Response::builder()
            .status(StatusCode::OK)
            .header(header::CONTENT_TYPE, mime.first_or_text_plain().to_string())
            .body(Body::from(content))
            .unwrap(),
        Err(_) => {
            let body = Body::from(std::fs::read(format!(""{root}/404.html"")).unwrap_or_default());
            Response::builder()
                .status(StatusCode::NOT_FOUND)
                .header(header::CONTENT_TYPE, ""text/html"")
                .body(body)
                .unwrap()
        }
    }
}

#[derive(Clone, Debug)]
pub struct AppState {
    pub state_thing: String,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ServerInfo {
    #[serde(skip_serializing_if = ""Option::is_none"")]
    #[serde(rename = ""node_version"")]
    node: Option<String>,
    #[serde(rename = ""api_server"")]
    server: ApiServer,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    chat_model: Option<ModelConfig>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    embedding_model: Option<ModelConfig>,
    extras: HashMap<String, String>,
}

#[derive(Debug, Serialize, Deserialize)]
pub(crate) struct ApiServer {
    #[serde(rename = ""type"")]
    ty: String,
    version: String,
    #[serde(rename = ""ggml_plugin_version"")]
    plugin_version: String,
    port: String,
}

#[derive(Debug, Default, Serialize, Deserialize)]
pub(crate) struct ModelConfig {
    // model name
    name: String,
    // type: chat or embedding
    #[serde(rename = ""type"")]
    ty: String,
    pub ctx_size: u64,
    pub batch_size: u64,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub prompt_template: Option<PromptTemplateType>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_predict: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub reverse_prompt: Option<String>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub n_gpu_layers: Option<u64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub use_mmap: Option<bool>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub temperature: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub top_p: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub repeat_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub presence_penalty: Option<f64>,
    #[serde(skip_serializing_if = ""Option::is_none"")]
    pub frequency_penalty: Option<f64>,
}

```","Q: 10. In what format are the `ModelConfig` struct fields serialized when creating a new response?
A: The `ModelConfig` struct fields are serialized as JSON when creating a new response. This is because the `Serialize` trait is derived for the `ModelConfig` struct, which enables serialization to various formats including JSON.

When serializing an instance of `ModelConfig`, the following rules apply:

- The `name` and `ty` fields will be serialized as strings.
- The `ctx_size`, `batch_size`, `n_predict`, and `n_gpu_layers` fields will be serialized as integers (u64).
- If the value for a field is `None`, it will not be included in the serialized JSON.

Here's an example of what the serialized JSON might look like:

```
{
    ""name"": ""Model1"",
    ""type"": ""chat"",
    ""ctx_size"": 4096,
    ""batch_size"": 512
}
```

Or

```
{
    ""name"": ""Model2"",
    ""type"": ""embedding""
}
```

Note that the `prompt_template`, `reverse_prompt`, `use_mmap`, `temperature`, `top_p`, `repeat_penalty`, `presence_penalty`, and `frequency_penalty` fields are not included in this example because they have a value of `None`."
"```rust:llama-api-server/src/utils.rs
use serde::{Deserialize, Serialize};

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

#[derive(
    Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, clap::ValueEnum, Serialize, Deserialize,
)]
#[serde(rename_all = ""lowercase"")]
pub(crate) enum LogLevel {
    /// Describes messages about the values of variables and the flow of
    /// control within a program.
    Trace,

    /// Describes messages likely to be of interest to someone debugging a
    /// program.
    Debug,

    /// Describes messages likely to be of interest to someone monitoring a
    /// program.
    Info,

    /// Describes messages indicating hazardous situations.
    Warn,

    /// Describes messages indicating serious errors.
    Error,

    /// Describes messages indicating fatal errors.
    Critical,
}
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error,
        }
    }
}
impl std::fmt::Display for LogLevel {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            LogLevel::Trace => write!(f, ""trace""),
            LogLevel::Debug => write!(f, ""debug""),
            LogLevel::Info => write!(f, ""info""),
            LogLevel::Warn => write!(f, ""warn""),
            LogLevel::Error => write!(f, ""error""),
            LogLevel::Critical => write!(f, ""critical""),
        }
    }
}
impl std::str::FromStr for LogLevel {
    type Err = String;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""trace"" => Ok(LogLevel::Trace),
            ""debug"" => Ok(LogLevel::Debug),
            ""info"" => Ok(LogLevel::Info),
            ""warn"" => Ok(LogLevel::Warn),
            ""error"" => Ok(LogLevel::Error),
            ""critical"" => Ok(LogLevel::Critical),
            _ => Err(format!(""Invalid log level: {}"", s)),
        }
    }
}

```","Summary:
**Code-related Files: utils.rs**

The `utils.rs` file contains utility functions for the Llama API Server. Here's a summary of the key changes:

### 1. `gen_chat_id()` Function

* Generates a unique chat ID using the `uuid::Uuid::new_v4()` function.
* Returns a string in the format ""chatcmpl-<unique_id>""

**Purpose:** To provide a unique identifier for each chat session.

**Dependencies:** None.

### 2. `LogLevel` Enum

* A custom enum to represent different log levels (Trace, Debug, Info, Warn, Error, Critical).
* Each variant is associated with a corresponding log level filter from the `log` crate.
* Implements `From<LogLevel>` and `Display` traits for easy conversion between `LogLevel` and its string representation.

**Purpose:** To standardize log levels across the application.

**Dependencies:**

* `serde`
* `clap`
* `log`

**Note:** The `LogLevel` enum is designed to work with the `log` crate, which provides a unified logging interface for Rust applications.

### 3. `From<LogLevel>` and `Display` Implementations

* Allows conversion between `LogLevel` variants and their string representations.
* Enables easy formatting of log levels as strings.

**Purpose:** To make it easier to work with log levels in the application.

**Dependencies:**

* `serde`
* `clap`
* `log`

In summary, the `utils.rs` file provides utility functions for generating unique chat IDs and standardizing log levels across the application. These changes have implications for error handling and logging throughout the project."
"```rust:llama-api-server/src/utils.rs
use serde::{Deserialize, Serialize};

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

#[derive(
    Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, clap::ValueEnum, Serialize, Deserialize,
)]
#[serde(rename_all = ""lowercase"")]
pub(crate) enum LogLevel {
    /// Describes messages about the values of variables and the flow of
    /// control within a program.
    Trace,

    /// Describes messages likely to be of interest to someone debugging a
    /// program.
    Debug,

    /// Describes messages likely to be of interest to someone monitoring a
    /// program.
    Info,

    /// Describes messages indicating hazardous situations.
    Warn,

    /// Describes messages indicating serious errors.
    Error,

    /// Describes messages indicating fatal errors.
    Critical,
}
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error,
        }
    }
}
impl std::fmt::Display for LogLevel {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            LogLevel::Trace => write!(f, ""trace""),
            LogLevel::Debug => write!(f, ""debug""),
            LogLevel::Info => write!(f, ""info""),
            LogLevel::Warn => write!(f, ""warn""),
            LogLevel::Error => write!(f, ""error""),
            LogLevel::Critical => write!(f, ""critical""),
        }
    }
}
impl std::str::FromStr for LogLevel {
    type Err = String;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""trace"" => Ok(LogLevel::Trace),
            ""debug"" => Ok(LogLevel::Debug),
            ""info"" => Ok(LogLevel::Info),
            ""warn"" => Ok(LogLevel::Warn),
            ""error"" => Ok(LogLevel::Error),
            ""critical"" => Ok(LogLevel::Critical),
            _ => Err(format!(""Invalid log level: {}"", s)),
        }
    }
}

```","Q: What is the name of the library used for serialization and deserialization?
A: The library used for serialization and deserialization in this code is Serde.

Bonus Tip: Serde is a popular Rust library for serialization and deserialization. It supports multiple formats, including JSON, MessagePack, Protobuf, etc., and can be easily integrated with other Rust libraries. If you're working with data exchange or persistence in your project, consider using Serde to simplify the process of serializing and deserializing your data.

Example use case: Suppose you want to serialize a custom struct `Person` into JSON format using Serde:
```rust
use serde::{Serialize, Deserialize};
use serde_json;

#[derive(Serialize, Deserialize)]
struct Person {
    name: String,
    age: u32,
}

fn main() {
    let person = Person { name: ""John"".to_string(), age: 30 };
    let json = serde_json::to_string(&person).unwrap();
    println!(""{}"", json); // Output: {""name"":""John"",""age"":30}
}
```"
"```rust:llama-api-server/src/utils.rs
use serde::{Deserialize, Serialize};

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

#[derive(
    Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, clap::ValueEnum, Serialize, Deserialize,
)]
#[serde(rename_all = ""lowercase"")]
pub(crate) enum LogLevel {
    /// Describes messages about the values of variables and the flow of
    /// control within a program.
    Trace,

    /// Describes messages likely to be of interest to someone debugging a
    /// program.
    Debug,

    /// Describes messages likely to be of interest to someone monitoring a
    /// program.
    Info,

    /// Describes messages indicating hazardous situations.
    Warn,

    /// Describes messages indicating serious errors.
    Error,

    /// Describes messages indicating fatal errors.
    Critical,
}
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error,
        }
    }
}
impl std::fmt::Display for LogLevel {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            LogLevel::Trace => write!(f, ""trace""),
            LogLevel::Debug => write!(f, ""debug""),
            LogLevel::Info => write!(f, ""info""),
            LogLevel::Warn => write!(f, ""warn""),
            LogLevel::Error => write!(f, ""error""),
            LogLevel::Critical => write!(f, ""critical""),
        }
    }
}
impl std::str::FromStr for LogLevel {
    type Err = String;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""trace"" => Ok(LogLevel::Trace),
            ""debug"" => Ok(LogLevel::Debug),
            ""info"" => Ok(LogLevel::Info),
            ""warn"" => Ok(LogLevel::Warn),
            ""error"" => Ok(LogLevel::Error),
            ""critical"" => Ok(LogLevel::Critical),
            _ => Err(format!(""Invalid log level: {}"", s)),
        }
    }
}

```","Q: Where in the code are log levels being converted to log::LevelFilter enum values?
A: In the code, log levels are being converted to `log::LevelFilter` enum values in the following line:

```rust
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error, // Note: Critical is being mapped to Error here
        }
    }
}
```

This implementation defines an associated function `From` which takes a `LogLevel` value and returns the corresponding `log::LevelFilter` enum value. This conversion is done using a `match` statement that pattern matches on the input `level` and returns the corresponding `log::LevelFilter` value.

**Bonus Tip:** When defining custom conversions between enums, consider using the `Into` trait instead of `From` if the conversion is not an exact identity mapping (like this one). The `Into` trait allows for more flexibility in how the conversion is defined, and can be used to handle cases where the source type cannot be safely converted into a destination type."
"```rust:llama-api-server/src/utils.rs
use serde::{Deserialize, Serialize};

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

#[derive(
    Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, clap::ValueEnum, Serialize, Deserialize,
)]
#[serde(rename_all = ""lowercase"")]
pub(crate) enum LogLevel {
    /// Describes messages about the values of variables and the flow of
    /// control within a program.
    Trace,

    /// Describes messages likely to be of interest to someone debugging a
    /// program.
    Debug,

    /// Describes messages likely to be of interest to someone monitoring a
    /// program.
    Info,

    /// Describes messages indicating hazardous situations.
    Warn,

    /// Describes messages indicating serious errors.
    Error,

    /// Describes messages indicating fatal errors.
    Critical,
}
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error,
        }
    }
}
impl std::fmt::Display for LogLevel {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            LogLevel::Trace => write!(f, ""trace""),
            LogLevel::Debug => write!(f, ""debug""),
            LogLevel::Info => write!(f, ""info""),
            LogLevel::Warn => write!(f, ""warn""),
            LogLevel::Error => write!(f, ""error""),
            LogLevel::Critical => write!(f, ""critical""),
        }
    }
}
impl std::str::FromStr for LogLevel {
    type Err = String;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""trace"" => Ok(LogLevel::Trace),
            ""debug"" => Ok(LogLevel::Debug),
            ""info"" => Ok(LogLevel::Info),
            ""warn"" => Ok(LogLevel::Warn),
            ""error"" => Ok(LogLevel::Error),
            ""critical"" => Ok(LogLevel::Critical),
            _ => Err(format!(""Invalid log level: {}"", s)),
        }
    }
}

```","Q: What does the uuid::Uuid::new_v4() function return?
A: The `uuid::Uuid::new_v4()` function returns a new, randomly generated Universally Unique Identifier (UUID).

In the context of this code, it is used to generate a unique chat ID. UUIDs are 128-bit numbers that can be used to identify objects in a way that is highly likely to be unique across different systems and organizations.

When you run `uuid::Uuid::new_v4()`, it generates a random 128-bit number that looks something like this: `123e4567-e89b-12d3-a456-426655440000`.

**Bonus Tip:** UUIDs are not just for chat IDs. They can be used in many other situations where you need to ensure uniqueness, such as:

* Identifying users or customers
* Generating unique keys or tokens
* Creating identifiers for data entries or records
* And more!

When using UUIDs, keep in mind that there is a small chance (1 in 2^128) of generating a duplicate UUID. However, this is extremely rare and unlikely to occur in practice."
"```rust:llama-api-server/src/utils.rs
use serde::{Deserialize, Serialize};

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

#[derive(
    Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, clap::ValueEnum, Serialize, Deserialize,
)]
#[serde(rename_all = ""lowercase"")]
pub(crate) enum LogLevel {
    /// Describes messages about the values of variables and the flow of
    /// control within a program.
    Trace,

    /// Describes messages likely to be of interest to someone debugging a
    /// program.
    Debug,

    /// Describes messages likely to be of interest to someone monitoring a
    /// program.
    Info,

    /// Describes messages indicating hazardous situations.
    Warn,

    /// Describes messages indicating serious errors.
    Error,

    /// Describes messages indicating fatal errors.
    Critical,
}
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error,
        }
    }
}
impl std::fmt::Display for LogLevel {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            LogLevel::Trace => write!(f, ""trace""),
            LogLevel::Debug => write!(f, ""debug""),
            LogLevel::Info => write!(f, ""info""),
            LogLevel::Warn => write!(f, ""warn""),
            LogLevel::Error => write!(f, ""error""),
            LogLevel::Critical => write!(f, ""critical""),
        }
    }
}
impl std::str::FromStr for LogLevel {
    type Err = String;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""trace"" => Ok(LogLevel::Trace),
            ""debug"" => Ok(LogLevel::Debug),
            ""info"" => Ok(LogLevel::Info),
            ""warn"" => Ok(LogLevel::Warn),
            ""error"" => Ok(LogLevel::Error),
            ""critical"" => Ok(LogLevel::Critical),
            _ => Err(format!(""Invalid log level: {}"", s)),
        }
    }
}

```","Q: How many variants are defined in the LogLevel enum?
A: There are 6 variants defined in the LogLevel enum:

1. `Trace`
2. `Debug`
3. `Info`
4. `Warn`
5. `Error`
6. `Critical`

These variants represent different levels of severity for log messages, ranging from informational (Info) to critical errors (Critical)."
"```rust:llama-api-server/src/utils.rs
use serde::{Deserialize, Serialize};

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

#[derive(
    Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, clap::ValueEnum, Serialize, Deserialize,
)]
#[serde(rename_all = ""lowercase"")]
pub(crate) enum LogLevel {
    /// Describes messages about the values of variables and the flow of
    /// control within a program.
    Trace,

    /// Describes messages likely to be of interest to someone debugging a
    /// program.
    Debug,

    /// Describes messages likely to be of interest to someone monitoring a
    /// program.
    Info,

    /// Describes messages indicating hazardous situations.
    Warn,

    /// Describes messages indicating serious errors.
    Error,

    /// Describes messages indicating fatal errors.
    Critical,
}
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error,
        }
    }
}
impl std::fmt::Display for LogLevel {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            LogLevel::Trace => write!(f, ""trace""),
            LogLevel::Debug => write!(f, ""debug""),
            LogLevel::Info => write!(f, ""info""),
            LogLevel::Warn => write!(f, ""warn""),
            LogLevel::Error => write!(f, ""error""),
            LogLevel::Critical => write!(f, ""critical""),
        }
    }
}
impl std::str::FromStr for LogLevel {
    type Err = String;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""trace"" => Ok(LogLevel::Trace),
            ""debug"" => Ok(LogLevel::Debug),
            ""info"" => Ok(LogLevel::Info),
            ""warn"" => Ok(LogLevel::Warn),
            ""error"" => Ok(LogLevel::Error),
            ""critical"" => Ok(LogLevel::Critical),
            _ => Err(format!(""Invalid log level: {}"", s)),
        }
    }
}

```","Q: What is the default value of the ""critical"" log level in terms of its display string?
A: According to the `Display` implementation for `LogLevel`, when you try to display the ""critical"" log level, it writes the string ""critical"" to the formatter. Therefore, in terms of its display string, the default value of the ""critical"" log level is ""critical"".

Bonus Tip: In Rust, implementing the `Display` trait allows you to customize how your type's values are printed when used with the `{}` format specifier or the `println!()` macro. This can be useful for logging, debugging, and even creating human-readable output from your program.

Example usage:

```rust
fn main() {
    let log_level = LogLevel::Critical;
    println!(""The critical log level is: {}"", log_level);  // Outputs: The critical log level is: critical
}
```

Note that if you try to use the `critical` string with an uppercase ""C"", it won't match, because all log levels in this codebase are converted to lowercase before matching. This helps make the logging more robust and less case-sensitive:

```rust
fn main() {
    let log_level = ""Critical"".to_lowercase();
    println!(""Is critical a valid log level? {}"", LogLevel::from_str(&log_level).is_ok());
    // Outputs: Is critical a valid log level? false
}
```"
"```rust:llama-api-server/src/utils.rs
use serde::{Deserialize, Serialize};

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

#[derive(
    Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, clap::ValueEnum, Serialize, Deserialize,
)]
#[serde(rename_all = ""lowercase"")]
pub(crate) enum LogLevel {
    /// Describes messages about the values of variables and the flow of
    /// control within a program.
    Trace,

    /// Describes messages likely to be of interest to someone debugging a
    /// program.
    Debug,

    /// Describes messages likely to be of interest to someone monitoring a
    /// program.
    Info,

    /// Describes messages indicating hazardous situations.
    Warn,

    /// Describes messages indicating serious errors.
    Error,

    /// Describes messages indicating fatal errors.
    Critical,
}
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error,
        }
    }
}
impl std::fmt::Display for LogLevel {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            LogLevel::Trace => write!(f, ""trace""),
            LogLevel::Debug => write!(f, ""debug""),
            LogLevel::Info => write!(f, ""info""),
            LogLevel::Warn => write!(f, ""warn""),
            LogLevel::Error => write!(f, ""error""),
            LogLevel::Critical => write!(f, ""critical""),
        }
    }
}
impl std::str::FromStr for LogLevel {
    type Err = String;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""trace"" => Ok(LogLevel::Trace),
            ""debug"" => Ok(LogLevel::Debug),
            ""info"" => Ok(LogLevel::Info),
            ""warn"" => Ok(LogLevel::Warn),
            ""error"" => Ok(LogLevel::Error),
            ""critical"" => Ok(LogLevel::Critical),
            _ => Err(format!(""Invalid log level: {}"", s)),
        }
    }
}

```","Q: In which format is a chat ID generated in the gen_chat_id() function?
A: The chat ID is generated in the format ""chatcmpl-<UUID>"" where `<UUID>` is a randomly generated Universally Unique Identifier (UUID) created using the `new_v4()` method from the `uuid` crate.

So, an example of a generated chat ID would be:

`""chatcmpl-f3b1a44d-e5ca-40ec-b5ed-be14c7df78f6""`

This format is used to uniquely identify each chat in the system."
"```rust:llama-api-server/src/utils.rs
use serde::{Deserialize, Serialize};

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

#[derive(
    Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, clap::ValueEnum, Serialize, Deserialize,
)]
#[serde(rename_all = ""lowercase"")]
pub(crate) enum LogLevel {
    /// Describes messages about the values of variables and the flow of
    /// control within a program.
    Trace,

    /// Describes messages likely to be of interest to someone debugging a
    /// program.
    Debug,

    /// Describes messages likely to be of interest to someone monitoring a
    /// program.
    Info,

    /// Describes messages indicating hazardous situations.
    Warn,

    /// Describes messages indicating serious errors.
    Error,

    /// Describes messages indicating fatal errors.
    Critical,
}
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error,
        }
    }
}
impl std::fmt::Display for LogLevel {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            LogLevel::Trace => write!(f, ""trace""),
            LogLevel::Debug => write!(f, ""debug""),
            LogLevel::Info => write!(f, ""info""),
            LogLevel::Warn => write!(f, ""warn""),
            LogLevel::Error => write!(f, ""error""),
            LogLevel::Critical => write!(f, ""critical""),
        }
    }
}
impl std::str::FromStr for LogLevel {
    type Err = String;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""trace"" => Ok(LogLevel::Trace),
            ""debug"" => Ok(LogLevel::Debug),
            ""info"" => Ok(LogLevel::Info),
            ""warn"" => Ok(LogLevel::Warn),
            ""error"" => Ok(LogLevel::Error),
            ""critical"" => Ok(LogLevel::Critical),
            _ => Err(format!(""Invalid log level: {}"", s)),
        }
    }
}

```","Q: What is the name of the module being implemented for standard library traits?
A: The module being implemented for standard library traits is `log`. 

In Rust, the `std::log` module provides a way to handle logging in your program. It allows you to control how messages are output and can be configured to write to different places (e.g., console, file) with different levels of verbosity.

The `log` module is part of the Rust standard library, and its traits such as `LevelFilter`, `Log` etc., can be used to configure logging in your program. 

Here's a bonus tip: When using the `log` module, you should usually import it like this:

```rust
use log::{info, debug, error};
```

This allows you to use the various logging macros (e.g., `info!`, `debug!`, `error!`) without having to prefix them with `std::log::*`."
"```rust:llama-api-server/src/utils.rs
use serde::{Deserialize, Serialize};

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

#[derive(
    Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, clap::ValueEnum, Serialize, Deserialize,
)]
#[serde(rename_all = ""lowercase"")]
pub(crate) enum LogLevel {
    /// Describes messages about the values of variables and the flow of
    /// control within a program.
    Trace,

    /// Describes messages likely to be of interest to someone debugging a
    /// program.
    Debug,

    /// Describes messages likely to be of interest to someone monitoring a
    /// program.
    Info,

    /// Describes messages indicating hazardous situations.
    Warn,

    /// Describes messages indicating serious errors.
    Error,

    /// Describes messages indicating fatal errors.
    Critical,
}
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error,
        }
    }
}
impl std::fmt::Display for LogLevel {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            LogLevel::Trace => write!(f, ""trace""),
            LogLevel::Debug => write!(f, ""debug""),
            LogLevel::Info => write!(f, ""info""),
            LogLevel::Warn => write!(f, ""warn""),
            LogLevel::Error => write!(f, ""error""),
            LogLevel::Critical => write!(f, ""critical""),
        }
    }
}
impl std::str::FromStr for LogLevel {
    type Err = String;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""trace"" => Ok(LogLevel::Trace),
            ""debug"" => Ok(LogLevel::Debug),
            ""info"" => Ok(LogLevel::Info),
            ""warn"" => Ok(LogLevel::Warn),
            ""error"" => Ok(LogLevel::Error),
            ""critical"" => Ok(LogLevel::Critical),
            _ => Err(format!(""Invalid log level: {}"", s)),
        }
    }
}

```","Q: Which standard library trait's implementation involves using a match statement to perform a type conversion?
A: The `impl From<LogLevel> for log::LevelFilter` involves using a match statement to perform a type conversion.

In this case, the match statement is used in the `From` implementation block to convert an enum value of type `LogLevel` into a `log::LevelFilter`. The specific types that can be converted from `LogLevel` are then matched against and their corresponding values are returned as the result of the conversion.

Here's how it works:

```rust
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error,
        }
    }
}
```

**Bonus Tip:** When implementing type conversions in Rust, consider using `std::convert` traits like `From`, `Into`, and `TryInto`. These traits provide a way to specify how one type can be converted into another, making your code more flexible and easier to reason about."
"```rust:llama-api-server/src/utils.rs
use serde::{Deserialize, Serialize};

pub(crate) fn gen_chat_id() -> String {
    format!(""chatcmpl-{}"", uuid::Uuid::new_v4())
}

#[derive(
    Debug, Copy, Clone, PartialEq, Eq, PartialOrd, Ord, clap::ValueEnum, Serialize, Deserialize,
)]
#[serde(rename_all = ""lowercase"")]
pub(crate) enum LogLevel {
    /// Describes messages about the values of variables and the flow of
    /// control within a program.
    Trace,

    /// Describes messages likely to be of interest to someone debugging a
    /// program.
    Debug,

    /// Describes messages likely to be of interest to someone monitoring a
    /// program.
    Info,

    /// Describes messages indicating hazardous situations.
    Warn,

    /// Describes messages indicating serious errors.
    Error,

    /// Describes messages indicating fatal errors.
    Critical,
}
impl From<LogLevel> for log::LevelFilter {
    fn from(level: LogLevel) -> Self {
        match level {
            LogLevel::Trace => log::LevelFilter::Trace,
            LogLevel::Debug => log::LevelFilter::Debug,
            LogLevel::Info => log::LevelFilter::Info,
            LogLevel::Warn => log::LevelFilter::Warn,
            LogLevel::Error => log::LevelFilter::Error,
            LogLevel::Critical => log::LevelFilter::Error,
        }
    }
}
impl std::fmt::Display for LogLevel {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            LogLevel::Trace => write!(f, ""trace""),
            LogLevel::Debug => write!(f, ""debug""),
            LogLevel::Info => write!(f, ""info""),
            LogLevel::Warn => write!(f, ""warn""),
            LogLevel::Error => write!(f, ""error""),
            LogLevel::Critical => write!(f, ""critical""),
        }
    }
}
impl std::str::FromStr for LogLevel {
    type Err = String;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""trace"" => Ok(LogLevel::Trace),
            ""debug"" => Ok(LogLevel::Debug),
            ""info"" => Ok(LogLevel::Info),
            ""warn"" => Ok(LogLevel::Warn),
            ""error"" => Ok(LogLevel::Error),
            ""critical"" => Ok(LogLevel::Critical),
            _ => Err(format!(""Invalid log level: {}"", s)),
        }
    }
}

```","Q: How many possible error values are returned by the FromStr implementation for LogLevel?
A: The `FromStr` implementation for `LogLevel` returns an error type of `String`. According to the `match` statement in the `from_str` function, there is only one possible value that will cause an error: if the input string does not match any of the valid log level names (""trace"", ""debug"", ""info"", ""warn"", ""error"", or ""critical"").

However, it's worth noting that the error message generated by this implementation will always be in the same format: `""Invalid log level: <input_string>""`. Therefore, there is only 1 unique possible error value (excluding the input string itself), which is the string `""Invalid log level""`.

**Bonus Tip**: When implementing a custom `FromStr` trait for an enum, consider using a more specific error type than `String`. For example, you could create an enum with variants for each type of parsing error and return one of those variants if the input string cannot be parsed correctly. This can make it easier to handle errors in your code and provide more informative error messages. 

Here's an example:

```rust
enum ParseError {
    InvalidLogLevel,
    // Add other possible parse errors here...
}

impl std::str::FromStr for LogLevel {
    type Err = ParseError;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            ""trace"" => Ok(LogLevel::Trace),
            // ...
            _ => Err(ParseError::InvalidLogLevel),
        }
    }
}
```"
"```rust:llama-api-server/src/error.rs
use hyper::{Body, Response};
use thiserror::Error;

#[allow(dead_code)]
pub(crate) fn not_implemented() -> Response<Body> {
    // log error
    error!(target: ""stdout"", ""501 Not Implemented"");

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_IMPLEMENTED)
        .body(Body::from(""501 Not Implemented""))
        .unwrap()
}

pub(crate) fn internal_server_error(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""500 Internal Server Error"".to_string(),
        false => format!(""500 Internal Server Error: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::INTERNAL_SERVER_ERROR)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn bad_request(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""400 Bad Request"".to_string(),
        false => format!(""400 Bad Request: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::BAD_REQUEST)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn invalid_endpoint(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""404 The requested service endpoint is not found"".to_string(),
        false => format!(
            ""404 The requested service endpoint is not found: {}"",
            msg.as_ref()
        ),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND)
        .body(Body::from(err_msg))
        .unwrap()
}

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum ServerError {
    /// Error returned while parsing CLI options failed
    #[error(""{0}"")]
    ArgumentError(String),
    /// Generic error returned while performing an operation
    #[error(""{0}"")]
    Operation(String),
}

```","Summary:
**Summary of Code-related Files:**

The code snippet is from a Rust file named `error.rs` in the `llama-api-server/src` directory. It appears to be part of an API server implementation.

**Key Changes and Additions:**

* The file introduces several new functions for handling HTTP errors:
	+ `not_implemented`: Returns a 501 Not Implemented response.
	+ `internal_server_error`: Returns a 500 Internal Server Error response with an optional error message.
	+ `bad_request`: Returns a 400 Bad Request response with an optional error message.
	+ `invalid_endpoint`: Returns a 404 The requested service endpoint is not found response with an optional error message.
* Each function creates an HTTP response using the `hyper` crate and sets specific headers for CORS (Cross-Origin Resource Sharing) support.
* All functions log errors to the stdout target using the `error!` macro from the `thiserror` crate.

**New Functions:**

* The file introduces a new enum `ServerError` with two variants:
	+ `ArgumentError`: Represents an error returned while parsing CLI options failed.
	+ `Operation`: Represents a generic error returned while performing an operation.

**Impact on the Project:**

* These functions provide a centralized way to handle HTTP errors, making it easier to manage and debug issues in the API server.
* The use of the `hyper` crate for creating HTTP responses and the `thiserror` crate for error handling follows best practices for building robust APIs.
* The addition of CORS headers ensures that the API server can be accessed from different origins.

**Dependencies:**

* `hyper`: A Rust library for building HTTP servers and clients.
* `thiserror`: A Rust library for creating custom error types using macros.

**Error Handling and Performance Implications:**

* The functions handle errors by logging them to stdout and returning an HTTP response with a suitable status code.
* This approach ensures that errors are properly handled without blocking or crashing the API server, maintaining its overall performance."
"```rust:llama-api-server/src/error.rs
use hyper::{Body, Response};
use thiserror::Error;

#[allow(dead_code)]
pub(crate) fn not_implemented() -> Response<Body> {
    // log error
    error!(target: ""stdout"", ""501 Not Implemented"");

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_IMPLEMENTED)
        .body(Body::from(""501 Not Implemented""))
        .unwrap()
}

pub(crate) fn internal_server_error(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""500 Internal Server Error"".to_string(),
        false => format!(""500 Internal Server Error: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::INTERNAL_SERVER_ERROR)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn bad_request(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""400 Bad Request"".to_string(),
        false => format!(""400 Bad Request: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::BAD_REQUEST)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn invalid_endpoint(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""404 The requested service endpoint is not found"".to_string(),
        false => format!(
            ""404 The requested service endpoint is not found: {}"",
            msg.as_ref()
        ),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND)
        .body(Body::from(err_msg))
        .unwrap()
}

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum ServerError {
    /// Error returned while parsing CLI options failed
    #[error(""{0}"")]
    ArgumentError(String),
    /// Generic error returned while performing an operation
    #[error(""{0}"")]
    Operation(String),
}

```","Q: What is the status code of a ""501 Not Implemented"" response?
A: According to the code provided, the status code of a ""501 Not Implemented"" response is `hyper::StatusCode::NOT_IMPLEMENTED`, which is equivalent to 501.

This is specified in the `not_implemented` function, where it says:
```rust
Response::builder()
    .status(hyper::StatusCode::NOT_IMPLEMENTED)
    .body(Body::from(""501 Not Implemented""))
    .unwrap()
```
So, the status code for a ""501 Not Implemented"" response is indeed 501. 

**Bonus Tip:** The HTTP Status Code 501 is often referred to as ""Not Implemented"". This status code indicates that the server understands the request but refuses to attempt to fulfill it because the method is not implemented or is otherwise not supported by this server.

Example of when you might encounter a 501 error: You try to access a new feature on your favorite website, but the site's developers haven't implemented the functionality yet. The website returns a ""501 Not Implemented"" error, indicating that they're still working on it!"
"```rust:llama-api-server/src/error.rs
use hyper::{Body, Response};
use thiserror::Error;

#[allow(dead_code)]
pub(crate) fn not_implemented() -> Response<Body> {
    // log error
    error!(target: ""stdout"", ""501 Not Implemented"");

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_IMPLEMENTED)
        .body(Body::from(""501 Not Implemented""))
        .unwrap()
}

pub(crate) fn internal_server_error(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""500 Internal Server Error"".to_string(),
        false => format!(""500 Internal Server Error: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::INTERNAL_SERVER_ERROR)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn bad_request(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""400 Bad Request"".to_string(),
        false => format!(""400 Bad Request: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::BAD_REQUEST)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn invalid_endpoint(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""404 The requested service endpoint is not found"".to_string(),
        false => format!(
            ""404 The requested service endpoint is not found: {}"",
            msg.as_ref()
        ),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND)
        .body(Body::from(err_msg))
        .unwrap()
}

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum ServerError {
    /// Error returned while parsing CLI options failed
    #[error(""{0}"")]
    ArgumentError(String),
    /// Generic error returned while performing an operation
    #[error(""{0}"")]
    Operation(String),
}

```","Q: Is the `not_implemented` function in the provided code logging any errors to the console?
A: Yes, the `not_implemented` function is logging an error to the console using the `error!` macro.

```rust
// log error
error!(target: ""stdout"", ""501 Not Implemented"");
```

In this line of code:

- The `target: ""stdout""` argument specifies that the log should be printed to the standard output (i.e., the console).
- `""501 Not Implemented""` is the error message being logged.

This means when the function is called, it will print a message saying ""501 Not Implemented"" to the console.

**Bonus Tip:** When using logging macros in Rust, make sure that the target destination actually exists. In this case, since `stdout` refers to the standard output (console), you should be fine. However, if you were targeting an invalid or non-existent file descriptor, like `invalid_target`, you would get a runtime error.

Example: This would fail at runtime with an error message indicating that the target destination is not valid.
```rust
error!(target: ""non_existent_file"", ""Hello world!"");
```
This might look something like this:
```text
thread 'main' panicked at 'Invalid target for log output', path/to/your/rust/project/error.rs:42:30
```"
"```rust:llama-api-server/src/error.rs
use hyper::{Body, Response};
use thiserror::Error;

#[allow(dead_code)]
pub(crate) fn not_implemented() -> Response<Body> {
    // log error
    error!(target: ""stdout"", ""501 Not Implemented"");

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_IMPLEMENTED)
        .body(Body::from(""501 Not Implemented""))
        .unwrap()
}

pub(crate) fn internal_server_error(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""500 Internal Server Error"".to_string(),
        false => format!(""500 Internal Server Error: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::INTERNAL_SERVER_ERROR)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn bad_request(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""400 Bad Request"".to_string(),
        false => format!(""400 Bad Request: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::BAD_REQUEST)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn invalid_endpoint(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""404 The requested service endpoint is not found"".to_string(),
        false => format!(
            ""404 The requested service endpoint is not found: {}"",
            msg.as_ref()
        ),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND)
        .body(Body::from(err_msg))
        .unwrap()
}

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum ServerError {
    /// Error returned while parsing CLI options failed
    #[error(""{0}"")]
    ArgumentError(String),
    /// Generic error returned while performing an operation
    #[error(""{0}"")]
    Operation(String),
}

```","Q: Does the `internal_server_error` function return an error message that can be formatted with a custom string?
A: Yes, the `internal_server_error` function returns a response that includes an error message which can be formatted with a custom string. 

The line where it does this is: 
```rust
let err_msg = match msg.as_ref().is_empty() {
    true => ""500 Internal Server Error"".to_string(),
    false => format!(""500 Internal Server Error: {}"", msg.as_ref()),
};
```
Here, `msg` is a string that can be used to customize the error message. 

When it's not empty (the `false` case), it formats the error message with `msg` by using `format!`, which returns a new string containing the formatted text.

So you could call this function and pass a custom error message like so:
```rust
let err_msg = internal_server_error(""Invalid token"");
```
This would return a response with the following body:
```
""500 Internal Server Error: Invalid token""
```

Bonus Tip: When implementing functions that handle errors, consider using pattern matching to ensure that all potential error cases are handled. This can help catch unexpected issues and prevent your program from crashing unexpectedly."
"```rust:llama-api-server/src/error.rs
use hyper::{Body, Response};
use thiserror::Error;

#[allow(dead_code)]
pub(crate) fn not_implemented() -> Response<Body> {
    // log error
    error!(target: ""stdout"", ""501 Not Implemented"");

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_IMPLEMENTED)
        .body(Body::from(""501 Not Implemented""))
        .unwrap()
}

pub(crate) fn internal_server_error(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""500 Internal Server Error"".to_string(),
        false => format!(""500 Internal Server Error: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::INTERNAL_SERVER_ERROR)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn bad_request(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""400 Bad Request"".to_string(),
        false => format!(""400 Bad Request: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::BAD_REQUEST)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn invalid_endpoint(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""404 The requested service endpoint is not found"".to_string(),
        false => format!(
            ""404 The requested service endpoint is not found: {}"",
            msg.as_ref()
        ),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND)
        .body(Body::from(err_msg))
        .unwrap()
}

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum ServerError {
    /// Error returned while parsing CLI options failed
    #[error(""{0}"")]
    ArgumentError(String),
    /// Generic error returned while performing an operation
    #[error(""{0}"")]
    Operation(String),
}

```","Q: Can a user pass a custom error message when calling the `bad_request` function?
A: Yes, a user can pass a custom error message when calling the `bad_request` function.

Here is an example:

```rust
let err_msg = ""Your email address is invalid"".to_string();  // Custom error message
let response = bad_request(err_msg);
```

In this case, when `bad_request` is called with a custom error message (in this instance `""Your email address is invalid""`), the function will return an HTTP response with the corresponding `400 Bad Request` status code and the provided custom error message as the body of the response.

The custom error message will be logged to stdout along with the default ""400 Bad Request"" message for debugging purposes:

```bash
400 Bad Request: Your email address is invalid
```

**Bonus Tip**: It's also possible to use `bad_request` function with a custom error message and other error types like string slice by passing it through `as_ref()` method. For example, 

```rust
let response = bad_request(""Custom Error"");
or 
let response = bad_request(&""Custom Error""[..]);
```
Both will produce the same result: an HTTP response with `400 Bad Request` status code and a custom error message as body."
"```rust:llama-api-server/src/error.rs
use hyper::{Body, Response};
use thiserror::Error;

#[allow(dead_code)]
pub(crate) fn not_implemented() -> Response<Body> {
    // log error
    error!(target: ""stdout"", ""501 Not Implemented"");

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_IMPLEMENTED)
        .body(Body::from(""501 Not Implemented""))
        .unwrap()
}

pub(crate) fn internal_server_error(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""500 Internal Server Error"".to_string(),
        false => format!(""500 Internal Server Error: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::INTERNAL_SERVER_ERROR)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn bad_request(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""400 Bad Request"".to_string(),
        false => format!(""400 Bad Request: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::BAD_REQUEST)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn invalid_endpoint(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""404 The requested service endpoint is not found"".to_string(),
        false => format!(
            ""404 The requested service endpoint is not found: {}"",
            msg.as_ref()
        ),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND)
        .body(Body::from(err_msg))
        .unwrap()
}

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum ServerError {
    /// Error returned while parsing CLI options failed
    #[error(""{0}"")]
    ArgumentError(String),
    /// Generic error returned while performing an operation
    #[error(""{0}"")]
    Operation(String),
}

```","Q: Does the `invalid_endpoint` function use the `NOT_FOUND` status code to indicate that a requested endpoint is not found?
A: Yes, the `invalid_endpoint` function does use the `NOT_FOUND` status code (HTTP 404) to indicate that a requested service endpoint is not found. This aligns with the standard HTTP protocol specification, which defines the `NOT_FOUND` status code as indicating that ""the server has no knowledge of the target resource"". 

However, it's worth noting that the function also includes an additional error message in the response body, which provides more information about the specific endpoint that was not found. This is a common practice when handling HTTP errors to provide more context and debugging information.

**Bonus Tip:** When building APIs or web services, it's essential to handle edge cases like this carefully, as they can often be indicative of issues in the application code or configuration. The `invalid_endpoint` function serves as a good example of how to properly handle such situations by including informative error messages and using standardized HTTP status codes.

**Example Outside of Context:** Consider a scenario where an e-commerce website uses RESTful API endpoints to manage product information. If the customer requests data for a product that does not exist in the database, the `invalid_endpoint` function would return a 404 response with a message indicating that ""The requested product is not found."" This allows clients to handle such errors gracefully and provide a better user experience."
"```rust:llama-api-server/src/error.rs
use hyper::{Body, Response};
use thiserror::Error;

#[allow(dead_code)]
pub(crate) fn not_implemented() -> Response<Body> {
    // log error
    error!(target: ""stdout"", ""501 Not Implemented"");

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_IMPLEMENTED)
        .body(Body::from(""501 Not Implemented""))
        .unwrap()
}

pub(crate) fn internal_server_error(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""500 Internal Server Error"".to_string(),
        false => format!(""500 Internal Server Error: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::INTERNAL_SERVER_ERROR)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn bad_request(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""400 Bad Request"".to_string(),
        false => format!(""400 Bad Request: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::BAD_REQUEST)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn invalid_endpoint(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""404 The requested service endpoint is not found"".to_string(),
        false => format!(
            ""404 The requested service endpoint is not found: {}"",
            msg.as_ref()
        ),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND)
        .body(Body::from(err_msg))
        .unwrap()
}

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum ServerError {
    /// Error returned while parsing CLI options failed
    #[error(""{0}"")]
    ArgumentError(String),
    /// Generic error returned while performing an operation
    #[error(""{0}"")]
    Operation(String),
}

```","Q: Is there any code in this snippet that returns a `Response` object without checking whether it's an empty string first?
A: Yes, there is code in this snippet that returns a `Response` object without checking whether it's an empty string first.

The method `not_implemented()` directly creates and returns a `Response` object using the expression `Body::from(""501 Not Implemented"")`. The error message ""501 Not Implemented"" is hardcoded as a string, which might not be what you expect if you're trying to display a specific error message based on certain conditions.

However, in this case, it's actually checking whether the argument `msg` is empty before creating the error response. Here are a few instances where this check happens:

* In the function `not_implemented()`, there doesn't seem to be an explicit check for an empty string.
* In the functions `internal_server_error(msg: impl AsRef<str>)`, `bad_request(msg: impl AsRef<str>)`, and `invalid_endpoint(msg: impl AsRef<str>)`, there is a check using `match msg.as_ref().is_empty()`. If the message is an empty string, it will return a default error message. Otherwise, it uses the provided `msg` as the error message.

So while the question was phrased in a way that suggests the code doesn't do this check, the reality is more nuanced.

**Bonus tip:** Consider using a more robust solution to handle errors with empty messages. Instead of hardcoding an error message for empty strings, you could use a default value or throw a custom exception specifically designed for handling such situations. This can make your code more modular and easier to debug in the future."
"```rust:llama-api-server/src/error.rs
use hyper::{Body, Response};
use thiserror::Error;

#[allow(dead_code)]
pub(crate) fn not_implemented() -> Response<Body> {
    // log error
    error!(target: ""stdout"", ""501 Not Implemented"");

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_IMPLEMENTED)
        .body(Body::from(""501 Not Implemented""))
        .unwrap()
}

pub(crate) fn internal_server_error(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""500 Internal Server Error"".to_string(),
        false => format!(""500 Internal Server Error: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::INTERNAL_SERVER_ERROR)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn bad_request(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""400 Bad Request"".to_string(),
        false => format!(""400 Bad Request: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::BAD_REQUEST)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn invalid_endpoint(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""404 The requested service endpoint is not found"".to_string(),
        false => format!(
            ""404 The requested service endpoint is not found: {}"",
            msg.as_ref()
        ),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND)
        .body(Body::from(err_msg))
        .unwrap()
}

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum ServerError {
    /// Error returned while parsing CLI options failed
    #[error(""{0}"")]
    ArgumentError(String),
    /// Generic error returned while performing an operation
    #[error(""{0}"")]
    Operation(String),
}

```","Q: Can the status code of a response be changed dynamically based on some condition?
A: Yes, the status code of a response can be changed dynamically based on some condition. In fact, it's common to do so in web development.

In Rust and the Hyper framework used here, you can use the `status` method of the `ResponseBuilder` to set the status code of the response. This method takes an instance of `StatusCode` as an argument, which represents the desired HTTP status code.

Here is an example:

```rust
let condition = some_condition(); // Replace with your actual condition

match condition {
    true => Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::FOUND) // Status code 302
        .body(Body::from(""Found""))
        .unwrap(),
    false => Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND) // Status code 404
        .body(Body::from(""Not Found""))
        .unwrap(),
}
```

However, this approach can lead to code duplication if you have multiple conditions that require different status codes. To avoid this issue, consider extracting a function for each condition and status code combination:

```rust
fn found_status() -> Response<Body> {
    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::FOUND) // Status code 302
        .body(Body::from(""Found""))
        .unwrap()
}

fn not_found_status() -> Response<Body> {
    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND) // Status code 404
        .body(Body::from(""Not Found""))
        .unwrap()
}
```

Then, use these functions in your main logic:

```rust
let condition = some_condition(); // Replace with your actual condition

match condition {
    true => found_status(),
    false => not_found_status(),
}
```

This approach is cleaner and more maintainable."
"```rust:llama-api-server/src/error.rs
use hyper::{Body, Response};
use thiserror::Error;

#[allow(dead_code)]
pub(crate) fn not_implemented() -> Response<Body> {
    // log error
    error!(target: ""stdout"", ""501 Not Implemented"");

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_IMPLEMENTED)
        .body(Body::from(""501 Not Implemented""))
        .unwrap()
}

pub(crate) fn internal_server_error(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""500 Internal Server Error"".to_string(),
        false => format!(""500 Internal Server Error: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::INTERNAL_SERVER_ERROR)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn bad_request(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""400 Bad Request"".to_string(),
        false => format!(""400 Bad Request: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::BAD_REQUEST)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn invalid_endpoint(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""404 The requested service endpoint is not found"".to_string(),
        false => format!(
            ""404 The requested service endpoint is not found: {}"",
            msg.as_ref()
        ),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND)
        .body(Body::from(err_msg))
        .unwrap()
}

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum ServerError {
    /// Error returned while parsing CLI options failed
    #[error(""{0}"")]
    ArgumentError(String),
    /// Generic error returned while performing an operation
    #[error(""{0}"")]
    Operation(String),
}

```","Q: What type does the `error!` macro return in Rust, and how is it used here?
A: The `error!` macro in Rust is used to log errors and returns a type called `std::fmt::Result`. The `std::fmt::Result` type has two variants: `Ok` and `Err`, but specifically for the `error!` macro, it returns `()`, indicating that there's no specific result value to return. 

Here is how it is used in the provided code snippet:

```rust
error!(target: ""stdout"", ""501 Not Implemented"");
```

In this example, the `error!` macro logs a message with the string literal `""501 Not Implemented""` to the target specified as `""stdout""`. The `target: ""stdout""` syntax tells `log` crate (which is assumed to be used in this project for logging purposes) to print the error to the standard output."
"```rust:llama-api-server/src/error.rs
use hyper::{Body, Response};
use thiserror::Error;

#[allow(dead_code)]
pub(crate) fn not_implemented() -> Response<Body> {
    // log error
    error!(target: ""stdout"", ""501 Not Implemented"");

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_IMPLEMENTED)
        .body(Body::from(""501 Not Implemented""))
        .unwrap()
}

pub(crate) fn internal_server_error(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""500 Internal Server Error"".to_string(),
        false => format!(""500 Internal Server Error: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::INTERNAL_SERVER_ERROR)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn bad_request(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""400 Bad Request"".to_string(),
        false => format!(""400 Bad Request: {}"", msg.as_ref()),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::BAD_REQUEST)
        .body(Body::from(err_msg))
        .unwrap()
}

pub(crate) fn invalid_endpoint(msg: impl AsRef<str>) -> Response<Body> {
    let err_msg = match msg.as_ref().is_empty() {
        true => ""404 The requested service endpoint is not found"".to_string(),
        false => format!(
            ""404 The requested service endpoint is not found: {}"",
            msg.as_ref()
        ),
    };

    // log error
    error!(target: ""stdout"", ""{}"", &err_msg);

    Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .status(hyper::StatusCode::NOT_FOUND)
        .body(Body::from(err_msg))
        .unwrap()
}

#[derive(Error, Clone, Debug, PartialEq, Eq)]
pub enum ServerError {
    /// Error returned while parsing CLI options failed
    #[error(""{0}"")]
    ArgumentError(String),
    /// Generic error returned while performing an operation
    #[error(""{0}"")]
    Operation(String),
}

```","Q: Does the `error!` macro log errors to stdout by default?
A: No, the `error!` macro does not log errors to stdout by default. The behavior in this case is due to the custom usage of the `error!` macro with a specific target: `""stdout""`.

When you use the `error!` macro without specifying a target, it logs the error using the default logger provided by the underlying framework (in this case, it seems to be Hyper). However, in the code snippet you provided, the target is explicitly set to `""stdout""` when logging errors:

```rust
// log error
error!(target: ""stdout"", ""{}"", &err_msg);
```

This means that any error logged using this `error!` macro will be written to stdout instead of being handled by the default logger. This allows for custom logging behavior in specific situations, such as when handling errors returned from a server.

**Bonus Tip:** When working with log macros like `error!`, it's essential to understand how they interact with the underlying logging framework. Always check the documentation and source code of the library you're using to ensure you're using these macros correctly.

**Example outside of this content:** If you wanted to customize the logging behavior even further, you could use a different target, such as `""stderr""`, or specify a custom logger implementation that writes to a file or database. For instance:

```rust
// log error to stderr instead of stdout
error!(target: ""stderr"", ""{}"", &err_msg);
```

This would write the errors to the standard error stream (usually used for outputting diagnostic messages) rather than the standard output stream."
"```rust:llama-api-server/src/backend/mod.rs
pub(crate) mod ggml;

use crate::error;
use hyper::{Body, Request, Response};

pub(crate) async fn handle_llama_request(req: Request<Body>) -> Response<Body> {
    match req.uri().path() {
        ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
        ""/v1/completions"" => ggml::completions_handler(req).await,
        ""/v1/models"" => ggml::models_handler().await,
        ""/v1/embeddings"" => ggml::embeddings_handler(req).await,
        ""/v1/files"" => ggml::files_handler(req).await,
        ""/v1/chunks"" => ggml::chunks_handler(req).await,
        ""/v1/info"" => ggml::server_info_handler().await,
        path => {
            if path.starts_with(""/v1/files/"") {
                ggml::files_handler(req).await
            } else {
                error::invalid_endpoint(path)
            }
        }
    }
}

```","Summary:
**Code-related File Summary**

The code is located in the `llama-api-server/src/backend/mod.rs` file. This module appears to be a part of an API server written in Rust.

**Key Changes:**

1. **Imported modules:** The module imports `error`, `hyper::Body`, `hyper::Request`, and `hyper::Response`. These are likely used for handling errors and HTTP requests/responses.
2. **`handle_llama_request` function:** This is the main entry point of the module, which handles incoming HTTP requests.

**Functionality:**

The `handle_llama_request` function takes a `Request<Body>` as input and returns a `Response<Body>`. It uses a match statement to determine which endpoint was requested based on the URL path. The endpoints are handled by separate functions in the `ggml` module, which is imported from the same crate.

**Endpoints:**

The code defines several endpoints:

1. `/v1/chat/completions`
2. `/v1/completions`
3. `/v1/models`
4. `/v1/embeddings`
5. `/v1/files`
6. `/v1/chunks`
7. `/v1/info`

Each endpoint is handled by a separate function in the `ggml` module:

1. `chat_completions_handler`
2. `completions_handler`
3. `models_handler`
4. `embeddings_handler`
5. `files_handler`
6. `chunks_handler`
7. `server_info_handler`

**Error Handling:**

The code catches any unexpected endpoint requests and returns an error response using the `error::invalid_endpoint` function.

**Dependencies:**

The code uses the `hyper` crate for HTTP request and response handling, as well as the `ggml` module for endpoint-specific functionality. The `error` module is also used for error handling.

Overall, this code appears to be a simple API server that routes incoming requests to specific endpoints handled by separate functions in the `ggml` module."
"```rust:llama-api-server/src/backend/mod.rs
pub(crate) mod ggml;

use crate::error;
use hyper::{Body, Request, Response};

pub(crate) async fn handle_llama_request(req: Request<Body>) -> Response<Body> {
    match req.uri().path() {
        ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
        ""/v1/completions"" => ggml::completions_handler(req).await,
        ""/v1/models"" => ggml::models_handler().await,
        ""/v1/embeddings"" => ggml::embeddings_handler(req).await,
        ""/v1/files"" => ggml::files_handler(req).await,
        ""/v1/chunks"" => ggml::chunks_handler(req).await,
        ""/v1/info"" => ggml::server_info_handler().await,
        path => {
            if path.starts_with(""/v1/files/"") {
                ggml::files_handler(req).await
            } else {
                error::invalid_endpoint(path)
            }
        }
    }
}

```","Q: What is the programming language used in this code snippet?
A: The programming language used in this code snippet is Rust.

Bonus Tip: Rust is a systems programming language that focuses on safety, performance, and concurrency. It's known for its strong focus on memory safety through its ownership model, which helps prevent common errors like null pointer dereferences and data races. If you're new to Rust, it's worth exploring the official Rust book and tutorials to get started with the language.

Example use case outside of this code snippet: Consider building a real-time collaborative text editor using Rust. With Rust's concurrency features, you could create a multi-threaded application that allows multiple users to edit a document simultaneously while maintaining consistency across all edits."
"```rust:llama-api-server/src/backend/mod.rs
pub(crate) mod ggml;

use crate::error;
use hyper::{Body, Request, Response};

pub(crate) async fn handle_llama_request(req: Request<Body>) -> Response<Body> {
    match req.uri().path() {
        ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
        ""/v1/completions"" => ggml::completions_handler(req).await,
        ""/v1/models"" => ggml::models_handler().await,
        ""/v1/embeddings"" => ggml::embeddings_handler(req).await,
        ""/v1/files"" => ggml::files_handler(req).await,
        ""/v1/chunks"" => ggml::chunks_handler(req).await,
        ""/v1/info"" => ggml::server_info_handler().await,
        path => {
            if path.starts_with(""/v1/files/"") {
                ggml::files_handler(req).await
            } else {
                error::invalid_endpoint(path)
            }
        }
    }
}

```","Q: Which framework is being used for building the API server?
A: Based on the code snippet provided, it appears that the API server is being built using the `hyper` framework. This is evident from the fact that the function `handle_llama_request` takes a `Request<Body>` as an argument and returns a `Response<Body>`. The `hyper` library provides a high-level interface for building web applications, including APIs.

In particular, the `hyper::Request` type represents an incoming HTTP request, and the `hyper::Response` type represents an outgoing HTTP response. These types are used in the function to handle the API requests and generate responses accordingly.

**Bonus Tip:** When using `hyper`, it's essential to understand how to handle errors properly. In this code snippet, the `error::invalid_endpoint(path)` function is used to handle invalid endpoint requests. However, you can customize error handling further by implementing your own error handler middleware.

For example, consider a simple error handling middleware that logs error messages and returns a JSON response with a custom error message:
```rust
use hyper::{Request, Response, Service};
use serde_json::json;

async fn error_handler(err: impl std::error::Error + Send) -> Result<Response, err> {
    let body = json!({""message"": ""Internal Server Error""});
    Ok(Response::new(Body::from(body.to_string())))
}

struct MyService {}

impl Service for MyService {
    type Request = Request;
    type Response = Response;
    type Error = Box<dyn std::error::Error + Send>;
    type Future = Pin<Box<dyn Future<Output = Result<Self::Response, Self::Error>>>>;

    async fn call(&mut self, req: Self::Request) -> Result<Self::Response, Self::Error> {
        // Call the original handler
        let res = handle_llama_request(req).await?;

        if !res.status().is_success() {
            // Catch and log errors
            error!(""[{}] {}"", req.method(), req.uri());
            return Ok(error_handler(""Internal Server Error"")?);
        }

        Ok(res)
    }
}
```
In this example, we've defined a custom `error_handler` function that logs the original request method and URI. We then call the original handler (`handle_llama_request`) and check if the response status is successful. If not, we catch the error, log it, and return a JSON response with an internal server error message.

This approach allows for more fine-grained control over error handling and logging in your API server."
"```rust:llama-api-server/src/backend/mod.rs
pub(crate) mod ggml;

use crate::error;
use hyper::{Body, Request, Response};

pub(crate) async fn handle_llama_request(req: Request<Body>) -> Response<Body> {
    match req.uri().path() {
        ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
        ""/v1/completions"" => ggml::completions_handler(req).await,
        ""/v1/models"" => ggml::models_handler().await,
        ""/v1/embeddings"" => ggml::embeddings_handler(req).await,
        ""/v1/files"" => ggml::files_handler(req).await,
        ""/v1/chunks"" => ggml::chunks_handler(req).await,
        ""/v1/info"" => ggml::server_info_handler().await,
        path => {
            if path.starts_with(""/v1/files/"") {
                ggml::files_handler(req).await
            } else {
                error::invalid_endpoint(path)
            }
        }
    }
}

```","Q: Is the error handling handled by a separate module named 'error'?
A: Yes, the error handling in this code is handled by a separate module named 'error'. 

In the `handle_llama_request` function, when an invalid endpoint path is encountered, it calls the `invalid_endpoint` function from the `error` module to handle the error. The `invalid_endpoint` function likely returns an appropriate response to the client indicating that the requested endpoint does not exist.

Here's a bonus tip:

In real-world applications, you might want to consider using a more robust error handling mechanism, such as implementing a custom error type and using it throughout your application. This would allow you to handle errors in a centralized way and provide more informative error messages to the clients.

For example, if you had a `CustomError` enum like this:

```rust
enum CustomError {
    InvalidEndpoint,
    OtherError,
}

impl std::fmt::Display for CustomError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::InvalidEndpoint => write!(f, ""Invalid endpoint""),
            Self::OtherError => write!(f, ""Another error occurred""),
        }
    }
}
```

You could then use it in the `handle_llama_request` function like this:

```rust
match path {
    ... // other cases...
    path => {
        if !path.starts_with(""/v1/files/"") && ![""/v1/models"", ""/v1/info""].contains(&path) {
            return error::error_response(CustomError::InvalidEndpoint);
        }
        ggml::files_handler(req).await
    },
}
```

This way, you can handle the `CustomError` in a centralized way and provide more informative error messages to your clients."
"```rust:llama-api-server/src/backend/mod.rs
pub(crate) mod ggml;

use crate::error;
use hyper::{Body, Request, Response};

pub(crate) async fn handle_llama_request(req: Request<Body>) -> Response<Body> {
    match req.uri().path() {
        ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
        ""/v1/completions"" => ggml::completions_handler(req).await,
        ""/v1/models"" => ggml::models_handler().await,
        ""/v1/embeddings"" => ggml::embeddings_handler(req).await,
        ""/v1/files"" => ggml::files_handler(req).await,
        ""/v1/chunks"" => ggml::chunks_handler(req).await,
        ""/v1/info"" => ggml::server_info_handler().await,
        path => {
            if path.starts_with(""/v1/files/"") {
                ggml::files_handler(req).await
            } else {
                error::invalid_endpoint(path)
            }
        }
    }
}

```","Q: Does the ggml module contain functions that handle different types of requests to the LLaMA API server?
A: Yes, the `ggml` module appears to contain functions that handle different types of requests to the LLaMA API server. This can be inferred from the `match` statement in the `handle_llama_request` function.

In this `match` statement, various paths from the request URI are matched and handled by specific functions within the `ggml` module, such as:

* `chat_completions_handler`: Handles requests to `/v1/chat/completions`
* `completions_handler`: Handles requests to `/v1/completions`
* `models_handler`: Handles requests to `/v1/models`
* `embeddings_handler`: Handles requests to `/v1/embeddings`
* `files_handler`: Handles requests to `/v1/files` and its subpaths
* `chunks_handler`: Handles requests to `/v1/chunks`
* `server_info_handler`: Handles requests to `/v1/info`

This modular design allows for the separation of concerns between different types of API requests, making it easier to maintain and extend the codebase.

**Bonus Tip:** This modular design is also an example of the **Single Responsibility Principle (SRP)** in software development. Each module has a single responsibility: handling a specific type of request to the LLaMA API server. This makes the code more maintainable, scalable, and easier to understand.

For example, imagine you want to add support for another endpoint, say `/v1/custom`. You would simply create a new function within the `ggml` module that handles this new endpoint, without affecting any of the existing code."
"```rust:llama-api-server/src/backend/mod.rs
pub(crate) mod ggml;

use crate::error;
use hyper::{Body, Request, Response};

pub(crate) async fn handle_llama_request(req: Request<Body>) -> Response<Body> {
    match req.uri().path() {
        ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
        ""/v1/completions"" => ggml::completions_handler(req).await,
        ""/v1/models"" => ggml::models_handler().await,
        ""/v1/embeddings"" => ggml::embeddings_handler(req).await,
        ""/v1/files"" => ggml::files_handler(req).await,
        ""/v1/chunks"" => ggml::chunks_handler(req).await,
        ""/v1/info"" => ggml::server_info_handler().await,
        path => {
            if path.starts_with(""/v1/files/"") {
                ggml::files_handler(req).await
            } else {
                error::invalid_endpoint(path)
            }
        }
    }
}

```","Q: Is the hyper library used for creating and manipulating HTTP requests and responses?
A: Yes, that's correct. The `hyper` library in Rust is used for creating and manipulating HTTP requests and responses. It provides a high-level interface for building and processing HTTP messages.

In the given code snippet, `handle_llama_request` function uses `hyper` to process incoming HTTP requests. The `Request<Body>` type from `hyper` represents an HTTP request that has been received by the server, and it contains information about the request method, URI, headers, body, etc.

The `Response<Body>` type from `hyper` is used to represent the HTTP response sent back to the client by the server. The `Body` type in both cases represents a stream of data that can be sent over the network.

Here's a bonus tip: When using `hyper`, you don't need to manually buffer the entire request body into memory, as it uses an asynchronous API with streaming support. This makes it suitable for handling large requests without consuming excessive memory.

Example outside of this content:

```rust
use hyper::Client;
use tokio;

async fn fetch_website(url: &str) -> Result<(), Box<dyn std::error::Error>> {
    let client = Client::new();
    let res = client.get(url).await?;
    
    println!(""Response status: {}"", res.status());
    
    Ok(())
}

#[tokio::main]
async fn main() {
    tokio::spawn(fetch_website(""https://www.rust-lang.org""));
}
```

In this example, `hyper` is used to send a GET request to the specified URL, and then print out the response status. The `tokio` crate is used for asynchronous I/O operations, as it provides an efficient way to handle concurrent requests in Rust."
"```rust:llama-api-server/src/backend/mod.rs
pub(crate) mod ggml;

use crate::error;
use hyper::{Body, Request, Response};

pub(crate) async fn handle_llama_request(req: Request<Body>) -> Response<Body> {
    match req.uri().path() {
        ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
        ""/v1/completions"" => ggml::completions_handler(req).await,
        ""/v1/models"" => ggml::models_handler().await,
        ""/v1/embeddings"" => ggml::embeddings_handler(req).await,
        ""/v1/files"" => ggml::files_handler(req).await,
        ""/v1/chunks"" => ggml::chunks_handler(req).await,
        ""/v1/info"" => ggml::server_info_handler().await,
        path => {
            if path.starts_with(""/v1/files/"") {
                ggml::files_handler(req).await
            } else {
                error::invalid_endpoint(path)
            }
        }
    }
}

```","Q: Are there any nested matches or if-else statements in this code snippet?
A: No, there are not any nested matches or if-else statements in this code snippet.

However, within the `match` statement, we have multiple arms (or cases) that handle different URI paths. This is a good way to handle multiple conditions in Rust, making it more concise and easier to read than using nested if-else statements.

Here's an example of how this could be done with nested if-else statements:

```rust
pub(crate) async fn handle_llama_request(req: Request<Body>) -> Response<Body> {
    match req.uri().path() {
        ""/v1/chat/completions"" | ""/v1/completions"" => {
            if path.starts_with(""/v1/files/"") {
                ggml::files_handler(req).await
            } else {
                error::invalid_endpoint(path)
            }
        }
        // Other paths...
    }
}
```

But, as you can see, it is harder to read and maintain than the original code snippet."
"```rust:llama-api-server/src/backend/mod.rs
pub(crate) mod ggml;

use crate::error;
use hyper::{Body, Request, Response};

pub(crate) async fn handle_llama_request(req: Request<Body>) -> Response<Body> {
    match req.uri().path() {
        ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
        ""/v1/completions"" => ggml::completions_handler(req).await,
        ""/v1/models"" => ggml::models_handler().await,
        ""/v1/embeddings"" => ggml::embeddings_handler(req).await,
        ""/v1/files"" => ggml::files_handler(req).await,
        ""/v1/chunks"" => ggml::chunks_handler(req).await,
        ""/v1/info"" => ggml::server_info_handler().await,
        path => {
            if path.starts_with(""/v1/files/"") {
                ggml::files_handler(req).await
            } else {
                error::invalid_endpoint(path)
            }
        }
    }
}

```","Q: Does the function ""handle_llama_request"" return an asynchronous response?
A: Yes, the function `handle_llama_request` returns an asynchronous response.

The `async` keyword in front of the function signature indicates that it is an asynchronous function. This means that it will return a `Future` object when called, rather than actually executing any code synchronously.

In this specific case, the function uses several other asynchronous functions (e.g., `ggml::chat_completions_handler`, `models_handler`, etc.) to handle different endpoints of the request. Each of these inner functions is also marked as `async` and returns a future. The outer function then uses the `.await` keyword to wait for each of these futures to complete before returning its own response.

Here's an excerpt from the code where this behavior can be seen:
```rust
match req.uri().path() {
    ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
    ...
}
```
The `.await` keyword is what allows the `handle_llama_request` function to wait for the results of these inner asynchronous functions before returning its own response.

**Bonus tip:** If you're working with asynchronous code in Rust, be sure to use the `tokio` crate or another async runtime to manage the execution of your futures. This will help you avoid common pitfalls like blocking the event loop and make your code more efficient and scalable."
"```rust:llama-api-server/src/backend/mod.rs
pub(crate) mod ggml;

use crate::error;
use hyper::{Body, Request, Response};

pub(crate) async fn handle_llama_request(req: Request<Body>) -> Response<Body> {
    match req.uri().path() {
        ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
        ""/v1/completions"" => ggml::completions_handler(req).await,
        ""/v1/models"" => ggml::models_handler().await,
        ""/v1/embeddings"" => ggml::embeddings_handler(req).await,
        ""/v1/files"" => ggml::files_handler(req).await,
        ""/v1/chunks"" => ggml::chunks_handler(req).await,
        ""/v1/info"" => ggml::server_info_handler().await,
        path => {
            if path.starts_with(""/v1/files/"") {
                ggml::files_handler(req).await
            } else {
                error::invalid_endpoint(path)
            }
        }
    }
}

```","Q: Which path is not handled by a dedicated handler function, but instead returns an error message?
A: According to the code, the path that is not handled by a dedicated handler function but instead returns an error message is:

```r
path => {
    if path.starts_with(""/v1/files/"") {
        ggml::files_handler(req).await
    } else {
        error::invalid_endpoint(path)
    }
}
```

This code block handles all the remaining paths that do not match any of the specific handler functions. If the path does not start with `/v1/files/`, it returns an `error::invalid_endpoint` message.

**Bonus Tip:** To improve this code, you can consider using a more robust way to handle unknown paths. Instead of just returning an error message, you could return a default response or provide a helpful hint for the user on how to access specific resources. 

Here is an example of how you could modify the code:

```rust
path => {
    if path.starts_with(""/v1/files/"") {
        ggml::files_handler(req).await
    } else {
        return Response::builder()
            .status(error::StatusCode::NOT_FOUND)
            .body(Body::from(format!(""No endpoint available for: {}"", path)))
            .unwrap();
    }
}
```

This modified code returns a `404 Not Found` response with a helpful error message that indicates the requested path was not found."
"```rust:llama-api-server/src/backend/mod.rs
pub(crate) mod ggml;

use crate::error;
use hyper::{Body, Request, Response};

pub(crate) async fn handle_llama_request(req: Request<Body>) -> Response<Body> {
    match req.uri().path() {
        ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
        ""/v1/completions"" => ggml::completions_handler(req).await,
        ""/v1/models"" => ggml::models_handler().await,
        ""/v1/embeddings"" => ggml::embeddings_handler(req).await,
        ""/v1/files"" => ggml::files_handler(req).await,
        ""/v1/chunks"" => ggml::chunks_handler(req).await,
        ""/v1/info"" => ggml::server_info_handler().await,
        path => {
            if path.starts_with(""/v1/files/"") {
                ggml::files_handler(req).await
            } else {
                error::invalid_endpoint(path)
            }
        }
    }
}

```","Q: Can you find a pattern in how different endpoints are matched with their corresponding handlers?
A: After examining the code, it appears that there is a pattern in how different endpoints are matched with their corresponding handlers. Here's what I found:

1. **Exact Matching**: For most endpoints (e.g., `/v1/chat/completions`, `/v1/models`, etc.), they are matched exactly using a simple `match` statement.
2. **Prefix Matching**: There is one case where a prefix match is used: when the endpoint starts with `/v1/files/`. In this case, the handler for `/v1/files/` will be executed regardless of the exact path.

Here's an excerpt from the code to illustrate this:

```rust
match req.uri().path() {
    // Exact matches
    ""/v1/chat/completions"" => ggml::chat_completions_handler(req).await,
    ""/v1/completions"" => ggml::completions_handler(req).await,
    ""/v1/models"" => ggml::models_handler().await,
    ""/v1/embeddings"" => ggml::embeddings_handler(req).await,
    // ...
}

// Prefix match
if path.starts_with(""/v1/files/"") {
    ggml::files_handler(req).await
} else {
    error::invalid_endpoint(path)
}
```

This pattern suggests that the author of this code has intentionally chosen to use exact matches for most endpoints and a prefix match only when it's necessary.

**Bonus Tip**: When dealing with a large number of API endpoints, consider using a more robust routing mechanism like [actix-web](https://actix.rs/) or [Rocket](https://rocket.rs/) which provide built-in support for path parameters and query string parameters. This can make your code easier to maintain and extend.

**Example Outside of the Content**: In other programming languages like Python, you might use a library like [Flask](http://flask.pocoo.org/) or [Django](https://www.djangoproject.com/) which also have built-in support for routing API endpoints. For instance:

```python
from flask import Flask

app = Flask(__name__)

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions_handler():
    # Handle the endpoint
    pass

if __name__ == '__main__':
    app.run()
```

In this example, we define a single function `chat_completions_handler` which will be executed when the `/v1/chat/completions` endpoint is accessed. This is similar to the Rust code snippet shown earlier."
"```rust:llama-api-server/src/backend/ggml.rs
use crate::{error, utils::gen_chat_id, SERVER_INFO};
use endpoints::{
    chat::ChatCompletionRequest,
    completions::CompletionRequest,
    embeddings::EmbeddingRequest,
    files::{DeleteFileStatus, FileObject, ListFilesResponse},
    rag::{ChunksRequest, ChunksResponse},
};
use futures_util::TryStreamExt;
use hyper::{body::to_bytes, Body, Method, Request, Response};
use multipart::server::{Multipart, ReadEntry, ReadEntryResult};
use multipart_2021 as multipart;
use std::{
    fs::{self, File},
    io::{Cursor, Read, Write},
    path::Path,
    time::SystemTime,
};
use walkdir::{DirEntry, WalkDir};

/// List all models available.
pub(crate) async fn models_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming model list request."");

    let list_models_response = match llama_core::models::models().await {
        Ok(list_models_response) => list_models_response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // serialize response
    let s = match serde_json::to_string(&list_models_response) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Failed to serialize the model list result. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the model list response."");

    res
}

/// Compute embeddings for the input text and return the embeddings object.
pub(crate) async fn embeddings_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming embeddings request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""embeddings_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut embedding_request: EmbeddingRequest = match serde_json::from_slice(&body_bytes) {
        Ok(embedding_request) => embedding_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize embedding request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if embedding_request.user.is_none() {
        embedding_request.user = Some(gen_chat_id())
    };
    let id = embedding_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::embeddings::embeddings(&embedding_request).await {
        Ok(embedding_response) => {
            // serialize embedding object
            match serde_json::to_string(&embedding_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .header(""user"", id)
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize embedding object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the embeddings response"");

    res
}

/// Process a completion request and returns a completion response with the answer from the model.
pub(crate) async fn completions_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming completions request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""completions_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut completion_request: CompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(completion_request) => completion_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize completions request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if completion_request.user.is_none() {
        completion_request.user = Some(gen_chat_id())
    };
    let id = completion_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::completions::completions(&completion_request).await {
        Ok(completion_object) => {
            // serialize completion object
            let s = match serde_json::to_string(&completion_object) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Fail to serialize completion object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .header(""user"", id)
                .body(Body::from(s));
            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the completions response."");

    res
}

/// Process a chat-completion request and returns a chat-completion response with the answer from the model.
pub(crate) async fn chat_completions_handler(mut req: Request<Body>) -> Response<Body> {
    info!(target: ""stdout"", ""Handling the coming chat completion request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    info!(target: ""stdout"", ""Prepare the chat completion request."");

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut chat_request: ChatCompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chat_request) => chat_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chat completion request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the user id is provided
    if chat_request.user.is_none() {
        chat_request.user = Some(gen_chat_id())
    };
    let id = chat_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", chat_request.user.clone().unwrap());

    let res = match llama_core::chat::chat(&mut chat_request).await {
        Ok(result) => match result {
            either::Left(stream) => {
                let stream = stream.map_err(|e| e.to_string());

                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""text/event-stream"")
                    .header(""Cache-Control"", ""no-cache"")
                    .header(""Connection"", ""keep-alive"")
                    .header(""user"", id)
                    .body(Body::wrap_stream(stream));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""finish chat completions in stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            either::Right(chat_completion_object) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&chat_completion_object) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize chat completion object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .header(""user"", id)
                    .body(Body::from(s));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""Finish chat completions in non-stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in non-stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
        },
        Err(e) => {
            let err_msg = format!(""Failed to get chat completions. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the chat completion response."");

    res
}

/// Upload files and return the file object.
pub(crate) async fn files_handler(req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming files request"");

    let res = if req.method() == Method::POST {
        let boundary = ""boundary="";

        let boundary = req.headers().get(""content-type"").and_then(|ct| {
            let ct = ct.to_str().ok()?;
            let idx = ct.find(boundary)?;
            Some(ct[idx + boundary.len()..].to_string())
        });

        let req_body = req.into_body();
        let body_bytes = match to_bytes(req_body).await {
            Ok(body_bytes) => body_bytes,
            Err(e) => {
                let err_msg = format!(""Fail to read buffer from request body. {}"", e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        let cursor = Cursor::new(body_bytes.to_vec());

        let mut multipart = Multipart::with_body(cursor, boundary.unwrap());

        let mut file_object: Option<FileObject> = None;
        while let ReadEntryResult::Entry(mut field) = multipart.read_entry_mut() {
            if &*field.headers.name == ""file"" {
                let filename = match field.headers.filename {
                    Some(filename) => filename,
                    None => {
                        let err_msg =
                            ""Failed to upload the target file. The filename is not provided."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                if !((filename).to_lowercase().ends_with("".txt"")
                    || (filename).to_lowercase().ends_with("".md""))
                    || (filename).to_lowercase().ends_with("".png"")
                {
                    let err_msg = format!(
                        ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported. The file extension is {}."",
                        &filename
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }

                let mut buffer = Vec::new();
                let size_in_bytes = match field.data.read_to_end(&mut buffer) {
                    Ok(size_in_bytes) => size_in_bytes,
                    Err(e) => {
                        let err_msg = format!(""Failed to read the target file. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a unique file id
                let id = format!(""file_{}"", uuid::Uuid::new_v4());

                // save the file
                let path = Path::new(""archives"");
                if !path.exists() {
                    fs::create_dir(path).unwrap();
                }
                let file_path = path.join(&id);
                if !file_path.exists() {
                    fs::create_dir(&file_path).unwrap();
                }
                let mut file = match File::create(file_path.join(&filename)) {
                    Ok(file) => file,
                    Err(e) => {
                        let err_msg =
                            format!(""Failed to create archive document {}. {}"", &filename, e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };
                file.write_all(&buffer[..]).unwrap();

                // log
                info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

                let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a file object
                file_object = Some(FileObject {
                    id,
                    bytes: size_in_bytes as u64,
                    created_at,
                    filename,
                    object: ""file"".to_string(),
                    purpose: ""assistants"".to_string(),
                });

                break;
            }
        }

        match file_object {
            Some(fo) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&fo) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize file object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .body(Body::from(s));

                match result {
                    Ok(response) => response,
                    Err(e) => {
                        let err_msg = e.to_string();

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            None => {
                let err_msg = ""Failed to upload the target file. Not found the target file."";

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::GET {
        let uri_path = req.uri().path();

        if uri_path == ""/v1/files"" {
            let mut file_objects: Vec<FileObject> = Vec::new();
            for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let id = entry
                        .path()
                        .parent()
                        .and_then(|p| p.file_name())
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .to_string();

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    let fo = FileObject {
                        id,
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    };

                    file_objects.push(fo);
                }
            }

            info!(target: ""stdout"", ""Found {} archive files"", file_objects.len());

            let file_objects = ListFilesResponse {
                object: ""list"".to_string(),
                data: file_objects,
            };

            // serialize chat completion object
            let s = match serde_json::to_string(&file_objects) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Failed to serialize file object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(s));

            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        } else {
            let id = uri_path.trim_start_matches(""/v1/files/"");
            let root = format!(""archives/{}"", id);
            let mut file_object: Option<FileObject> = None;
            for entry in WalkDir::new(root).into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    file_object = Some(FileObject {
                        id: id.into(),
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    });

                    break;
                }
            }

            match file_object {
                Some(fo) => {
                    // serialize chat completion object
                    let s = match serde_json::to_string(&fo) {
                        Ok(s) => s,
                        Err(e) => {
                            let err_msg = format!(""Failed to serialize file object. {}"", e);

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            return error::internal_server_error(err_msg);
                        }
                    };

                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));

                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                None => {
                    let err_msg = format!(
                        ""Failed to retrieve the target file. Not found the target file with id {}."",
                        id
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
    } else if req.method() == Method::DELETE {
        let id = req.uri().path().trim_start_matches(""/v1/files/"");
        let root = format!(""archives/{}"", id);
        let status = match fs::remove_dir_all(root) {
            Ok(_) => {
                info!(target: ""stdout"", ""Successfully deleted the target file with id {}."", id);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: true,
                }
            }
            Err(e) => {
                let err_msg = format!(""Failed to delete the target file with id {}. {}"", id, e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: false,
                }
            }
        };

        // serialize status
        let s = match serde_json::to_string(&status) {
            Ok(s) => s,
            Err(e) => {
                let err_msg = format!(
                    ""Failed to serialize the status of the file deletion operation. {}"",
                    e
                );

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        // return response
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::from(s));

        match result {
            Ok(response) => response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::OPTIONS {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""files_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    } else {
        let err_msg = ""Invalid HTTP Method."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        error::internal_server_error(err_msg)
    };

    info!(target: ""stdout"", ""Send the files response"");

    res
}

/// Segment the text into chunks and return the chunks response.
pub(crate) async fn chunks_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming chunks request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""chunks_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    let chunks_request: ChunksRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chunks_request) => chunks_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chunks request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the archives directory exists
    let path = Path::new(""archives"");
    if !path.exists() {
        let err_msg = ""The `archives` directory does not exist."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the archive id exists
    let archive_path = path.join(&chunks_request.id);
    if !archive_path.exists() {
        let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the file exists
    let file_path = archive_path.join(&chunks_request.filename);
    if !file_path.exists() {
        let err_msg = format!(
            ""Not found file: {} in archive id: {}"",
            &chunks_request.filename, &chunks_request.id
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // log
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &chunks_request.id, &chunks_request.filename);

    // get the extension of the archived file
    let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
        Some(extension) => extension,
        None => {
            let err_msg = format!(
                ""Failed to get the extension of the archived `{}`."",
                &chunks_request.filename
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // open the file
    let mut file = match File::open(&file_path) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to open `{}`. {}"", &chunks_request.filename, e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // read the file
    let mut contents = String::new();
    if let Err(e) = file.read_to_string(&mut contents) {
        let err_msg = format!(""Failed to read `{}`. {}"", &chunks_request.filename, e);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
    {
        Ok(chunks) => {
            let chunks_response = ChunksResponse {
                id: chunks_request.id,
                filename: chunks_request.filename,
                chunks,
            };

            // serialize embedding object
            match serde_json::to_string(&chunks_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize chunks response. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the chunks response."");

    res
}

/// Return the server info.
pub(crate) async fn server_info_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming server info request."");

    // get the server info
    let server_info = match SERVER_INFO.get() {
        Some(server_info) => server_info,
        None => {
            let err_msg = ""The server info is not set."";

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(""The server info is not set."");
        }
    };

    // serialize server info
    let s = match serde_json::to_string(&server_info) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Fail to serialize server info. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the server info response."");

    res
}

fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}

```","Summary:
Here's a summary of the code in the provided file:

**Overview**

The code appears to be a Rust implementation of a web server using the Hyper framework. The server provides several endpoints for handling HTTP requests, including API calls, file uploads, and responses.

**Endpoints**

1. `models_handler`: Handles GET requests to retrieve a list of available models.
2. `completions_handler`: Handles POST requests to generate completions based on user input.
3. `chat_completions_handler`: Handles POST requests to generate chat-style completions based on user input.
4. `files_handler`: Handles GET, POST, and DELETE requests to interact with uploaded files.
5. `chunks_handler`: Handles GET requests to segment a text into chunks for further processing.
6. `server_info_handler`: Returns server information.

**Functions**

1. `models_handler`: Deserializes the request body to extract user input and uses it to retrieve models from an unknown source.
2. `completions_handler`: Deserializes the request body to extract user input, generates completions using an external service (LlamaCore), and returns the result.
3. `chat_completions_handler`: Similar to `completions_handler`, but generates chat-style completions instead.
4. `files_handler`: Handles file uploads by deserializing the request body to extract metadata, creates a new directory for each uploaded file, and stores the file contents within it.
5. `chunks_handler`: Deserializes the request body to extract user input (text), determines the text's encoding, and uses an external service (LlamaCore) to segment the text into chunks.
6. `server_info_handler`: Returns server information.

**Notes**

* The code uses several external dependencies, including Hyper, serde_json, and walkdir.
* Some functions appear to be handling errors implicitly or using a custom error type (e.g., `error::internal_server_error`).
* There are many log statements throughout the code, which may indicate that this is a debugging or testing environment.

Overall, the code seems to be implementing a web server with several endpoints for generating completions, uploading files, and retrieving server information. However, without additional context or information about the external services being used (e.g., LlamaCore), it's difficult to provide more specific insights into the code's purpose or functionality."
"```rust:llama-api-server/src/backend/ggml.rs
use crate::{error, utils::gen_chat_id, SERVER_INFO};
use endpoints::{
    chat::ChatCompletionRequest,
    completions::CompletionRequest,
    embeddings::EmbeddingRequest,
    files::{DeleteFileStatus, FileObject, ListFilesResponse},
    rag::{ChunksRequest, ChunksResponse},
};
use futures_util::TryStreamExt;
use hyper::{body::to_bytes, Body, Method, Request, Response};
use multipart::server::{Multipart, ReadEntry, ReadEntryResult};
use multipart_2021 as multipart;
use std::{
    fs::{self, File},
    io::{Cursor, Read, Write},
    path::Path,
    time::SystemTime,
};
use walkdir::{DirEntry, WalkDir};

/// List all models available.
pub(crate) async fn models_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming model list request."");

    let list_models_response = match llama_core::models::models().await {
        Ok(list_models_response) => list_models_response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // serialize response
    let s = match serde_json::to_string(&list_models_response) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Failed to serialize the model list result. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the model list response."");

    res
}

/// Compute embeddings for the input text and return the embeddings object.
pub(crate) async fn embeddings_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming embeddings request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""embeddings_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut embedding_request: EmbeddingRequest = match serde_json::from_slice(&body_bytes) {
        Ok(embedding_request) => embedding_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize embedding request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if embedding_request.user.is_none() {
        embedding_request.user = Some(gen_chat_id())
    };
    let id = embedding_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::embeddings::embeddings(&embedding_request).await {
        Ok(embedding_response) => {
            // serialize embedding object
            match serde_json::to_string(&embedding_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .header(""user"", id)
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize embedding object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the embeddings response"");

    res
}

/// Process a completion request and returns a completion response with the answer from the model.
pub(crate) async fn completions_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming completions request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""completions_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut completion_request: CompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(completion_request) => completion_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize completions request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if completion_request.user.is_none() {
        completion_request.user = Some(gen_chat_id())
    };
    let id = completion_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::completions::completions(&completion_request).await {
        Ok(completion_object) => {
            // serialize completion object
            let s = match serde_json::to_string(&completion_object) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Fail to serialize completion object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .header(""user"", id)
                .body(Body::from(s));
            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the completions response."");

    res
}

/// Process a chat-completion request and returns a chat-completion response with the answer from the model.
pub(crate) async fn chat_completions_handler(mut req: Request<Body>) -> Response<Body> {
    info!(target: ""stdout"", ""Handling the coming chat completion request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    info!(target: ""stdout"", ""Prepare the chat completion request."");

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut chat_request: ChatCompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chat_request) => chat_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chat completion request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the user id is provided
    if chat_request.user.is_none() {
        chat_request.user = Some(gen_chat_id())
    };
    let id = chat_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", chat_request.user.clone().unwrap());

    let res = match llama_core::chat::chat(&mut chat_request).await {
        Ok(result) => match result {
            either::Left(stream) => {
                let stream = stream.map_err(|e| e.to_string());

                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""text/event-stream"")
                    .header(""Cache-Control"", ""no-cache"")
                    .header(""Connection"", ""keep-alive"")
                    .header(""user"", id)
                    .body(Body::wrap_stream(stream));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""finish chat completions in stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            either::Right(chat_completion_object) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&chat_completion_object) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize chat completion object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .header(""user"", id)
                    .body(Body::from(s));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""Finish chat completions in non-stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in non-stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
        },
        Err(e) => {
            let err_msg = format!(""Failed to get chat completions. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the chat completion response."");

    res
}

/// Upload files and return the file object.
pub(crate) async fn files_handler(req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming files request"");

    let res = if req.method() == Method::POST {
        let boundary = ""boundary="";

        let boundary = req.headers().get(""content-type"").and_then(|ct| {
            let ct = ct.to_str().ok()?;
            let idx = ct.find(boundary)?;
            Some(ct[idx + boundary.len()..].to_string())
        });

        let req_body = req.into_body();
        let body_bytes = match to_bytes(req_body).await {
            Ok(body_bytes) => body_bytes,
            Err(e) => {
                let err_msg = format!(""Fail to read buffer from request body. {}"", e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        let cursor = Cursor::new(body_bytes.to_vec());

        let mut multipart = Multipart::with_body(cursor, boundary.unwrap());

        let mut file_object: Option<FileObject> = None;
        while let ReadEntryResult::Entry(mut field) = multipart.read_entry_mut() {
            if &*field.headers.name == ""file"" {
                let filename = match field.headers.filename {
                    Some(filename) => filename,
                    None => {
                        let err_msg =
                            ""Failed to upload the target file. The filename is not provided."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                if !((filename).to_lowercase().ends_with("".txt"")
                    || (filename).to_lowercase().ends_with("".md""))
                    || (filename).to_lowercase().ends_with("".png"")
                {
                    let err_msg = format!(
                        ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported. The file extension is {}."",
                        &filename
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }

                let mut buffer = Vec::new();
                let size_in_bytes = match field.data.read_to_end(&mut buffer) {
                    Ok(size_in_bytes) => size_in_bytes,
                    Err(e) => {
                        let err_msg = format!(""Failed to read the target file. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a unique file id
                let id = format!(""file_{}"", uuid::Uuid::new_v4());

                // save the file
                let path = Path::new(""archives"");
                if !path.exists() {
                    fs::create_dir(path).unwrap();
                }
                let file_path = path.join(&id);
                if !file_path.exists() {
                    fs::create_dir(&file_path).unwrap();
                }
                let mut file = match File::create(file_path.join(&filename)) {
                    Ok(file) => file,
                    Err(e) => {
                        let err_msg =
                            format!(""Failed to create archive document {}. {}"", &filename, e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };
                file.write_all(&buffer[..]).unwrap();

                // log
                info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

                let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a file object
                file_object = Some(FileObject {
                    id,
                    bytes: size_in_bytes as u64,
                    created_at,
                    filename,
                    object: ""file"".to_string(),
                    purpose: ""assistants"".to_string(),
                });

                break;
            }
        }

        match file_object {
            Some(fo) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&fo) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize file object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .body(Body::from(s));

                match result {
                    Ok(response) => response,
                    Err(e) => {
                        let err_msg = e.to_string();

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            None => {
                let err_msg = ""Failed to upload the target file. Not found the target file."";

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::GET {
        let uri_path = req.uri().path();

        if uri_path == ""/v1/files"" {
            let mut file_objects: Vec<FileObject> = Vec::new();
            for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let id = entry
                        .path()
                        .parent()
                        .and_then(|p| p.file_name())
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .to_string();

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    let fo = FileObject {
                        id,
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    };

                    file_objects.push(fo);
                }
            }

            info!(target: ""stdout"", ""Found {} archive files"", file_objects.len());

            let file_objects = ListFilesResponse {
                object: ""list"".to_string(),
                data: file_objects,
            };

            // serialize chat completion object
            let s = match serde_json::to_string(&file_objects) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Failed to serialize file object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(s));

            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        } else {
            let id = uri_path.trim_start_matches(""/v1/files/"");
            let root = format!(""archives/{}"", id);
            let mut file_object: Option<FileObject> = None;
            for entry in WalkDir::new(root).into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    file_object = Some(FileObject {
                        id: id.into(),
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    });

                    break;
                }
            }

            match file_object {
                Some(fo) => {
                    // serialize chat completion object
                    let s = match serde_json::to_string(&fo) {
                        Ok(s) => s,
                        Err(e) => {
                            let err_msg = format!(""Failed to serialize file object. {}"", e);

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            return error::internal_server_error(err_msg);
                        }
                    };

                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));

                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                None => {
                    let err_msg = format!(
                        ""Failed to retrieve the target file. Not found the target file with id {}."",
                        id
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
    } else if req.method() == Method::DELETE {
        let id = req.uri().path().trim_start_matches(""/v1/files/"");
        let root = format!(""archives/{}"", id);
        let status = match fs::remove_dir_all(root) {
            Ok(_) => {
                info!(target: ""stdout"", ""Successfully deleted the target file with id {}."", id);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: true,
                }
            }
            Err(e) => {
                let err_msg = format!(""Failed to delete the target file with id {}. {}"", id, e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: false,
                }
            }
        };

        // serialize status
        let s = match serde_json::to_string(&status) {
            Ok(s) => s,
            Err(e) => {
                let err_msg = format!(
                    ""Failed to serialize the status of the file deletion operation. {}"",
                    e
                );

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        // return response
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::from(s));

        match result {
            Ok(response) => response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::OPTIONS {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""files_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    } else {
        let err_msg = ""Invalid HTTP Method."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        error::internal_server_error(err_msg)
    };

    info!(target: ""stdout"", ""Send the files response"");

    res
}

/// Segment the text into chunks and return the chunks response.
pub(crate) async fn chunks_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming chunks request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""chunks_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    let chunks_request: ChunksRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chunks_request) => chunks_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chunks request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the archives directory exists
    let path = Path::new(""archives"");
    if !path.exists() {
        let err_msg = ""The `archives` directory does not exist."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the archive id exists
    let archive_path = path.join(&chunks_request.id);
    if !archive_path.exists() {
        let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the file exists
    let file_path = archive_path.join(&chunks_request.filename);
    if !file_path.exists() {
        let err_msg = format!(
            ""Not found file: {} in archive id: {}"",
            &chunks_request.filename, &chunks_request.id
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // log
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &chunks_request.id, &chunks_request.filename);

    // get the extension of the archived file
    let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
        Some(extension) => extension,
        None => {
            let err_msg = format!(
                ""Failed to get the extension of the archived `{}`."",
                &chunks_request.filename
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // open the file
    let mut file = match File::open(&file_path) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to open `{}`. {}"", &chunks_request.filename, e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // read the file
    let mut contents = String::new();
    if let Err(e) = file.read_to_string(&mut contents) {
        let err_msg = format!(""Failed to read `{}`. {}"", &chunks_request.filename, e);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
    {
        Ok(chunks) => {
            let chunks_response = ChunksResponse {
                id: chunks_request.id,
                filename: chunks_request.filename,
                chunks,
            };

            // serialize embedding object
            match serde_json::to_string(&chunks_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize chunks response. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the chunks response."");

    res
}

/// Return the server info.
pub(crate) async fn server_info_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming server info request."");

    // get the server info
    let server_info = match SERVER_INFO.get() {
        Some(server_info) => server_info,
        None => {
            let err_msg = ""The server info is not set."";

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(""The server info is not set."");
        }
    };

    // serialize server info
    let s = match serde_json::to_string(&server_info) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Fail to serialize server info. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the server info response."");

    res
}

fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}

```","Q: Here are the questions that need to be answered based on the code:
A: Please go ahead and ask your questions about the code. I'll do my best to provide detailed and accurate answers. 

(Note: Since you didn't mention any specific question in your message, please feel free to add them as needed!)"
"```rust:llama-api-server/src/backend/ggml.rs
use crate::{error, utils::gen_chat_id, SERVER_INFO};
use endpoints::{
    chat::ChatCompletionRequest,
    completions::CompletionRequest,
    embeddings::EmbeddingRequest,
    files::{DeleteFileStatus, FileObject, ListFilesResponse},
    rag::{ChunksRequest, ChunksResponse},
};
use futures_util::TryStreamExt;
use hyper::{body::to_bytes, Body, Method, Request, Response};
use multipart::server::{Multipart, ReadEntry, ReadEntryResult};
use multipart_2021 as multipart;
use std::{
    fs::{self, File},
    io::{Cursor, Read, Write},
    path::Path,
    time::SystemTime,
};
use walkdir::{DirEntry, WalkDir};

/// List all models available.
pub(crate) async fn models_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming model list request."");

    let list_models_response = match llama_core::models::models().await {
        Ok(list_models_response) => list_models_response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // serialize response
    let s = match serde_json::to_string(&list_models_response) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Failed to serialize the model list result. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the model list response."");

    res
}

/// Compute embeddings for the input text and return the embeddings object.
pub(crate) async fn embeddings_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming embeddings request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""embeddings_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut embedding_request: EmbeddingRequest = match serde_json::from_slice(&body_bytes) {
        Ok(embedding_request) => embedding_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize embedding request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if embedding_request.user.is_none() {
        embedding_request.user = Some(gen_chat_id())
    };
    let id = embedding_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::embeddings::embeddings(&embedding_request).await {
        Ok(embedding_response) => {
            // serialize embedding object
            match serde_json::to_string(&embedding_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .header(""user"", id)
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize embedding object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the embeddings response"");

    res
}

/// Process a completion request and returns a completion response with the answer from the model.
pub(crate) async fn completions_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming completions request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""completions_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut completion_request: CompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(completion_request) => completion_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize completions request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if completion_request.user.is_none() {
        completion_request.user = Some(gen_chat_id())
    };
    let id = completion_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::completions::completions(&completion_request).await {
        Ok(completion_object) => {
            // serialize completion object
            let s = match serde_json::to_string(&completion_object) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Fail to serialize completion object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .header(""user"", id)
                .body(Body::from(s));
            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the completions response."");

    res
}

/// Process a chat-completion request and returns a chat-completion response with the answer from the model.
pub(crate) async fn chat_completions_handler(mut req: Request<Body>) -> Response<Body> {
    info!(target: ""stdout"", ""Handling the coming chat completion request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    info!(target: ""stdout"", ""Prepare the chat completion request."");

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut chat_request: ChatCompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chat_request) => chat_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chat completion request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the user id is provided
    if chat_request.user.is_none() {
        chat_request.user = Some(gen_chat_id())
    };
    let id = chat_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", chat_request.user.clone().unwrap());

    let res = match llama_core::chat::chat(&mut chat_request).await {
        Ok(result) => match result {
            either::Left(stream) => {
                let stream = stream.map_err(|e| e.to_string());

                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""text/event-stream"")
                    .header(""Cache-Control"", ""no-cache"")
                    .header(""Connection"", ""keep-alive"")
                    .header(""user"", id)
                    .body(Body::wrap_stream(stream));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""finish chat completions in stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            either::Right(chat_completion_object) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&chat_completion_object) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize chat completion object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .header(""user"", id)
                    .body(Body::from(s));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""Finish chat completions in non-stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in non-stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
        },
        Err(e) => {
            let err_msg = format!(""Failed to get chat completions. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the chat completion response."");

    res
}

/// Upload files and return the file object.
pub(crate) async fn files_handler(req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming files request"");

    let res = if req.method() == Method::POST {
        let boundary = ""boundary="";

        let boundary = req.headers().get(""content-type"").and_then(|ct| {
            let ct = ct.to_str().ok()?;
            let idx = ct.find(boundary)?;
            Some(ct[idx + boundary.len()..].to_string())
        });

        let req_body = req.into_body();
        let body_bytes = match to_bytes(req_body).await {
            Ok(body_bytes) => body_bytes,
            Err(e) => {
                let err_msg = format!(""Fail to read buffer from request body. {}"", e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        let cursor = Cursor::new(body_bytes.to_vec());

        let mut multipart = Multipart::with_body(cursor, boundary.unwrap());

        let mut file_object: Option<FileObject> = None;
        while let ReadEntryResult::Entry(mut field) = multipart.read_entry_mut() {
            if &*field.headers.name == ""file"" {
                let filename = match field.headers.filename {
                    Some(filename) => filename,
                    None => {
                        let err_msg =
                            ""Failed to upload the target file. The filename is not provided."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                if !((filename).to_lowercase().ends_with("".txt"")
                    || (filename).to_lowercase().ends_with("".md""))
                    || (filename).to_lowercase().ends_with("".png"")
                {
                    let err_msg = format!(
                        ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported. The file extension is {}."",
                        &filename
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }

                let mut buffer = Vec::new();
                let size_in_bytes = match field.data.read_to_end(&mut buffer) {
                    Ok(size_in_bytes) => size_in_bytes,
                    Err(e) => {
                        let err_msg = format!(""Failed to read the target file. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a unique file id
                let id = format!(""file_{}"", uuid::Uuid::new_v4());

                // save the file
                let path = Path::new(""archives"");
                if !path.exists() {
                    fs::create_dir(path).unwrap();
                }
                let file_path = path.join(&id);
                if !file_path.exists() {
                    fs::create_dir(&file_path).unwrap();
                }
                let mut file = match File::create(file_path.join(&filename)) {
                    Ok(file) => file,
                    Err(e) => {
                        let err_msg =
                            format!(""Failed to create archive document {}. {}"", &filename, e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };
                file.write_all(&buffer[..]).unwrap();

                // log
                info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

                let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a file object
                file_object = Some(FileObject {
                    id,
                    bytes: size_in_bytes as u64,
                    created_at,
                    filename,
                    object: ""file"".to_string(),
                    purpose: ""assistants"".to_string(),
                });

                break;
            }
        }

        match file_object {
            Some(fo) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&fo) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize file object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .body(Body::from(s));

                match result {
                    Ok(response) => response,
                    Err(e) => {
                        let err_msg = e.to_string();

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            None => {
                let err_msg = ""Failed to upload the target file. Not found the target file."";

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::GET {
        let uri_path = req.uri().path();

        if uri_path == ""/v1/files"" {
            let mut file_objects: Vec<FileObject> = Vec::new();
            for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let id = entry
                        .path()
                        .parent()
                        .and_then(|p| p.file_name())
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .to_string();

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    let fo = FileObject {
                        id,
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    };

                    file_objects.push(fo);
                }
            }

            info!(target: ""stdout"", ""Found {} archive files"", file_objects.len());

            let file_objects = ListFilesResponse {
                object: ""list"".to_string(),
                data: file_objects,
            };

            // serialize chat completion object
            let s = match serde_json::to_string(&file_objects) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Failed to serialize file object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(s));

            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        } else {
            let id = uri_path.trim_start_matches(""/v1/files/"");
            let root = format!(""archives/{}"", id);
            let mut file_object: Option<FileObject> = None;
            for entry in WalkDir::new(root).into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    file_object = Some(FileObject {
                        id: id.into(),
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    });

                    break;
                }
            }

            match file_object {
                Some(fo) => {
                    // serialize chat completion object
                    let s = match serde_json::to_string(&fo) {
                        Ok(s) => s,
                        Err(e) => {
                            let err_msg = format!(""Failed to serialize file object. {}"", e);

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            return error::internal_server_error(err_msg);
                        }
                    };

                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));

                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                None => {
                    let err_msg = format!(
                        ""Failed to retrieve the target file. Not found the target file with id {}."",
                        id
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
    } else if req.method() == Method::DELETE {
        let id = req.uri().path().trim_start_matches(""/v1/files/"");
        let root = format!(""archives/{}"", id);
        let status = match fs::remove_dir_all(root) {
            Ok(_) => {
                info!(target: ""stdout"", ""Successfully deleted the target file with id {}."", id);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: true,
                }
            }
            Err(e) => {
                let err_msg = format!(""Failed to delete the target file with id {}. {}"", id, e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: false,
                }
            }
        };

        // serialize status
        let s = match serde_json::to_string(&status) {
            Ok(s) => s,
            Err(e) => {
                let err_msg = format!(
                    ""Failed to serialize the status of the file deletion operation. {}"",
                    e
                );

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        // return response
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::from(s));

        match result {
            Ok(response) => response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::OPTIONS {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""files_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    } else {
        let err_msg = ""Invalid HTTP Method."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        error::internal_server_error(err_msg)
    };

    info!(target: ""stdout"", ""Send the files response"");

    res
}

/// Segment the text into chunks and return the chunks response.
pub(crate) async fn chunks_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming chunks request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""chunks_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    let chunks_request: ChunksRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chunks_request) => chunks_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chunks request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the archives directory exists
    let path = Path::new(""archives"");
    if !path.exists() {
        let err_msg = ""The `archives` directory does not exist."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the archive id exists
    let archive_path = path.join(&chunks_request.id);
    if !archive_path.exists() {
        let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the file exists
    let file_path = archive_path.join(&chunks_request.filename);
    if !file_path.exists() {
        let err_msg = format!(
            ""Not found file: {} in archive id: {}"",
            &chunks_request.filename, &chunks_request.id
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // log
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &chunks_request.id, &chunks_request.filename);

    // get the extension of the archived file
    let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
        Some(extension) => extension,
        None => {
            let err_msg = format!(
                ""Failed to get the extension of the archived `{}`."",
                &chunks_request.filename
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // open the file
    let mut file = match File::open(&file_path) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to open `{}`. {}"", &chunks_request.filename, e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // read the file
    let mut contents = String::new();
    if let Err(e) = file.read_to_string(&mut contents) {
        let err_msg = format!(""Failed to read `{}`. {}"", &chunks_request.filename, e);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
    {
        Ok(chunks) => {
            let chunks_response = ChunksResponse {
                id: chunks_request.id,
                filename: chunks_request.filename,
                chunks,
            };

            // serialize embedding object
            match serde_json::to_string(&chunks_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize chunks response. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the chunks response."");

    res
}

/// Return the server info.
pub(crate) async fn server_info_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming server info request."");

    // get the server info
    let server_info = match SERVER_INFO.get() {
        Some(server_info) => server_info,
        None => {
            let err_msg = ""The server info is not set."";

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(""The server info is not set."");
        }
    };

    // serialize server info
    let s = match serde_json::to_string(&server_info) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Fail to serialize server info. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the server info response."");

    res
}

fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}

```","Q: 1. What is the purpose of the `is_hidden` function?
A: The `is_hidden` function checks if a file or directory is hidden. It takes a `DirEntry` object as an argument and returns `true` if the name of the entry starts with a dot (`.`), indicating that it's hidden, and `false` otherwise.

This function is used in several places within the codebase to filter out hidden files and directories from the list of entries. For example, when listing files or directories, only non-hidden entries are included in the result.

Here's an example:
```rust
let mut file_objects: Vec<FileObject> = Vec::new();
for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
    if !is_hidden(&entry) && entry.path().is_file() {
        // process the non-hidden file
    }
}
```
In this example, `is_hidden` is used to filter out hidden files from the list of entries returned by `WalkDir::new(""archives"")`."
"```rust:llama-api-server/src/backend/ggml.rs
use crate::{error, utils::gen_chat_id, SERVER_INFO};
use endpoints::{
    chat::ChatCompletionRequest,
    completions::CompletionRequest,
    embeddings::EmbeddingRequest,
    files::{DeleteFileStatus, FileObject, ListFilesResponse},
    rag::{ChunksRequest, ChunksResponse},
};
use futures_util::TryStreamExt;
use hyper::{body::to_bytes, Body, Method, Request, Response};
use multipart::server::{Multipart, ReadEntry, ReadEntryResult};
use multipart_2021 as multipart;
use std::{
    fs::{self, File},
    io::{Cursor, Read, Write},
    path::Path,
    time::SystemTime,
};
use walkdir::{DirEntry, WalkDir};

/// List all models available.
pub(crate) async fn models_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming model list request."");

    let list_models_response = match llama_core::models::models().await {
        Ok(list_models_response) => list_models_response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // serialize response
    let s = match serde_json::to_string(&list_models_response) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Failed to serialize the model list result. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the model list response."");

    res
}

/// Compute embeddings for the input text and return the embeddings object.
pub(crate) async fn embeddings_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming embeddings request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""embeddings_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut embedding_request: EmbeddingRequest = match serde_json::from_slice(&body_bytes) {
        Ok(embedding_request) => embedding_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize embedding request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if embedding_request.user.is_none() {
        embedding_request.user = Some(gen_chat_id())
    };
    let id = embedding_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::embeddings::embeddings(&embedding_request).await {
        Ok(embedding_response) => {
            // serialize embedding object
            match serde_json::to_string(&embedding_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .header(""user"", id)
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize embedding object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the embeddings response"");

    res
}

/// Process a completion request and returns a completion response with the answer from the model.
pub(crate) async fn completions_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming completions request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""completions_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut completion_request: CompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(completion_request) => completion_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize completions request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if completion_request.user.is_none() {
        completion_request.user = Some(gen_chat_id())
    };
    let id = completion_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::completions::completions(&completion_request).await {
        Ok(completion_object) => {
            // serialize completion object
            let s = match serde_json::to_string(&completion_object) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Fail to serialize completion object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .header(""user"", id)
                .body(Body::from(s));
            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the completions response."");

    res
}

/// Process a chat-completion request and returns a chat-completion response with the answer from the model.
pub(crate) async fn chat_completions_handler(mut req: Request<Body>) -> Response<Body> {
    info!(target: ""stdout"", ""Handling the coming chat completion request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    info!(target: ""stdout"", ""Prepare the chat completion request."");

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut chat_request: ChatCompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chat_request) => chat_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chat completion request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the user id is provided
    if chat_request.user.is_none() {
        chat_request.user = Some(gen_chat_id())
    };
    let id = chat_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", chat_request.user.clone().unwrap());

    let res = match llama_core::chat::chat(&mut chat_request).await {
        Ok(result) => match result {
            either::Left(stream) => {
                let stream = stream.map_err(|e| e.to_string());

                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""text/event-stream"")
                    .header(""Cache-Control"", ""no-cache"")
                    .header(""Connection"", ""keep-alive"")
                    .header(""user"", id)
                    .body(Body::wrap_stream(stream));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""finish chat completions in stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            either::Right(chat_completion_object) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&chat_completion_object) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize chat completion object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .header(""user"", id)
                    .body(Body::from(s));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""Finish chat completions in non-stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in non-stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
        },
        Err(e) => {
            let err_msg = format!(""Failed to get chat completions. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the chat completion response."");

    res
}

/// Upload files and return the file object.
pub(crate) async fn files_handler(req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming files request"");

    let res = if req.method() == Method::POST {
        let boundary = ""boundary="";

        let boundary = req.headers().get(""content-type"").and_then(|ct| {
            let ct = ct.to_str().ok()?;
            let idx = ct.find(boundary)?;
            Some(ct[idx + boundary.len()..].to_string())
        });

        let req_body = req.into_body();
        let body_bytes = match to_bytes(req_body).await {
            Ok(body_bytes) => body_bytes,
            Err(e) => {
                let err_msg = format!(""Fail to read buffer from request body. {}"", e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        let cursor = Cursor::new(body_bytes.to_vec());

        let mut multipart = Multipart::with_body(cursor, boundary.unwrap());

        let mut file_object: Option<FileObject> = None;
        while let ReadEntryResult::Entry(mut field) = multipart.read_entry_mut() {
            if &*field.headers.name == ""file"" {
                let filename = match field.headers.filename {
                    Some(filename) => filename,
                    None => {
                        let err_msg =
                            ""Failed to upload the target file. The filename is not provided."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                if !((filename).to_lowercase().ends_with("".txt"")
                    || (filename).to_lowercase().ends_with("".md""))
                    || (filename).to_lowercase().ends_with("".png"")
                {
                    let err_msg = format!(
                        ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported. The file extension is {}."",
                        &filename
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }

                let mut buffer = Vec::new();
                let size_in_bytes = match field.data.read_to_end(&mut buffer) {
                    Ok(size_in_bytes) => size_in_bytes,
                    Err(e) => {
                        let err_msg = format!(""Failed to read the target file. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a unique file id
                let id = format!(""file_{}"", uuid::Uuid::new_v4());

                // save the file
                let path = Path::new(""archives"");
                if !path.exists() {
                    fs::create_dir(path).unwrap();
                }
                let file_path = path.join(&id);
                if !file_path.exists() {
                    fs::create_dir(&file_path).unwrap();
                }
                let mut file = match File::create(file_path.join(&filename)) {
                    Ok(file) => file,
                    Err(e) => {
                        let err_msg =
                            format!(""Failed to create archive document {}. {}"", &filename, e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };
                file.write_all(&buffer[..]).unwrap();

                // log
                info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

                let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a file object
                file_object = Some(FileObject {
                    id,
                    bytes: size_in_bytes as u64,
                    created_at,
                    filename,
                    object: ""file"".to_string(),
                    purpose: ""assistants"".to_string(),
                });

                break;
            }
        }

        match file_object {
            Some(fo) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&fo) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize file object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .body(Body::from(s));

                match result {
                    Ok(response) => response,
                    Err(e) => {
                        let err_msg = e.to_string();

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            None => {
                let err_msg = ""Failed to upload the target file. Not found the target file."";

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::GET {
        let uri_path = req.uri().path();

        if uri_path == ""/v1/files"" {
            let mut file_objects: Vec<FileObject> = Vec::new();
            for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let id = entry
                        .path()
                        .parent()
                        .and_then(|p| p.file_name())
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .to_string();

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    let fo = FileObject {
                        id,
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    };

                    file_objects.push(fo);
                }
            }

            info!(target: ""stdout"", ""Found {} archive files"", file_objects.len());

            let file_objects = ListFilesResponse {
                object: ""list"".to_string(),
                data: file_objects,
            };

            // serialize chat completion object
            let s = match serde_json::to_string(&file_objects) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Failed to serialize file object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(s));

            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        } else {
            let id = uri_path.trim_start_matches(""/v1/files/"");
            let root = format!(""archives/{}"", id);
            let mut file_object: Option<FileObject> = None;
            for entry in WalkDir::new(root).into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    file_object = Some(FileObject {
                        id: id.into(),
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    });

                    break;
                }
            }

            match file_object {
                Some(fo) => {
                    // serialize chat completion object
                    let s = match serde_json::to_string(&fo) {
                        Ok(s) => s,
                        Err(e) => {
                            let err_msg = format!(""Failed to serialize file object. {}"", e);

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            return error::internal_server_error(err_msg);
                        }
                    };

                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));

                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                None => {
                    let err_msg = format!(
                        ""Failed to retrieve the target file. Not found the target file with id {}."",
                        id
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
    } else if req.method() == Method::DELETE {
        let id = req.uri().path().trim_start_matches(""/v1/files/"");
        let root = format!(""archives/{}"", id);
        let status = match fs::remove_dir_all(root) {
            Ok(_) => {
                info!(target: ""stdout"", ""Successfully deleted the target file with id {}."", id);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: true,
                }
            }
            Err(e) => {
                let err_msg = format!(""Failed to delete the target file with id {}. {}"", id, e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: false,
                }
            }
        };

        // serialize status
        let s = match serde_json::to_string(&status) {
            Ok(s) => s,
            Err(e) => {
                let err_msg = format!(
                    ""Failed to serialize the status of the file deletion operation. {}"",
                    e
                );

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        // return response
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::from(s));

        match result {
            Ok(response) => response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::OPTIONS {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""files_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    } else {
        let err_msg = ""Invalid HTTP Method."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        error::internal_server_error(err_msg)
    };

    info!(target: ""stdout"", ""Send the files response"");

    res
}

/// Segment the text into chunks and return the chunks response.
pub(crate) async fn chunks_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming chunks request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""chunks_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    let chunks_request: ChunksRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chunks_request) => chunks_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chunks request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the archives directory exists
    let path = Path::new(""archives"");
    if !path.exists() {
        let err_msg = ""The `archives` directory does not exist."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the archive id exists
    let archive_path = path.join(&chunks_request.id);
    if !archive_path.exists() {
        let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the file exists
    let file_path = archive_path.join(&chunks_request.filename);
    if !file_path.exists() {
        let err_msg = format!(
            ""Not found file: {} in archive id: {}"",
            &chunks_request.filename, &chunks_request.id
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // log
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &chunks_request.id, &chunks_request.filename);

    // get the extension of the archived file
    let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
        Some(extension) => extension,
        None => {
            let err_msg = format!(
                ""Failed to get the extension of the archived `{}`."",
                &chunks_request.filename
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // open the file
    let mut file = match File::open(&file_path) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to open `{}`. {}"", &chunks_request.filename, e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // read the file
    let mut contents = String::new();
    if let Err(e) = file.read_to_string(&mut contents) {
        let err_msg = format!(""Failed to read `{}`. {}"", &chunks_request.filename, e);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
    {
        Ok(chunks) => {
            let chunks_response = ChunksResponse {
                id: chunks_request.id,
                filename: chunks_request.filename,
                chunks,
            };

            // serialize embedding object
            match serde_json::to_string(&chunks_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize chunks response. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the chunks response."");

    res
}

/// Return the server info.
pub(crate) async fn server_info_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming server info request."");

    // get the server info
    let server_info = match SERVER_INFO.get() {
        Some(server_info) => server_info,
        None => {
            let err_msg = ""The server info is not set."";

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(""The server info is not set."");
        }
    };

    // serialize server info
    let s = match serde_json::to_string(&server_info) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Fail to serialize server info. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the server info response."");

    res
}

fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}

```","Q: 2. How does the `chunks_handler` function handle file extensions when segmenting text into chunks?
A: The `chunks_handler` function in the provided code snippet handles file extensions when segmenting text into chunks using the following logic:

1. After opening and reading the file, it checks if the extension of the archived file is available:
```rust
let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
    Some(extension) => extension,
    None => {
        let err_msg = format!(
            ""Failed to get the extension of the archived `{}`."",
            &chunks_request.filename
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }
};
```

2. If the extension is available, it is then passed as an argument to the `chunk_text` function:
```rust
let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
{
    Ok(chunks) => { ... },
    Err(e) => { ... }
}
```
The `chunk_text` function is responsible for segmenting the text into chunks based on the provided extension. However, without the implementation details of the `chunk_text` function, it's unclear exactly how file extensions are used in this process.

If I had to make an educated guess based on common practices, here's what might be happening:

* The `chunk_text` function uses the file extension to determine which chunking algorithm or strategy to apply.
* For example, if the file has a `.txt` extension, it might use a simple whitespace-based chunking algorithm. On the other hand, if the file has a `.md` extension, it might use a more complex algorithm that takes into account markdown formatting and structure.

Without the implementation details of `chunk_text`, this is purely speculative. However, I hope this helps clarify how file extensions are used in the `chunks_handler` function!"
"```rust:llama-api-server/src/backend/ggml.rs
use crate::{error, utils::gen_chat_id, SERVER_INFO};
use endpoints::{
    chat::ChatCompletionRequest,
    completions::CompletionRequest,
    embeddings::EmbeddingRequest,
    files::{DeleteFileStatus, FileObject, ListFilesResponse},
    rag::{ChunksRequest, ChunksResponse},
};
use futures_util::TryStreamExt;
use hyper::{body::to_bytes, Body, Method, Request, Response};
use multipart::server::{Multipart, ReadEntry, ReadEntryResult};
use multipart_2021 as multipart;
use std::{
    fs::{self, File},
    io::{Cursor, Read, Write},
    path::Path,
    time::SystemTime,
};
use walkdir::{DirEntry, WalkDir};

/// List all models available.
pub(crate) async fn models_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming model list request."");

    let list_models_response = match llama_core::models::models().await {
        Ok(list_models_response) => list_models_response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // serialize response
    let s = match serde_json::to_string(&list_models_response) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Failed to serialize the model list result. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the model list response."");

    res
}

/// Compute embeddings for the input text and return the embeddings object.
pub(crate) async fn embeddings_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming embeddings request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""embeddings_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut embedding_request: EmbeddingRequest = match serde_json::from_slice(&body_bytes) {
        Ok(embedding_request) => embedding_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize embedding request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if embedding_request.user.is_none() {
        embedding_request.user = Some(gen_chat_id())
    };
    let id = embedding_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::embeddings::embeddings(&embedding_request).await {
        Ok(embedding_response) => {
            // serialize embedding object
            match serde_json::to_string(&embedding_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .header(""user"", id)
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize embedding object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the embeddings response"");

    res
}

/// Process a completion request and returns a completion response with the answer from the model.
pub(crate) async fn completions_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming completions request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""completions_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut completion_request: CompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(completion_request) => completion_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize completions request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if completion_request.user.is_none() {
        completion_request.user = Some(gen_chat_id())
    };
    let id = completion_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::completions::completions(&completion_request).await {
        Ok(completion_object) => {
            // serialize completion object
            let s = match serde_json::to_string(&completion_object) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Fail to serialize completion object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .header(""user"", id)
                .body(Body::from(s));
            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the completions response."");

    res
}

/// Process a chat-completion request and returns a chat-completion response with the answer from the model.
pub(crate) async fn chat_completions_handler(mut req: Request<Body>) -> Response<Body> {
    info!(target: ""stdout"", ""Handling the coming chat completion request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    info!(target: ""stdout"", ""Prepare the chat completion request."");

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut chat_request: ChatCompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chat_request) => chat_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chat completion request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the user id is provided
    if chat_request.user.is_none() {
        chat_request.user = Some(gen_chat_id())
    };
    let id = chat_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", chat_request.user.clone().unwrap());

    let res = match llama_core::chat::chat(&mut chat_request).await {
        Ok(result) => match result {
            either::Left(stream) => {
                let stream = stream.map_err(|e| e.to_string());

                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""text/event-stream"")
                    .header(""Cache-Control"", ""no-cache"")
                    .header(""Connection"", ""keep-alive"")
                    .header(""user"", id)
                    .body(Body::wrap_stream(stream));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""finish chat completions in stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            either::Right(chat_completion_object) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&chat_completion_object) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize chat completion object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .header(""user"", id)
                    .body(Body::from(s));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""Finish chat completions in non-stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in non-stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
        },
        Err(e) => {
            let err_msg = format!(""Failed to get chat completions. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the chat completion response."");

    res
}

/// Upload files and return the file object.
pub(crate) async fn files_handler(req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming files request"");

    let res = if req.method() == Method::POST {
        let boundary = ""boundary="";

        let boundary = req.headers().get(""content-type"").and_then(|ct| {
            let ct = ct.to_str().ok()?;
            let idx = ct.find(boundary)?;
            Some(ct[idx + boundary.len()..].to_string())
        });

        let req_body = req.into_body();
        let body_bytes = match to_bytes(req_body).await {
            Ok(body_bytes) => body_bytes,
            Err(e) => {
                let err_msg = format!(""Fail to read buffer from request body. {}"", e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        let cursor = Cursor::new(body_bytes.to_vec());

        let mut multipart = Multipart::with_body(cursor, boundary.unwrap());

        let mut file_object: Option<FileObject> = None;
        while let ReadEntryResult::Entry(mut field) = multipart.read_entry_mut() {
            if &*field.headers.name == ""file"" {
                let filename = match field.headers.filename {
                    Some(filename) => filename,
                    None => {
                        let err_msg =
                            ""Failed to upload the target file. The filename is not provided."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                if !((filename).to_lowercase().ends_with("".txt"")
                    || (filename).to_lowercase().ends_with("".md""))
                    || (filename).to_lowercase().ends_with("".png"")
                {
                    let err_msg = format!(
                        ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported. The file extension is {}."",
                        &filename
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }

                let mut buffer = Vec::new();
                let size_in_bytes = match field.data.read_to_end(&mut buffer) {
                    Ok(size_in_bytes) => size_in_bytes,
                    Err(e) => {
                        let err_msg = format!(""Failed to read the target file. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a unique file id
                let id = format!(""file_{}"", uuid::Uuid::new_v4());

                // save the file
                let path = Path::new(""archives"");
                if !path.exists() {
                    fs::create_dir(path).unwrap();
                }
                let file_path = path.join(&id);
                if !file_path.exists() {
                    fs::create_dir(&file_path).unwrap();
                }
                let mut file = match File::create(file_path.join(&filename)) {
                    Ok(file) => file,
                    Err(e) => {
                        let err_msg =
                            format!(""Failed to create archive document {}. {}"", &filename, e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };
                file.write_all(&buffer[..]).unwrap();

                // log
                info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

                let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a file object
                file_object = Some(FileObject {
                    id,
                    bytes: size_in_bytes as u64,
                    created_at,
                    filename,
                    object: ""file"".to_string(),
                    purpose: ""assistants"".to_string(),
                });

                break;
            }
        }

        match file_object {
            Some(fo) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&fo) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize file object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .body(Body::from(s));

                match result {
                    Ok(response) => response,
                    Err(e) => {
                        let err_msg = e.to_string();

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            None => {
                let err_msg = ""Failed to upload the target file. Not found the target file."";

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::GET {
        let uri_path = req.uri().path();

        if uri_path == ""/v1/files"" {
            let mut file_objects: Vec<FileObject> = Vec::new();
            for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let id = entry
                        .path()
                        .parent()
                        .and_then(|p| p.file_name())
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .to_string();

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    let fo = FileObject {
                        id,
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    };

                    file_objects.push(fo);
                }
            }

            info!(target: ""stdout"", ""Found {} archive files"", file_objects.len());

            let file_objects = ListFilesResponse {
                object: ""list"".to_string(),
                data: file_objects,
            };

            // serialize chat completion object
            let s = match serde_json::to_string(&file_objects) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Failed to serialize file object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(s));

            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        } else {
            let id = uri_path.trim_start_matches(""/v1/files/"");
            let root = format!(""archives/{}"", id);
            let mut file_object: Option<FileObject> = None;
            for entry in WalkDir::new(root).into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    file_object = Some(FileObject {
                        id: id.into(),
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    });

                    break;
                }
            }

            match file_object {
                Some(fo) => {
                    // serialize chat completion object
                    let s = match serde_json::to_string(&fo) {
                        Ok(s) => s,
                        Err(e) => {
                            let err_msg = format!(""Failed to serialize file object. {}"", e);

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            return error::internal_server_error(err_msg);
                        }
                    };

                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));

                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                None => {
                    let err_msg = format!(
                        ""Failed to retrieve the target file. Not found the target file with id {}."",
                        id
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
    } else if req.method() == Method::DELETE {
        let id = req.uri().path().trim_start_matches(""/v1/files/"");
        let root = format!(""archives/{}"", id);
        let status = match fs::remove_dir_all(root) {
            Ok(_) => {
                info!(target: ""stdout"", ""Successfully deleted the target file with id {}."", id);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: true,
                }
            }
            Err(e) => {
                let err_msg = format!(""Failed to delete the target file with id {}. {}"", id, e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: false,
                }
            }
        };

        // serialize status
        let s = match serde_json::to_string(&status) {
            Ok(s) => s,
            Err(e) => {
                let err_msg = format!(
                    ""Failed to serialize the status of the file deletion operation. {}"",
                    e
                );

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        // return response
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::from(s));

        match result {
            Ok(response) => response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::OPTIONS {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""files_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    } else {
        let err_msg = ""Invalid HTTP Method."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        error::internal_server_error(err_msg)
    };

    info!(target: ""stdout"", ""Send the files response"");

    res
}

/// Segment the text into chunks and return the chunks response.
pub(crate) async fn chunks_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming chunks request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""chunks_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    let chunks_request: ChunksRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chunks_request) => chunks_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chunks request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the archives directory exists
    let path = Path::new(""archives"");
    if !path.exists() {
        let err_msg = ""The `archives` directory does not exist."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the archive id exists
    let archive_path = path.join(&chunks_request.id);
    if !archive_path.exists() {
        let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the file exists
    let file_path = archive_path.join(&chunks_request.filename);
    if !file_path.exists() {
        let err_msg = format!(
            ""Not found file: {} in archive id: {}"",
            &chunks_request.filename, &chunks_request.id
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // log
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &chunks_request.id, &chunks_request.filename);

    // get the extension of the archived file
    let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
        Some(extension) => extension,
        None => {
            let err_msg = format!(
                ""Failed to get the extension of the archived `{}`."",
                &chunks_request.filename
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // open the file
    let mut file = match File::open(&file_path) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to open `{}`. {}"", &chunks_request.filename, e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // read the file
    let mut contents = String::new();
    if let Err(e) = file.read_to_string(&mut contents) {
        let err_msg = format!(""Failed to read `{}`. {}"", &chunks_request.filename, e);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
    {
        Ok(chunks) => {
            let chunks_response = ChunksResponse {
                id: chunks_request.id,
                filename: chunks_request.filename,
                chunks,
            };

            // serialize embedding object
            match serde_json::to_string(&chunks_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize chunks response. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the chunks response."");

    res
}

/// Return the server info.
pub(crate) async fn server_info_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming server info request."");

    // get the server info
    let server_info = match SERVER_INFO.get() {
        Some(server_info) => server_info,
        None => {
            let err_msg = ""The server info is not set."";

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(""The server info is not set."");
        }
    };

    // serialize server info
    let s = match serde_json::to_string(&server_info) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Fail to serialize server info. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the server info response."");

    res
}

fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}

```","Q: 3. In the `server_info_handler` function, how does it retrieve and serialize server information?
A: In the `server_info_handler` function, it retrieves and serializes server information using the following steps:

1. **Get the server info**: It tries to get a value from a global variable or a static field called `SERVER_INFO`. If this is successful, it stores the value in the `server_info` variable.

   ```rust
   let server_info = match SERVER_INFO.get() {
       Some(server_info) => server_info,
       None => {
           // Handle the case where the server info is not set.
           let err_msg = ""The server info is not set."";

           // log
           error!(target: ""stdout"", ""{}"", &err_msg);

           return error::internal_server_error(""The server info is not set."");
       }
   };
   ```

2. **Serialize server info**: It uses the `serde_json` library to serialize the `server_info` object into a JSON string.

   ```rust
   let s = match serde_json::to_string(&server_info) {
       Ok(s) => s,
       Err(e) => {
           // Handle the case where serialization fails.
           let err_msg = format!(""Fail to serialize server info. {}"", e);

           // log
           error!(target: ""stdout"", ""{}"", &err_msg);

           return error::internal_server_error(err_msg);
       }
   };
   ```

The serialized JSON string is then returned as the response body.

```rust
// return response
let result = Response::builder()
    .header(""Access-Control-Allow-Origin"", ""*"")
    .header(""Access-Control-Allow-Methods"", ""*"")
    .header(""Access-Control-Allow-Headers"", ""*"")
    .header(""Content-Type"", ""application/json"")
    .body(Body::from(s));
```

**Example use case:**

To call this API endpoint, you can send a `GET` request to the server with the base URL `/v1/server_info`. The response will be a JSON string containing the server information.

```bash
curl http://localhost:8000/v1/server_info
```"
"```rust:llama-api-server/src/backend/ggml.rs
use crate::{error, utils::gen_chat_id, SERVER_INFO};
use endpoints::{
    chat::ChatCompletionRequest,
    completions::CompletionRequest,
    embeddings::EmbeddingRequest,
    files::{DeleteFileStatus, FileObject, ListFilesResponse},
    rag::{ChunksRequest, ChunksResponse},
};
use futures_util::TryStreamExt;
use hyper::{body::to_bytes, Body, Method, Request, Response};
use multipart::server::{Multipart, ReadEntry, ReadEntryResult};
use multipart_2021 as multipart;
use std::{
    fs::{self, File},
    io::{Cursor, Read, Write},
    path::Path,
    time::SystemTime,
};
use walkdir::{DirEntry, WalkDir};

/// List all models available.
pub(crate) async fn models_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming model list request."");

    let list_models_response = match llama_core::models::models().await {
        Ok(list_models_response) => list_models_response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // serialize response
    let s = match serde_json::to_string(&list_models_response) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Failed to serialize the model list result. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the model list response."");

    res
}

/// Compute embeddings for the input text and return the embeddings object.
pub(crate) async fn embeddings_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming embeddings request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""embeddings_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut embedding_request: EmbeddingRequest = match serde_json::from_slice(&body_bytes) {
        Ok(embedding_request) => embedding_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize embedding request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if embedding_request.user.is_none() {
        embedding_request.user = Some(gen_chat_id())
    };
    let id = embedding_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::embeddings::embeddings(&embedding_request).await {
        Ok(embedding_response) => {
            // serialize embedding object
            match serde_json::to_string(&embedding_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .header(""user"", id)
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize embedding object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the embeddings response"");

    res
}

/// Process a completion request and returns a completion response with the answer from the model.
pub(crate) async fn completions_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming completions request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""completions_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut completion_request: CompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(completion_request) => completion_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize completions request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if completion_request.user.is_none() {
        completion_request.user = Some(gen_chat_id())
    };
    let id = completion_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::completions::completions(&completion_request).await {
        Ok(completion_object) => {
            // serialize completion object
            let s = match serde_json::to_string(&completion_object) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Fail to serialize completion object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .header(""user"", id)
                .body(Body::from(s));
            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the completions response."");

    res
}

/// Process a chat-completion request and returns a chat-completion response with the answer from the model.
pub(crate) async fn chat_completions_handler(mut req: Request<Body>) -> Response<Body> {
    info!(target: ""stdout"", ""Handling the coming chat completion request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    info!(target: ""stdout"", ""Prepare the chat completion request."");

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut chat_request: ChatCompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chat_request) => chat_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chat completion request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the user id is provided
    if chat_request.user.is_none() {
        chat_request.user = Some(gen_chat_id())
    };
    let id = chat_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", chat_request.user.clone().unwrap());

    let res = match llama_core::chat::chat(&mut chat_request).await {
        Ok(result) => match result {
            either::Left(stream) => {
                let stream = stream.map_err(|e| e.to_string());

                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""text/event-stream"")
                    .header(""Cache-Control"", ""no-cache"")
                    .header(""Connection"", ""keep-alive"")
                    .header(""user"", id)
                    .body(Body::wrap_stream(stream));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""finish chat completions in stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            either::Right(chat_completion_object) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&chat_completion_object) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize chat completion object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .header(""user"", id)
                    .body(Body::from(s));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""Finish chat completions in non-stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in non-stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
        },
        Err(e) => {
            let err_msg = format!(""Failed to get chat completions. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the chat completion response."");

    res
}

/// Upload files and return the file object.
pub(crate) async fn files_handler(req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming files request"");

    let res = if req.method() == Method::POST {
        let boundary = ""boundary="";

        let boundary = req.headers().get(""content-type"").and_then(|ct| {
            let ct = ct.to_str().ok()?;
            let idx = ct.find(boundary)?;
            Some(ct[idx + boundary.len()..].to_string())
        });

        let req_body = req.into_body();
        let body_bytes = match to_bytes(req_body).await {
            Ok(body_bytes) => body_bytes,
            Err(e) => {
                let err_msg = format!(""Fail to read buffer from request body. {}"", e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        let cursor = Cursor::new(body_bytes.to_vec());

        let mut multipart = Multipart::with_body(cursor, boundary.unwrap());

        let mut file_object: Option<FileObject> = None;
        while let ReadEntryResult::Entry(mut field) = multipart.read_entry_mut() {
            if &*field.headers.name == ""file"" {
                let filename = match field.headers.filename {
                    Some(filename) => filename,
                    None => {
                        let err_msg =
                            ""Failed to upload the target file. The filename is not provided."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                if !((filename).to_lowercase().ends_with("".txt"")
                    || (filename).to_lowercase().ends_with("".md""))
                    || (filename).to_lowercase().ends_with("".png"")
                {
                    let err_msg = format!(
                        ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported. The file extension is {}."",
                        &filename
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }

                let mut buffer = Vec::new();
                let size_in_bytes = match field.data.read_to_end(&mut buffer) {
                    Ok(size_in_bytes) => size_in_bytes,
                    Err(e) => {
                        let err_msg = format!(""Failed to read the target file. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a unique file id
                let id = format!(""file_{}"", uuid::Uuid::new_v4());

                // save the file
                let path = Path::new(""archives"");
                if !path.exists() {
                    fs::create_dir(path).unwrap();
                }
                let file_path = path.join(&id);
                if !file_path.exists() {
                    fs::create_dir(&file_path).unwrap();
                }
                let mut file = match File::create(file_path.join(&filename)) {
                    Ok(file) => file,
                    Err(e) => {
                        let err_msg =
                            format!(""Failed to create archive document {}. {}"", &filename, e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };
                file.write_all(&buffer[..]).unwrap();

                // log
                info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

                let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a file object
                file_object = Some(FileObject {
                    id,
                    bytes: size_in_bytes as u64,
                    created_at,
                    filename,
                    object: ""file"".to_string(),
                    purpose: ""assistants"".to_string(),
                });

                break;
            }
        }

        match file_object {
            Some(fo) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&fo) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize file object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .body(Body::from(s));

                match result {
                    Ok(response) => response,
                    Err(e) => {
                        let err_msg = e.to_string();

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            None => {
                let err_msg = ""Failed to upload the target file. Not found the target file."";

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::GET {
        let uri_path = req.uri().path();

        if uri_path == ""/v1/files"" {
            let mut file_objects: Vec<FileObject> = Vec::new();
            for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let id = entry
                        .path()
                        .parent()
                        .and_then(|p| p.file_name())
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .to_string();

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    let fo = FileObject {
                        id,
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    };

                    file_objects.push(fo);
                }
            }

            info!(target: ""stdout"", ""Found {} archive files"", file_objects.len());

            let file_objects = ListFilesResponse {
                object: ""list"".to_string(),
                data: file_objects,
            };

            // serialize chat completion object
            let s = match serde_json::to_string(&file_objects) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Failed to serialize file object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(s));

            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        } else {
            let id = uri_path.trim_start_matches(""/v1/files/"");
            let root = format!(""archives/{}"", id);
            let mut file_object: Option<FileObject> = None;
            for entry in WalkDir::new(root).into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    file_object = Some(FileObject {
                        id: id.into(),
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    });

                    break;
                }
            }

            match file_object {
                Some(fo) => {
                    // serialize chat completion object
                    let s = match serde_json::to_string(&fo) {
                        Ok(s) => s,
                        Err(e) => {
                            let err_msg = format!(""Failed to serialize file object. {}"", e);

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            return error::internal_server_error(err_msg);
                        }
                    };

                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));

                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                None => {
                    let err_msg = format!(
                        ""Failed to retrieve the target file. Not found the target file with id {}."",
                        id
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
    } else if req.method() == Method::DELETE {
        let id = req.uri().path().trim_start_matches(""/v1/files/"");
        let root = format!(""archives/{}"", id);
        let status = match fs::remove_dir_all(root) {
            Ok(_) => {
                info!(target: ""stdout"", ""Successfully deleted the target file with id {}."", id);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: true,
                }
            }
            Err(e) => {
                let err_msg = format!(""Failed to delete the target file with id {}. {}"", id, e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: false,
                }
            }
        };

        // serialize status
        let s = match serde_json::to_string(&status) {
            Ok(s) => s,
            Err(e) => {
                let err_msg = format!(
                    ""Failed to serialize the status of the file deletion operation. {}"",
                    e
                );

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        // return response
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::from(s));

        match result {
            Ok(response) => response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::OPTIONS {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""files_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    } else {
        let err_msg = ""Invalid HTTP Method."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        error::internal_server_error(err_msg)
    };

    info!(target: ""stdout"", ""Send the files response"");

    res
}

/// Segment the text into chunks and return the chunks response.
pub(crate) async fn chunks_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming chunks request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""chunks_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    let chunks_request: ChunksRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chunks_request) => chunks_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chunks request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the archives directory exists
    let path = Path::new(""archives"");
    if !path.exists() {
        let err_msg = ""The `archives` directory does not exist."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the archive id exists
    let archive_path = path.join(&chunks_request.id);
    if !archive_path.exists() {
        let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the file exists
    let file_path = archive_path.join(&chunks_request.filename);
    if !file_path.exists() {
        let err_msg = format!(
            ""Not found file: {} in archive id: {}"",
            &chunks_request.filename, &chunks_request.id
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // log
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &chunks_request.id, &chunks_request.filename);

    // get the extension of the archived file
    let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
        Some(extension) => extension,
        None => {
            let err_msg = format!(
                ""Failed to get the extension of the archived `{}`."",
                &chunks_request.filename
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // open the file
    let mut file = match File::open(&file_path) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to open `{}`. {}"", &chunks_request.filename, e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // read the file
    let mut contents = String::new();
    if let Err(e) = file.read_to_string(&mut contents) {
        let err_msg = format!(""Failed to read `{}`. {}"", &chunks_request.filename, e);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
    {
        Ok(chunks) => {
            let chunks_response = ChunksResponse {
                id: chunks_request.id,
                filename: chunks_request.filename,
                chunks,
            };

            // serialize embedding object
            match serde_json::to_string(&chunks_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize chunks response. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the chunks response."");

    res
}

/// Return the server info.
pub(crate) async fn server_info_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming server info request."");

    // get the server info
    let server_info = match SERVER_INFO.get() {
        Some(server_info) => server_info,
        None => {
            let err_msg = ""The server info is not set."";

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(""The server info is not set."");
        }
    };

    // serialize server info
    let s = match serde_json::to_string(&server_info) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Fail to serialize server info. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the server info response."");

    res
}

fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}

```","Q: 4. What is the purpose of the `gen_chat_id` function used in the `embeddings_handler`, `completions_handler`, and `chat_completions_handler` functions?
A: The `gen_chat_id` function is used to generate a unique identifier for the user making the API request. This id is stored in the `user` field of the request object, which is then used to associate the response with the correct user.

In other words, when a client makes an API request to any of these functions (`embeddings_handler`, `completions_handler`, and `chat_completions_handler`), it may not always include a valid `user` field in the request body. To ensure that each response can be uniquely associated with the user who made the request, this function generates a new unique id and sets it as the value of the `user` field.

This is likely done for several reasons:

*   It allows the API to maintain user session state, even if the client does not provide a valid `user` field in its requests.
*   It provides an additional layer of security by ensuring that responses cannot be easily tied back to a specific client making the request.
*   It makes it easier for the server-side code to track and manage user-specific data.

However, without more context or information about how this API is used and integrated with other systems, it's difficult to say exactly why this function was implemented."
"```rust:llama-api-server/src/backend/ggml.rs
use crate::{error, utils::gen_chat_id, SERVER_INFO};
use endpoints::{
    chat::ChatCompletionRequest,
    completions::CompletionRequest,
    embeddings::EmbeddingRequest,
    files::{DeleteFileStatus, FileObject, ListFilesResponse},
    rag::{ChunksRequest, ChunksResponse},
};
use futures_util::TryStreamExt;
use hyper::{body::to_bytes, Body, Method, Request, Response};
use multipart::server::{Multipart, ReadEntry, ReadEntryResult};
use multipart_2021 as multipart;
use std::{
    fs::{self, File},
    io::{Cursor, Read, Write},
    path::Path,
    time::SystemTime,
};
use walkdir::{DirEntry, WalkDir};

/// List all models available.
pub(crate) async fn models_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming model list request."");

    let list_models_response = match llama_core::models::models().await {
        Ok(list_models_response) => list_models_response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // serialize response
    let s = match serde_json::to_string(&list_models_response) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Failed to serialize the model list result. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the model list response."");

    res
}

/// Compute embeddings for the input text and return the embeddings object.
pub(crate) async fn embeddings_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming embeddings request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""embeddings_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut embedding_request: EmbeddingRequest = match serde_json::from_slice(&body_bytes) {
        Ok(embedding_request) => embedding_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize embedding request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if embedding_request.user.is_none() {
        embedding_request.user = Some(gen_chat_id())
    };
    let id = embedding_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::embeddings::embeddings(&embedding_request).await {
        Ok(embedding_response) => {
            // serialize embedding object
            match serde_json::to_string(&embedding_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .header(""user"", id)
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize embedding object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the embeddings response"");

    res
}

/// Process a completion request and returns a completion response with the answer from the model.
pub(crate) async fn completions_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming completions request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""completions_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut completion_request: CompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(completion_request) => completion_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize completions request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if completion_request.user.is_none() {
        completion_request.user = Some(gen_chat_id())
    };
    let id = completion_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::completions::completions(&completion_request).await {
        Ok(completion_object) => {
            // serialize completion object
            let s = match serde_json::to_string(&completion_object) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Fail to serialize completion object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .header(""user"", id)
                .body(Body::from(s));
            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the completions response."");

    res
}

/// Process a chat-completion request and returns a chat-completion response with the answer from the model.
pub(crate) async fn chat_completions_handler(mut req: Request<Body>) -> Response<Body> {
    info!(target: ""stdout"", ""Handling the coming chat completion request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    info!(target: ""stdout"", ""Prepare the chat completion request."");

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut chat_request: ChatCompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chat_request) => chat_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chat completion request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the user id is provided
    if chat_request.user.is_none() {
        chat_request.user = Some(gen_chat_id())
    };
    let id = chat_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", chat_request.user.clone().unwrap());

    let res = match llama_core::chat::chat(&mut chat_request).await {
        Ok(result) => match result {
            either::Left(stream) => {
                let stream = stream.map_err(|e| e.to_string());

                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""text/event-stream"")
                    .header(""Cache-Control"", ""no-cache"")
                    .header(""Connection"", ""keep-alive"")
                    .header(""user"", id)
                    .body(Body::wrap_stream(stream));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""finish chat completions in stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            either::Right(chat_completion_object) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&chat_completion_object) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize chat completion object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .header(""user"", id)
                    .body(Body::from(s));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""Finish chat completions in non-stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in non-stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
        },
        Err(e) => {
            let err_msg = format!(""Failed to get chat completions. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the chat completion response."");

    res
}

/// Upload files and return the file object.
pub(crate) async fn files_handler(req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming files request"");

    let res = if req.method() == Method::POST {
        let boundary = ""boundary="";

        let boundary = req.headers().get(""content-type"").and_then(|ct| {
            let ct = ct.to_str().ok()?;
            let idx = ct.find(boundary)?;
            Some(ct[idx + boundary.len()..].to_string())
        });

        let req_body = req.into_body();
        let body_bytes = match to_bytes(req_body).await {
            Ok(body_bytes) => body_bytes,
            Err(e) => {
                let err_msg = format!(""Fail to read buffer from request body. {}"", e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        let cursor = Cursor::new(body_bytes.to_vec());

        let mut multipart = Multipart::with_body(cursor, boundary.unwrap());

        let mut file_object: Option<FileObject> = None;
        while let ReadEntryResult::Entry(mut field) = multipart.read_entry_mut() {
            if &*field.headers.name == ""file"" {
                let filename = match field.headers.filename {
                    Some(filename) => filename,
                    None => {
                        let err_msg =
                            ""Failed to upload the target file. The filename is not provided."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                if !((filename).to_lowercase().ends_with("".txt"")
                    || (filename).to_lowercase().ends_with("".md""))
                    || (filename).to_lowercase().ends_with("".png"")
                {
                    let err_msg = format!(
                        ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported. The file extension is {}."",
                        &filename
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }

                let mut buffer = Vec::new();
                let size_in_bytes = match field.data.read_to_end(&mut buffer) {
                    Ok(size_in_bytes) => size_in_bytes,
                    Err(e) => {
                        let err_msg = format!(""Failed to read the target file. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a unique file id
                let id = format!(""file_{}"", uuid::Uuid::new_v4());

                // save the file
                let path = Path::new(""archives"");
                if !path.exists() {
                    fs::create_dir(path).unwrap();
                }
                let file_path = path.join(&id);
                if !file_path.exists() {
                    fs::create_dir(&file_path).unwrap();
                }
                let mut file = match File::create(file_path.join(&filename)) {
                    Ok(file) => file,
                    Err(e) => {
                        let err_msg =
                            format!(""Failed to create archive document {}. {}"", &filename, e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };
                file.write_all(&buffer[..]).unwrap();

                // log
                info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

                let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a file object
                file_object = Some(FileObject {
                    id,
                    bytes: size_in_bytes as u64,
                    created_at,
                    filename,
                    object: ""file"".to_string(),
                    purpose: ""assistants"".to_string(),
                });

                break;
            }
        }

        match file_object {
            Some(fo) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&fo) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize file object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .body(Body::from(s));

                match result {
                    Ok(response) => response,
                    Err(e) => {
                        let err_msg = e.to_string();

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            None => {
                let err_msg = ""Failed to upload the target file. Not found the target file."";

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::GET {
        let uri_path = req.uri().path();

        if uri_path == ""/v1/files"" {
            let mut file_objects: Vec<FileObject> = Vec::new();
            for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let id = entry
                        .path()
                        .parent()
                        .and_then(|p| p.file_name())
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .to_string();

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    let fo = FileObject {
                        id,
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    };

                    file_objects.push(fo);
                }
            }

            info!(target: ""stdout"", ""Found {} archive files"", file_objects.len());

            let file_objects = ListFilesResponse {
                object: ""list"".to_string(),
                data: file_objects,
            };

            // serialize chat completion object
            let s = match serde_json::to_string(&file_objects) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Failed to serialize file object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(s));

            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        } else {
            let id = uri_path.trim_start_matches(""/v1/files/"");
            let root = format!(""archives/{}"", id);
            let mut file_object: Option<FileObject> = None;
            for entry in WalkDir::new(root).into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    file_object = Some(FileObject {
                        id: id.into(),
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    });

                    break;
                }
            }

            match file_object {
                Some(fo) => {
                    // serialize chat completion object
                    let s = match serde_json::to_string(&fo) {
                        Ok(s) => s,
                        Err(e) => {
                            let err_msg = format!(""Failed to serialize file object. {}"", e);

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            return error::internal_server_error(err_msg);
                        }
                    };

                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));

                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                None => {
                    let err_msg = format!(
                        ""Failed to retrieve the target file. Not found the target file with id {}."",
                        id
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
    } else if req.method() == Method::DELETE {
        let id = req.uri().path().trim_start_matches(""/v1/files/"");
        let root = format!(""archives/{}"", id);
        let status = match fs::remove_dir_all(root) {
            Ok(_) => {
                info!(target: ""stdout"", ""Successfully deleted the target file with id {}."", id);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: true,
                }
            }
            Err(e) => {
                let err_msg = format!(""Failed to delete the target file with id {}. {}"", id, e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: false,
                }
            }
        };

        // serialize status
        let s = match serde_json::to_string(&status) {
            Ok(s) => s,
            Err(e) => {
                let err_msg = format!(
                    ""Failed to serialize the status of the file deletion operation. {}"",
                    e
                );

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        // return response
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::from(s));

        match result {
            Ok(response) => response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::OPTIONS {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""files_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    } else {
        let err_msg = ""Invalid HTTP Method."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        error::internal_server_error(err_msg)
    };

    info!(target: ""stdout"", ""Send the files response"");

    res
}

/// Segment the text into chunks and return the chunks response.
pub(crate) async fn chunks_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming chunks request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""chunks_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    let chunks_request: ChunksRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chunks_request) => chunks_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chunks request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the archives directory exists
    let path = Path::new(""archives"");
    if !path.exists() {
        let err_msg = ""The `archives` directory does not exist."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the archive id exists
    let archive_path = path.join(&chunks_request.id);
    if !archive_path.exists() {
        let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the file exists
    let file_path = archive_path.join(&chunks_request.filename);
    if !file_path.exists() {
        let err_msg = format!(
            ""Not found file: {} in archive id: {}"",
            &chunks_request.filename, &chunks_request.id
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // log
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &chunks_request.id, &chunks_request.filename);

    // get the extension of the archived file
    let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
        Some(extension) => extension,
        None => {
            let err_msg = format!(
                ""Failed to get the extension of the archived `{}`."",
                &chunks_request.filename
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // open the file
    let mut file = match File::open(&file_path) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to open `{}`. {}"", &chunks_request.filename, e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // read the file
    let mut contents = String::new();
    if let Err(e) = file.read_to_string(&mut contents) {
        let err_msg = format!(""Failed to read `{}`. {}"", &chunks_request.filename, e);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
    {
        Ok(chunks) => {
            let chunks_response = ChunksResponse {
                id: chunks_request.id,
                filename: chunks_request.filename,
                chunks,
            };

            // serialize embedding object
            match serde_json::to_string(&chunks_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize chunks response. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the chunks response."");

    res
}

/// Return the server info.
pub(crate) async fn server_info_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming server info request."");

    // get the server info
    let server_info = match SERVER_INFO.get() {
        Some(server_info) => server_info,
        None => {
            let err_msg = ""The server info is not set."";

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(""The server info is not set."");
        }
    };

    // serialize server info
    let s = match serde_json::to_string(&server_info) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Fail to serialize server info. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the server info response."");

    res
}

fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}

```","Q: 5. How does the `files_handler` function handle file deletion operations?
A: The `files_handler` function handles file deletion operations as follows:

1. When a DELETE request is made to the `/v1/files` endpoint, the function checks if the `id` parameter in the request's URI path matches an existing file in the `archives` directory.
2. If a match is found, the function attempts to remove the corresponding directory and all its contents using the `fs::remove_dir_all()` method.
3. If the deletion operation is successful, the function creates a `DeleteFileStatus` object with the `id`, `object`, and `deleted` fields set accordingly. The `deleted` field is set to `true`.
4. The function then serializes the `DeleteFileStatus` object into JSON using the `serde_json::to_string()` method.
5. Finally, the function returns a HTTP response with a 200 OK status code and the serialized JSON as its body.

However, if the file deletion operation fails for any reason (e.g., permissions error, directory not empty), the function catches the error, logs an error message, and returns a `DeleteFileStatus` object with the `deleted` field set to `false`. This allows the client to determine whether the deletion operation was successful or not.

Here's an example of how the code handles file deletion operations:
```rust
if req.method() == Method::DELETE {
    let id = req.uri().path().trim_start_matches(""/v1/files/"");
    let root = format!(""archives/{}"", id);
    let status = match fs::remove_dir_all(root) {
        Ok(_) => DeleteFileStatus {
            id: id.into(),
            object: ""file"".to_string(),
            deleted: true,
        },
        Err(e) => DeleteFileStatus {
            id: id.into(),
            object: ""file"".to_string(),
            deleted: false,
        },
    };

    // ...
}
```
In this example, the `files_handler` function uses a `match` statement to handle the outcome of the file deletion operation. If the deletion is successful, it creates a `DeleteFileStatus` object with the `deleted` field set to `true`. Otherwise, it creates an error message and logs it using the `error!()` macro."
"```rust:llama-api-server/src/backend/ggml.rs
use crate::{error, utils::gen_chat_id, SERVER_INFO};
use endpoints::{
    chat::ChatCompletionRequest,
    completions::CompletionRequest,
    embeddings::EmbeddingRequest,
    files::{DeleteFileStatus, FileObject, ListFilesResponse},
    rag::{ChunksRequest, ChunksResponse},
};
use futures_util::TryStreamExt;
use hyper::{body::to_bytes, Body, Method, Request, Response};
use multipart::server::{Multipart, ReadEntry, ReadEntryResult};
use multipart_2021 as multipart;
use std::{
    fs::{self, File},
    io::{Cursor, Read, Write},
    path::Path,
    time::SystemTime,
};
use walkdir::{DirEntry, WalkDir};

/// List all models available.
pub(crate) async fn models_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming model list request."");

    let list_models_response = match llama_core::models::models().await {
        Ok(list_models_response) => list_models_response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // serialize response
    let s = match serde_json::to_string(&list_models_response) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Failed to serialize the model list result. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the model list response."");

    res
}

/// Compute embeddings for the input text and return the embeddings object.
pub(crate) async fn embeddings_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming embeddings request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""embeddings_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut embedding_request: EmbeddingRequest = match serde_json::from_slice(&body_bytes) {
        Ok(embedding_request) => embedding_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize embedding request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if embedding_request.user.is_none() {
        embedding_request.user = Some(gen_chat_id())
    };
    let id = embedding_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::embeddings::embeddings(&embedding_request).await {
        Ok(embedding_response) => {
            // serialize embedding object
            match serde_json::to_string(&embedding_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .header(""user"", id)
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize embedding object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the embeddings response"");

    res
}

/// Process a completion request and returns a completion response with the answer from the model.
pub(crate) async fn completions_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming completions request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""completions_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut completion_request: CompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(completion_request) => completion_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize completions request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if completion_request.user.is_none() {
        completion_request.user = Some(gen_chat_id())
    };
    let id = completion_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::completions::completions(&completion_request).await {
        Ok(completion_object) => {
            // serialize completion object
            let s = match serde_json::to_string(&completion_object) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Fail to serialize completion object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .header(""user"", id)
                .body(Body::from(s));
            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the completions response."");

    res
}

/// Process a chat-completion request and returns a chat-completion response with the answer from the model.
pub(crate) async fn chat_completions_handler(mut req: Request<Body>) -> Response<Body> {
    info!(target: ""stdout"", ""Handling the coming chat completion request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    info!(target: ""stdout"", ""Prepare the chat completion request."");

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut chat_request: ChatCompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chat_request) => chat_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chat completion request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the user id is provided
    if chat_request.user.is_none() {
        chat_request.user = Some(gen_chat_id())
    };
    let id = chat_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", chat_request.user.clone().unwrap());

    let res = match llama_core::chat::chat(&mut chat_request).await {
        Ok(result) => match result {
            either::Left(stream) => {
                let stream = stream.map_err(|e| e.to_string());

                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""text/event-stream"")
                    .header(""Cache-Control"", ""no-cache"")
                    .header(""Connection"", ""keep-alive"")
                    .header(""user"", id)
                    .body(Body::wrap_stream(stream));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""finish chat completions in stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            either::Right(chat_completion_object) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&chat_completion_object) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize chat completion object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .header(""user"", id)
                    .body(Body::from(s));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""Finish chat completions in non-stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in non-stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
        },
        Err(e) => {
            let err_msg = format!(""Failed to get chat completions. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the chat completion response."");

    res
}

/// Upload files and return the file object.
pub(crate) async fn files_handler(req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming files request"");

    let res = if req.method() == Method::POST {
        let boundary = ""boundary="";

        let boundary = req.headers().get(""content-type"").and_then(|ct| {
            let ct = ct.to_str().ok()?;
            let idx = ct.find(boundary)?;
            Some(ct[idx + boundary.len()..].to_string())
        });

        let req_body = req.into_body();
        let body_bytes = match to_bytes(req_body).await {
            Ok(body_bytes) => body_bytes,
            Err(e) => {
                let err_msg = format!(""Fail to read buffer from request body. {}"", e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        let cursor = Cursor::new(body_bytes.to_vec());

        let mut multipart = Multipart::with_body(cursor, boundary.unwrap());

        let mut file_object: Option<FileObject> = None;
        while let ReadEntryResult::Entry(mut field) = multipart.read_entry_mut() {
            if &*field.headers.name == ""file"" {
                let filename = match field.headers.filename {
                    Some(filename) => filename,
                    None => {
                        let err_msg =
                            ""Failed to upload the target file. The filename is not provided."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                if !((filename).to_lowercase().ends_with("".txt"")
                    || (filename).to_lowercase().ends_with("".md""))
                    || (filename).to_lowercase().ends_with("".png"")
                {
                    let err_msg = format!(
                        ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported. The file extension is {}."",
                        &filename
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }

                let mut buffer = Vec::new();
                let size_in_bytes = match field.data.read_to_end(&mut buffer) {
                    Ok(size_in_bytes) => size_in_bytes,
                    Err(e) => {
                        let err_msg = format!(""Failed to read the target file. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a unique file id
                let id = format!(""file_{}"", uuid::Uuid::new_v4());

                // save the file
                let path = Path::new(""archives"");
                if !path.exists() {
                    fs::create_dir(path).unwrap();
                }
                let file_path = path.join(&id);
                if !file_path.exists() {
                    fs::create_dir(&file_path).unwrap();
                }
                let mut file = match File::create(file_path.join(&filename)) {
                    Ok(file) => file,
                    Err(e) => {
                        let err_msg =
                            format!(""Failed to create archive document {}. {}"", &filename, e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };
                file.write_all(&buffer[..]).unwrap();

                // log
                info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

                let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a file object
                file_object = Some(FileObject {
                    id,
                    bytes: size_in_bytes as u64,
                    created_at,
                    filename,
                    object: ""file"".to_string(),
                    purpose: ""assistants"".to_string(),
                });

                break;
            }
        }

        match file_object {
            Some(fo) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&fo) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize file object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .body(Body::from(s));

                match result {
                    Ok(response) => response,
                    Err(e) => {
                        let err_msg = e.to_string();

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            None => {
                let err_msg = ""Failed to upload the target file. Not found the target file."";

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::GET {
        let uri_path = req.uri().path();

        if uri_path == ""/v1/files"" {
            let mut file_objects: Vec<FileObject> = Vec::new();
            for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let id = entry
                        .path()
                        .parent()
                        .and_then(|p| p.file_name())
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .to_string();

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    let fo = FileObject {
                        id,
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    };

                    file_objects.push(fo);
                }
            }

            info!(target: ""stdout"", ""Found {} archive files"", file_objects.len());

            let file_objects = ListFilesResponse {
                object: ""list"".to_string(),
                data: file_objects,
            };

            // serialize chat completion object
            let s = match serde_json::to_string(&file_objects) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Failed to serialize file object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(s));

            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        } else {
            let id = uri_path.trim_start_matches(""/v1/files/"");
            let root = format!(""archives/{}"", id);
            let mut file_object: Option<FileObject> = None;
            for entry in WalkDir::new(root).into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    file_object = Some(FileObject {
                        id: id.into(),
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    });

                    break;
                }
            }

            match file_object {
                Some(fo) => {
                    // serialize chat completion object
                    let s = match serde_json::to_string(&fo) {
                        Ok(s) => s,
                        Err(e) => {
                            let err_msg = format!(""Failed to serialize file object. {}"", e);

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            return error::internal_server_error(err_msg);
                        }
                    };

                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));

                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                None => {
                    let err_msg = format!(
                        ""Failed to retrieve the target file. Not found the target file with id {}."",
                        id
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
    } else if req.method() == Method::DELETE {
        let id = req.uri().path().trim_start_matches(""/v1/files/"");
        let root = format!(""archives/{}"", id);
        let status = match fs::remove_dir_all(root) {
            Ok(_) => {
                info!(target: ""stdout"", ""Successfully deleted the target file with id {}."", id);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: true,
                }
            }
            Err(e) => {
                let err_msg = format!(""Failed to delete the target file with id {}. {}"", id, e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: false,
                }
            }
        };

        // serialize status
        let s = match serde_json::to_string(&status) {
            Ok(s) => s,
            Err(e) => {
                let err_msg = format!(
                    ""Failed to serialize the status of the file deletion operation. {}"",
                    e
                );

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        // return response
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::from(s));

        match result {
            Ok(response) => response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::OPTIONS {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""files_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    } else {
        let err_msg = ""Invalid HTTP Method."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        error::internal_server_error(err_msg)
    };

    info!(target: ""stdout"", ""Send the files response"");

    res
}

/// Segment the text into chunks and return the chunks response.
pub(crate) async fn chunks_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming chunks request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""chunks_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    let chunks_request: ChunksRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chunks_request) => chunks_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chunks request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the archives directory exists
    let path = Path::new(""archives"");
    if !path.exists() {
        let err_msg = ""The `archives` directory does not exist."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the archive id exists
    let archive_path = path.join(&chunks_request.id);
    if !archive_path.exists() {
        let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the file exists
    let file_path = archive_path.join(&chunks_request.filename);
    if !file_path.exists() {
        let err_msg = format!(
            ""Not found file: {} in archive id: {}"",
            &chunks_request.filename, &chunks_request.id
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // log
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &chunks_request.id, &chunks_request.filename);

    // get the extension of the archived file
    let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
        Some(extension) => extension,
        None => {
            let err_msg = format!(
                ""Failed to get the extension of the archived `{}`."",
                &chunks_request.filename
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // open the file
    let mut file = match File::open(&file_path) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to open `{}`. {}"", &chunks_request.filename, e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // read the file
    let mut contents = String::new();
    if let Err(e) = file.read_to_string(&mut contents) {
        let err_msg = format!(""Failed to read `{}`. {}"", &chunks_request.filename, e);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
    {
        Ok(chunks) => {
            let chunks_response = ChunksResponse {
                id: chunks_request.id,
                filename: chunks_request.filename,
                chunks,
            };

            // serialize embedding object
            match serde_json::to_string(&chunks_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize chunks response. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the chunks response."");

    res
}

/// Return the server info.
pub(crate) async fn server_info_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming server info request."");

    // get the server info
    let server_info = match SERVER_INFO.get() {
        Some(server_info) => server_info,
        None => {
            let err_msg = ""The server info is not set."";

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(""The server info is not set."");
        }
    };

    // serialize server info
    let s = match serde_json::to_string(&server_info) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Fail to serialize server info. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the server info response."");

    res
}

fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}

```","Q: 6. In the `chunks_handler` function, what happens when the requested archive ID or filename is not found?
A: In the `chunks_handler` function, when the requested archive ID or filename is not found, an error message is logged using the `error!(target: ""stdout"", ""{}"", &err_msg)` macro. This error message includes a description of the issue.

Specifically:

- For missing archive IDs:
  ```rust
let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

// log
error!(target: ""stdout"", ""{}"", &err_msg);
```

- For missing file names within an archive ID:
  ```rust
let err_msg = format!(
    ""Not found file: {} in archive id: {}"",
    &chunks_request.filename, &chunks_request.id
);
// log
error!(target: ""stdout"", ""{}"", &err_msg);
```

After logging the error message, the function returns an internal server error response using `error::internal_server_error(err_msg)`. This ensures that the server handles the situation where requested archive IDs or file names are not found properly."
"```rust:llama-api-server/src/backend/ggml.rs
use crate::{error, utils::gen_chat_id, SERVER_INFO};
use endpoints::{
    chat::ChatCompletionRequest,
    completions::CompletionRequest,
    embeddings::EmbeddingRequest,
    files::{DeleteFileStatus, FileObject, ListFilesResponse},
    rag::{ChunksRequest, ChunksResponse},
};
use futures_util::TryStreamExt;
use hyper::{body::to_bytes, Body, Method, Request, Response};
use multipart::server::{Multipart, ReadEntry, ReadEntryResult};
use multipart_2021 as multipart;
use std::{
    fs::{self, File},
    io::{Cursor, Read, Write},
    path::Path,
    time::SystemTime,
};
use walkdir::{DirEntry, WalkDir};

/// List all models available.
pub(crate) async fn models_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming model list request."");

    let list_models_response = match llama_core::models::models().await {
        Ok(list_models_response) => list_models_response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // serialize response
    let s = match serde_json::to_string(&list_models_response) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Failed to serialize the model list result. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the model list response."");

    res
}

/// Compute embeddings for the input text and return the embeddings object.
pub(crate) async fn embeddings_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming embeddings request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""embeddings_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut embedding_request: EmbeddingRequest = match serde_json::from_slice(&body_bytes) {
        Ok(embedding_request) => embedding_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize embedding request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if embedding_request.user.is_none() {
        embedding_request.user = Some(gen_chat_id())
    };
    let id = embedding_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::embeddings::embeddings(&embedding_request).await {
        Ok(embedding_response) => {
            // serialize embedding object
            match serde_json::to_string(&embedding_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .header(""user"", id)
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize embedding object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the embeddings response"");

    res
}

/// Process a completion request and returns a completion response with the answer from the model.
pub(crate) async fn completions_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming completions request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""completions_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut completion_request: CompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(completion_request) => completion_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize completions request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if completion_request.user.is_none() {
        completion_request.user = Some(gen_chat_id())
    };
    let id = completion_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::completions::completions(&completion_request).await {
        Ok(completion_object) => {
            // serialize completion object
            let s = match serde_json::to_string(&completion_object) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Fail to serialize completion object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .header(""user"", id)
                .body(Body::from(s));
            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the completions response."");

    res
}

/// Process a chat-completion request and returns a chat-completion response with the answer from the model.
pub(crate) async fn chat_completions_handler(mut req: Request<Body>) -> Response<Body> {
    info!(target: ""stdout"", ""Handling the coming chat completion request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    info!(target: ""stdout"", ""Prepare the chat completion request."");

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut chat_request: ChatCompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chat_request) => chat_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chat completion request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the user id is provided
    if chat_request.user.is_none() {
        chat_request.user = Some(gen_chat_id())
    };
    let id = chat_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", chat_request.user.clone().unwrap());

    let res = match llama_core::chat::chat(&mut chat_request).await {
        Ok(result) => match result {
            either::Left(stream) => {
                let stream = stream.map_err(|e| e.to_string());

                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""text/event-stream"")
                    .header(""Cache-Control"", ""no-cache"")
                    .header(""Connection"", ""keep-alive"")
                    .header(""user"", id)
                    .body(Body::wrap_stream(stream));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""finish chat completions in stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            either::Right(chat_completion_object) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&chat_completion_object) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize chat completion object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .header(""user"", id)
                    .body(Body::from(s));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""Finish chat completions in non-stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in non-stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
        },
        Err(e) => {
            let err_msg = format!(""Failed to get chat completions. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the chat completion response."");

    res
}

/// Upload files and return the file object.
pub(crate) async fn files_handler(req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming files request"");

    let res = if req.method() == Method::POST {
        let boundary = ""boundary="";

        let boundary = req.headers().get(""content-type"").and_then(|ct| {
            let ct = ct.to_str().ok()?;
            let idx = ct.find(boundary)?;
            Some(ct[idx + boundary.len()..].to_string())
        });

        let req_body = req.into_body();
        let body_bytes = match to_bytes(req_body).await {
            Ok(body_bytes) => body_bytes,
            Err(e) => {
                let err_msg = format!(""Fail to read buffer from request body. {}"", e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        let cursor = Cursor::new(body_bytes.to_vec());

        let mut multipart = Multipart::with_body(cursor, boundary.unwrap());

        let mut file_object: Option<FileObject> = None;
        while let ReadEntryResult::Entry(mut field) = multipart.read_entry_mut() {
            if &*field.headers.name == ""file"" {
                let filename = match field.headers.filename {
                    Some(filename) => filename,
                    None => {
                        let err_msg =
                            ""Failed to upload the target file. The filename is not provided."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                if !((filename).to_lowercase().ends_with("".txt"")
                    || (filename).to_lowercase().ends_with("".md""))
                    || (filename).to_lowercase().ends_with("".png"")
                {
                    let err_msg = format!(
                        ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported. The file extension is {}."",
                        &filename
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }

                let mut buffer = Vec::new();
                let size_in_bytes = match field.data.read_to_end(&mut buffer) {
                    Ok(size_in_bytes) => size_in_bytes,
                    Err(e) => {
                        let err_msg = format!(""Failed to read the target file. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a unique file id
                let id = format!(""file_{}"", uuid::Uuid::new_v4());

                // save the file
                let path = Path::new(""archives"");
                if !path.exists() {
                    fs::create_dir(path).unwrap();
                }
                let file_path = path.join(&id);
                if !file_path.exists() {
                    fs::create_dir(&file_path).unwrap();
                }
                let mut file = match File::create(file_path.join(&filename)) {
                    Ok(file) => file,
                    Err(e) => {
                        let err_msg =
                            format!(""Failed to create archive document {}. {}"", &filename, e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };
                file.write_all(&buffer[..]).unwrap();

                // log
                info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

                let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a file object
                file_object = Some(FileObject {
                    id,
                    bytes: size_in_bytes as u64,
                    created_at,
                    filename,
                    object: ""file"".to_string(),
                    purpose: ""assistants"".to_string(),
                });

                break;
            }
        }

        match file_object {
            Some(fo) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&fo) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize file object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .body(Body::from(s));

                match result {
                    Ok(response) => response,
                    Err(e) => {
                        let err_msg = e.to_string();

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            None => {
                let err_msg = ""Failed to upload the target file. Not found the target file."";

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::GET {
        let uri_path = req.uri().path();

        if uri_path == ""/v1/files"" {
            let mut file_objects: Vec<FileObject> = Vec::new();
            for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let id = entry
                        .path()
                        .parent()
                        .and_then(|p| p.file_name())
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .to_string();

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    let fo = FileObject {
                        id,
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    };

                    file_objects.push(fo);
                }
            }

            info!(target: ""stdout"", ""Found {} archive files"", file_objects.len());

            let file_objects = ListFilesResponse {
                object: ""list"".to_string(),
                data: file_objects,
            };

            // serialize chat completion object
            let s = match serde_json::to_string(&file_objects) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Failed to serialize file object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(s));

            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        } else {
            let id = uri_path.trim_start_matches(""/v1/files/"");
            let root = format!(""archives/{}"", id);
            let mut file_object: Option<FileObject> = None;
            for entry in WalkDir::new(root).into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    file_object = Some(FileObject {
                        id: id.into(),
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    });

                    break;
                }
            }

            match file_object {
                Some(fo) => {
                    // serialize chat completion object
                    let s = match serde_json::to_string(&fo) {
                        Ok(s) => s,
                        Err(e) => {
                            let err_msg = format!(""Failed to serialize file object. {}"", e);

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            return error::internal_server_error(err_msg);
                        }
                    };

                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));

                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                None => {
                    let err_msg = format!(
                        ""Failed to retrieve the target file. Not found the target file with id {}."",
                        id
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
    } else if req.method() == Method::DELETE {
        let id = req.uri().path().trim_start_matches(""/v1/files/"");
        let root = format!(""archives/{}"", id);
        let status = match fs::remove_dir_all(root) {
            Ok(_) => {
                info!(target: ""stdout"", ""Successfully deleted the target file with id {}."", id);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: true,
                }
            }
            Err(e) => {
                let err_msg = format!(""Failed to delete the target file with id {}. {}"", id, e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: false,
                }
            }
        };

        // serialize status
        let s = match serde_json::to_string(&status) {
            Ok(s) => s,
            Err(e) => {
                let err_msg = format!(
                    ""Failed to serialize the status of the file deletion operation. {}"",
                    e
                );

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        // return response
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::from(s));

        match result {
            Ok(response) => response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::OPTIONS {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""files_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    } else {
        let err_msg = ""Invalid HTTP Method."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        error::internal_server_error(err_msg)
    };

    info!(target: ""stdout"", ""Send the files response"");

    res
}

/// Segment the text into chunks and return the chunks response.
pub(crate) async fn chunks_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming chunks request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""chunks_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    let chunks_request: ChunksRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chunks_request) => chunks_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chunks request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the archives directory exists
    let path = Path::new(""archives"");
    if !path.exists() {
        let err_msg = ""The `archives` directory does not exist."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the archive id exists
    let archive_path = path.join(&chunks_request.id);
    if !archive_path.exists() {
        let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the file exists
    let file_path = archive_path.join(&chunks_request.filename);
    if !file_path.exists() {
        let err_msg = format!(
            ""Not found file: {} in archive id: {}"",
            &chunks_request.filename, &chunks_request.id
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // log
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &chunks_request.id, &chunks_request.filename);

    // get the extension of the archived file
    let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
        Some(extension) => extension,
        None => {
            let err_msg = format!(
                ""Failed to get the extension of the archived `{}`."",
                &chunks_request.filename
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // open the file
    let mut file = match File::open(&file_path) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to open `{}`. {}"", &chunks_request.filename, e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // read the file
    let mut contents = String::new();
    if let Err(e) = file.read_to_string(&mut contents) {
        let err_msg = format!(""Failed to read `{}`. {}"", &chunks_request.filename, e);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
    {
        Ok(chunks) => {
            let chunks_response = ChunksResponse {
                id: chunks_request.id,
                filename: chunks_request.filename,
                chunks,
            };

            // serialize embedding object
            match serde_json::to_string(&chunks_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize chunks response. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the chunks response."");

    res
}

/// Return the server info.
pub(crate) async fn server_info_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming server info request."");

    // get the server info
    let server_info = match SERVER_INFO.get() {
        Some(server_info) => server_info,
        None => {
            let err_msg = ""The server info is not set."";

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(""The server info is not set."");
        }
    };

    // serialize server info
    let s = match serde_json::to_string(&server_info) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Fail to serialize server info. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the server info response."");

    res
}

fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}

```","Q: 7. What is the purpose of the `error::internal_server_error` function used throughout the code to handle internal server errors?
A: The `error::internal_server_error` function is used to handle internal server errors by returning a response with an HTTP status code of 500, indicating that the request failed due to an error on the server side.

Here's an example of how it's typically implemented:

```rust
pub(crate) fn internal_server_error(msg: String) -> Response<Body> {
    let res = Response::builder()
        .status(StatusCode::INTERNAL_SERVER_ERROR)
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(msg));

    match res {
        Ok(response) => response,
        Err(e) => {
            error!(target: ""error::internal_server_error"", ""{}"", e);

            Response::builder()
                .status(StatusCode::INTERNAL_SERVER_ERROR)
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(""Internal server error""))
        }
    }
}
```

In this example, the function takes a `String` message as an argument and returns a response with a 500 status code. The body of the response is set to the provided error message.

However, in the provided code snippet, the implementation is slightly different:

```rust
error::internal_server_error(err_msg)
```

It seems like there might be an issue with this line as `err_msg` is not defined anywhere in the code snippet. It's likely that it should be replaced with a variable containing the actual error message.

The purpose of this function is to provide a standardized way of handling internal server errors and returning a response with a consistent format, regardless of the specific error that occurred.

**Bonus Tip**: In production environments, it's essential to log the actual error messages and stack traces for debugging purposes. This can be achieved by adding logging statements or using a centralized logging framework like `logrus` or `serilog`."
"```rust:llama-api-server/src/backend/ggml.rs
use crate::{error, utils::gen_chat_id, SERVER_INFO};
use endpoints::{
    chat::ChatCompletionRequest,
    completions::CompletionRequest,
    embeddings::EmbeddingRequest,
    files::{DeleteFileStatus, FileObject, ListFilesResponse},
    rag::{ChunksRequest, ChunksResponse},
};
use futures_util::TryStreamExt;
use hyper::{body::to_bytes, Body, Method, Request, Response};
use multipart::server::{Multipart, ReadEntry, ReadEntryResult};
use multipart_2021 as multipart;
use std::{
    fs::{self, File},
    io::{Cursor, Read, Write},
    path::Path,
    time::SystemTime,
};
use walkdir::{DirEntry, WalkDir};

/// List all models available.
pub(crate) async fn models_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming model list request."");

    let list_models_response = match llama_core::models::models().await {
        Ok(list_models_response) => list_models_response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // serialize response
    let s = match serde_json::to_string(&list_models_response) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Failed to serialize the model list result. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the model list response."");

    res
}

/// Compute embeddings for the input text and return the embeddings object.
pub(crate) async fn embeddings_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming embeddings request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""embeddings_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut embedding_request: EmbeddingRequest = match serde_json::from_slice(&body_bytes) {
        Ok(embedding_request) => embedding_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize embedding request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if embedding_request.user.is_none() {
        embedding_request.user = Some(gen_chat_id())
    };
    let id = embedding_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::embeddings::embeddings(&embedding_request).await {
        Ok(embedding_response) => {
            // serialize embedding object
            match serde_json::to_string(&embedding_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .header(""user"", id)
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize embedding object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the embeddings response"");

    res
}

/// Process a completion request and returns a completion response with the answer from the model.
pub(crate) async fn completions_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming completions request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""completions_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut completion_request: CompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(completion_request) => completion_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize completions request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if completion_request.user.is_none() {
        completion_request.user = Some(gen_chat_id())
    };
    let id = completion_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::completions::completions(&completion_request).await {
        Ok(completion_object) => {
            // serialize completion object
            let s = match serde_json::to_string(&completion_object) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Fail to serialize completion object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .header(""user"", id)
                .body(Body::from(s));
            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the completions response."");

    res
}

/// Process a chat-completion request and returns a chat-completion response with the answer from the model.
pub(crate) async fn chat_completions_handler(mut req: Request<Body>) -> Response<Body> {
    info!(target: ""stdout"", ""Handling the coming chat completion request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    info!(target: ""stdout"", ""Prepare the chat completion request."");

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut chat_request: ChatCompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chat_request) => chat_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chat completion request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the user id is provided
    if chat_request.user.is_none() {
        chat_request.user = Some(gen_chat_id())
    };
    let id = chat_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", chat_request.user.clone().unwrap());

    let res = match llama_core::chat::chat(&mut chat_request).await {
        Ok(result) => match result {
            either::Left(stream) => {
                let stream = stream.map_err(|e| e.to_string());

                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""text/event-stream"")
                    .header(""Cache-Control"", ""no-cache"")
                    .header(""Connection"", ""keep-alive"")
                    .header(""user"", id)
                    .body(Body::wrap_stream(stream));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""finish chat completions in stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            either::Right(chat_completion_object) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&chat_completion_object) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize chat completion object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .header(""user"", id)
                    .body(Body::from(s));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""Finish chat completions in non-stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in non-stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
        },
        Err(e) => {
            let err_msg = format!(""Failed to get chat completions. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the chat completion response."");

    res
}

/// Upload files and return the file object.
pub(crate) async fn files_handler(req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming files request"");

    let res = if req.method() == Method::POST {
        let boundary = ""boundary="";

        let boundary = req.headers().get(""content-type"").and_then(|ct| {
            let ct = ct.to_str().ok()?;
            let idx = ct.find(boundary)?;
            Some(ct[idx + boundary.len()..].to_string())
        });

        let req_body = req.into_body();
        let body_bytes = match to_bytes(req_body).await {
            Ok(body_bytes) => body_bytes,
            Err(e) => {
                let err_msg = format!(""Fail to read buffer from request body. {}"", e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        let cursor = Cursor::new(body_bytes.to_vec());

        let mut multipart = Multipart::with_body(cursor, boundary.unwrap());

        let mut file_object: Option<FileObject> = None;
        while let ReadEntryResult::Entry(mut field) = multipart.read_entry_mut() {
            if &*field.headers.name == ""file"" {
                let filename = match field.headers.filename {
                    Some(filename) => filename,
                    None => {
                        let err_msg =
                            ""Failed to upload the target file. The filename is not provided."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                if !((filename).to_lowercase().ends_with("".txt"")
                    || (filename).to_lowercase().ends_with("".md""))
                    || (filename).to_lowercase().ends_with("".png"")
                {
                    let err_msg = format!(
                        ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported. The file extension is {}."",
                        &filename
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }

                let mut buffer = Vec::new();
                let size_in_bytes = match field.data.read_to_end(&mut buffer) {
                    Ok(size_in_bytes) => size_in_bytes,
                    Err(e) => {
                        let err_msg = format!(""Failed to read the target file. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a unique file id
                let id = format!(""file_{}"", uuid::Uuid::new_v4());

                // save the file
                let path = Path::new(""archives"");
                if !path.exists() {
                    fs::create_dir(path).unwrap();
                }
                let file_path = path.join(&id);
                if !file_path.exists() {
                    fs::create_dir(&file_path).unwrap();
                }
                let mut file = match File::create(file_path.join(&filename)) {
                    Ok(file) => file,
                    Err(e) => {
                        let err_msg =
                            format!(""Failed to create archive document {}. {}"", &filename, e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };
                file.write_all(&buffer[..]).unwrap();

                // log
                info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

                let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a file object
                file_object = Some(FileObject {
                    id,
                    bytes: size_in_bytes as u64,
                    created_at,
                    filename,
                    object: ""file"".to_string(),
                    purpose: ""assistants"".to_string(),
                });

                break;
            }
        }

        match file_object {
            Some(fo) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&fo) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize file object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .body(Body::from(s));

                match result {
                    Ok(response) => response,
                    Err(e) => {
                        let err_msg = e.to_string();

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            None => {
                let err_msg = ""Failed to upload the target file. Not found the target file."";

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::GET {
        let uri_path = req.uri().path();

        if uri_path == ""/v1/files"" {
            let mut file_objects: Vec<FileObject> = Vec::new();
            for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let id = entry
                        .path()
                        .parent()
                        .and_then(|p| p.file_name())
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .to_string();

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    let fo = FileObject {
                        id,
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    };

                    file_objects.push(fo);
                }
            }

            info!(target: ""stdout"", ""Found {} archive files"", file_objects.len());

            let file_objects = ListFilesResponse {
                object: ""list"".to_string(),
                data: file_objects,
            };

            // serialize chat completion object
            let s = match serde_json::to_string(&file_objects) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Failed to serialize file object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(s));

            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        } else {
            let id = uri_path.trim_start_matches(""/v1/files/"");
            let root = format!(""archives/{}"", id);
            let mut file_object: Option<FileObject> = None;
            for entry in WalkDir::new(root).into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    file_object = Some(FileObject {
                        id: id.into(),
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    });

                    break;
                }
            }

            match file_object {
                Some(fo) => {
                    // serialize chat completion object
                    let s = match serde_json::to_string(&fo) {
                        Ok(s) => s,
                        Err(e) => {
                            let err_msg = format!(""Failed to serialize file object. {}"", e);

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            return error::internal_server_error(err_msg);
                        }
                    };

                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));

                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                None => {
                    let err_msg = format!(
                        ""Failed to retrieve the target file. Not found the target file with id {}."",
                        id
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
    } else if req.method() == Method::DELETE {
        let id = req.uri().path().trim_start_matches(""/v1/files/"");
        let root = format!(""archives/{}"", id);
        let status = match fs::remove_dir_all(root) {
            Ok(_) => {
                info!(target: ""stdout"", ""Successfully deleted the target file with id {}."", id);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: true,
                }
            }
            Err(e) => {
                let err_msg = format!(""Failed to delete the target file with id {}. {}"", id, e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: false,
                }
            }
        };

        // serialize status
        let s = match serde_json::to_string(&status) {
            Ok(s) => s,
            Err(e) => {
                let err_msg = format!(
                    ""Failed to serialize the status of the file deletion operation. {}"",
                    e
                );

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        // return response
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::from(s));

        match result {
            Ok(response) => response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::OPTIONS {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""files_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    } else {
        let err_msg = ""Invalid HTTP Method."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        error::internal_server_error(err_msg)
    };

    info!(target: ""stdout"", ""Send the files response"");

    res
}

/// Segment the text into chunks and return the chunks response.
pub(crate) async fn chunks_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming chunks request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""chunks_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    let chunks_request: ChunksRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chunks_request) => chunks_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chunks request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the archives directory exists
    let path = Path::new(""archives"");
    if !path.exists() {
        let err_msg = ""The `archives` directory does not exist."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the archive id exists
    let archive_path = path.join(&chunks_request.id);
    if !archive_path.exists() {
        let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the file exists
    let file_path = archive_path.join(&chunks_request.filename);
    if !file_path.exists() {
        let err_msg = format!(
            ""Not found file: {} in archive id: {}"",
            &chunks_request.filename, &chunks_request.id
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // log
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &chunks_request.id, &chunks_request.filename);

    // get the extension of the archived file
    let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
        Some(extension) => extension,
        None => {
            let err_msg = format!(
                ""Failed to get the extension of the archived `{}`."",
                &chunks_request.filename
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // open the file
    let mut file = match File::open(&file_path) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to open `{}`. {}"", &chunks_request.filename, e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // read the file
    let mut contents = String::new();
    if let Err(e) = file.read_to_string(&mut contents) {
        let err_msg = format!(""Failed to read `{}`. {}"", &chunks_request.filename, e);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
    {
        Ok(chunks) => {
            let chunks_response = ChunksResponse {
                id: chunks_request.id,
                filename: chunks_request.filename,
                chunks,
            };

            // serialize embedding object
            match serde_json::to_string(&chunks_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize chunks response. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the chunks response."");

    res
}

/// Return the server info.
pub(crate) async fn server_info_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming server info request."");

    // get the server info
    let server_info = match SERVER_INFO.get() {
        Some(server_info) => server_info,
        None => {
            let err_msg = ""The server info is not set."";

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(""The server info is not set."");
        }
    };

    // serialize server info
    let s = match serde_json::to_string(&server_info) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Fail to serialize server info. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the server info response."");

    res
}

fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}

```","Q: 8. How does the code handle hidden files in the `walkdir` context?
A: The code handles hidden files in the `walkdir` context using a helper function named `is_hidden`. This function takes a `DirEntry` as input and returns a boolean indicating whether the file is hidden.

```rust
fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}
```

This function checks if the filename starts with a dot (`.`). In Unix-like systems, filenames that start with a dot are considered hidden. By using this convention, the code can easily identify and exclude hidden files from its processing.

In the context of `walkdir`, when iterating over directory entries, the code calls `is_hidden` to check each entry before considering it for further processing. If an entry is marked as hidden (i.e., its filename starts with a dot), it is skipped, preventing those files from being included in the results.

Here's an example of how this works:

Let's say you have a directory containing the following files and subdirectories:

```
.
 .hidden_file.txt
 file1.txt
 dir/
     .hidden_subdir/file2.txt
```

When `walkdir` iterates over this directory, it will call `is_hidden` for each entry. The function will return `true` for the `.hidden_file.txt` and `.hidden_subdir/` entries because their filenames start with a dot. As a result, these hidden entries will be skipped by `walkdir`, leaving only the non-hidden entries (`.file1.txt`) to be processed.

In summary, the code uses the `is_hidden` function to efficiently identify and exclude hidden files from its processing in the `walkdir` context, making it easier to manage file system operations while avoiding unnecessary processing of hidden files."
"```rust:llama-api-server/src/backend/ggml.rs
use crate::{error, utils::gen_chat_id, SERVER_INFO};
use endpoints::{
    chat::ChatCompletionRequest,
    completions::CompletionRequest,
    embeddings::EmbeddingRequest,
    files::{DeleteFileStatus, FileObject, ListFilesResponse},
    rag::{ChunksRequest, ChunksResponse},
};
use futures_util::TryStreamExt;
use hyper::{body::to_bytes, Body, Method, Request, Response};
use multipart::server::{Multipart, ReadEntry, ReadEntryResult};
use multipart_2021 as multipart;
use std::{
    fs::{self, File},
    io::{Cursor, Read, Write},
    path::Path,
    time::SystemTime,
};
use walkdir::{DirEntry, WalkDir};

/// List all models available.
pub(crate) async fn models_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming model list request."");

    let list_models_response = match llama_core::models::models().await {
        Ok(list_models_response) => list_models_response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // serialize response
    let s = match serde_json::to_string(&list_models_response) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Failed to serialize the model list result. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = format!(""Failed to get model list. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the model list response."");

    res
}

/// Compute embeddings for the input text and return the embeddings object.
pub(crate) async fn embeddings_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming embeddings request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""embeddings_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut embedding_request: EmbeddingRequest = match serde_json::from_slice(&body_bytes) {
        Ok(embedding_request) => embedding_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize embedding request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if embedding_request.user.is_none() {
        embedding_request.user = Some(gen_chat_id())
    };
    let id = embedding_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::embeddings::embeddings(&embedding_request).await {
        Ok(embedding_response) => {
            // serialize embedding object
            match serde_json::to_string(&embedding_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .header(""user"", id)
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize embedding object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the embeddings response"");

    res
}

/// Process a completion request and returns a completion response with the answer from the model.
pub(crate) async fn completions_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming completions request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""completions_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut completion_request: CompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(completion_request) => completion_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize completions request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    if completion_request.user.is_none() {
        completion_request.user = Some(gen_chat_id())
    };
    let id = completion_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", &id);

    let res = match llama_core::completions::completions(&completion_request).await {
        Ok(completion_object) => {
            // serialize completion object
            let s = match serde_json::to_string(&completion_object) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Fail to serialize completion object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .header(""user"", id)
                .body(Body::from(s));
            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the completions response."");

    res
}

/// Process a chat-completion request and returns a chat-completion response with the answer from the model.
pub(crate) async fn chat_completions_handler(mut req: Request<Body>) -> Response<Body> {
    info!(target: ""stdout"", ""Handling the coming chat completion request."");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    info!(target: ""stdout"", ""Prepare the chat completion request."");

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };
    let mut chat_request: ChatCompletionRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chat_request) => chat_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chat completion request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the user id is provided
    if chat_request.user.is_none() {
        chat_request.user = Some(gen_chat_id())
    };
    let id = chat_request.user.clone().unwrap();

    // log user id
    info!(target: ""stdout"", ""user: {}"", chat_request.user.clone().unwrap());

    let res = match llama_core::chat::chat(&mut chat_request).await {
        Ok(result) => match result {
            either::Left(stream) => {
                let stream = stream.map_err(|e| e.to_string());

                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""text/event-stream"")
                    .header(""Cache-Control"", ""no-cache"")
                    .header(""Connection"", ""keep-alive"")
                    .header(""user"", id)
                    .body(Body::wrap_stream(stream));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""finish chat completions in stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            either::Right(chat_completion_object) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&chat_completion_object) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize chat completion object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .header(""user"", id)
                    .body(Body::from(s));

                match result {
                    Ok(response) => {
                        // log
                        info!(target: ""stdout"", ""Finish chat completions in non-stream mode"");

                        response
                    }
                    Err(e) => {
                        let err_msg =
                            format!(""Failed chat completions in non-stream mode. Reason: {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
        },
        Err(e) => {
            let err_msg = format!(""Failed to get chat completions. Reason: {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    // log
    info!(target: ""stdout"", ""Send the chat completion response."");

    res
}

/// Upload files and return the file object.
pub(crate) async fn files_handler(req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming files request"");

    let res = if req.method() == Method::POST {
        let boundary = ""boundary="";

        let boundary = req.headers().get(""content-type"").and_then(|ct| {
            let ct = ct.to_str().ok()?;
            let idx = ct.find(boundary)?;
            Some(ct[idx + boundary.len()..].to_string())
        });

        let req_body = req.into_body();
        let body_bytes = match to_bytes(req_body).await {
            Ok(body_bytes) => body_bytes,
            Err(e) => {
                let err_msg = format!(""Fail to read buffer from request body. {}"", e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        let cursor = Cursor::new(body_bytes.to_vec());

        let mut multipart = Multipart::with_body(cursor, boundary.unwrap());

        let mut file_object: Option<FileObject> = None;
        while let ReadEntryResult::Entry(mut field) = multipart.read_entry_mut() {
            if &*field.headers.name == ""file"" {
                let filename = match field.headers.filename {
                    Some(filename) => filename,
                    None => {
                        let err_msg =
                            ""Failed to upload the target file. The filename is not provided."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                if !((filename).to_lowercase().ends_with("".txt"")
                    || (filename).to_lowercase().ends_with("".md""))
                    || (filename).to_lowercase().ends_with("".png"")
                {
                    let err_msg = format!(
                        ""Failed to upload the target file. Only files with 'txt' and 'md' extensions are supported. The file extension is {}."",
                        &filename
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }

                let mut buffer = Vec::new();
                let size_in_bytes = match field.data.read_to_end(&mut buffer) {
                    Ok(size_in_bytes) => size_in_bytes,
                    Err(e) => {
                        let err_msg = format!(""Failed to read the target file. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a unique file id
                let id = format!(""file_{}"", uuid::Uuid::new_v4());

                // save the file
                let path = Path::new(""archives"");
                if !path.exists() {
                    fs::create_dir(path).unwrap();
                }
                let file_path = path.join(&id);
                if !file_path.exists() {
                    fs::create_dir(&file_path).unwrap();
                }
                let mut file = match File::create(file_path.join(&filename)) {
                    Ok(file) => file,
                    Err(e) => {
                        let err_msg =
                            format!(""Failed to create archive document {}. {}"", &filename, e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };
                file.write_all(&buffer[..]).unwrap();

                // log
                info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &id, &filename);

                let created_at = match SystemTime::now().duration_since(std::time::UNIX_EPOCH) {
                    Ok(n) => n.as_secs(),
                    Err(_) => {
                        let err_msg = ""Failed to get the current time."";

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // create a file object
                file_object = Some(FileObject {
                    id,
                    bytes: size_in_bytes as u64,
                    created_at,
                    filename,
                    object: ""file"".to_string(),
                    purpose: ""assistants"".to_string(),
                });

                break;
            }
        }

        match file_object {
            Some(fo) => {
                // serialize chat completion object
                let s = match serde_json::to_string(&fo) {
                    Ok(s) => s,
                    Err(e) => {
                        let err_msg = format!(""Failed to serialize file object. {}"", e);

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        return error::internal_server_error(err_msg);
                    }
                };

                // return response
                let result = Response::builder()
                    .header(""Access-Control-Allow-Origin"", ""*"")
                    .header(""Access-Control-Allow-Methods"", ""*"")
                    .header(""Access-Control-Allow-Headers"", ""*"")
                    .header(""Content-Type"", ""application/json"")
                    .body(Body::from(s));

                match result {
                    Ok(response) => response,
                    Err(e) => {
                        let err_msg = e.to_string();

                        // log
                        error!(target: ""stdout"", ""{}"", &err_msg);

                        error::internal_server_error(err_msg)
                    }
                }
            }
            None => {
                let err_msg = ""Failed to upload the target file. Not found the target file."";

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::GET {
        let uri_path = req.uri().path();

        if uri_path == ""/v1/files"" {
            let mut file_objects: Vec<FileObject> = Vec::new();
            for entry in WalkDir::new(""archives"").into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let id = entry
                        .path()
                        .parent()
                        .and_then(|p| p.file_name())
                        .unwrap()
                        .to_str()
                        .unwrap()
                        .to_string();

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    let fo = FileObject {
                        id,
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    };

                    file_objects.push(fo);
                }
            }

            info!(target: ""stdout"", ""Found {} archive files"", file_objects.len());

            let file_objects = ListFilesResponse {
                object: ""list"".to_string(),
                data: file_objects,
            };

            // serialize chat completion object
            let s = match serde_json::to_string(&file_objects) {
                Ok(s) => s,
                Err(e) => {
                    let err_msg = format!(""Failed to serialize file object. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    return error::internal_server_error(err_msg);
                }
            };

            // return response
            let result = Response::builder()
                .header(""Access-Control-Allow-Origin"", ""*"")
                .header(""Access-Control-Allow-Methods"", ""*"")
                .header(""Access-Control-Allow-Headers"", ""*"")
                .header(""Content-Type"", ""application/json"")
                .body(Body::from(s));

            match result {
                Ok(response) => response,
                Err(e) => {
                    let err_msg = e.to_string();

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        } else {
            let id = uri_path.trim_start_matches(""/v1/files/"");
            let root = format!(""archives/{}"", id);
            let mut file_object: Option<FileObject> = None;
            for entry in WalkDir::new(root).into_iter().filter_map(|e| e.ok()) {
                if !is_hidden(&entry) && entry.path().is_file() {
                    info!(target: ""stdout"", ""archive file: {}"", entry.path().display());

                    let filename = entry
                        .path()
                        .file_name()
                        .and_then(|n| n.to_str())
                        .unwrap()
                        .to_string();

                    let metadata = entry.path().metadata().unwrap();

                    let created_at = metadata
                        .created()
                        .unwrap()
                        .duration_since(std::time::UNIX_EPOCH)
                        .unwrap()
                        .as_secs();

                    let bytes = metadata.len();

                    file_object = Some(FileObject {
                        id: id.into(),
                        bytes,
                        created_at,
                        filename,
                        object: ""file"".to_string(),
                        purpose: ""assistants"".to_string(),
                    });

                    break;
                }
            }

            match file_object {
                Some(fo) => {
                    // serialize chat completion object
                    let s = match serde_json::to_string(&fo) {
                        Ok(s) => s,
                        Err(e) => {
                            let err_msg = format!(""Failed to serialize file object. {}"", e);

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            return error::internal_server_error(err_msg);
                        }
                    };

                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));

                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                None => {
                    let err_msg = format!(
                        ""Failed to retrieve the target file. Not found the target file with id {}."",
                        id
                    );

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
    } else if req.method() == Method::DELETE {
        let id = req.uri().path().trim_start_matches(""/v1/files/"");
        let root = format!(""archives/{}"", id);
        let status = match fs::remove_dir_all(root) {
            Ok(_) => {
                info!(target: ""stdout"", ""Successfully deleted the target file with id {}."", id);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: true,
                }
            }
            Err(e) => {
                let err_msg = format!(""Failed to delete the target file with id {}. {}"", id, e);

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                DeleteFileStatus {
                    id: id.into(),
                    object: ""file"".to_string(),
                    deleted: false,
                }
            }
        };

        // serialize status
        let s = match serde_json::to_string(&status) {
            Ok(s) => s,
            Err(e) => {
                let err_msg = format!(
                    ""Failed to serialize the status of the file deletion operation. {}"",
                    e
                );

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        };

        // return response
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::from(s));

        match result {
            Ok(response) => response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""stdout"", ""{}"", &err_msg);

                error::internal_server_error(err_msg)
            }
        }
    } else if req.method() == Method::OPTIONS {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""files_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    } else {
        let err_msg = ""Invalid HTTP Method."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        error::internal_server_error(err_msg)
    };

    info!(target: ""stdout"", ""Send the files response"");

    res
}

/// Segment the text into chunks and return the chunks response.
pub(crate) async fn chunks_handler(mut req: Request<Body>) -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming chunks request"");

    if req.method().eq(&hyper::http::Method::OPTIONS) {
        let result = Response::builder()
            .header(""Access-Control-Allow-Origin"", ""*"")
            .header(""Access-Control-Allow-Methods"", ""*"")
            .header(""Access-Control-Allow-Headers"", ""*"")
            .header(""Content-Type"", ""application/json"")
            .body(Body::empty());

        match result {
            Ok(response) => return response,
            Err(e) => {
                let err_msg = e.to_string();

                // log
                error!(target: ""chunks_handler"", ""{}"", &err_msg);

                return error::internal_server_error(err_msg);
            }
        }
    }

    // parse request
    let body_bytes = match to_bytes(req.body_mut()).await {
        Ok(body_bytes) => body_bytes,
        Err(e) => {
            let err_msg = format!(""Fail to read buffer from request body. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    let chunks_request: ChunksRequest = match serde_json::from_slice(&body_bytes) {
        Ok(chunks_request) => chunks_request,
        Err(e) => {
            let mut err_msg = format!(""Fail to deserialize chunks request: {}."", e);

            if let Ok(json_value) = serde_json::from_slice::<serde_json::Value>(&body_bytes) {
                err_msg = format!(""{}\njson_value: {}"", err_msg, json_value);
            }

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::bad_request(err_msg);
        }
    };

    // check if the archives directory exists
    let path = Path::new(""archives"");
    if !path.exists() {
        let err_msg = ""The `archives` directory does not exist."";

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the archive id exists
    let archive_path = path.join(&chunks_request.id);
    if !archive_path.exists() {
        let err_msg = format!(""Not found archive id: {}"", &chunks_request.id);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // check if the file exists
    let file_path = archive_path.join(&chunks_request.filename);
    if !file_path.exists() {
        let err_msg = format!(
            ""Not found file: {} in archive id: {}"",
            &chunks_request.filename, &chunks_request.id
        );

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    // log
    info!(target: ""stdout"", ""file_id: {}, file_name: {}"", &chunks_request.id, &chunks_request.filename);

    // get the extension of the archived file
    let extension = match file_path.extension().and_then(std::ffi::OsStr::to_str) {
        Some(extension) => extension,
        None => {
            let err_msg = format!(
                ""Failed to get the extension of the archived `{}`."",
                &chunks_request.filename
            );

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // open the file
    let mut file = match File::open(&file_path) {
        Ok(file) => file,
        Err(e) => {
            let err_msg = format!(""Failed to open `{}`. {}"", &chunks_request.filename, e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // read the file
    let mut contents = String::new();
    if let Err(e) = file.read_to_string(&mut contents) {
        let err_msg = format!(""Failed to read `{}`. {}"", &chunks_request.filename, e);

        // log
        error!(target: ""stdout"", ""{}"", &err_msg);

        return error::internal_server_error(err_msg);
    }

    let res = match llama_core::rag::chunk_text(&contents, extension, chunks_request.chunk_capacity)
    {
        Ok(chunks) => {
            let chunks_response = ChunksResponse {
                id: chunks_request.id,
                filename: chunks_request.filename,
                chunks,
            };

            // serialize embedding object
            match serde_json::to_string(&chunks_response) {
                Ok(s) => {
                    // return response
                    let result = Response::builder()
                        .header(""Access-Control-Allow-Origin"", ""*"")
                        .header(""Access-Control-Allow-Methods"", ""*"")
                        .header(""Access-Control-Allow-Headers"", ""*"")
                        .header(""Content-Type"", ""application/json"")
                        .body(Body::from(s));
                    match result {
                        Ok(response) => response,
                        Err(e) => {
                            let err_msg = e.to_string();

                            // log
                            error!(target: ""stdout"", ""{}"", &err_msg);

                            error::internal_server_error(err_msg)
                        }
                    }
                }
                Err(e) => {
                    let err_msg = format!(""Fail to serialize chunks response. {}"", e);

                    // log
                    error!(target: ""stdout"", ""{}"", &err_msg);

                    error::internal_server_error(err_msg)
                }
            }
        }
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the chunks response."");

    res
}

/// Return the server info.
pub(crate) async fn server_info_handler() -> Response<Body> {
    // log
    info!(target: ""stdout"", ""Handling the coming server info request."");

    // get the server info
    let server_info = match SERVER_INFO.get() {
        Some(server_info) => server_info,
        None => {
            let err_msg = ""The server info is not set."";

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(""The server info is not set."");
        }
    };

    // serialize server info
    let s = match serde_json::to_string(&server_info) {
        Ok(s) => s,
        Err(e) => {
            let err_msg = format!(""Fail to serialize server info. {}"", e);

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            return error::internal_server_error(err_msg);
        }
    };

    // return response
    let result = Response::builder()
        .header(""Access-Control-Allow-Origin"", ""*"")
        .header(""Access-Control-Allow-Methods"", ""*"")
        .header(""Access-Control-Allow-Headers"", ""*"")
        .header(""Content-Type"", ""application/json"")
        .body(Body::from(s));
    let res = match result {
        Ok(response) => response,
        Err(e) => {
            let err_msg = e.to_string();

            // log
            error!(target: ""stdout"", ""{}"", &err_msg);

            error::internal_server_error(err_msg)
        }
    };

    info!(target: ""stdout"", ""Send the server info response."");

    res
}

fn is_hidden(entry: &DirEntry) -> bool {
    entry
        .file_name()
        .to_str()
        .map(|s| s.starts_with("".""))
        .unwrap_or(false)
}

```","Q: Please answer these questions based on your understanding of the provided Rust code snippet.
A: I'm ready to answer your questions about the Rust code snippet. Go ahead and ask away! 

(Also, please let me know if you'd like me to clarify any part of my answers)"
