path,content
Aff-wild-exps/config.py,"import argparse
parser = argparse.ArgumentParser(description=""PyTorch implementation of two stream emotion network"")
parser.add_argument('--label_name', type=str, choices = ['arousal', 'valence', 'arousal_valence'])
parser.add_argument('--store_name', type=str, default="""")
parser.add_argument('--save_root', type=str, default="""")
# ========================= Model Configs ==========================
parser.add_argument('--cnn', type=str, default = 'resnet50', choices = ['resnet50'])
parser.add_argument('--root_path', type=str, default ='/media/newssd/Aff-Wild_experiments/Aligned_Faces_train')
parser.add_argument('--annot_dir', type=str, default ='/media/newssd/Aff-Wild_experiments/annotations')
parser.add_argument('--loss_type', type=str, default=""ccc"",
                    choices=['mse', 'ccc'])
parser.add_argument('--hidden_units', default=[2048, 256, 256], type=int, nargs=""+"",
                    help='hidden units set up')
parser.add_argument('--length', type = int, default = 64)
parser.add_argument('-L', type=int, default=12, help='the number of input phase difference images')
# ========================= Learning Configs ==========================
parser.add_argument('--epochs', default= 25, type=int, metavar='N',
                    help='number of total epochs to run')
parser.add_argument('--early_stop', type=int, default=5) # if validation loss didn't improve over 5 epochs, stop
parser.add_argument('-b', '--batch_size', default= 16, type=int,
                    metavar='N', help='mini-batch size (default: 16)')
parser.add_argument('--lr', default=1e-3, type=float)
parser.add_argument('--lr_steps', default=[ 10, 20], type=float, nargs=""+"",
                    metavar='LRSteps', help='epochs to decay learning rate by 10')
parser.add_argument('--momentum', default=0.9, type=float, metavar='M',
                    help='momentum')
parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float,
                    metavar='W', help='weight decay (default: 5e-4)')
parser.add_argument('--clip-gradient', '--gd', default=20, type=float,
                    metavar='W', help='gradient norm clipping (default: 20)')

# ========================= Monitor Configs ==========================
parser.add_argument('--print-freq', '-p', default=250, type=int,
                    metavar='N', help='print frequency (default: 50) iteration')
parser.add_argument('--eval-freq', '-ef', default=1, type=int,
                    metavar='N', help='evaluation frequency (default: 50) epochs')
# ========================= Runtime Configs ==========================
parser.add_argument('-j', '--workers', default=0, type=int, metavar='N',
                    help='number of data loading workers (default: 4)')
parser.add_argument('--resume', default='', type=str, metavar='PATH',
                    help='path to latest checkpoint (default: none)')
parser.add_argument('-t', '--test', dest='test', action='store_true',
                    help='evaluate model on test set')
parser.add_argument('--snapshot_pref', type=str, default="""")
parser.add_argument('--start-epoch', default=0, type=int, metavar='N',
                    help='manual epoch number (useful on restarts)')
parser.add_argument('--gpus', nargs='+', type=int, default=None)
parser.add_argument('--root_log',type=str, default='log')
parser.add_argument('--root_model', type=str, default='model')
parser.add_argument('--root_output',type=str, default='output')
parser.add_argument('--root_tensorboard', type=str, default='runs')
"
Aff-wild-exps/dataloader.py,"import torch.utils.data as data
from PIL import Image
import os
import os.path
import numpy as np
import glob
from torch import nn as nn
from tqdm import tqdm
import torch
import torchvision
from transforms import *
from utils import Steerable_Pyramid_Phase, get_device
class VideoRecord(object):
    def __init__(self, video, feature_dir, annot_dir, label_name, test_mode = False):
        self.video = video
        self.feature_dir = feature_dir
        self.annot_dir = annot_dir
        self.label_name = label_name
        self.test_mode = test_mode
        self.path_label = self.get_path_label()
     
    def get_path_label(self):
        frames = glob.glob(os.path.join(self.feature_dir, self.video, '*.npy'))
        frames = sorted(frames, key  = lambda x: os.path.basename(x).split(""."")[0])
        if len(frames)==0:
            raise ValueError(""number of frames of video {} should not be zero."".format(self.video))
        if '_' in self.label_name:
            self.label_name = self.label_name.split(""_"")
        else:
            self.label_name = [self.label_name]
        annot_file = [os.path.join(self.annot_dir, 'train', ln, self.video+"".txt"") for ln in self.label_name]
        if (not self.test_mode) and (any([not os.path.exists(file) for file in annot_file])):
            raise ValueError(""Annotation file not found: the training mode should always has annotation file!"")
        if self.test_mode:
            return [frames, np.array([[-100] * len(self.label_name)]*len(frames))]
        else:
            total_labels = []
            for file in annot_file:
                f = open(file, ""r"")
                corr_frames, labels = [], []
                for i, x in enumerate(f):
                    label = float(x)
                    corr_frame = os.path.join(self.feature_dir, self.video,  '{0:05d}.npy'.format(i+1))
                    if os.path.exists(corr_frame):
                        corr_frames.append(corr_frame)
                        labels.append(label)
                    else:
                        # skip those frames and labels
                        continue
                f.close()
                total_labels.append(labels)
            assert len(corr_frames) == len(labels)
            total_labels = np.asarray(total_labels)
            total_labels = total_labels.transpose(1, 0)
            return [corr_frames, total_labels]
    def __str__(self):
        string = ''
        for key, record in self.utterance_dict.items():
            string += str(record)+'\n'
        return string
def phase_2_output( phase_batch, steerable_pyramid,return_phase=False):
    """"""
    phase_batch dim: bs, num_phase, W, H
    """"""
    sp = steerable_pyramid
    num_frames,num_phases, W, H = phase_batch.size()
    coeff_batch = sp.build_pyramid(phase_batch)
    assert isinstance(coeff_batch, list)
    phase_batch_0 = sp.extract_phase(coeff_batch[0], return_phase=return_phase)
    num_frames, n_ch, n_ph, W, H= phase_batch_0.size()
    phase_batch_0 = phase_batch_0.view(num_frames, -1, W, H)
    phase_batch_1 = sp.extract_phase(coeff_batch[1], return_phase=return_phase)
    num_frames, n_ch, n_ph, W, H= phase_batch_1.size()
    phase_batch_1 = phase_batch_1.view(num_frames, -1, W, H)
    return phase_batch_0,phase_batch_1
class Face_Dataset(data.Dataset):
    def __init__(self, root_path, feature_path, annot_dir, video_name_list,  label_name, py_level=4, py_nbands=2, 
                 test_mode =False, num_phase=12, phase_size = 48, length=64, stride=32, return_phase=False):
        self.root_path = root_path
        self.feature_path = feature_path
        self.annot_dir = annot_dir
        self.video_name_list = video_name_list
        self.label_name = label_name
        self.test_mode = test_mode
        self.length = length # length of sequence as input to the RNN
        self.stride = stride # 
        self.num_phase = num_phase
        self.phase_size = phase_size
        self.return_phase = return_phase
        device = get_device('cuda:0')
        self.steerable_pyramid = Steerable_Pyramid_Phase(height=py_level, nbands=py_nbands, scale_factor=2, device=device, extract_level=[1,2], visualize=False)
        print(""sample stride {} is only applicable when test_mode=False."".format(stride))
        self.parse_videos()

    def parse_videos(self):
        videos = self.video_name_list      
        self.video_list = list()
        self.sequence_ranges = []
        self.video_ids = []
        self.total_labels = []
        vid_ids =0
        for vid in tqdm(videos):
            v_record = VideoRecord(vid, self.feature_path, self.annot_dir, self.label_name, self.test_mode)
            frames, labels = v_record.path_label
            self.total_labels.append(labels)
            if len(frames) !=0 and (len(frames)==len(labels)):
                self.video_list.append(v_record)
                if self.test_mode:
                    n_seq = len(frames)//self.length
                    if len(frames)%self.length !=0:
                        n_seq +=1
                    seq_range = []
                    for i in range(n_seq):
                        if (i+1)*self.length<=len(frames):
                            seq_range.append([i*self.length, (i+1)*self.length] )
                        else:
                            seq_range.append([len(frames)-self.length, len(frames)])
                    self.sequence_ranges.extend(seq_range)
                    self.video_ids.extend([vid_ids]*n_seq)
                    vid_ids +=1
                else:
                    n_seq = 0
                    start, end = 0, self.length
                    seq_range = []
                    while end < len(frames) and (start<len(frames)):
                        seq_range.append([start, end])
                        n_seq +=1
                        start +=self.stride
                        end = start+self.length
                    self.sequence_ranges.extend(seq_range)
                    self.video_ids.extend([vid_ids]*n_seq) 
                    vid_ids +=1
        self.total_labels = np.concatenate(self.total_labels, axis=0)                
        print(""number of videos:{}, number of seqs:{}"".format(len(self.video_list), len(self)))

    def __len__(self):
        return len(self.sequence_ranges)
    def __getitem__(self, index):
        seq_ranges = self.sequence_ranges[index]
        start, end = seq_ranges
        video_record = self.video_list[self.video_ids[index]]
        frames, labels = video_record.path_label
        seq_frames, seq_labels = frames[start:end], labels[start:end]
        imgs = []
        for f in seq_frames:
            imgs.append(np.load(f))
        # phase image sample
        sample_f_ids = []
        for f_id in range(start, end):
            phase_ids = []
            for i in range(self.num_phase+1):
                step = i-self.num_phase//2
                id_0 = max(0,f_id + step)
                id_0 = min(id_0, len(frames)-1) 
                phase_ids.append(id_0)
            sample_f_ids.append(phase_ids)
        sample_frames = [[frames[id] for id in ids] for ids in sample_f_ids]
        phase_images= []
        for frames in sample_frames:
            phase_img_list = []
            for frame in frames:
                f_index = int(os.path.basename(frame).split(""."")[0])
                img_frame = os.path.join(self.root_path, video_record.video, video_record.video+""_aligned"", 'frame_det_00_{:06d}.bmp'.format(f_index))
                try:
                   img = Image.open(img_frame).convert('L')
                except:
                    raise ValueError(""incorrect face path"")    
                phase_img_list.append(img)
            phase_images.append(phase_img_list)
        if not self.test_mode:
            random_seed = np.random.randint(250)
            phase_transform = torchvision.transforms.Compose([GroupRandomHorizontalFlip(seed=random_seed),
                                   GroupRandomCrop(size=int(self.phase_size*0.85), seed=random_seed),
                                   GroupScale(size=self.phase_size),
                                   Stack(),
                                   ToTorchFormatTensor()])
        else:
            phase_transform = torchvision.transforms.Compose([
                                   GroupScale(size=self.phase_size),
                                   Stack(),
                                   ToTorchFormatTensor()]) 
        flat_phase_images = []
        for sublist in phase_images:
            flat_phase_images.extend(sublist)
        flat_phase_images = phase_transform(flat_phase_images)
        phase_images = flat_phase_images.view(len(phase_images), self.num_phase+1, self.phase_size, self.phase_size)
        phase_images = phase_images.type('torch.FloatTensor').cuda()
        phase_batch_0,phase_batch_1 = phase_2_output( phase_images, self.steerable_pyramid, return_phase=self.return_phase)
        
        return [[phase_batch_0,phase_batch_1], np.array(imgs), np.array(seq_labels), np.array([start, end]), video_record.video]
    

if __name__ == '__main__':
    root_path = '/media/newssd/Aff-Wild_experiments/Aligned_Faces_train'
    feature_path = '/media/newssd/Aff-Wild_experiments/Extracted_Features/Aff_wild_train/resnet50_ferplus_features_fps=30_pool5_7x7_s1'
    annot_dir = '/media/newssd/Aff-Wild_experiments/annotations'
    video_names = os.listdir(feature_path)[:25]

    train_dataset = Face_Dataset(root_path, feature_path, annot_dir, video_names, label_name='arousal_valence',  num_phase=12 , phase_size=48, test_mode=True)
    train_loader = torch.utils.data.DataLoader(
        train_dataset, 
        batch_size = 4, 
        num_workers=0, pin_memory=False )
    for phase_f, rgb_f, label, seq_range, video_names in train_loader:
        phase_0, phase_1 = phase_f
        "
Aff-wild-exps/main.py,"#!/usr/bin/env python3
# -*- coding: utf-8 -*-
""""""
Created on Mon Jul  8 19:37:28 2019
main
@author: ddeng
""""""

import os
import time
import shutil
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import pickle
import torch.optim
from torch.nn.utils import clip_grad_norm_
import numpy as np
from config import parser
args = parser.parse_args()
from dataloader import Face_Dataset, VideoRecord
from sklearn.metrics import mean_squared_error
from torch.autograd import Variable as Variable
from network import Two_Stream_RNN
import pandas as pd
from tqdm import tqdm
class My_loss(torch.nn.Module): 
    def __init__(self):
        super().__init__()   
    def forward(self, x, y): 
        cccs = 0
        for i in range(x.size(-1)):
            x_i = x[::, i]
            y_i = y[::, i]
            if len(x_i.size())==2 or len(y_i.size())==2:
                x_i = x_i.contiguous()
                y_i = y_i.contiguous()
                x_i = x_i.view(-1)
                y_i = y_i.view(-1)
            vx = x_i - torch.mean(x_i)
            vy = y_i - torch.mean(y_i)
            rho =  torch.sum(vx * vy) / (torch.sqrt(torch.sum(torch.pow(vx, 2))) * torch.sqrt(torch.sum(torch.pow(vy, 2))))
            x_m = torch.mean(x_i)
            y_m = torch.mean(y_i)
            x_s = torch.std(x_i)
            y_s = torch.std(y_i)
            ccc = 2*rho*x_s*y_s/(torch.pow(x_s, 2) + torch.pow(y_s, 2) + torch.pow(x_m - y_m, 2))
            cccs+=ccc
        return -cccs
class Squeeze(nn.Module):
    def __init__(self):
        super(Squeeze, self).__init__()
    def forward(self, x):
        bs = x.size(0)
        if bs==1:
            x = x.squeeze()
            x = x.unsqueeze(0)
        else:
            x = x.squeeze()
        return x
class AverageMeter(object):
    """"""Computes and stores the average and current value""""""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def ccc(y_true, y_pred):
    true_mean = np.mean(y_true)
    pred_mean = np.mean(y_pred)
    v_pred = y_pred - pred_mean
    v_true = y_true - true_mean
    
    rho =  np.sum(v_pred*v_true) / (np.sqrt(np.sum(v_pred**2)) * np.sqrt(np.sum(v_true**2)))
    std_predictions = np.std(y_pred)
    std_gt = np.std(y_true)
    
    ccc = 2 * rho * std_gt * std_predictions / (
        std_predictions ** 2 + std_gt ** 2 +
        (pred_mean - true_mean) ** 2)
    return ccc, rho 

def correct(pred_val_y, train_mean, train_std):
    try:
        val_std = np.std(pred_val_y)
        mean = np.mean(pred_val_y)
        pred_val_y = np.array(pred_val_y)
    except:
        val_std = torch.std(pred_val_y)
        mean = torch.mean(pred_val_y)
    pred_val_y = mean + (pred_val_y - mean) * train_std / val_std
    return pred_val_y
def check_rootfolders():
    """"""Create log and model folder""""""
    folders_util = [args.root_log, args.root_model, args.root_output, args.root_tensorboard]
    folders_util = [""%s/""%(args.save_root) +folder for folder in folders_util]
    for folder in folders_util:
        if not os.path.exists(folder):
            print('creating folder ' + folder)
            os.makedirs(folder)
def train(dataloader, model, criterion, optimizer, epoch, log): 
    
    batch_time = AverageMeter()
    data_time = AverageMeter()
    losses = AverageMeter()
    end = time.time()
    optimizer.zero_grad()
    model.train()
    for i, data_batch in enumerate(dataloader):
        data_time.update(time.time() - end)
        phase_f, rgb_f, label, ranges, videos = data_batch
        phase_0, phase_1 = phase_f
        rgb_f = Variable(rgb_f.type('torch.FloatTensor').cuda())
        phase_0 = Variable(phase_0.type('torch.FloatTensor').cuda())
        phase_1 = Variable(phase_1.type('torch.FloatTensor').cuda())
        label_var = Variable(label.type('torch.FloatTensor').cuda())
        out = model([phase_0,phase_1], rgb_f)
        loss= criterion(out, label_var)
        loss.backward()
        optimizer.step() # We have accumulated enought gradients
        optimizer.zero_grad()
        # measure elapsed time
        batch_time.update(time.time() - end)
        losses.update(loss.item(), rgb_f.size(0))
        end = time.time()

        if i % args.print_freq == 0:
            output = ('Epoch: [{0}][{1}/{2}], lr: {lr:.6f}\t'
                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                    'Data {data_time.val:.3f} ({data_time.avg:.3f})\t'
                    'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                    .format( epoch, i, len(dataloader), batch_time=batch_time,
                        data_time=data_time, loss=losses, lr=optimizer.param_groups[-1]['lr']))
            print(output)
            log.write(output + '\n')
            log.flush()

def validate(dataloader, model, criterion, iter, log, train_mean=None, train_std=None): 
    batch_time = AverageMeter()
    losses = AverageMeter()
    # switch to evaluate mode
    model.eval()
#    df = pd.DataFrame(columns = ['video','utterance',str(args.label_name)+'_target', str(args.label_name)+'_prediction'])
    end = time.time()
    targets, preds = [], []
    for i, data_batch in enumerate(dataloader):
        phase_f, rgb_f, label, ranges, videos = data_batch
        with torch.no_grad():
            phase_0, phase_1 = phase_f
            rgb_f = Variable(rgb_f.type('torch.FloatTensor').cuda())
            phase_0 = Variable(phase_0.type('torch.FloatTensor').cuda())
            phase_1 = Variable(phase_1.type('torch.FloatTensor').cuda())
            label_var = Variable(label.type('torch.FloatTensor').cuda())
        output = model([phase_0,phase_1], rgb_f)
        targets.append(label_var.data.cpu().numpy() )
        preds.append(output.data.cpu().numpy())
        loss = criterion(output, label_var)  
        losses.update(loss.item(), rgb_f.size(0))
        batch_time.update(time.time() - end)
        end = time.time()
        if i % args.print_freq == 0:
            output = ('Test: [{0}/{1}]\t'
                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                  .format(
                   i, len(dataloader), batch_time=batch_time, loss=losses))
            print(output)
            log.write(output + '\n')
            log.flush()
        torch.cuda.empty_cache()
    targets, preds = np.concatenate([array for array in targets], axis=0), np.concatenate([array for array in preds], axis=0)
    targets = targets.reshape(-1, targets.shape[-1])
    preds =  preds.reshape(-1, targets.shape[-1])
    mse_func =  mean_squared_error
    ccc_score = [ccc(targets[:, i], preds[:, i])[0] for i in range(targets.shape[-1])]
    mse_loss = [mse_func(targets[:, i], preds[:, i]) for i in range(targets.shape[-1])]
    ccc_corr =  [ccc(targets[:, i], correct(preds[:, i], train_mean[i], train_std[i]))[0]  for i in range(targets.shape[-1])]
    mse_corr = [mse_func(targets[:, i], correct(preds[:, i], train_mean[i], train_std[i])) for i in range(targets.shape[-1])]
    labels = args.label_name.split(""_"")
    list0 = ['ccc_{}: {:.4f}({:.4f}),'.format(labels[i], ccc_score[i], ccc_corr[i]) for i in range(targets.shape[-1])]
    list1 = ['mse_{}: {:.4f}({:.4f}),'.format(labels[i], mse_loss[i], mse_corr[i]) for i in range(targets.shape[-1])]
    output = ' '.join(['Validation : [{0}][{1}],'.format( i, len(dataloader)),
                        *list0, 
                        *list1])  
    ccc_score = ccc_corr
    print(output)
    log.write(output + '\n')
    log.flush() 
    return np.mean(mse_corr), np.mean(ccc_corr)
def test(dataloader, model, train_mean, train_std):
    model.eval()
    sample_names = []
    sample_preds = []
    sample_ranges = []
    for i, data_batch in tqdm(enumerate(dataloader)):
        phase_f, rgb_f, label, ranges, names = data_batch
        with torch.no_grad():
            phase_0, phase_1 = phase_f
            rgb_f = Variable(rgb_f.type('torch.FloatTensor').cuda())
            phase_0 = Variable(phase_0.type('torch.FloatTensor').cuda())
            phase_1 = Variable(phase_1.type('torch.FloatTensor').cuda())
        output = model([phase_0,phase_1], rgb_f)
        sample_names.append(names)
        sample_ranges.append(ranges)
        sample_preds.append(output.cpu().data.numpy())
    sample_names = np.concatenate([arr for arr in sample_names], axis=0)
    sample_preds = np.concatenate([arr for arr in sample_preds], axis=0)
    n_sample, n_length, n_labels = sample_preds.shape
    # scale 
    trans_sample_preds = sample_preds.reshape(-1, n_labels)
    trans_sample_preds = np.array([correct(trans_sample_preds[:, i], train_mean[i], train_std[i]) for i  in range(n_labels)])
    sample_preds = trans_sample_preds.reshape(n_sample, n_length, n_labels)
    sample_ranges = np.concatenate([arr for arr in sample_ranges], axis=0)
    video_dict = {}
    for video in sample_names:
        mask = sample_names==video
        video_ranges = sample_ranges[mask]
        if video not in video_dict.keys():
            max_len = max([ranges[-1] for ranges in video_ranges])
            video_dict[video] = np.zeros((max_len, n_labels))
        video_preds = sample_preds[mask]
        # make sure the dataset returns full range of video frames
        min_f, max_f = 0, 0
        for rg, pred in zip(video_ranges, video_preds):
            start, end = rg
            video_dict[video][start:end, :] = pred
            min_f = min(min_f, start)
            max_f = max(max_f, end)
        assert (min_f==0) and (max_f == max_len)
    return video_dict        
def main():
    if args.cnn == 'vgg':
        feature_path = '/media/newssd/Aff-Wild_experiments/Extracted_Features/Aff_wild_train/vgg_vd_face_fer_dag_features_fps=30_fc7'
    elif args.cnn =='resnet50':
        feature_path = '/media/newssd/Aff-Wild_experiments/Extracted_Features/Aff_wild_train/resnet50_ferplus_features_fps=30_pool5_7x7_s1'
    label_name = args.label_name
    if len(args.store_name)==0:
        args.store_name = '_'.join( [label_name,
                                     'loss_type:{}'.format(args.loss_type),
                                      'batch_size:{}'.format(args.batch_size), 
                                      'length:{}'.format(args.length),
                                      'cnn:{}'.format(args.cnn),
                                      'mlp:{}'.format(args.hidden_units),
                                      'L:{}'.format(args.L)]) 
    setattr(args, 'save_root', args.store_name)
    print(""save experiment to :{}"".format(args.save_root))
    check_rootfolders()
    num_class = 1 if not ""_"" in args.label_name else 2
    setattr(args, 'num_class', num_class)
    if args.loss_type == 'mse':
        criterion = nn.MSELoss().cuda()
    elif args.loss_type=='ccc':
        criterion = My_loss().cuda()
    else: # another loss is mse or mae
        raise ValueError(""Unknown loss type"")           
    video_names = os.listdir(feature_path)
    # predict on test set. use the five fold models, and their average prediction as final results
    if args.test:
        test_lost_frames = '/media/newssd/Aff-Wild_experiments/test_set_lost_frames.pkl'
        test_lost_frames = pickle.load(open(test_lost_frames, 'rb'))
        if args.cnn =='resnet50':
            feature_path_test = '/media/newssd/Aff-Wild_experiments/Extracted_Features/Aff_wild_test/resnet50_ferplus_features_fps=30_pool5_7x7_s1'
        test_video_names = os.listdir(feature_path_test)
        test_root_path = '/media/newssd/Aff-Wild_experiments/Aligned_Faces_test'
        test_dataset = Face_Dataset(test_root_path, feature_path_test, '', test_video_names, label_name, test_mode=True, length=args.length, stride  = args.length, num_phase=args.L)
        test_loader = torch.utils.data.DataLoader(
                       test_dataset,
                       batch_size=args.batch_size//2,
                       num_workers=args.workers, pin_memory=False)
        for i in range(5):
            length = len(video_names)//5
            # five fold cross validation
            val_video_names = video_names[i*length:(i+1)*length]
            if i==4:
                val_video_names = video_names[i*length:]
            train_video_names = [name for name in video_names if name not in val_video_names]
            train_dataset = Face_Dataset(args.root_path, feature_path,  args.annot_dir, train_video_names, label_name, test_mode=False, length=args.length, stride=args.length)
            train_labels = train_dataset.total_labels
            train_mean, train_std = np.mean(train_labels, axis=0), np.std(train_labels, axis=0)
            model_path = os.path.join(args.save_root, args.root_model, 'fold_{}_best_ccc.pth.tar'.format(i))
            assert os.path.exists(model_path)
            model = Two_Stream_RNN(args.hidden_units, fold_id=i, num_phase=args.L)
            checkpoint = torch.load(model_path)
            model.load_state_dict(checkpoint['state_dict'])
            start_epoch = checkpoint['epoch']
            print(""load checkpoint from {}, epoch:{}"".format(model_path, start_epoch))
            model.cuda()
            video_dict = test(test_loader, model, train_mean, train_std)
            for video_name in video_dict:
                save_dir = os.path.join(args.save_root, args.root_log, 'fold_{}'.format(i))
                if not os.path.exists(save_dir):
                    os.makedirs(save_dir)
                save_path = os.path.join(save_dir, '{}_{}.csv'.format(label_name, video_name))
                predictions = video_dict[video_name]
                test_frames = test_lost_frames[video_name]
                if test_frames['length'] == len(predictions):
                    # that means the prediction has the same length as original frames
                    pass
                elif test_frames['length'] > len(predictions):
                    # some frames are lost
                    success_frames = test_frames['success']
                    assert len(success_frames) == len(predictions)
                    new_predictions = np.ones(test_frames['length'])*(-100)
                    prev_pred = 0.
                    id = 0
                    for j in range(test_frames['length']):
                        if j+1 not in success_frames:
                            new_predictions[j] = prev_pred
                        else:
                            new_predictions[j] = predictions[id]
                            prev_pred = predictions[id]
                            id+=1 
                    predictions = new_predictions
                    assert id == len(success_frames)
                elif test_frames['length'] < len(predictions):
                    raise ValueError(""prediction length incorrect!"")
                assert predictions.shape[0] == test_frames['length']
                df = pd.DataFrame(predictions)
                df.to_csv(save_path, index=False, header=None)
        return
                
    for i in range(5):
    
        ###########################  Modify the classifier ###################       
        model = Two_Stream_RNN(args.hidden_units, fold_id=i, label_name=args.label_name, num_phase=args.L)
        ###########################  Modify the classifier ###################     
        pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(""Total Params: {}"".format(pytorch_total_params))
        model.cuda()
        optimizer = torch.optim.SGD(model.parameters(), 
                                args.lr,
                                momentum=args.momentum,
                                weight_decay=args.weight_decay)  
        
        length = len(video_names)//5
        # five fold cross validation
        val_video_names = video_names[i*length:(i+1)*length]
        if i==4:
            val_video_names = video_names[i*length:]
        train_video_names = [name for name in video_names if name not in val_video_names]
        train_dataset = Face_Dataset(args.root_path, feature_path, args.annot_dir, train_video_names, label_name, test_mode=False, length=args.length, stride=args.length//2, num_phase=args.L)
        train_labels = train_dataset.total_labels
        train_mean, train_std = np.mean(train_labels, axis=0), np.std(train_labels, axis=0)
        val_dataset = Face_Dataset(args.root_path, feature_path, args.annot_dir, val_video_names, label_name, test_mode=False, length=args.length, stride=args.length//2, num_phase=args.L)
        train_loader =  torch.utils.data.DataLoader(
                        train_dataset, shuffle=True, 
                        batch_size = args.batch_size, drop_last=True,
                        num_workers=args.workers, pin_memory=False )
        val_loader = torch.utils.data.DataLoader(
                       val_dataset,
                       batch_size=args.batch_size//2,
                       num_workers=args.workers, pin_memory=False, drop_last=True)
        log = open(os.path.join(args.save_root, args.root_log, 'fold_{}.txt'.format(i)), 'w')
        output = ""\n Fold: {}\n"".format(i)
        log.write(output)
        log.flush()
        best_loss = 1000
        best_ccc = -100
        val_accum_epochs = 0
        for epoch in range(args.epochs):
            adjust_learning_rate(optimizer, epoch, args.lr_steps)
            train(train_loader, model, criterion, optimizer, epoch, log)
            torch.cuda.empty_cache() 
            if (epoch + 1) % args.eval_freq == 0 or epoch == args.epochs - 1:
                loss_val, ccc_current_val = validate(val_loader, model, criterion, (epoch + 1) * len(train_loader), log, train_mean, train_std)
                is_best_loss = loss_val< best_loss
                best_loss = min(loss_val, best_loss)
                is_best_ccc = ccc_current_val >best_ccc
                best_ccc  = max(ccc_current_val , best_ccc)
                save_checkpoint({
                    'epoch': epoch + 1,
                    'state_dict': model.state_dict(),
                }, is_best_loss, is_best_ccc, filename='fold_{}'.format(i))
                if not is_best_ccc:
                    val_accum_epochs+=1
                else:
                    val_accum_epochs=0
                if val_accum_epochs>=args.early_stop:
                    print(""validation ccc did not improve over {} epochs, stop"".format(args.early_stop))
                    break
    
def adjust_learning_rate(optimizer, epoch, lr_steps):
    """"""Sets the learning rate to the initial LR decayed by 10 every N epochs""""""
    decay = 0.1 ** (sum(epoch >= np.array(lr_steps)))
    lr = args.lr * decay
    decay = args.weight_decay
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr 
        param_group['weight_decay'] = decay         
def save_checkpoint(state, is_best_loss, is_best_ccc,filename='fold'):
    torch.save(state, '%s/%s/%s_checkpoint.pth.tar' % (args.save_root, args.root_model, filename))
    if is_best_loss:
        shutil.copyfile('%s/%s/%s_checkpoint.pth.tar' % (args.save_root, args.root_model, filename),
                        '%s/%s/%s_best_loss.pth.tar' % (args.save_root, args.root_model, filename)) 
        print(""checkpoint saved to"",  '%s/%s/%s_best_loss.pth.tar' % (args.save_root, args.root_model, filename))           
    if is_best_ccc:
        shutil.copyfile('%s/%s/%s_checkpoint.pth.tar' % (args.save_root, args.root_model, filename),
                        '%s/%s/%s_best_ccc.pth.tar' % (args.save_root, args.root_model, filename)) 
        print(""checkpoint saved to"",  '%s/%s/%s_best_ccc.pth.tar' % (args.save_root, args.root_model, filename))        
        
if __name__=='__main__':
    main()
"
Aff-wild-exps/merge_test_predictions.py,"import pandas as pd
import os
import glob
import numpy as np
import matplotlib.pyplot as plt
arousal_input_dir = ''
valence_input_dir = ''
assert os.path.exists(arousal_input_dir)
output_dir = ''
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
video_dict = {}
for i in range(5):
    arousal_cur_fold = os.path.join(arousal_input_dir, 'fold_{}'.format(i))
    valence_cur_fold = os.path.join(valence_input_dir, 'fold_{}'.format(i))
    video_names =  os.listdir(arousal_cur_fold)
    video_names = [file.split(""."")[0] for file in video_names]
    for vid in video_names:
        vid_id = vid.split(""_"")[1]
        arousal_df = pd.read_csv(os.path.join(arousal_cur_fold, 'arousal_{}.csv'.format(vid_id)))
        valence_df = pd.read_csv(os.path.join(valence_cur_fold, 'valence_{}.csv'.format(vid_id)))
        predictions = np.concatenate([valence_df.values, arousal_df.values], axis=-1)
        if vid_id not in video_dict.keys():
            video_dict[vid_id] = []
        video_dict[vid_id].append(predictions)
num_frames = 0
for video_name in video_dict.keys():
    preds_5_folds = np.asarray(video_dict[video_name])
    assert preds_5_folds.shape[0] == 5
    preds_mean = np.mean(preds_5_folds, axis=0)
    num_frames +=preds_mean.shape[0]
    new_df = pd.DataFrame(preds_mean)
    new_df.to_csv(os.path.join(output_dir, '{}.csv'.format(video_name)), index=False, header=None)
print(""num_frames in test set:{}"".format(num_frames))
    
"
Aff-wild-exps/network.py,"import torch
import torch.nn as nn
import torch.nn.functional as F
import glob
import os
class Path(object):
    def __init__(self, label_name, fold_id):
        self.label_name = label_name
        self.fold_id = fold_id
        self.phase_path = {}
        a_root = '/media/newssd/Aff-Wild_experiments/phase_diff_5_fold/valence_loss_type:ccc_batch_size:64_alpha:1.0/model'
        v_root = '/media/newssd/Aff-Wild_experiments/phase_diff_5_fold/arousal_loss_type:ccc_batch_size:256_alpha:1.0/model'
        self.root_dir = a_root if label_name=='arousal' else v_root
    def path(self):
        model_path = glob.glob(os.path.join(self.root_dir, '*best_ccc*'))
        model_path = [path  for path in model_path if 'fold_{}'.format(self.fold_id) in path]
        assert len(model_path)==1
        return model_path[0]
class MLP(nn.Module):
    def __init__(self, hidden_units, dropout=0.3):
        super(MLP, self).__init__()
        input_feature_dim = hidden_units[0]
        num_layers = len(hidden_units)-1
        assert num_layers>0
        assert hidden_units[-1]==256
        fc_list = []
        for hidden_dim in hidden_units[1:]:
            fc_list += [ nn.Dropout(dropout),
                        nn.Linear(input_feature_dim, hidden_dim),
                        nn.BatchNorm1d(hidden_dim),
                        nn.ReLU(inplace=True)
                        ]
            input_feature_dim = hidden_dim
        self.mlp = nn.Sequential(*fc_list)
    def forward(self, input_tensor):
        bs, num_frames, feature_dim = input_tensor.size()
        input_tensor = input_tensor.view(bs*num_frames, feature_dim)
        out = self.mlp(input_tensor)
        return out.view(bs, num_frames, -1)
class PhaseNet(nn.Module):
    def __init__(self, input_size, num_channels, hidden_units=[256, 256, 1] , dropout=0.3, feature=False):
        super(PhaseNet,self).__init__()
        # input size : 2**i times 6 or 7
        if input_size not in [48, 96, 112]:
               raise ValueError(""Incorrect input size"")
        if input_size==48:
            num_conv_layers = 3
        else:
            num_conv_layers = 4
        if input_size==48 or input_size==96:
            last_conv_width = 6
        else:
            last_conv_width = 7
        self.conv_net = []
        for i in range(num_conv_layers):
            if i==0:
                self.conv_net.append(self._make_conv_layer(num_channels, 2**(i+6), kernel_size=3, stride=2))
            elif i==1:
                self.conv_net.append(self._make_conv_layer(num_channels+2**(i-1+6), 2**(i+6), kernel_size=3, stride=2))
            else:
                self.conv_net.append(self._make_conv_layer(2**(i-1+6), 2**(i+6), kernel_size=3, stride=2))
        last_conv_dim = 2**(i+6)
        self.conv_net = nn.ModuleList(self.conv_net)
        self.dropout = nn.Dropout2d(p=0.2)
        self.avgpool = nn.AvgPool2d(kernel_size=[last_conv_width, last_conv_width])
        fc_list =[]
        fc_list += [nn.Linear(last_conv_dim, hidden_units[0]),
                       nn.ReLU(inplace=True),
                       nn.BatchNorm1d(hidden_units[1]),
                       nn.Dropout(dropout)]
        for i in range(0, len(hidden_units)-2):
            fc_list += [nn.Linear(hidden_units[i], hidden_units[i+1]),
                       nn.ReLU(inplace=True),
                       nn.BatchNorm1d(hidden_units[i+1]),
                       nn.Dropout(dropout)]
        self.fc = nn.Sequential(*fc_list)
        final_norm = nn.BatchNorm1d(1, eps=1e-6, momentum=0.1) 
        self.classifier = nn.Sequential(nn.Linear(hidden_units[-2], hidden_units[-1]),
                                 final_norm )
        self.feature = feature
    def _make_conv_layer(self, in_c, out_c, kernel_size = 3, stride = 2):
        ks = kernel_size 
        conv_layer = nn.Sequential(
        nn.Conv2d(in_c, out_c, kernel_size=(ks, ks), padding=ks//2),
        nn.BatchNorm2d(out_c, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
        nn.ReLU(inplace=True),
        nn.Conv2d(out_c, out_c, kernel_size=(ks, ks), padding=ks//2,stride=stride),
        nn.BatchNorm2d(out_c, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
        nn.ReLU(inplace=True),
        )
        return conv_layer
    def forward(self, data_level0, data_level1):
        bs, num_frames,  num_channel, W0, H0 = data_level0.size()
        bs, num_frames, num_channel,  W1, H1 = data_level1.size()
        trans_data_level0 = data_level0.view(bs*num_frames, num_channel, W0, H0 )
        trans_data_level1 = data_level1.view(bs* num_frames, num_channel,  W1, H1)
        conv1 = self.conv_net[0](trans_data_level0)
        conv_out = torch.cat([conv1, trans_data_level1], dim=1) 
        for layer in self.conv_net[1:]:
            conv_out = self.dropout(layer(conv_out))
        avgpool = self.avgpool(conv_out)
        avgpool = avgpool.view(bs*num_frames, -1)
        out = self.fc(avgpool)
        if self.feature:
            return out
        else:
            out = self.classifier(out)
            return out
class Two_Stream_RNN(nn.Module):
    def __init__(self, mlp_hidden_units=[2048, 256, 256], dropout=0.5, label_name = 'arousal', fold_id = 0, num_phase=12):
        super(Two_Stream_RNN, self).__init__()
        self.mlp = MLP(mlp_hidden_units)
        self.num_phase = num_phase
        self.phasenet = PhaseNet(48, 2*num_phase, hidden_units =[256, 256, 1], dropout=0.3, feature=True)
        if '_' not in label_name:
            pretrained_path = Path(label_name, fold_id)
            self.phasenet = self.load_model_weights(self.phasenet, pretrained_path.path())
        self.transform = nn.Sequential(nn.Linear(512, 256),
                                      nn.ReLU(inplace=True),
                                      nn.BatchNorm1d(256),
                                      nn.Dropout(dropout))
        self.rnns = nn.GRU(256, 128, bidirectional=True, num_layers=2, dropout = 0.3)
        self.classifier = nn.Sequential(nn.Dropout(dropout),
                                        nn.Linear(256, len(label_name.split(""_""))),
                                        nn.BatchNorm1d(len(label_name.split(""_""))))
    def load_model_weights(self, model, model_path):
        ckp = torch.load(model_path)
        net_key = [key for key in ckp.keys() if (key !='epoch') and (key !='iter')][0]
        state_dict = ckp[net_key]
        model.load_state_dict(state_dict)
        return model
    def forward(self, phase_data, rgb_data):
        bs, num_frames = rgb_data.size(0), rgb_data.size(1)
        features_cnn = self.mlp(rgb_data)
        features_spatial = features_cnn.view(bs, num_frames, -1)
        phase_0, phase_1 = phase_data
        features_temporal = self.phasenet(phase_0, phase_1)
        features_temporal = features_temporal.view(bs, num_frames, -1)
        features = torch.cat([features_spatial, features_temporal], dim=-1)
        features = self.transform(features.view(bs*num_frames, -1))
        features = features.view(bs, num_frames, -1)
        outputs_rnns,  _ = self.rnns(features)
        outputs_rnns = outputs_rnns.view(bs* num_frames, -1)
        out = self.classifier(outputs_rnns)
        out = out.view(bs, num_frames, -1)
        return out
    
if __name__ == ""__main__"":

    from dataloader import Face_Dataset
    import os
    root_path = '/media/newssd/Aff-Wild_experiments/Aligned_Faces_train'
    feature_path = '/media/newssd/Aff-Wild_experiments/Extracted_Features/Aff_wild_train/resnet50_ferplus_features_fps=30_pool5_7x7_s1'
    annot_dir = '/media/newssd/Aff-Wild_experiments/annotations'
    video_names = os.listdir(root_path)[:50]
    train_dataset = Face_Dataset(root_path, feature_path, annot_dir, video_names, label_name='arousal', test_mode=False, num_phase=12, length=64, stride=32)
    train_loader = torch.utils.data.DataLoader(
        train_dataset, 
        batch_size = 2, 
        num_workers=0, pin_memory=False )
    model = Two_Stream_RNN(mlp_hidden_units=[2048, 256, 256], fold_id=0)
    model.cuda()
    model.train()
    for phase_f, rgb_f, labels, ranges, videos in train_loader:
        phase_0, phase_1 = phase_f
        phase_0 = phase_0.type('torch.FloatTensor').cuda()
        phase_1 = phase_1.type('torch.FloatTensor').cuda()
        rgb_f = rgb_f.type('torch.FloatTensor').cuda()
        labels = labels.type('torch.FloatTensor').cuda()
        out = model([phase_0,phase_1], rgb_f)
        
        
"
Aff-wild-exps/read_fold_txt.py,"import os
import numpy as np
def read_val_ccc(txt_file):
    with open(txt_file) as f:
        content = f.readlines()
    content = [x.strip() for x in content] 
    val_lines = [x for x in content if 'Validation' in x]
    val_cccs_v = [x.split('ccc_valence:')[-1].split(',')[0] for x in val_lines]
    val_cccs_v = [float(x.split('(')[-1].split(')')[0]) for x in val_cccs_v]
    val_cccs_a = [x.split('ccc_arousal:')[-1].split(',')[0] for x in val_lines]
    val_cccs_a = [float(x.split('(')[-1].split(')')[0]) for x in val_cccs_a]
    return [max(val_cccs_a), max(val_cccs_v)]
dirs = os.listdir('.')
for dir_path in dirs:
    if dir_path.startswith('arousal') or dir_path.startswith('valence'):
        fold_dir = os.path.join(dir_path, 'log')
        fold_txts = [os.path.join(fold_dir, 'fold_{}.txt'.format(i)) for i in range(5)]
        if all([os.path.exists(path) for path in fold_txts]):
            cccs = np.array([read_val_ccc(txt) for txt in fold_txts])
            print(""name:{}"".format(dir_path))
            print(""5 fold arousal ccc {}, average ccc:{}"".format(cccs[:, 0], np.mean(cccs[:, 0], axis=0)))
            print(""5 fold valence ccc {}, average ccc:{}"".format(cccs[:, 1], np.mean(cccs[:, 1], axis=0)))

"
Aff-wild-exps/readme,"example:
```
python main.py --label_name arousal --save_root arousal/
```
using 'merge_test_predictions.py' to get the average prediction of 5 models in five folds.

using 'read_fold_txt.py' to print out the average CCC of five-fold validation experiments.
"
Aff-wild-exps/transforms.py,"import torchvision
import random
from PIL import Image, ImageOps
import numpy as np
import numbers
import math
import torch


class GroupRandomCrop(object):
    def __init__(self, size, seed=None):
        if isinstance(size, numbers.Number):
            self.size = (int(size), int(size))
        else:
            self.size = size
        self.seed = seed
    def __call__(self, img_group):

        w, h = img_group[0].size
        th, tw = self.size

        out_images = list()
        if self.seed is not None:
            random.seed(self.seed)
        x1 = random.randint(0, w - tw)
        y1 = random.randint(0, h - th)

        for img in img_group:
            assert(img.size[0] == w and img.size[1] == h)
            if w == tw and h == th:
                out_images.append(img)
            else:
                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))

        return out_images

class GroupRandomHorizontalFlip(object):
    """"""Randomly horizontally flips the given PIL.Image with a probability of 0.5
    """"""
    def __init__(self, seed=None):
        self.seed = seed

    def __call__(self, img_group, is_flow=False):
        if self.seed is not None:
            random.seed(self.seed)
        v = random.random()
        if v < 0.5:
            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]
            return ret
        else:
            return img_group


class GroupNormalize(object):
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def __call__(self, tensor):
        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))
        rep_std = self.std * (tensor.size()[0]//len(self.std))

        # TODO: make efficient
        for t, m, s in zip(tensor, rep_mean, rep_std):
            t.sub_(m).div_(s)

        return tensor


class GroupScale(object):
    """""" Rescales the input PIL.Image to the given 'size'.
    'size' will be the size of the smaller edge.
    For example, if height > width, then image will be
    rescaled to (size * height / width, size)
    size: size of the smaller edge
    interpolation: Default: PIL.Image.BILINEAR
    """"""

    def __init__(self, size, interpolation=Image.LANCZOS):
        self.worker = torchvision.transforms.Resize(size, interpolation)

    def __call__(self, img_group):
        return [self.worker(img) for img in img_group]



class Stack(object):

    def __init__(self, roll=False):
        self.roll = roll

    def __call__(self, img_group):
        if img_group[0].mode == 'L':
            return np.stack(img_group, axis=2)
        elif img_group[0].mode == 'RGB':
            if self.roll:
                return np.stack([np.array(x)[:, :, ::-1] for x in img_group], axis=3)
            else:
                return np.stack(img_group, axis=3)


class ToTorchFormatTensor(object):
    """""" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]
    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] """"""
    def __init__(self, div=True):
        self.div = div

    def __call__(self, pic):
        if isinstance(pic, np.ndarray):
            # handle numpy array
            if len(pic.shape)==3:
                W, H, n_frames =pic.shape
                img = torch.from_numpy(pic).permute(2, 0, 1).contiguous() #frames, W, H
            elif len(pic.shape)==4:
                W, H, channel, n_frames = pic.shape
                img = torch.from_numpy(pic).permute(3, 2, 0, 1).contiguous() #frames, channels, W, H
        else:
            raise ValueError(""input image has to be ndarray!"")
        return img.float().div(255) if self.div else img.float()


class IdentityTransform(object):

    def __call__(self, data):
        return data


if __name__ == ""__main__"":
    trans = torchvision.transforms.Compose([
        GroupScale(256),
        GroupRandomCrop(224),
        Stack(),
        ToTorchFormatTensor(),
        GroupNormalize(
            mean=[.485, .456, .406],
            std=[.229, .224, .225]
        )]
    )

    im = Image.open('../tensorflow-model-zoo.torch/lena_299.png')

    color_group = [im] * 3
    rst = trans(color_group)

    gray_group = [im.convert('L')] * 9
    gray_rst = trans(gray_group)

    trans2 = torchvision.transforms.Compose([
        Stack(),
        ToTorchFormatTensor(),
        GroupNormalize(
            mean=[.485, .456, .406],
            std=[.229, .224, .225])
    ])
    print(trans2(color_group))
"
Aff-wild-exps/utils.py,"# -*- coding: utf-8 -*-
import os
import sys
import six
import torch
from os.path import join as pjoin
from steerable.SCFpyr_PyTorch import SCFpyr_PyTorch
import numpy as np
import math
from PIL import Image
from torch.nn import functional as F
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from matplotlib import cm
from time import time
def load_module_2or3(model_name, model_def_path):
    """"""Load model definition module in a manner that is compatible with
    both Python2 and Python3

    Args:
        model_name: The name of the model to be loaded
        model_def_path: The filepath of the module containing the definition

    Return:
        The loaded python module.""""""
    if six.PY3:
        import importlib.util
        spec = importlib.util.spec_from_file_location(model_name, model_def_path)
        mod = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(mod)
    else:
        import importlib
        dirname = os.path.dirname(model_def_path)
        sys.path.insert(0, dirname)
        module_name = os.path.splitext(os.path.basename(model_def_path))[0]
        mod = importlib.import_module(module_name)
    return mod
def load_model(model_name, MODEL_DIR):
    """"""Load imoprted PyTorch model by name

    Args:
        model_name (str): the name of the model to be loaded

    Return:
        nn.Module: the loaded network
    """"""
    model_def_path = pjoin(MODEL_DIR, model_name + '.py')
    weights_path = pjoin(MODEL_DIR, model_name + '.pth')
    mod = load_module_2or3(model_name, model_def_path)
    func = getattr(mod, model_name)
    net = func(weights_path=weights_path)
    return net
def extract_from_batch(coeff_batch, example_idx=0, symmetry = True):
    '''
    Given the batched Complex Steerable Pyramid, extract the coefficients
    for a single example from the batch. Additionally, it converts all
    torch.Tensor's to np.ndarrays' and changes creates proper np.complex
    objects for all the orientation bands. 

    Args:
        coeff_batch (list): list containing low-pass, high-pass and pyr levels
        example_idx (int, optional): Defaults to 0. index in batch to extract
    
    Returns:
        list: list containing low-pass, high-pass and pyr levels as np.ndarray
    '''
    if not isinstance(coeff_batch, list):
        raise ValueError('Batch of coefficients must be a list')
    coeff = []  # coefficient for single example
    for coeff_level in coeff_batch:
        if isinstance(coeff_level, torch.Tensor):
            # Low- or High-Pass
            coeff_level_numpy = coeff_level[example_idx].cpu().numpy()
            if symmetry:
                W, H = coeff_level_numpy.shape
                coeff_level_numpy = coeff_level_numpy[:W//2, :H//2]
            coeff.append(coeff_level_numpy)
        elif isinstance(coeff_level, list):
            coeff_orientations_numpy = []
            for coeff_orientation in coeff_level:
                coeff_orientation_numpy = coeff_orientation[example_idx].cpu().numpy()
                coeff_orientation_numpy = coeff_orientation_numpy[:,:,0] + 1j*coeff_orientation_numpy[:,:,1]
                if symmetry:
                    W, H = coeff_orientation_numpy.shape
                    coeff_orientation_numpy= coeff_orientation_numpy[:W//2, :H//2]
                coeff_orientations_numpy.append(coeff_orientation_numpy)
            coeff.append(coeff_orientations_numpy)
        else:
            raise ValueError('coeff leve must be of type (list, torch.Tensor)')
    return coeff
def get_device(device='cuda:0'):
    assert isinstance(device, str)
    num_cuda = torch.cuda.device_count()

    if 'cuda' in device:
        if num_cuda > 0:
            # Found CUDA device, use the GPU
            return torch.device(device)
        # Fallback to CPU
        print('No CUDA devices found, falling back to CPU')
        device = 'cpu'

    if not torch.backends.mkl.is_available():
        raise NotImplementedError(
            'torch.fft on the CPU requires MKL back-end. ' +
            'Please recompile your PyTorch distribution.')
    return torch.device('cpu')
def make_grid_coeff(coeff, normalize=True):
    '''
    Visualization function for building a large image that contains the
    low-pass, high-pass and all intermediate levels in the steerable pyramid. 
    For the complex intermediate bands, the real part is visualized.
    
    Args:
        coeff (list): complex pyramid stored as list containing all levels
        normalize (bool, optional): Defaults to True. Whether to normalize each band
    
    Returns:
        np.ndarray: large image that contains grid of all bands and orientations
    '''
    M, N = coeff[1][0].shape
    Norients = len(coeff[1])
    out = np.zeros((M * 3 - coeff[-1].shape[0], Norients * N *2))
    currentx, currenty = 0, 0
    m, n = coeff[0].shape
    out[currentx: currentx+m, currenty: currenty+n] = 255 * coeff[0]/coeff[0].max()
    currentx, currenty = m, 0
    for i in range(1, len(coeff[:-1])):
        for j in range(len(coeff[1])):
            tmp_real = coeff[i][j].real
            tmp_imag = coeff[i][j].imag
            phase = np.arctan2(tmp_imag,tmp_real)
            amp = np.sqrt(np.power(tmp_imag,2) + np.power(tmp_real, 2))
            m, n = tmp_real.shape
            if normalize:
                amp = 255*(amp-amp.min())/(amp.max()-amp.min())
                phase = 255*(phase - phase.min())/(phase.max()-phase.min())
            amp[m-1,:] = 255
            amp[:,n-1]=255
            phase[m-1,:]=255
            phase[:,n-1] = 255
            out[currentx:currentx+m, currenty:currenty+n] = amp
            out[currentx:currentx+m, currenty+n:currenty+2*n] = phase
            currenty += 2*n
        currentx += m
        currenty = 0

    m, n = coeff[-1].shape
    out[currentx: currentx+m, currenty: currenty+n] = 255 * coeff[-1]/coeff[-1].max()
    out[0,:] = 255
    out[:,0] = 255
    return out.astype(np.uint8)
def rgb2gray(rgb):
    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])
#def show_image_3D(image_2d, save_PATH=None):
#    if save_PATH is not None:
#        if not os.path.isdir(os.path.dirname(save_PATH)):
#            os.makedirs(os.path.dirname(save_PATH))
#    M,N = image_2d.shape[:2]
#    X, Y = range(1, M+1), range(1, N+1)
#    Xm, Ym = np.meshgrid(X, Y)
#    fig = plt.figure()
#    ax = Axes3D(plt.gcf())
#    surf = ax.plot_surface(Xm, Ym, image_2d, cmap=cm.coolwarm,)
#    fig.colorbar(surf, shrink=0.5, aspect=10)
#    if save_PATH is not None:
#        plt.savefig(save_PATH)
#    plt.show()
#def windowing_batch(img_batch, device=None): #reference: https://blogs.mathworks.com/steve/2009/12/04/fourier-transform-visualization-using-windowing/
#    M,N  =img_batch.size()[-2:]
#    w1 = np.expand_dims(np.cos(np.linspace(-np.pi/2, np.pi/2, M)), axis=0)
#    w2 = np.expand_dims(np.cos(np.linspace(-np.pi/2, np.pi/2, N)), axis=0)
##    np.copyto(w1, 0.5, where=w1>1/2.)
##    w1 = w1*2
##    np.copyto(w2, 0.5, where=w2>1/2.)
##    w2  = w2*2
#    window = np.dot(w1.T, w2)
#    window = torch.from_numpy(window).type('torch.FloatTensor').to(device)
#    img_batch= torch.mul(img_batch, window)
#    return img_batch
#def extract_phase_mag_from_coeff(coeff):
#    '''
#    Extracting the phase and magnitude for all intermediate levels in the steerable pyramid. 
#    
#    Args:
#        coeff (list): complex pyramid stored as list containing all levels
#    
#    Returns:
#        torch.Tensor (list): magnitude and phase are stacked in the same dimension
#    '''
#    bs, M, N, _ = coeff[1][0].shape
#    inter_coeff = coeff[1:-1]
#    returns = []
#    for coeff_level in inter_coeff:
#        new_coeff_level = []
#        for subband in coeff_level:
#            subband_ = torch.unbind(subband, -1)
#            subband_real, subband_imag = subband_
#            # computing local phase at each scale and orientation
#            subband_phase = torch.atan2(subband_imag, subband_real) # -pi to pi
#            subband_mag = torch.sqrt(torch.pow(subband_real,2)+torch.pow(subband_imag,2))
#            subband_polar = torch.stack((subband_mag, subband_phase),-1)
#            new_coeff_level.append(subband_polar)
#        returns.append(new_coeff_level)
#    returns.insert(0, coeff[0]) # high pass response real
#    returns.append(coeff[-1]) # low pass response
#    return returns
def torch_unwrap(tensor, discont=math.pi, dim=-1):
    nd = len(tensor.size())
    dd = torch_diff(tensor, dim=dim)
    slice1 = [slice(None, None)]*nd     # full slices
    slice1[dim] = slice(1, None)
    slice1 = tuple(slice1)
    PI = math.pi
    ddmod = torch.fmod(dd + PI, 2*PI) - PI
    id1 = (ddmod == -PI) & (dd > 0)
    ddmod[id1] = PI
    ph_correct = ddmod - dd
    id2 = torch.abs(dd) < discont
    ph_correct[id2] = 0
    up = tensor.clone().detach()
    up[slice1] = tensor[slice1] + ph_correct.cumsum(dim=dim)
    return up
def torch_diff(tensor,n=1, dim=-1):
    """"""
    tensor : Input Tensor
    n : int, optional
        The number of times values are differenced. If zero, the input
        is returned as-is.
    axis : int, optional
        The axis along which the difference is taken, default is the
        last axis.
    """"""
    nd = len(tensor.size())
    slice1 = [slice(None)] * nd
    slice2 = [slice(None)] * nd
    slice1[dim] = slice(1, None)
    slice2[dim] = slice(None, -1)
    slice1 = tuple(slice1)
    slice2 = tuple(slice2)
    for _ in range(n):
        tensor = tensor[slice1] - tensor[slice2]
    return tensor
    
def amplitude_based_gaussian_blur(mag, phase, g_kernel):
    bs, n_frames, W, H = phase.size()
    mag_phase = torch.mul(mag, phase)
    in_channel = n_frames
    out_channel = n_frames
    m, n = g_kernel.size()
    filters = torch.stack([g_kernel]*out_channel, dim=0)
    filters = torch.unsqueeze(filters, 1) # (output_channel, input_channel/groups, W, H)
    filters =filters.type('torch.FloatTensor').cuda(async=True) if phase.is_cuda else filters
    mag_phase_blurred = F.conv2d(mag_phase, filters, groups = in_channel, padding=m//2)
    
    mag_blurred = F.conv2d(mag, filters, groups = in_channel, padding=m//2)
    result = torch.div(mag_phase_blurred, mag_blurred)
    return result
def amplitude_based_gaussian_blurcoeff_batch_numpy(mag, phase, g_kernel):
    bs, n_frames, W, H = phase.size()
    phase = phase.cpu().numpy()
    mag = mag.cpu().numpy()
    g_kernel = g_kernel.cpu().numpy()
    from scipy.signal import convolve2d
    new_phase = []
    for b in range(bs):
        new_phase_b = []
        for f in range(n_frames):
            p = phase[b,f,...]
            m = mag[b, f,...]
            denoised_phase =  convolve2d(np.multiply(p, m), g_kernel, mode='same')/convolve2d(m, g_kernel, mode='same')
            new_phase_b.append(denoised_phase)
        new_phase.append(new_phase_b)
    new_phase = np.asarray(new_phase)
    return torch.Tensor(new_phase).type('torch.FloatTensor').cuda(async=True)
def gaussian_kernel(std, tap = 11):
    kernel = np.zeros((tap, tap))
    for x in range(tap):
        for y in range(tap):
            x0 = x - tap//2
            y0 = y - tap//2
            kernel[x, y] = np.exp(- (x0**2+y0**2)/(2*std**2))
    return kernel
def symmetric_extension_batch(img_batch):
    #  img_batch, None, W, H (the last two aixes are two dimensional image)
    img_batch_inverse_col = img_batch.clone().detach()
    inv_idx_col = torch.arange(img_batch.size(-1)-1, -1, -1).long()
    img_batch_inverse_col = img_batch_inverse_col[..., :, inv_idx_col]
    img_batch_inverse_row = img_batch.clone().detach()
    inv_idx_row = torch.arange(img_batch.size(-2)-1, -1, -1).long()
    img_batch_inverse_row = img_batch_inverse_row[..., inv_idx_row, :]
    img_batch_inverse_row_col = img_batch_inverse_col.clone().detach()
    img_batch_inverse_row_col = img_batch_inverse_row_col[:,:, inv_idx_row, :]
    img_batch_0 = torch.cat([img_batch, img_batch_inverse_col], dim=-1)
    img_batch_1 = torch.cat([img_batch_inverse_row, img_batch_inverse_row_col], dim=-1)
    new_img_batch = torch.cat([img_batch_0, img_batch_1], dim=-2)
    return new_img_batch
class Steerable_Pyramid_Phase(object):
    def __init__(self, height=5, nbands=4, scale_factor=2, device=None, extract_level=1, visualize=False):
        self.pyramid = SCFpyr_PyTorch(
            height=height, 
            nbands=nbands,
            scale_factor=scale_factor, 
            device=device
        )
        self.height = height
        self.nbands = nbands
        self.scale_factor = scale_factor
        self.device = device
        self.extract_level = extract_level
        self.visualize = visualize
    def build_pyramid(self, im_batch, symmetry = True):
        """"""
        input image batch has 4 dimensions: batch size,  number of phase images, W, H
        """"""
        bs, num_phase_frames, W, H =im_batch.size()
        trans_im_batch = im_batch.view(bs*num_phase_frames, 1, W, H) # the second dim is 1, indicating it's grayscale image
        if symmetry:
            trans_im_batch = symmetric_extension_batch(trans_im_batch)
        #tic= time()
        coeff_batch = self.pyramid.build(trans_im_batch)
        #print(""process {} images for {}"".format(bs*num_phase_frames, time()-tic))
        if not isinstance(coeff_batch, list):
            raise ValueError('Batch of coefficients must be a list')

        if self.visualize :
            example_id = 10 # the 10th image from number of phase images
            example_coeff = extract_from_batch(coeff_batch , example_id, symmetry)
            example_coeff = make_grid_coeff(example_coeff)
            example_coeff = Image.fromarray(example_coeff)
            example_img = trans_im_batch[example_id,0,...].cpu().numpy()
            example_img = Image.fromarray(255*example_img/example_img.max())
            example_img.show()
            example_img_remove_symm = trans_im_batch[example_id,0,...].cpu().numpy()
            example_img_remove_symm = 255*example_img_remove_symm/example_img_remove_symm.max()
            if symmetry:
                W, H = example_img_remove_symm.shape
                example_img_remove_symm = example_img_remove_symm[:W//2, :H//2]
                example_img_remove_symm = Image.fromarray(example_img_remove_symm)
                example_img_remove_symm.show()
            example_coeff.show()
        if isinstance(self.extract_level, int):
            extr_level_coeff_batch = self.extract_coeff_level(self.extract_level, coeff_batch)
            W, H, _  = extr_level_coeff_batch.size()[-3:]
            nbands = extr_level_coeff_batch.size()[0]
            extr_level_coeff_batch = extr_level_coeff_batch.view(nbands, bs, num_phase_frames,  W, H, 2)
            extr_level_coeff_batch = extr_level_coeff_batch.permute(1, 0, 2, 3, 4, 5 ).contiguous()
            if symmetry:
                extr_level_coeff_batch = extr_level_coeff_batch[..., :W//2, :H//2, :]
        elif isinstance(self.extract_level, list):
            extr_level_coeff_batch = []
            for level in self.extract_level:
                level_coeff_batch = self.extract_coeff_level(level, coeff_batch)
                W, H, _  = level_coeff_batch.size()[-3:]
                nbands = level_coeff_batch.size()[0]
                level_coeff_batch = level_coeff_batch.view(nbands, bs, num_phase_frames,  W, H, 2)
                level_coeff_batch = level_coeff_batch.permute(1, 0, 2, 3, 4, 5 ).contiguous()
                if symmetry:
                    level_coeff_batch = level_coeff_batch[..., :W//2, :H//2, :]
                extr_level_coeff_batch.append(level_coeff_batch)
        return extr_level_coeff_batch
    def extract_coeff_level(self, level, coeff_batch):
        extr_level_coeff_batch = coeff_batch[level]
        assert isinstance(extr_level_coeff_batch, list)
        extr_level_coeff_batch = torch.stack(extr_level_coeff_batch, 0)
        return extr_level_coeff_batch
    def extract_phase(self, coeff_batch, return_phase=False, return_both = False):
        """"""
        coeff batch has dimension: batch size, nbands, number phase frames (17), W, H, 2   (2 is for real part and imaginary part) 
        """"""
        bs, n_bands, n_phase_frames, W,H,_ = coeff_batch.size()
        trans_coeff_batch = coeff_batch.view(bs*  n_bands* n_phase_frames, W, H, -1)
        real_coeff_batch, imag_coeff_batch = torch.unbind(trans_coeff_batch, -1)
        phase_batch = torch.atan2(imag_coeff_batch, real_coeff_batch)
        mag_batch = torch.sqrt(torch.pow(imag_coeff_batch, 2)+torch.pow(real_coeff_batch, 2))
        phase_batch = phase_batch.view(bs*n_bands, n_phase_frames, W, H)
        EPS = 1e-10
        mag_batch = mag_batch.view(bs*n_bands, n_phase_frames, W, H) +EPS # TO avoid mag==0
        assert (mag_batch<=0.0).nonzero().size(0)==0
        
        # phase unwrap over time
        phase_batch = torch_unwrap(phase_batch, discont = math.pi, dim=-3)
        # phase denoising (amplitude-based gaussian blur)
        g_kernel = torch.from_numpy(gaussian_kernel(std=2, tap=11))
        #denoised_phase_batch = amplitude_based_gaussian_blur_numpy(mag_batch, phase_batch, g_kernel)
        denoised_phase_batch = amplitude_based_gaussian_blur(mag_batch, phase_batch, g_kernel)
        denoised_phase_batch = denoised_phase_batch.view(bs, n_bands, n_phase_frames, W, H)
        # phase difference 
        phase_difference_batch = torch_diff(denoised_phase_batch, dim=2)
        phase_difference_batch =  phase_difference_batch.view(bs, n_bands, n_phase_frames-1, W, H)
        if self.visualize:
            phase_example = phase_batch.view(bs, n_bands, n_phase_frames, W, H)[0,...]
            mag_example = mag_batch.view(bs, n_bands, n_phase_frames, W, H)[0,...]
            denoised_phase_example = denoised_phase_batch.view(bs, n_bands, n_phase_frames, W, H)[0,...]
            phase_diff_example = phase_difference_batch.view(bs, n_bands, n_phase_frames-1, W, H)[0,...]
            self.show_3D_subplots(phase_example, title=""phase example"", first_k_frames=2)
            self.show_3D_subplots(mag_example, title=""magnitude example"", first_k_frames=2)
            self.show_3D_subplots(denoised_phase_example, title=""denoised phase example"", first_k_frames=2)
            self.show_3D_subplots(phase_diff_example, title=""phase difference example"", first_k_frames=2)
        # denoised phase centered
        mean = denoised_phase_batch.mean(-1).mean(-1)
        mean = mean.unsqueeze(-1).unsqueeze(-1)
        denoised_phase_batch = denoised_phase_batch - mean 
        mean = phase_difference_batch.mean(-1).mean(-1)
        mean = mean.unsqueeze(-1).unsqueeze(-1)
        phase_difference_batch = phase_difference_batch - mean
        phase_difference_batch = torch.clamp(phase_difference_batch, -5*math.pi, 5*math.pi)
        if return_both:
            # remove one phase image
            denoised_phase_batch = denoised_phase_batch[:,:,1:, :]
            assert phase_difference_batch.size() == denoised_phase_batch.size()
            result = self.insert_tensors(phase_difference_batch, denoised_phase_batch, dim=2) 
            result = result.cuda()
            return result
        if return_phase:
            return denoised_phase_batch 
        else:
            return phase_difference_batch
    def insert_tensors(self,t_a, t_b, dim):
        size = list(t_a.size())
        size[dim] = 2*size[dim]
        result = torch.zeros(size)
        length = t_a.size(dim)
        for i in range(length):
            slice0 = [slice(None,None)]*len(size)
            slice0[dim]= slice(i, i+1)
            slice1 = [slice(None,None)]*len(size)
            slice1[dim]= slice(i//2, i//2+1)
            if i%2==0:
                result[slice0] = t_a[slice1]
            else:
                result[slice0] = t_b[slice1]
        return result
    def show_3D_subplots(self, data, title, first_k_frames = None):
        """"""
        data has dimensions: nbands, n_phase_frames, W, H
        """"""
        nbands, n_phase_frames, W, H = data.size()
        m = nbands
        n = first_k_frames if first_k_frames is not None else n_phase_frames
        
        X, Y = range(1, W+1), range(1, H+1)
        Xm, Ym = np.meshgrid(X, Y)
        for i in range(m):
            fig, ax = plt.subplots(nrows=1, ncols=n, subplot_kw={'projection':""3d""})
            for j in range(n):
                img = data[i, j , ...].cpu().numpy()
                surf = ax[j].plot_surface(Xm, Ym,img , rstride=1, cstride=1, cmap=cm.coolwarm,linewidth=0, antialiased=False)
            fig.colorbar(surf, shrink=0.5, aspect=10)
            fig.suptitle(title+"": orientation {}"".format(i))
        plt.show()
            
# def PadSequence(batch):
#     # Let's assume that ""batch"" is a list of tuples: (data0, data1, data2, label, flag, names).
#     # data has dimensionality: bs, num_frames, channels, W, H
#     # Sort the batch in the descending order
#     sorted_batch = sorted(batch, key=lambda x: x[1].shape[0], reverse=True)
# 	# Get each sequence and pad it
#     num_sequences = len(sorted_batch[0])-1
#     list_seqs = []
#     list_of_padded_seqs = []
#     for i in range(num_sequences):
#         if len(sorted_batch[0][i].size())!=1:
#             sequences = [x[i] for x in sorted_batch ]
#         else:
#             # for label or flag
#             sequences = [x[i].unsqueeze(-1) for x in sorted_batch ]
#         sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True).contiguous()
#         list_seqs.append(sequences)
#         list_of_padded_seqs.append(sequences_padded)
#     lengths = torch.LongTensor([len(x) for x in list_seqs[0]])
#     names = [x[-1] for x in sorted_batch] # names, do not need to be padded
#     list_of_padded_seqs.append(names)
#     return list_of_padded_seqs, lengths
            
"
LICENSE,"MIT License

Copyright (c) 2020 Hong University of Science and Technology

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
"
OMG-exps/Same_Length_Sampler.py,"import torch
from torch._six import int_classes as _int_classes
class Sampler(object):
    """"""Base class for all Samplers.

    Every Sampler subclass has to provide an __iter__ method, providing a way
    to iterate over indices of dataset elements, and a __len__ method that
    returns the length of the returned iterators.
    """"""

    def __init__(self, data_source):
        pass

    def __iter__(self):
        raise NotImplementedError

    def __len__(self):
        raise NotImplementedError
        
class SubsetRandomSampler(Sampler):
    """"""Samples elements randomly from a given list of indices, without replacement.

    Arguments:
        indices (sequence): a sequence of indices
    """"""
    def __init__(self, indices):
        self.indices = indices

    def __iter__(self):
        return (self.indices[i] for i in torch.randperm(len(self.indices)))

    def __len__(self):
        return len(self.indices)
    
class SameLengthBatchSampler(Sampler):
    """"""indices_list is a list, each element is another list, consisting of all indices of same lengths. Each time sample from the same list
       Arguments:
           indices_list: a list of sublists, each sublist is a sequence of indices
    """"""
    def __init__(self, indices_list, batch_size, drop_last, random=True):
        self.indices_list = indices_list
        if not isinstance(batch_size, _int_classes) or isinstance(batch_size, bool) or \
                batch_size <= 0:
            raise ValueError(""batch_size should be a positive integeral value, ""
                             ""but got batch_size={}"".format(batch_size))
        if not isinstance(drop_last, bool):
            raise ValueError(""drop_last should be a boolean value, but got ""
                             ""drop_last={}"".format(drop_last))
        self.batch_size = batch_size
        self.drop_last = drop_last
        self.random = random
    def shuffle(self, indices):
        length = len(indices)
        r = torch.randperm(length)
        return [indices[id] for id in r]
    def __iter__(self):
        for indices in self.indices_list:
            batch = []
            for idx in indices:
                batch.append(idx)
                if len(batch) == self.batch_size:
                    yield batch if not self.random else self.shuffle(batch)
                    batch = []
            if len(batch) > 0 and not self.drop_last:
                yield batch if not self.random else self.shuffle(batch)
    def __len__(self):
        indices_num = [len(indices) for indices in self.indices_list]
        if self.drop_last:
            return sum(indices_num) // self.batch_size
        else:
            return (sum(indices_num) + self.batch_size - 1) // self.batch_size"
OMG-exps/config.py,"import argparse
parser = argparse.ArgumentParser(description=""PyTorch implementation of two stream emotion network"")
parser.add_argument('--label_name', type=str, choices = ['arousal', 'valence', 'arousal_valence'])
parser.add_argument('--train_dict', type=str,default=""../exps/train_dict.pkl"") 
parser.add_argument('--val_dict', type=str, default=""../exps/val_dict.pkl"")
parser.add_argument('--test_dict', type=str, default = '../exps/test_dict.pkl')
parser.add_argument('--store_name', type=str, default="""")
parser.add_argument(""--save_root"", type=str, default="""")
# ========================= Model Configs ==========================
parser.add_argument(""--cnn"", type =str, default = ""resnet50"", choices=[ 'resnet50', 'vgg'])
parser.add_argument('--root_path', type=str, default='/media/newssd/OMG_experiments/OMG_OpenFace')
parser.add_argument('--loss_type', type=str, default=""ccc"",
                    choices=['mse', 'ccc'])
parser.add_argument('--fusion', type=str, default= 'cat', choices =['cat'])
parser.add_argument('--py_level', type=int, default=4) # including highpass and lowpass residual
parser.add_argument('--py_nbands', type=int, default=2) # number of orientations
parser.add_argument('--hidden_units', default=[2048, 256, 256, 1], type=int, nargs=""+"",
                    help='hidden units set up for MLP') # for spatial stream
parser.add_argument('--sample_rate', type=int, default = 1)
parser.add_argument('--length', type=int, default = 12)
parser.add_argument('--cat_before_gru', action='store_true')
parser.add_argument('--freeze', action='store_true')
# ========================= Learning Configs ==========================
parser.add_argument('--epochs', default=5, type=int, metavar='N',
                    help='number of total epochs to run')
parser.add_argument('--early_stop', type=int, default=3) # if validation loss didn't improve over 5 epochs, stop
parser.add_argument('-b', '--batch_size', default=4, type=int,
                    metavar='N', help='mini-batch size (default: 4)')
parser.add_argument( '--eval_batch_size', default=4, type=int,
                    metavar='N', help='mini-batch size (default: 4)')
parser.add_argument('--lr', default=1e-3, type=float)
parser.add_argument('--lr_steps', default=[3, 5], type=float, nargs=""+"",
                    metavar='LRSteps', help='epochs to decay learning rate by 10')
parser.add_argument('--momentum', default=0.9, type=float, metavar='M',
                    help='momentum')
parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float,
                    metavar='W', help='weight decay (default: 5e-4)')
parser.add_argument('--clip-gradient', '--gd', default=20, type=float,
                    metavar='W', help='gradient norm clipping (default: 20)')
parser.add_argument('--gradient_accumulation_steps', type=int, default = 1, 
                    help='accumulate gradient before loss backward.')
# ========================= Monitor Configs ==========================
parser.add_argument('--print-freq', '-p', default=25, type=int,
                    metavar='N', help='print frequency (default: 50) iteration')
parser.add_argument('--eval-freq', '-ef', default=1, type=int,
                    metavar='N', help='evaluation frequency (default: 50) epochs')
# ========================= Runtime Configs ==========================
parser.add_argument('-j', '--workers', default=0, type=int, metavar='N',
                    help='number of data loading workers (default: 4)')
parser.add_argument('--resume', default='', type=str, metavar='PATH',
                    help='path to latest checkpoint (default: none)')
parser.add_argument('-t', '--test', dest='test', action='store_true',
                    help='evaluate model on test set')
parser.add_argument('--snapshot_pref', type=str, default="""")
parser.add_argument('--start-epoch', default=0, type=int, metavar='N',
                    help='manual epoch number (useful on restarts)')
parser.add_argument('--gpus', nargs='+', type=int, default=None)
parser.add_argument('--root_log',type=str, default='log')
parser.add_argument('--root_model', type=str, default='model')
parser.add_argument('--root_output',type=str, default='output')
parser.add_argument('--root_tensorboard', type=str, default='runs')
"
OMG-exps/dataloader.py,"import torch.utils.data as data
from collections import OrderedDict
from itertools import islice
import pdb 
from PIL import Image
import os
import os.path
import numpy as np
from numpy.random import randint
import pickle
import glob
from copy import copy
import json
import torch
from torch import nn as nn
from tqdm import tqdm
from mpl_toolkits.axes_grid1 import ImageGrid
import matplotlib.pyplot as plt
from transforms import GroupRandomCrop,GroupRandomHorizontalFlip, GroupNormalize, GroupScale, Stack, ToTorchFormatTensor
import torchvision
from utils import Steerable_Pyramid_Phase, get_device
from random import shuffle
class UtteranceRecord(object):
    def __init__(self, video, utter, root_path, data_row, label_name, mini_num_frames = 17):
        self.video = video
        self.utterance = utter if 'mp4' not in utter else utter.split(""."")[0]
        self.mini_num_frames = mini_num_frames
        if not ""_"" in label_name:
            self._label_name = label_name
        else:
            self._label_name = label_name.split(""_"")
        self.root_path = root_path
        self.data_row = data_row
    @property
    def path(self):
        #paths includes all frames (has face)
        if isinstance(self.root_path, list):
            frames = sorted(glob.glob(os.path.join(self.root_path[0], self.video, self.utterance, self.utterance+""_aligned"", '*.bmp')), key = lambda x: int(x.split(""."")[0].split(""_"")[-1]))
            if len(frames)==0:
                frames = sorted(glob.glob(os.path.join(self.root_path[1], self.video, self.utterance, self.utterance+""_aligned"", '*.bmp')), key = lambda x: int(x.split(""."")[0].split(""_"")[-1]))
        else:
            frames = sorted(glob.glob(os.path.join(self.root_path, self.video, self.utterance, self.utterance+""_aligned"", '*.bmp')), key = lambda x: int(x.split(""."")[0].split(""_"")[-1]))
        if len(frames) >=self.mini_num_frames: # minimum number of frames to extract Phase
            return frames
        else:
            return None
    @property
    def label(self):
        if isinstance(self._label_name, list):
            return np.array([self.data_row[label_name] for label_name in self._label_name])
        elif isinstance(self._label_name, str):
            label = self.data_row[self._label_name]
            return label
    @property
    def name(self):
        return ""{} {}"".format(self.video, self.utterance)
    def __str__(self):
        return ""{} in {}, {}: {}, number of facial frames:{}"".format(self.utterance, self.video, self._label_name, self.label, len(self.path))
def phase_2_output( phase_batch, steerable_pyramid,return_phase=False):
    """"""
    phase_batch dim: bs, num_phase, W, H
    """"""
    sp = steerable_pyramid
    num_frames,num_phases, W, H = phase_batch.size()
    coeff_batch = sp.build_pyramid(phase_batch)
    assert isinstance(coeff_batch, list)
    phase_batch_0 = sp.extract_phase(coeff_batch[0], return_phase=return_phase)
    num_frames, n_ch, n_ph, W, H= phase_batch_0.size()
    phase_batch_0 = phase_batch_0.view(num_frames, -1, W, H)
    phase_batch_1 = sp.extract_phase(coeff_batch[1], return_phase=return_phase)
    num_frames, n_ch, n_ph, W, H= phase_batch_1.size()
    phase_batch_1 = phase_batch_1.view(num_frames, -1, W, H)
    return phase_batch_0,phase_batch_1
class Face_Dataset(data.Dataset):
    def __init__(self, root_path, pretrained_feature_root, dict_file, label_name, py_level, py_nbands,  
                 sample_rate=4, num_phase = 8, phase_size = 112, test_mode =False, return_phase=False,
                  max_len = 16): # the length distribution: <13s many, 16s: 2, 18s: 1, 19s:1, 23s:1, 24s:1
        """"""
        Args: 
            root_path: the directory that contains all facial images
            dict_file: data dictory
            label_name: arousal or valence or arousal_valence
            max_len: max sequence length for frames in one video (including many utterances)
            sample_rate: the sample rate for RGB images
            num_phase: number of phase images (to be extracted for phase and phase difference)
            phase_size: phase image size
            test_mode: for training, True, otherwise it's False
        """"""
        self.root_path = root_path
        self.pretrained_feature_root = pretrained_feature_root
        self.dict_file= dict_file
        self.label_name = label_name # one of 'arousal', 'valence' and 'EmotionMaxVote'
        self.test_mode = test_mode
        self.sample_rate = sample_rate # frames/second
        self.max_len = max_len*self.sample_rate
        self.fps=30
        self.num_phase = num_phase
        self.return_phase = return_phase
        self._parse_dict()
        self.utterance_list = self.sample_images(self.utterance_list)
        print(""total number of utterances:{}"".format(len(self.utterance_list)))
        self.phase_size = phase_size
        device = get_device('cuda:0')
        self.steerable_pyramid = Steerable_Pyramid_Phase(height=py_level, nbands=py_nbands, scale_factor=2, device=device, extract_level=[1,2], visualize=False)
        if not self.test_mode:
            shuffle(self.utterance_list) 
        seq_lens = [record.length for record in self.utterance_list ]
        unique_seq_lens = np.unique(seq_lens)
        lens_index_list= []
        for length in unique_seq_lens:
            indexes = [id for id, record in enumerate(self.utterance_list) if record.length==length]
            lens_index_list.append(indexes)
        self.indices_list =  lens_index_list
    def _parse_dict(self):
        data_dict = pickle.load(open(self.dict_file,'rb')) if isinstance(self.dict_file, str) else self.dict_file 
        self.utterance_list = list()
        num_videos = 0
        videos = list(data_dict.keys())
        for id, video in tqdm(enumerate(videos)):
            num_videos+=1
            for utter in data_dict[video].keys():
                u_record = UtteranceRecord(video, utter,self.root_path, data_dict[video][utter], self.label_name, mini_num_frames=self.num_phase+1)
                if u_record.path is not None:
                    self.utterance_list.append(u_record)
        print(""total number of videos:{}"".format(num_videos)) 
    def sample_images(self, utterance_list):
        new_utterance_list= []
        for utter_record in tqdm(utterance_list):
            total_frames = utter_record.path
            n_frames = self.fps//self.sample_rate
            if not self.test_mode: # train set has augmentation on sampling image
                max_aug = min(n_frames, len(total_frames))
                augment_utterance_list = []
                for i in range(max_aug-1):
                    frames  = total_frames[i:]
                    sampled_rgb_frames = []
                    sampled_phase_frames = []
                    sampled_rgb_f_index = np.arange(len(frames))[::n_frames]
                    while len(frames) - sampled_rgb_f_index[-1]<self.num_phase+1:
                        sampled_rgb_f_index = sampled_rgb_f_index[:-1] # remove the last rgb frame
                        if len(sampled_rgb_f_index)==0:
                            break
                    if len(sampled_rgb_f_index)==0:
                        break
                    sampled_phase_f_index = np.array([np.arange(id, id+self.num_phase+1) for id in sampled_rgb_f_index])
                    sampled_rgb_frames.extend([frames[id + self.num_phase//2] for id in sampled_rgb_f_index])
                    sampled_phase_frames.extend([np.array([frames[id] for id in ids]) for ids in sampled_phase_f_index]) 
                    length = min(len(sampled_rgb_frames), self.max_len) 
                    if length >0: # make sure that record in augment_utterance_list has the same length
                        setattr(utter_record, 'rgb_frames', sampled_rgb_frames[:length])
                        setattr(utter_record, 'phase_frames', sampled_phase_frames[:length])
                        setattr(utter_record, 'length', len(sampled_phase_frames[:length]))
                        augment_utterance_list.append(utter_record)
                if len(augment_utterance_list)!=0: 
                    augment_utterance_list = augment_utterance_list[::3] # save training time
                    new_utterance_list.extend(augment_utterance_list)
                else:
                    print(""Nothing in augment list"")
            else:
                frames = total_frames
                sampled_rgb_frames = []
                sampled_phase_frames = []
                sampled_rgb_f_index = np.arange(len(frames))[::n_frames]
                while len(frames) - sampled_rgb_f_index[-1]<self.num_phase+1:
                    sampled_rgb_f_index = sampled_rgb_f_index[:-1] # remove the last rgb frame
                    if len(sampled_rgb_f_index)==0:
                        break
                if len(sampled_rgb_f_index)==0:
                    raise ValueError(""Frame length incorrect"")
                sampled_phase_f_index = np.array([np.arange(id, id+self.num_phase+1) for id in sampled_rgb_f_index])
                sampled_rgb_frames.extend([frames[id + self.num_phase//2] for id in sampled_rgb_f_index])
                sampled_phase_frames.extend([np.array([frames[id] for id in ids]) for ids in sampled_phase_f_index]) 
                length = min(len(sampled_rgb_frames),self.max_len) 
                if length>0:
                    setattr(utter_record, 'rgb_frames', sampled_rgb_frames[:length])
                    setattr(utter_record, 'phase_frames', sampled_phase_frames[:length])
                    setattr(utter_record, 'length', len(sampled_phase_frames[:length]))
                    new_utterance_list.append(utter_record)
        return new_utterance_list
                        
    def __getitem__(self, index):
        u_record = self.utterance_list[index]
        rgb_frames = u_record.rgb_frames
        phase_frames = u_record.phase_frames
        label = u_record.label
        while len(rgb_frames)==0:
            id = np.random.randint(len(self.utterance_list))
            u_record = self.utterance_list[id] 
            rgb_frames = u_record.rgb_frames
            phase_frames = u_record.phase_frames

        name = u_record.name
        return_list = self.get(rgb_frames, phase_frames)
        return_list.append(label)
        return_list.append(name)
        return return_list     
    def get(self, rgb_frames, phase_frames):
        assert len(rgb_frames) == len(phase_frames) 
        assert len(rgb_frames)<=self.max_len  
        phase_images = []
        for frames in phase_frames:
            phase_img_list = []
            for frame in frames:
                img = Image.open(frame).convert('L')
                phase_img_list.append(img)
            phase_images.append(phase_img_list)

        if not self.test_mode:
            random_seed = np.random.randint(250)
            W,H = phase_images[0][0].size
            phase_transform = torchvision.transforms.Compose([GroupRandomHorizontalFlip(seed=random_seed),
                                   GroupRandomCrop(size=int(W*0.85), seed=random_seed),
                                   GroupScale(size=self.phase_size),
                                   Stack(),
                                   ToTorchFormatTensor()])
        else:
            phase_transform = torchvision.transforms.Compose([
                                   GroupScale(size=self.phase_size),
                                   Stack(),
                                   ToTorchFormatTensor()])   

        flat_phase_images =[]
        for sublist in phase_images:
            flat_phase_images.extend(sublist)
        flat_phase_images_trans = phase_transform(flat_phase_images)
        phase_images = flat_phase_images_trans.view(len(phase_images), self.num_phase+1, self.phase_size, self.phase_size)
        phase_images = phase_images.type('torch.FloatTensor').cuda()
        phase_batch_0,phase_batch_1 = phase_2_output( phase_images, self.steerable_pyramid, return_phase=self.return_phase)  
        rgb_features = []
        for frame in rgb_frames:
            video = frame.split('/')[-4]
            utter = frame.split(""/"")[-3]
            index = int(frame.split(""/"")[-1].split(""."")[0].split(""_"")[-1])/media/newssd/Aff-Wild_experiments/annotations
            path = os.path.join(self.pretrained_feature_root, video, utter+"".mp4"", ""{:05d}.npy"".format(index))
            try:
                rgb_features.append(np.load(path))
            except:
                raise ValueError(""Incorrect feature path!"")
        return [phase_batch_0,phase_batch_1, np.array(rgb_features)]
    def __len__(self):
        return len(self.utterance_list)

def imshow_grid(images, shape=[2, 9], name='default', save=False):
    """"""Plot images in a grid of a given shape.""""""
    fig = plt.figure()
    grid = ImageGrid(fig, 111, nrows_ncols=shape, axes_pad=0.05)

    size = shape[0] * shape[1]
    for i in range(size):
        if i==17:
            break
        grid[i].axis('off')
        grid[i].imshow(images[i], cmap='gray', vmin=0., vmax=1.)  # The AxesGrid object work as a list of axes.

    plt.show() 
if __name__ == ""__main__"":
    data_dict = '/media/newssd/OMG_experiments/exps/test_dict.pkl'
    feature_root = '/media/newssd/OMG_experiments/Extracted_features/vgg_fer_features_fps=30_pool5'
    root_path = '/media/newssd/OMG_experiments/OMG_OpenFace_nomask/Test'
    label_name = 'arousal'
    train_dataset = Face_Dataset(root_path, feature_root, data_dict, label_name, py_level=4, py_nbands=2, sample_rate = 1, 
                                 num_phase=12, phase_size=48, test_mode=False, return_phase=False)
    #phase_batch0, phase_batch1, rgb_features, label, names = train_dataset[34]
    from Same_Length_Sampler import SameLengthBatchSampler
    train_sampler = SameLengthBatchSampler(train_dataset.indices_list, batch_size=4, drop_last=True, random=True)
    train_loader = torch.utils.data.DataLoader(
        train_dataset, 
       batch_sampler= train_sampler, 
        num_workers=0, pin_memory=False )
    index= 0
    print([len(indices) for indices in train_dataset.indices_list])
    for data_batch in tqdm(train_loader):
        phase_batch0, phase_batch1, rgb_features, label, names = data_batch
        index+=1
        if index >=550:
            print()
    
    
"
OMG-exps/main.py,"import os
import time
import shutil
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.backends.cudnn as cudnn
import torch.optim
from torch.nn.utils import clip_grad_norm_
import numpy as np
from config import parser
args = parser.parse_args()
import pickle
from network import Two_Stream_RNN
from dataloader import Face_Dataset, UtteranceRecord
from sklearn.metrics import mean_squared_error
from torch.autograd import Variable as Variable
import copy
from tqdm import tqdm
import glob
from Same_Length_Sampler import SameLengthBatchSampler
import pandas as pd
class My_loss(torch.nn.Module): 
    def __init__(self):
        super().__init__()   
    def forward(self, x, y): 
        vx = x - torch.mean(x)
        vy = y - torch.mean(y)
        rho =  torch.sum(vx * vy) / (torch.sqrt(torch.sum(torch.pow(vx, 2))) * torch.sqrt(torch.sum(torch.pow(vy, 2))))
        x_m = torch.mean(x)
        y_m = torch.mean(y)
        x_s = torch.std(x)
        y_s = torch.std(y)
        ccc = 2*rho*x_s*y_s/(torch.pow(x_s, 2) + torch.pow(y_s, 2) + torch.pow(x_m - y_m, 2))
        return -ccc
class AverageMeter(object):
    """"""Computes and stores the average and current value""""""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def ccc(y_true, y_pred):
    true_mean = np.mean(y_true)
    pred_mean = np.mean(y_pred)
    v_pred = y_pred - pred_mean
    v_true = y_true - true_mean
    
    rho =  np.sum(v_pred*v_true) / (np.sqrt(np.sum(v_pred**2)) * np.sqrt(np.sum(v_true**2)))
    std_predictions = np.std(y_pred)
    std_gt = np.std(y_true)
    
    ccc = 2 * rho * std_gt * std_predictions / (
        std_predictions ** 2 + std_gt ** 2 +
        (pred_mean - true_mean) ** 2)
    return ccc, rho 

def check_rootfolders():
    """"""Create log and model folder""""""
    folders_util = [args.root_log, args.root_model, args.root_output, args.root_tensorboard]
    folders_util = [""%s/""%(args.save_root) +folder for folder in folders_util]
    for folder in folders_util:
        if not os.path.exists(folder):
            print('creating folder ' + folder)
            os.makedirs(folder)
         
def main():
    root_path = args.root_path
    label_name = args.label_name
    if args.cnn == 'resnet50':
        feature_root = '/media/newssd/OMG_experiments/Extracted_features/resnet50_ferplus_features_fps=30_pool5_7x7_s1'
    elif args.cnn == 'vgg':
        feature_root = '/media/newssd/OMG_experiments/Extracted_features/vgg_fer_features_fps=30_pool5'
    if len(args.store_name)==0:
        args.store_name = '_'.join( [label_name, 
                                     'cnn:{}'.format(args.cnn),
                                     'loss_type:{}'.format(args.loss_type),
                                      'batch_size:{}'.format(args.batch_size), 
                                      'cat_before_gru:{}'.format(args.cat_before_gru),
                                      'freeze:{}'.format(args.freeze),
                                      'fusion:{}'.format(args.fusion)]) 
    if len(args.save_root)==0:
        setattr(args, 'save_root', args.store_name)
    else:
        setattr(args, 'save_root', os.path.join(args.save_root, args.store_name))
    print(""save experiment to :{}"".format(args.save_root))
    check_rootfolders()
    num_class = 1 if not ""_"" in args.label_name else 2
    setattr(args, 'num_class', num_class)
    
    if args.loss_type == 'mse':
        criterion = nn.MSELoss().cuda()
    elif args.loss_type=='ccc':
        criterion = My_loss().cuda()
    else: # another loss is mse or mae
        raise ValueError(""Unknown loss type"")
    L = args.length
    train_dict = pickle.load(open(args.train_dict, 'rb'))
    val_dict = pickle.load(open(args.val_dict, 'rb'))
    train_dict.update(val_dict)
    train_val_dict = copy.copy(train_dict)
    video_names = sorted(list(train_dict.keys()))
    np.random.seed(0)
    video_indexes = np.random.permutation(len(video_names))
    video_names = [video_names[i] for i in video_indexes] 
    if args.test:
        run_5_fold_prediction_on_test_set(feature_root) 
    for i in range(5):
        ###########################  Modify the classifier ###################       
        model = Two_Stream_RNN(mlp_hidden_units=args.hidden_units, phase_size=48, phase_channels=2*L, 
                               phase_hidden_size=256, cat_before_gru=args.cat_before_gru, gru_hidden = 64, gru_num_layers=2, fusion=args.fusion)
        ###########################  Modify the classifier ###################   
        pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(""Total Params: {}"".format(pytorch_total_params))
        phasenet_param = sum(p.numel() for p in model.phase_net.parameters() if p.requires_grad)
        print(""Temporal Stream params: {} ({:.2f})"".format( phasenet_param, phasenet_param/float(pytorch_total_params)))
        mlp_param = sum(p.numel() for p in model.mlp.parameters() if p.requires_grad)
        print(""Spatial Stream params: {} ({:.2f})"".format( mlp_param, mlp_param/float(pytorch_total_params)))
        model.cuda()
        if args.cat_before_gru:
            params_dict = [{'params': model.rnns.parameters(), 'lr':args.lr}, 
                            {'params': model.classifier.parameters(), 'lr':args.lr}, 
                            {'params': model.fusion_module.parameters(), 'lr':args.lr}]
        else:
            params_dict = [{'params': model.rnns_spatial.parameters(), 'lr':args.lr}, 
                           {'params': model.rnns_temporal.parameters(), 'lr':args.lr}, 
                           {'params': model.classifier.parameters(), 'lr':args.lr},
                           {'params': model.fusion_module.parameters(), 'lr':args.lr}]
        if not args.freeze:
            params_dict += [{'params': model.mlp.parameters(), 'lr':args.lr},
                             {'params': model.phase_net.parameters(), 'lr':args.lr}]
        optimizer = torch.optim.SGD(params_dict, # do not set learn rate for mlp and 
                                args.lr,
                                momentum=args.momentum,
                                weight_decay=args.weight_decay) 
        torch.cuda.empty_cache()
        cudnn.benchmark = True  
        length = len(video_names)//5
        # five fold cross validation
        val_video_names = video_names[i*length:(i+1)*length]
        if i==4:
            val_video_names = video_names[i*length:]
        train_video_names = [name for name in video_names if name not in val_video_names]
        train_video_names = video_names #   delete it later
        train_dict = {key:train_val_dict[key] for key in train_video_names}
        val_dict = {key:train_val_dict[key] for key in val_video_names}
        train_dataset = Face_Dataset([os.path.join(root_path,'Train'), os.path.join(root_path,'Validation')], feature_root, train_dict, label_name, py_level=args.py_level, 
                                     py_nbands=args.py_nbands, sample_rate = args.sample_rate, num_phase=L, phase_size=48, test_mode=False,
                                     return_phase=False)
        val_dataset = Face_Dataset([os.path.join(root_path,'Train'), os.path.join(root_path,'Validation')], feature_root, val_dict, label_name, py_level=args.py_level, 
                                   py_nbands=args.py_nbands,  sample_rate = args.sample_rate, num_phase=L, phase_size=48, test_mode=True,
                                  return_phase=False)
        train_batch_sampler = SameLengthBatchSampler(train_dataset.indices_list, batch_size=args.batch_size, drop_last=True)
        val_batch_sampler  = SameLengthBatchSampler(val_dataset.indices_list, batch_size = args.eval_batch_size, drop_last=True, random=False)
        train_loader = torch.utils.data.DataLoader(
            train_dataset, 
            batch_sampler=train_batch_sampler,
            num_workers=args.workers, pin_memory=False)
        val_loader = torch.utils.data.DataLoader(
            val_dataset, 
            batch_sampler=val_batch_sampler,
            num_workers=args.workers, pin_memory=False)
         
        print(""train dataset:{}"".format(len(train_dataset)))
        print(""val dataset:{}"".format(len(val_dataset)))
        log = open(os.path.join(args.save_root, args.root_log, 'fold_{}.txt'.format(i)), 'w')
        output = ""\n Fold: {}\n"".format(i)
        log.write(output)
        log.flush()
        best_loss = 1000
        best_ccc = -100
        val_accum_epochs = 0
        for epoch in range(args.epochs):
            adjust_learning_rate(optimizer, epoch, args.lr_steps)
            train_mean, train_std = train(train_loader, model, criterion, optimizer, epoch, log)
            log_train_mean_std = open(os.path.join(args.save_root, args.root_log, 'mean_std_{}.txt'.format(i)), 'w')
            log_train_mean_std.write(""{} {}"".format(train_mean, train_std))
            log_train_mean_std.flush()
            torch.cuda.empty_cache() 
            if (epoch + 1) % args.eval_freq == 0 or epoch == args.epochs - 1:
                loss_val, ccc_current_val = validate(val_loader, model, criterion, (epoch + 1) * len(train_loader), log, train_mean, train_std)
                is_best_loss = loss_val< best_loss
                best_loss = min(loss_val, best_loss)
                is_best_ccc = ccc_current_val >best_ccc
                best_ccc  = max(ccc_current_val , best_ccc)
                save_checkpoint({
                    'epoch': epoch + 1,
                    'state_dict': model.state_dict(),
                }, is_best_loss, is_best_ccc, filename='fold_{}'.format(i))
                if not is_best_ccc:
                    val_accum_epochs+=1
                else:
                    val_accum_epochs=0
                if val_accum_epochs>=args.early_stop:
                    print(""validation ccc did not improve over {} epochs, stop"".format(args.early_stop))
                    break
    run_5_fold_prediction_on_test_set(feature_root)
def run_5_fold_prediction_on_test_set(feature_root):
    test_dataset = Face_Dataset(os.path.join(args.root_path,'Test'), feature_root, args.test_dict, args.label_name, py_level=args.py_level, 
                               py_nbands=args.py_nbands,  sample_rate = args.sample_rate, num_phase=args.num_phase, phase_size=48, test_mode=True,
                               return_phase=False)
    print(""test dataset:{}"".format(len(test_dataset)))
    test_batch_sampler =  SameLengthBatchSampler(test_dataset.indices_list, batch_size = args.eval_batch_size, drop_last=False, random=False)
    test_loader = torch.utils.data.DataLoader(
        test_dataset, 
        batch_sampler=test_batch_sampler,
        num_workers=args.workers, pin_memory=False)
    for i in range(5):
        file = open(os.path.join(args.save_root, args.root_log, 'mean_std_{}.txt'.format(i)), 'r')
        string = file.readline()
        train_mean, train_std = string.split("" "")
        train_mean = float(train_mean)
        train_std = float(train_std)
        # resume
        model = Two_Stream_RNN(mlp_hidden_units=args.hidden_units, phase_size=48, phase_channels=2*args.num_phase, phase_hidden_size=256, cat_before_gru=args.cat_before_gru)
        model.cuda()
        saved_model_path = os.path.join(args.save_root, args.root_model, 'fold_{}_best_ccc.pth.tar'.format(i))
        checkpoint = torch.load(saved_model_path)
        start_epoch = checkpoint['epoch']
        model.load_state_dict(checkpoint['state_dict'])
        print((""=> loading checkpoint '{}' epoch:{}"".format(saved_model_path, start_epoch)))
        preds, names = test(test_loader, model,train_mean=train_mean, train_std=train_std)
        df= pd.DataFrame()
        df['video'] = pd.Series([n.split("" "")[0] for n in names])
        df['utterance'] = pd.Series([n.split("" "")[1] for n in names])
        df[args.label_name] = pd.Series([v for v in preds])
        df.to_csv(os.path.join(args.save_root, args.root_log, 'test_predictions_{}.csv'.format(i)), index=False)
def train(dataloader, model, criterion, optimizer, epoch, log): 
    batch_time = AverageMeter()
    data_time = AverageMeter()
    losses = AverageMeter()
    end = time.time()
    optimizer.zero_grad()
    model.train()
    targets = []
    for i, data_batch in enumerate(dataloader):
        data_time.update(time.time() - end)
        phase_0, phase_1, rgb_features, label, names  = data_batch
        phase_0 = Variable(phase_0.type('torch.FloatTensor').cuda())
        phase_1 = Variable(phase_1.type('torch.FloatTensor').cuda())
        rgb_features = Variable(rgb_features.type('torch.FloatTensor').cuda())
        label_var = Variable(label.type('torch.FloatTensor').cuda())
        out = model([phase_0, phase_1, rgb_features])
        loss= criterion(out.squeeze(-1), label_var)
        loss.backward()
        optimizer.step() # We have accumulated enought gradients
        optimizer.zero_grad()
        targets.append(label_var.data.cpu().numpy() )
        # measure elapsed time
        batch_time.update(time.time() - end)
        losses.update(loss.item(), label_var.size(0))
        end = time.time()

        if i % args.print_freq == 0:
            output = ('Epoch: [{0}][{1}/{2}], lr: {lr:.6f}\t'
                    'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                    'Data {data_time.val:.3f} ({data_time.avg:.3f})\t'
                    'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                    .format( epoch, i, len(dataloader), batch_time=batch_time,
                        data_time=data_time, loss=losses, lr=optimizer.param_groups[-1]['lr']))
            print(output)
            log.write(output + '\n')
            log.flush()
        torch.cuda.empty_cache()
    targets = np.concatenate([array for array in targets], axis=0)
    train_mean, train_std = np.mean(targets), np.std(targets)
    return train_mean, train_std

def validate(dataloader, model, criterion, iter, log, train_mean=None, train_std=None): 
    batch_time = AverageMeter()
    losses = AverageMeter()
    # switch to evaluate mode
    model.eval()
#    df = pd.DataFrame(columns = ['video','utterance',str(args.label_name)+'_target', str(args.label_name)+'_prediction'])
    end = time.time()
    targets, preds = [], []
    for i, data_batch in enumerate(dataloader):
        phase_0, phase_1, rgb_features, label, names  = data_batch
        if (torch.sum(torch.isnan(phase_0))>0) or (torch.sum(torch.isnan(phase_1))>0) or (torch.sum(torch.isnan(rgb_features))>0):
            print()
        with torch.no_grad():
            phase_0 = Variable(phase_0.type('torch.FloatTensor').cuda())
            phase_1 = Variable(phase_1.type('torch.FloatTensor').cuda())
            rgb_features = Variable(rgb_features.type('torch.FloatTensor').cuda())
            label_var = Variable(label.type('torch.FloatTensor').cuda())
        out = model([phase_0, phase_1, rgb_features])
        targets.append(label_var.data.cpu().numpy() )
        preds.append(out.squeeze(-1).data.cpu().numpy())
        loss = criterion(out.squeeze(-1), label_var)  
        losses.update(loss.item(), label_var.size(0))
#        if np.isnan(losses.avg):
#            print() # caused by batch size =1
        batch_time.update(time.time() - end)
        end = time.time()
        if i % args.print_freq == 0:
            output = ('Test: [{0}/{1}]\t'
                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\t'
                  'Loss {loss.val:.4f} ({loss.avg:.4f})\t'
                  .format(
                   i, len(dataloader), batch_time=batch_time, loss=losses))
            print(output)
            log.write(output + '\n')
            log.flush()
        torch.cuda.empty_cache()
    targets, preds = np.concatenate([array for array in targets], axis=0), np.concatenate([array for array in preds], axis=0)
    mse_func =  mean_squared_error
    ccc_score = ccc(targets, preds)[0]
    mse_loss = mse_func(targets, preds)
    if train_mean is None:
        output = ' Validation : [{0}][{1}], ccc: {ccc_score:.4f} , loss:{loss_mse:.4f}'.format( i, len(dataloader), 
                          ccc_score=ccc_score, loss_mse = loss)  
    else:
        ccc_corr = ccc(targets, correct(preds, train_mean, train_std))[0]
        output = ' Validation : [{0}][{1}], ccc: {ccc_score:.4f}({ccc_corr:.4f}) , mse:{loss_mse:.4f}({loss_mse_c:.4f})'.format( i, len(dataloader), 
                          ccc_score=ccc_score, ccc_corr=ccc_corr, loss_mse = mse_loss, loss_mse_c = mse_func(targets, correct(preds, train_mean, train_std)))  
        ccc_score = ccc_corr
    print(output)
    log.write(output + '\n')
    log.flush() 
    return loss, ccc_score
def test(dataloader, model, train_mean=None, train_std=None): 
    print(""Testing..."")
    # switch to evaluate mode
    model.eval()
    preds = []
    names = []
    for i, data_batch in tqdm(enumerate(dataloader)):
        phase_0, phase_1, rgb_features, label, name_batch  = data_batch
        with torch.no_grad():
            phase_0 = Variable(phase_0.type('torch.FloatTensor').cuda())
            phase_1 = Variable(phase_1.type('torch.FloatTensor').cuda())
            rgb_features = Variable(rgb_features.type('torch.FloatTensor').cuda())
        out = model([phase_0, phase_1, rgb_features])
        preds.append(out.squeeze(-1).data.cpu().numpy())
        names.append(name_batch)
    preds =  np.concatenate([array for array in preds], axis=0)
    names =  np.concatenate([array for array in names], axis=0)
    preds = correct(preds, train_mean, train_std)
    return preds, names
def adjust_learning_rate(optimizer, epoch, lr_steps):
    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""
    decay = 0.1 ** (sum(epoch >= np.array(lr_steps)))
    lr = args.lr * decay
    decay = args.weight_decay
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr 
        param_group['weight_decay'] = decay 
def correct(pred_val_y, train_mean, train_std):
    try:
        val_std = np.std(pred_val_y)
        mean = np.mean(pred_val_y)
        pred_val_y = np.array(pred_val_y)
    except:
        val_std = torch.std(pred_val_y)
        mean = torch.mean(pred_val_y)
    pred_val_y = mean + (pred_val_y - mean) * train_std / val_std
    return pred_val_y
def save_checkpoint(state, is_best_loss, is_best_ccc, filename='fold'):
    torch.save(state, '%s/%s/%s_checkpoint.pth.tar' % (args.save_root, args.root_model,filename))       
    if is_best_ccc:
        shutil.copyfile('%s/%s/%s_checkpoint.pth.tar' % (args.save_root, args.root_model, filename),
                        '%s/%s/%s_best_ccc.pth.tar' % (args.save_root, args.root_model, filename)) 
        print(""checkpoint saved to"",  '%s/%s/%s_best_ccc.pth.tar' % (args.save_root, args.root_model,filename)) 
        
        
if __name__ == ""__main__"":
    main()
"
OMG-exps/network.py,"import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import glob
import numpy as np
class Pretrained_Model_Path(object):
    def __init__(self, label_name):
        self.label_name = label_name
    def get_mlp_path(self, five_fold=True):
        if self.label_name=='arousal':
            save_dir = '/media/newssd/OMG_experiments/two_layer_MLP_baseline_new_loss_5_fold/save_5_folds_results/label:arousal_cnn:resnet50_alpha:0_weight_decay:0.0005_momentum:0.9'
        elif self.label_name == 'valence':
            save_dir ='/media/newssd/OMG_experiments/two_layer_MLP_baseline_new_loss_5_fold/save_5_folds_results/label:valence_cnn:resnet50_alpha:0_weight_decay:0.001_momentum:0.9'
        if five_fold:
            return self.five_fold_model_path(save_dir)
        else:
            raise ValueError
    def get_phasenet_path(self, five_fold=True):
        if self.label_name =='arousal':
            save_dir = '/media/newssd/OMG_experiments/Phase_only_2DCNN_new_loss/sample_rate=1/phase_size=48/alpha=4/L=12/arousal_ccc_mse_py:4-2_loss_type:ccc_mse_gradient_accumulation_steps:1_batch_size:32_max_len:50_sample_rate:1_phase_size:48_alpha:4_L:12/model'
        elif self.label_name == 'valence':
            save_dir = '/media/newssd/OMG_experiments/Phase_only_2DCNN_new_loss/sample_rate=1/phase_size=48/alpha=4/L=12/valence_ccc_mse_py:4-2_loss_type:ccc_mse_gradient_accumulation_steps:1_batch_size:8_max_len:50_sample_rate:1_phase_size:48_alpha:4_L:12/model'
        if five_fold==True:
            return self.five_fold_model_path(save_dir)
        else:
            raise ValueError
        
    def five_fold_model_path(self, root_dir):
        model_paths = []
        for i in range(5):
            sub_folder = os.path.join(root_dir, 'fold_{}'.format(i))
            model_path = glob.glob(os.path.join(sub_folder, '*best_ccc*'))
            assert len(model_path)==1
            model_paths.append(model_path[0])
        return model_paths
class PhaseNet(nn.Module):
    def __init__(self, input_size, num_channels, hidden_size, output_dim , feature=False):
        super(PhaseNet,self).__init__()
        # input size : 2**i times 6 or 7
        if input_size not in [48, 96, 112]:
               raise ValueError(""Incorrect input size"")
        if input_size==48:
            num_conv_layers = 3
        else:
            num_conv_layers = 4
        if input_size==48 or input_size==96:
            last_conv_width = 6
        else:
            last_conv_width = 7
        self.conv_net = []
        for i in range(num_conv_layers):
            if i==0:
                self.conv_net.append(self._make_conv_layer(num_channels, 2**(i+6), kernel_size=3, stride=2))
            elif i==1:
                self.conv_net.append(self._make_conv_layer(num_channels+2**(i-1+6), 2**(i+6), kernel_size=3, stride=2))
            else:
                self.conv_net.append(self._make_conv_layer(2**(i-1+6), 2**(i+6), kernel_size=3, stride=2))
        last_conv_dim = 2**(i+6)
        self.conv_net = nn.ModuleList(self.conv_net)
        self.dropout = nn.Dropout2d(p=0.2)
        self.avgpool = nn.AvgPool2d(kernel_size=[last_conv_width, last_conv_width])
        fc4 = nn.Linear(last_conv_dim,hidden_size)
        fc5 = nn.Linear(hidden_size, output_dim)
        final_norm = nn.BatchNorm1d(1, eps=1e-6, momentum=0.1) # because 32/2400 =0.01
        self.mlp = nn.Sequential(fc4,
                                 nn.ReLU(inplace=True),
                                 nn.Dropout(0.2),
                                 fc5,
                                 final_norm
                                 )
        self.feature = feature
    def _make_conv_layer(self, in_c, out_c, kernel_size = 3, stride = 2):
        ks = kernel_size 
        conv_layer = nn.Sequential(
        nn.Conv2d(in_c, out_c, kernel_size=(ks, ks), padding=ks//2),
        nn.BatchNorm2d(out_c, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
        nn.ReLU(inplace=True),
        nn.Conv2d(out_c, out_c, kernel_size=(ks, ks), padding=ks//2,stride=stride),
        nn.BatchNorm2d(out_c, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
        nn.ReLU(inplace=True),
        )
        return conv_layer
    def forward(self, data_level0, data_level1):
        bs, num_frames,  num_channel, W0, H0 = data_level0.size()
        bs, num_frames, num_channel,  W1, H1 = data_level1.size()
        trans_data_level0 = data_level0.view(bs*num_frames, num_channel, W0, H0 )
        trans_data_level1 = data_level1.view(bs* num_frames, num_channel,  W1, H1)
        conv1 = self.conv_net[0](trans_data_level0)
        conv_out = torch.cat([conv1, trans_data_level1], dim=1) 
        for layer in self.conv_net[1:]:
            conv_out = self.dropout(layer(conv_out))
        avgpool = self.avgpool(conv_out)
        if self.feature:
            feature = avgpool.view(bs*num_frames, -1)
            for i, layer in enumerate(self.mlp):
                feature = layer(feature)
                if i==2:
                    return feature.view(bs, num_frames, -1)
        else:
            avgpool = avgpool.view(bs,num_frames, -1)
            avgpool_mean = avgpool.mean(1)
            out = self.mlp(avgpool_mean)
            return out
class MLP(nn.Module):
    def __init__(self, hidden_units, dropout=0.5, feature=False):
        super(MLP, self).__init__()
        self.feature = feature
        fc_list = []
        for i in range(len(hidden_units)-2):   
            fc_list += [nn.Linear(hidden_units[i], hidden_units[i+1]),
                       nn.ReLU(inplace=True),
                       nn.BatchNorm1d(hidden_units[i+1]),
                       nn.Dropout(dropout)]
        self.feature = feature
        self.fc = nn.Sequential(*fc_list) 
        self.classifier = nn.Sequential(nn.Linear(hidden_units[-2], hidden_units[-1]))
    def forward(self, input_tensor):
        out = self.fc(input_tensor)
        if not self.feature:
            out = self.classifier(out)
        return out
class  Two_Stream_RNN(nn.Module):
    def __init__(self, mlp_hidden_units, phase_size, phase_channels, phase_hidden_size, gru_hidden = 128, gru_num_layers=1, cat_before_gru=True, label_name='arousal', fusion='cat'):
        super(Two_Stream_RNN, self).__init__()
        self.mlp = MLP(mlp_hidden_units, feature=True)
        self.phase_net = PhaseNet(phase_size, phase_channels, phase_hidden_size, 1, feature=True)
        pretrained_path = Pretrained_Model_Path(label_name)
        mlp_paths =pretrained_path.get_mlp_path(five_fold=True)
        phasenet_paths = pretrained_path.get_phasenet_path(five_fold=True)
        self.mlp = self.load_model_weights(self.mlp, mlp_paths)
        self.phase_net = self.load_model_weights(self.phase_net, phasenet_paths)
        self.cat_before_gru = cat_before_gru
        self.fusion = fusion
        self.dropout = nn.Dropout(0.5)
        self.gru_hidden = gru_hidden
        self.gru_num_layers = gru_num_layers
        dp = 0.5 if self.gru_num_layers>1 else 0.
        if self.cat_before_gru:
            f_dim = 256
            self.rnns = nn.GRU(f_dim, self.gru_hidden, num_layers=self.gru_num_layers, bidirectional=True, dropout=dp)
            rnn_out_dim = self.gru_hidden
        else:
            f_dim = 256
            self.rnns_spatial = nn.GRU(f_dim, self.gru_hidden, num_layers=self.gru_num_layers, bidirectional=True, dropout=dp)
            self.rnns_temporal = nn.GRU(f_dim, self.gru_hidden, num_layers=self.gru_num_layers, bidirectional=True, dropout=dp)
            rnn_out_dim = self.gru_hidden*2
        self.classifier = nn.Sequential(nn.Dropout(0.5),
                                        nn.Linear(rnn_out_dim, 1),
                                        nn.BatchNorm1d(1))
    def load_model_weights(self, model, model_path):
        if isinstance(model_path, list):
            state_dict = {}
            state_dicts = []
            for m_p in model_path:
                ckp = torch.load(m_p)
                net_key = [key for key in ckp.keys() if key !='epoch'][0]
                m_state_dict = ckp[net_key]
                state_dicts.append(m_state_dict)
            for key in state_dicts[0].keys():
                try:
                    state_dict[key] = torch.mean(torch.stack([state_dicts[i][key] for i in range(len(model_path))], dim=0),dim=0)
                except:
                    #num_batches_tracked is int64 tensor, cannot use torch.mean
                    state_dict[key] = torch.mean(torch.stack([state_dicts[i][key].type('torch.FloatTensor') for i in range(len(model_path))], dim=0),dim=0)
                    state_dict[key] = state_dict[key].type('torch.LongTensor')
        else:
            ckp = torch.load(model_path)
            net_key = [key for key in ckp.keys() if (key !='epoch') and (key !='iter')][0]
            state_dict = ckp[net_key]
        model.load_state_dict(state_dict)
        return model
    def forward(self, data):
        phase_0 , phase_1, rgb_features = data
        bs, num_frames = phase_0.size(0), phase_0.size(1)
        rgb_features = rgb_features.view(bs* num_frames, -1)
        features_cnn = self.mlp(rgb_features)
        features_cnn = features_cnn.view(bs, num_frames,-1)
        features_phase = self.phase_net(phase_0, phase_1)
        if self.cat_before_gru:
           if self.fusion=='cat':
               features = torch.cat([features_cnn, features_phase], dim=-1)
            outputs_rnns = self.dropout(features)
            outputs_rnns, _ = self.rnns(outputs_rnns)
            outputs_rnns = F.relu(outputs_rnns)
            outputs_rnns = (outputs_rnns[:,:,:self.gru_hidden]+outputs_rnns[:,:,self.gru_hidden:])
        else:
            outputs_rnn_spatial = self.dropout(features_cnn)
            outputs_rnn_spatial, _ = self.rnns_spatial(outputs_rnn_spatial)
            outputs_rnn_spatial = F.relu(outputs_rnn_spatial)
            outputs_rnn_spatial = (outputs_rnn_spatial[:,:,:self.gru_hidden] + outputs_rnn_spatial[:,:,self.gru_hidden:])
            outputs_rnn_temporal = self.dropout(features_phase)
            outputs_rnn_temporal, _ = self.rnns_temporal(outputs_rnn_temporal)
            outputs_rnn_temporal = F.relu(outputs_rnn_temporal)
            outputs_rnn_temporal = (outputs_rnn_temporal[:,:,:self.gru_hidden] + outputs_rnn_temporal[:,:,self.gru_hidden:])
            if self.fusion=='cat':
                outputs_rnns = torch.cat([outputs_rnn_spatial, outputs_rnn_temporal], dim=-1)
        outputs_rnns = outputs_rnns[:,-1,:]
        out = self.classifier(outputs_rnns)
        return out

if __name__ == ""__main__"":
    root_path = '/media/newssd/OMG_experiments/OMG_OpenFace_nomask'
    feature_root = '/media/newssd/OMG_experiments/Extracted_features/resnet50_ferplus_features_fps=30_pool5_7x7_s1'
    label_name = 'arousal'
    from dataloader import Face_Dataset
    import os
    from Same_Length_Sampler import SameLengthBatchSampler
    L= 12
    batch_size = 16
    test_dataset = Face_Dataset(os.path.join(root_path,'Test'), feature_root, ""../exps/test_dict.pkl"", label_name, py_level=4, 
                               py_nbands=2,  sample_rate = 1, num_phase=L, phase_size=48, test_mode=True, return_phase=False)
    print(""test dataset:{}"".format(len(test_dataset)))
    test_batch_sampler =  SameLengthBatchSampler(test_dataset.indices_list, batch_size = batch_size, drop_last=False, random=False)
    test_loader = torch.utils.data.DataLoader(
        test_dataset, 
        batch_sampler=test_batch_sampler,
        num_workers=0, pin_memory=False)
    model = Two_Stream_RNN([2048, 256, 256, 1], 48, 12*2, 256, cat_before_gru=False, gru_hidden = 128, gru_num_layers=1, fusion='cat')
    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(""Total Params: {}"".format(pytorch_total_params))
    model.cuda()
    
    for i, data in enumerate(test_loader):
        phase_0 , phase_1, rgb_features, labels, names = data
        phase_0 = phase_0.type('torch.FloatTensor').cuda()
        phase_1 = phase_1.type('torch.FloatTensor').cuda()
        rgb_features = rgb_features.type('torch.FloatTensor').cuda()
        out = model([phase_0, phase_1, rgb_features])
#    
    
                
"
OMG-exps/readme,"
'train_dict.pkl' is a dictionary create from 'omg_TrainVideos.csv'. The structure is:
--video_name_1
  --utterance_1.mp4
    --arousal: 0.
    --valence: 0.
  --utterance_2.mp4
    --arousal: 0.
    --valence: 0.
  ...
--video_name_2
  ...
So are 'val_dict.pkl' and 'test_dict.pkl'

example:
```
python main.py --label_name arousal 
```
  
"
OMG-exps/transforms.py,"import torchvision
import random
from PIL import Image, ImageOps
import numpy as np
import numbers
import math
import torch


class GroupRandomCrop(object):
    def __init__(self, size, seed=None):
        if isinstance(size, numbers.Number):
            self.size = (int(size), int(size))
        else:
            self.size = size
        self.seed = seed
    def __call__(self, img_group):

        w, h = img_group[0].size
        th, tw = self.size

        out_images = list()
        if self.seed is not None:
            random.seed(self.seed)
        x1 = random.randint(0, w - tw)
        y1 = random.randint(0, h - th)

        for img in img_group:
            assert(img.size[0] == w and img.size[1] == h)
            if w == tw and h == th:
                out_images.append(img)
            else:
                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))

        return out_images

class GroupRandomHorizontalFlip(object):
    """"""Randomly horizontally flips the given PIL.Image with a probability of 0.5
    """"""
    def __init__(self, seed=None):
        self.seed = seed

    def __call__(self, img_group, is_flow=False):
        if self.seed is not None:
            random.seed(self.seed)
        v = random.random()
        if v < 0.5:
            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]
            return ret
        else:
            return img_group


class GroupNormalize(object):
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def __call__(self, tensor):
        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))
        rep_std = self.std * (tensor.size()[0]//len(self.std))

        # TODO: make efficient
        for t, m, s in zip(tensor, rep_mean, rep_std):
            t.sub_(m).div_(s)

        return tensor


class GroupScale(object):
    """""" Rescales the input PIL.Image to the given 'size'.
    'size' will be the size of the smaller edge.
    For example, if height > width, then image will be
    rescaled to (size * height / width, size)
    size: size of the smaller edge
    interpolation: Default: PIL.Image.BILINEAR
    """"""

    def __init__(self, size, interpolation=Image.LANCZOS):
        self.worker = torchvision.transforms.Resize(size, interpolation)

    def __call__(self, img_group):
        return [self.worker(img) for img in img_group]



class Stack(object):

    def __init__(self, roll=False):
        self.roll = roll

    def __call__(self, img_group):
        if img_group[0].mode == 'L':
            return np.stack(img_group, axis=2)
        elif img_group[0].mode == 'RGB':
            if self.roll:
                return np.stack([np.array(x)[:, :, ::-1] for x in img_group], axis=3)
            else:
                return np.stack(img_group, axis=3)


class ToTorchFormatTensor(object):
    """""" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]
    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] """"""
    def __init__(self, div=True):
        self.div = div

    def __call__(self, pic):
        if isinstance(pic, np.ndarray):
            # handle numpy array
            if len(pic.shape)==3:
                W, H, n_frames =pic.shape
                img = torch.from_numpy(pic).permute(2, 0, 1).contiguous() #frames, W, H
            elif len(pic.shape)==4:
                W, H, channel, n_frames = pic.shape
                img = torch.from_numpy(pic).permute(3, 2, 0, 1).contiguous() #frames, channels, W, H
        else:
            raise ValueError(""input image has to be ndarray!"")
        return img.float().div(255) if self.div else img.float()


class IdentityTransform(object):

    def __call__(self, data):
        return data


if __name__ == ""__main__"":
    trans = torchvision.transforms.Compose([
        GroupScale(256),
        GroupRandomCrop(224),
        Stack(),
        ToTorchFormatTensor(),
        GroupNormalize(
            mean=[.485, .456, .406],
            std=[.229, .224, .225]
        )]
    )

    im = Image.open('../tensorflow-model-zoo.torch/lena_299.png')

    color_group = [im] * 3
    rst = trans(color_group)

    gray_group = [im.convert('L')] * 9
    gray_rst = trans(gray_group)

    trans2 = torchvision.transforms.Compose([
        Stack(),
        ToTorchFormatTensor(),
        GroupNormalize(
            mean=[.485, .456, .406],
            std=[.229, .224, .225])
    ])
    print(trans2(color_group))
"
OMG-exps/utils.py,"import os
import sys
import six
import torch
from os.path import join as pjoin
from steerable.SCFpyr_PyTorch import SCFpyr_PyTorch
import numpy as np
import math
from PIL import Image
from torch.nn import functional as F
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
from matplotlib import cm
from time import time
def load_module_2or3(model_name, model_def_path):
    """"""Load model definition module in a manner that is compatible with
    both Python2 and Python3

    Args:
        model_name: The name of the model to be loaded
        model_def_path: The filepath of the module containing the definition

    Return:
        The loaded python module.""""""
    if six.PY3:
        import importlib.util
        spec = importlib.util.spec_from_file_location(model_name, model_def_path)
        mod = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(mod)
    else:
        import importlib
        dirname = os.path.dirname(model_def_path)
        sys.path.insert(0, dirname)
        module_name = os.path.splitext(os.path.basename(model_def_path))[0]
        mod = importlib.import_module(module_name)
    return mod
def load_model(model_name, MODEL_DIR):
    """"""Load imoprted PyTorch model by name

    Args:
        model_name (str): the name of the model to be loaded

    Return:
        nn.Module: the loaded network
    """"""
    model_def_path = pjoin(MODEL_DIR, model_name + '.py')
    weights_path = pjoin(MODEL_DIR, model_name + '.pth')
    mod = load_module_2or3(model_name, model_def_path)
    func = getattr(mod, model_name)
    net = func(weights_path=weights_path)
    return net
def extract_from_batch(coeff_batch, example_idx=0, symmetry = True):
    '''
    Given the batched Complex Steerable Pyramid, extract the coefficients
    for a single example from the batch. Additionally, it converts all
    torch.Tensor's to np.ndarrays' and changes creates proper np.complex
    objects for all the orientation bands. 

    Args:
        coeff_batch (list): list containing low-pass, high-pass and pyr levels
        example_idx (int, optional): Defaults to 0. index in batch to extract
    
    Returns:
        list: list containing low-pass, high-pass and pyr levels as np.ndarray
    '''
    if not isinstance(coeff_batch, list):
        raise ValueError('Batch of coefficients must be a list')
    coeff = []  # coefficient for single example
    for coeff_level in coeff_batch:
        if isinstance(coeff_level, torch.Tensor):
            # Low- or High-Pass
            coeff_level_numpy = coeff_level[example_idx].cpu().numpy()
            if symmetry:
                W, H = coeff_level_numpy.shape
                coeff_level_numpy = coeff_level_numpy[:W//2, :H//2]
            coeff.append(coeff_level_numpy)
        elif isinstance(coeff_level, list):
            coeff_orientations_numpy = []
            for coeff_orientation in coeff_level:
                coeff_orientation_numpy = coeff_orientation[example_idx].cpu().numpy()
                coeff_orientation_numpy = coeff_orientation_numpy[:,:,0] + 1j*coeff_orientation_numpy[:,:,1]
                if symmetry:
                    W, H = coeff_orientation_numpy.shape
                    coeff_orientation_numpy= coeff_orientation_numpy[:W//2, :H//2]
                coeff_orientations_numpy.append(coeff_orientation_numpy)
            coeff.append(coeff_orientations_numpy)
        else:
            raise ValueError('coeff leve must be of type (list, torch.Tensor)')
    return coeff
def get_device(device='cuda:0'):
    assert isinstance(device, str)
    num_cuda = torch.cuda.device_count()

    if 'cuda' in device:
        if num_cuda > 0:
            # Found CUDA device, use the GPU
            return torch.device(device)
        # Fallback to CPU
        print('No CUDA devices found, falling back to CPU')
        device = 'cpu'

    if not torch.backends.mkl.is_available():
        raise NotImplementedError(
            'torch.fft on the CPU requires MKL back-end. ' +
            'Please recompile your PyTorch distribution.')
    return torch.device('cpu')
def make_grid_coeff(coeff, normalize=True):
    '''
    Visualization function for building a large image that contains the
    low-pass, high-pass and all intermediate levels in the steerable pyramid. 
    For the complex intermediate bands, the real part is visualized.
    
    Args:
        coeff (list): complex pyramid stored as list containing all levels
        normalize (bool, optional): Defaults to True. Whether to normalize each band
    
    Returns:
        np.ndarray: large image that contains grid of all bands and orientations
    '''
    M, N = coeff[1][0].shape
    Norients = len(coeff[1])
    out = np.zeros((M * 3 - coeff[-1].shape[0], Norients * N *2))
    currentx, currenty = 0, 0
    m, n = coeff[0].shape
    out[currentx: currentx+m, currenty: currenty+n] = 255 * coeff[0]/coeff[0].max()
    currentx, currenty = m, 0
    for i in range(1, len(coeff[:-1])):
        for j in range(len(coeff[1])):
            tmp_real = coeff[i][j].real
            tmp_imag = coeff[i][j].imag
            phase = np.arctan2(tmp_imag,tmp_real)
            amp = np.sqrt(np.power(tmp_imag,2) + np.power(tmp_real, 2))
            m, n = tmp_real.shape
            if normalize:
                amp = 255*(amp-amp.min())/(amp.max()-amp.min())
                phase = 255*(phase - phase.min())/(phase.max()-phase.min())
            amp[m-1,:] = 255
            amp[:,n-1]=255
            phase[m-1,:]=255
            phase[:,n-1] = 255
            out[currentx:currentx+m, currenty:currenty+n] = amp
            out[currentx:currentx+m, currenty+n:currenty+2*n] = phase
            currenty += 2*n
        currentx += m
        currenty = 0

    m, n = coeff[-1].shape
    out[currentx: currentx+m, currenty: currenty+n] = 255 * coeff[-1]/coeff[-1].max()
    out[0,:] = 255
    out[:,0] = 255
    return out.astype(np.uint8)
def rgb2gray(rgb):
    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])
#def show_image_3D(image_2d, save_PATH=None):
#    if save_PATH is not None:
#        if not os.path.isdir(os.path.dirname(save_PATH)):
#            os.makedirs(os.path.dirname(save_PATH))
#    M,N = image_2d.shape[:2]
#    X, Y = range(1, M+1), range(1, N+1)
#    Xm, Ym = np.meshgrid(X, Y)
#    fig = plt.figure()
#    ax = Axes3D(plt.gcf())
#    surf = ax.plot_surface(Xm, Ym, image_2d, cmap=cm.coolwarm,)
#    fig.colorbar(surf, shrink=0.5, aspect=10)
#    if save_PATH is not None:
#        plt.savefig(save_PATH)
#    plt.show()
#def windowing_batch(img_batch, device=None): #reference: https://blogs.mathworks.com/steve/2009/12/04/fourier-transform-visualization-using-windowing/
#    M,N  =img_batch.size()[-2:]
#    w1 = np.expand_dims(np.cos(np.linspace(-np.pi/2, np.pi/2, M)), axis=0)
#    w2 = np.expand_dims(np.cos(np.linspace(-np.pi/2, np.pi/2, N)), axis=0)
##    np.copyto(w1, 0.5, where=w1>1/2.)
##    w1 = w1*2
##    np.copyto(w2, 0.5, where=w2>1/2.)
##    w2  = w2*2
#    window = np.dot(w1.T, w2)
#    window = torch.from_numpy(window).type('torch.FloatTensor').to(device)
#    img_batch= torch.mul(img_batch, window)
#    return img_batch
#def extract_phase_mag_from_coeff(coeff):
#    '''
#    Extracting the phase and magnitude for all intermediate levels in the steerable pyramid. 
#    
#    Args:
#        coeff (list): complex pyramid stored as list containing all levels
#    
#    Returns:
#        torch.Tensor (list): magnitude and phase are stacked in the same dimension
#    '''
#    bs, M, N, _ = coeff[1][0].shape
#    inter_coeff = coeff[1:-1]
#    returns = []
#    for coeff_level in inter_coeff:
#        new_coeff_level = []
#        for subband in coeff_level:
#            subband_ = torch.unbind(subband, -1)
#            subband_real, subband_imag = subband_
#            # computing local phase at each scale and orientation
#            subband_phase = torch.atan2(subband_imag, subband_real) # -pi to pi
#            subband_mag = torch.sqrt(torch.pow(subband_real,2)+torch.pow(subband_imag,2))
#            subband_polar = torch.stack((subband_mag, subband_phase),-1)
#            new_coeff_level.append(subband_polar)
#        returns.append(new_coeff_level)
#    returns.insert(0, coeff[0]) # high pass response real
#    returns.append(coeff[-1]) # low pass response
#    return returns
def torch_unwrap(tensor, discont=math.pi, dim=-1):
    nd = len(tensor.size())
    dd = torch_diff(tensor, dim=dim)
    slice1 = [slice(None, None)]*nd     # full slices
    slice1[dim] = slice(1, None)
    slice1 = tuple(slice1)
    PI = math.pi
    ddmod = torch.fmod(dd + PI, 2*PI) - PI
    id1 = (ddmod == -PI) & (dd > 0)
    ddmod[id1] = PI
    ph_correct = ddmod - dd
    id2 = torch.abs(dd) < discont
    ph_correct[id2] = 0
    up = tensor.clone().detach()
    up[slice1] = tensor[slice1] + ph_correct.cumsum(dim=dim)
    return up
def torch_diff(tensor,n=1, dim=-1):
    """"""
    tensor : Input Tensor
    n : int, optional
        The number of times values are differenced. If zero, the input
        is returned as-is.
    axis : int, optional
        The axis along which the difference is taken, default is the
        last axis.
    """"""
    nd = len(tensor.size())
    slice1 = [slice(None)] * nd
    slice2 = [slice(None)] * nd
    slice1[dim] = slice(1, None)
    slice2[dim] = slice(None, -1)
    slice1 = tuple(slice1)
    slice2 = tuple(slice2)
    for _ in range(n):
        tensor = tensor[slice1] - tensor[slice2]
    return tensor
    
def amplitude_based_gaussian_blur(mag, phase, g_kernel):
    bs, n_frames, W, H = phase.size()
    mag_phase = torch.mul(mag, phase)
    in_channel = n_frames
    out_channel = n_frames
    m, n = g_kernel.size()
    filters = torch.stack([g_kernel]*out_channel, dim=0)
    filters = torch.unsqueeze(filters, 1) # (output_channel, input_channel/groups, W, H)
    filters =filters.type('torch.FloatTensor').cuda(async=True) if phase.is_cuda else filters
    mag_phase_blurred = F.conv2d(mag_phase, filters, groups = in_channel, padding=m//2)
    
    mag_blurred = F.conv2d(mag, filters, groups = in_channel, padding=m//2)
    result = torch.div(mag_phase_blurred, mag_blurred)
    return result
def amplitude_based_gaussian_blurcoeff_batch_numpy(mag, phase, g_kernel):
    bs, n_frames, W, H = phase.size()
    phase = phase.cpu().numpy()
    mag = mag.cpu().numpy()
    g_kernel = g_kernel.cpu().numpy()
    from scipy.signal import convolve2d
    new_phase = []
    for b in range(bs):
        new_phase_b = []
        for f in range(n_frames):
            p = phase[b,f,...]
            m = mag[b, f,...]
            denoised_phase =  convolve2d(np.multiply(p, m), g_kernel, mode='same')/convolve2d(m, g_kernel, mode='same')
            new_phase_b.append(denoised_phase)
        new_phase.append(new_phase_b)
    new_phase = np.asarray(new_phase)
    return torch.Tensor(new_phase).type('torch.FloatTensor').cuda(async=True)
def gaussian_kernel(std, tap = 11):
    kernel = np.zeros((tap, tap))
    for x in range(tap):
        for y in range(tap):
            x0 = x - tap//2
            y0 = y - tap//2
            kernel[x, y] = np.exp(- (x0**2+y0**2)/(2*std**2))
    return kernel
def symmetric_extension_batch(img_batch):
    #  img_batch, None, W, H (the last two aixes are two dimensional image)
    img_batch_inverse_col = img_batch.clone().detach()
    inv_idx_col = torch.arange(img_batch.size(-1)-1, -1, -1).long()
    img_batch_inverse_col = img_batch_inverse_col[..., :, inv_idx_col]
    img_batch_inverse_row = img_batch.clone().detach()
    inv_idx_row = torch.arange(img_batch.size(-2)-1, -1, -1).long()
    img_batch_inverse_row = img_batch_inverse_row[..., inv_idx_row, :]
    img_batch_inverse_row_col = img_batch_inverse_col.clone().detach()
    img_batch_inverse_row_col = img_batch_inverse_row_col[:,:, inv_idx_row, :]
    img_batch_0 = torch.cat([img_batch, img_batch_inverse_col], dim=-1)
    img_batch_1 = torch.cat([img_batch_inverse_row, img_batch_inverse_row_col], dim=-1)
    new_img_batch = torch.cat([img_batch_0, img_batch_1], dim=-2)
    return new_img_batch
class Steerable_Pyramid_Phase(object):
    def __init__(self, height=5, nbands=4, scale_factor=2, device=None, extract_level=1, visualize=False):
        self.pyramid = SCFpyr_PyTorch(
            height=height, 
            nbands=nbands,
            scale_factor=scale_factor, 
            device=device
        )
        self.height = height
        self.nbands = nbands
        self.scale_factor = scale_factor
        self.device = device
        self.extract_level = extract_level
        self.visualize = visualize
    def build_pyramid(self, im_batch, symmetry = True):
        """"""
        input image batch has 4 dimensions: batch size,  number of phase images, W, H
        """"""
        bs, num_phase_frames, W, H =im_batch.size()
        trans_im_batch = im_batch.view(bs*num_phase_frames, 1, W, H) # the second dim is 1, indicating it's grayscale image
        if symmetry:
            trans_im_batch = symmetric_extension_batch(trans_im_batch)
        #tic= time()
        coeff_batch = self.pyramid.build(trans_im_batch)
        #print(""process {} images for {}"".format(bs*num_phase_frames, time()-tic))
        if not isinstance(coeff_batch, list):
            raise ValueError('Batch of coefficients must be a list')

        if self.visualize :
            example_id = 10 # the 10th image from number of phase images
            example_coeff = extract_from_batch(coeff_batch , example_id, symmetry)
            example_coeff = make_grid_coeff(example_coeff)
            example_coeff = Image.fromarray(example_coeff)
            example_img = trans_im_batch[example_id,0,...].cpu().numpy()
            example_img = Image.fromarray(255*example_img/example_img.max())
            example_img.show()
            example_img_remove_symm = trans_im_batch[example_id,0,...].cpu().numpy()
            example_img_remove_symm = 255*example_img_remove_symm/example_img_remove_symm.max()
            if symmetry:
                W, H = example_img_remove_symm.shape
                example_img_remove_symm = example_img_remove_symm[:W//2, :H//2]
                example_img_remove_symm = Image.fromarray(example_img_remove_symm)
                example_img_remove_symm.show()
            example_coeff.show()
        if isinstance(self.extract_level, int):
            extr_level_coeff_batch = self.extract_coeff_level(self.extract_level, coeff_batch)
            W, H, _  = extr_level_coeff_batch.size()[-3:]
            nbands = extr_level_coeff_batch.size()[0]
            extr_level_coeff_batch = extr_level_coeff_batch.view(nbands, bs, num_phase_frames,  W, H, 2)
            extr_level_coeff_batch = extr_level_coeff_batch.permute(1, 0, 2, 3, 4, 5 ).contiguous()
            if symmetry:
                extr_level_coeff_batch = extr_level_coeff_batch[..., :W//2, :H//2, :]
        elif isinstance(self.extract_level, list):
            extr_level_coeff_batch = []
            for level in self.extract_level:
                level_coeff_batch = self.extract_coeff_level(level, coeff_batch)
                W, H, _  = level_coeff_batch.size()[-3:]
                nbands = level_coeff_batch.size()[0]
                level_coeff_batch = level_coeff_batch.view(nbands, bs, num_phase_frames,  W, H, 2)
                level_coeff_batch = level_coeff_batch.permute(1, 0, 2, 3, 4, 5 ).contiguous()
                if symmetry:
                    level_coeff_batch = level_coeff_batch[..., :W//2, :H//2, :]
                extr_level_coeff_batch.append(level_coeff_batch)
        return extr_level_coeff_batch
    def extract_coeff_level(self, level, coeff_batch):
        extr_level_coeff_batch = coeff_batch[level]
        assert isinstance(extr_level_coeff_batch, list)
        extr_level_coeff_batch = torch.stack(extr_level_coeff_batch, 0)
        return extr_level_coeff_batch
    def extract_phase(self, coeff_batch, return_phase=False, return_both = False):
        """"""
        coeff batch has dimension: batch size, nbands, number phase frames (17), W, H, 2   (2 is for real part and imaginary part) 
        """"""
        bs, n_bands, n_phase_frames, W,H,_ = coeff_batch.size()
        trans_coeff_batch = coeff_batch.view(bs*  n_bands* n_phase_frames, W, H, -1)
        real_coeff_batch, imag_coeff_batch = torch.unbind(trans_coeff_batch, -1)
        phase_batch = torch.atan2(imag_coeff_batch, real_coeff_batch)
        mag_batch = torch.sqrt(torch.pow(imag_coeff_batch, 2)+torch.pow(real_coeff_batch, 2))
        phase_batch = phase_batch.view(bs*n_bands, n_phase_frames, W, H)
        EPS = 1e-10
        mag_batch = mag_batch.view(bs*n_bands, n_phase_frames, W, H) +EPS # TO avoid mag==0
        assert (mag_batch<=0.0).nonzero().size(0)==0
        
        # phase unwrap over time
        phase_batch = torch_unwrap(phase_batch, discont = math.pi, dim=-3)
        # phase denoising (amplitude-based gaussian blur)
        g_kernel = torch.from_numpy(gaussian_kernel(std=2, tap=11))
        #denoised_phase_batch = amplitude_based_gaussian_blur_numpy(mag_batch, phase_batch, g_kernel)
        denoised_phase_batch = amplitude_based_gaussian_blur(mag_batch, phase_batch, g_kernel)
        denoised_phase_batch = denoised_phase_batch.view(bs, n_bands, n_phase_frames, W, H)
        # phase difference 
        phase_difference_batch = torch_diff(denoised_phase_batch, dim=2)
        phase_difference_batch =  phase_difference_batch.view(bs, n_bands, n_phase_frames-1, W, H)
        if self.visualize:
            phase_example = phase_batch.view(bs, n_bands, n_phase_frames, W, H)[0,...]
            mag_example = mag_batch.view(bs, n_bands, n_phase_frames, W, H)[0,...]
            denoised_phase_example = denoised_phase_batch.view(bs, n_bands, n_phase_frames, W, H)[0,...]
            phase_diff_example = phase_difference_batch.view(bs, n_bands, n_phase_frames-1, W, H)[0,...]
            self.show_3D_subplots(phase_example, title=""phase example"", first_k_frames=2)
            self.show_3D_subplots(mag_example, title=""magnitude example"", first_k_frames=2)
            self.show_3D_subplots(denoised_phase_example, title=""denoised phase example"", first_k_frames=2)
            self.show_3D_subplots(phase_diff_example, title=""phase difference example"", first_k_frames=2)
        # denoised phase centered
        mean = denoised_phase_batch.mean(-1).mean(-1)
        mean = mean.unsqueeze(-1).unsqueeze(-1)
        denoised_phase_batch = denoised_phase_batch - mean 
        mean = phase_difference_batch.mean(-1).mean(-1)
        mean = mean.unsqueeze(-1).unsqueeze(-1)
        phase_difference_batch = phase_difference_batch - mean
        phase_difference_batch = torch.clamp(phase_difference_batch, -5*math.pi, 5*math.pi)
        if return_both:
            # remove one phase image
            denoised_phase_batch = denoised_phase_batch[:,:,1:, :]
            assert phase_difference_batch.size() == denoised_phase_batch.size()
            result = self.insert_tensors(phase_difference_batch, denoised_phase_batch, dim=2) 
            result = result.cuda()
            return result
        if return_phase:
            return denoised_phase_batch 
        else:
            return phase_difference_batch
    def insert_tensors(self,t_a, t_b, dim):
        size = list(t_a.size())
        size[dim] = 2*size[dim]
        result = torch.zeros(size)
        length = t_a.size(dim)
        for i in range(length):
            slice0 = [slice(None,None)]*len(size)
            slice0[dim]= slice(i, i+1)
            slice1 = [slice(None,None)]*len(size)
            slice1[dim]= slice(i//2, i//2+1)
            if i%2==0:
                result[slice0] = t_a[slice1]
            else:
                result[slice0] = t_b[slice1]
        return result
    def show_3D_subplots(self, data, title, first_k_frames = None):
        """"""
        data has dimensions: nbands, n_phase_frames, W, H
        """"""
        nbands, n_phase_frames, W, H = data.size()
        m = nbands
        n = first_k_frames if first_k_frames is not None else n_phase_frames
        
        X, Y = range(1, W+1), range(1, H+1)
        Xm, Ym = np.meshgrid(X, Y)
        for i in range(m):
            fig, ax = plt.subplots(nrows=1, ncols=n, subplot_kw={'projection':""3d""})
            for j in range(n):
                img = data[i, j , ...].cpu().numpy()
                surf = ax[j].plot_surface(Xm, Ym,img , rstride=1, cstride=1, cmap=cm.coolwarm,linewidth=0, antialiased=False)
            fig.colorbar(surf, shrink=0.5, aspect=10)
            fig.suptitle(title+"": orientation {}"".format(i))
        plt.show()
            
# def PadSequence(batch):
#     # Let's assume that ""batch"" is a list of tuples: (data0, data1, data2, label, flag, names).
#     # data has dimensionality: bs, num_frames, channels, W, H
#     # Sort the batch in the descending order
#     sorted_batch = sorted(batch, key=lambda x: x[1].shape[0], reverse=True)
# 	# Get each sequence and pad it
#     num_sequences = len(sorted_batch[0])-1
#     list_seqs = []
#     list_of_padded_seqs = []
#     for i in range(num_sequences):
#         if len(sorted_batch[0][i].size())!=1:
#             sequences = [x[i] for x in sorted_batch ]
#         else:
#             # for label or flag
#             sequences = [x[i].unsqueeze(-1) for x in sorted_batch ]
#         sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True).contiguous()
#         list_seqs.append(sequences)
#         list_of_padded_seqs.append(sequences_padded)
#     lengths = torch.LongTensor([len(x) for x in list_seqs[0]])
#     names = [x[-1] for x in sorted_batch] # names, do not need to be padded
#     list_of_padded_seqs.append(names)
#     return list_of_padded_seqs, lengths
            
"
README.md,"# MIMAMO-Net
MIMAMO Net: Integrating Micro- and Macro-motion for Video Emotion Recognition

Paper Link: https://arxiv.org/pdf/1911.09784.pdf

Requirements:

1. Pytorch 0.4.1 (or higher version)
2. Numpy
3. [PyTorchSteerablePyramid](https://github.com/tomrunia/PyTorchSteerablePyramid)
4. [pytorch-benchmarks](https://github.com/albanie/pytorch-benchmarks)
5. [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace)


In this paper, we propose to combine the micro- and macro-motion features to improve video emotion recognition, using a two-stream recurrrent network named MIMAMO (Micro-Macro-Motion) Net. This model structure is shown in the picture:
![alt text](https://github.com/wtomin/MIMA-Net/blob/master/model.png)

To run this project, 

(1) Download the pretrained ResNet50 model from this [webpage](https://www.robots.ox.ac.uk/~albanie/pytorch-models.html), which is pretrained on VGGFACE2 and FER_plus. Make sure the [pytorch-benchmarks](https://github.com/albanie/pytorch-benchmarks) is correctly installed and the pretrained model can be imported.

(2) Use [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace) toolkit to crop and align faces in videos, save aligned faces.

(3) Extracted the Pool5 features of ResNet50 model and save features. Using the python script in './scripts/CNN_feature_extraction.py':
```
python CNN_feature_extraction.py --fps 30 --layer_name pool5_7x7_s1 --save_root Extracted_Features --data_root dir-to-aligned-face
```

(4) Before running experiments on [Aff-wild dataset](https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge/) (or [OMG emotion dataset](https://github.com/knowledgetechnologyuhh/OMGEmotionChallenge)), make sure dataset is downloaded and processed in step (3).

Run scripts in 'Aff-wild-exps' or 'OMG-exps'.


"
api/examples/utterance_1.mp4,
api/mimamo_net.py,"import torch
import torch.nn as nn
import torch.nn.functional as F
import glob
import os
class MLP(nn.Module):
    def __init__(self, hidden_units, dropout=0.3):
        super(MLP, self).__init__()
        input_feature_dim = hidden_units[0]
        num_layers = len(hidden_units)-1
        assert num_layers>0
        assert hidden_units[-1]==256
        fc_list = []
        for hidden_dim in hidden_units[1:]:
            fc_list += [ nn.Dropout(dropout),
                        nn.Linear(input_feature_dim, hidden_dim),
                        nn.BatchNorm1d(hidden_dim),
                        nn.ReLU(inplace=True)
                        ]
            input_feature_dim = hidden_dim
        self.mlp = nn.Sequential(*fc_list)
    def forward(self, input_tensor):
        bs, num_frames, feature_dim = input_tensor.size()
        input_tensor = input_tensor.view(bs*num_frames, feature_dim)
        out = self.mlp(input_tensor)
        return out.view(bs, num_frames, -1)
class PhaseNet(nn.Module):
    def __init__(self, input_size, num_channels, hidden_units=[256, 256, 1] , dropout=0.3, feature=False):
        super(PhaseNet,self).__init__()
        # input size : 2**i times 6 or 7
        if input_size not in [48, 96, 112]:
               raise ValueError(""Incorrect input size"")
        if input_size==48:
            num_conv_layers = 3
        else:
            num_conv_layers = 4
        if input_size==48 or input_size==96:
            last_conv_width = 6
        else:
            last_conv_width = 7
        self.conv_net = []
        for i in range(num_conv_layers):
            if i==0:
                self.conv_net.append(self._make_conv_layer(num_channels, 2**(i+6), kernel_size=3, stride=2))
            elif i==1:
                self.conv_net.append(self._make_conv_layer(num_channels+2**(i-1+6), 2**(i+6), kernel_size=3, stride=2))
            else:
                self.conv_net.append(self._make_conv_layer(2**(i-1+6), 2**(i+6), kernel_size=3, stride=2))
        last_conv_dim = 2**(i+6)
        self.conv_net = nn.ModuleList(self.conv_net)
        self.dropout = nn.Dropout2d(p=0.2)
        self.avgpool = nn.AvgPool2d(kernel_size=[last_conv_width, last_conv_width])
        fc_list =[]
        fc_list += [nn.Linear(last_conv_dim, hidden_units[0]),
                       nn.ReLU(inplace=True),
                       nn.BatchNorm1d(hidden_units[1]),
                       nn.Dropout(dropout)]
        for i in range(0, len(hidden_units)-2):
            fc_list += [nn.Linear(hidden_units[i], hidden_units[i+1]),
                       nn.ReLU(inplace=True),
                       nn.BatchNorm1d(hidden_units[i+1]),
                       nn.Dropout(dropout)]
        self.fc = nn.Sequential(*fc_list)
        final_norm = nn.BatchNorm1d(1, eps=1e-6, momentum=0.1) 
        self.classifier = nn.Sequential(nn.Linear(hidden_units[-2], hidden_units[-1]),
                                 final_norm )
        self.feature = feature
    def _make_conv_layer(self, in_c, out_c, kernel_size = 3, stride = 2):
        ks = kernel_size 
        conv_layer = nn.Sequential(
        nn.Conv2d(in_c, out_c, kernel_size=(ks, ks), padding=ks//2),
        nn.BatchNorm2d(out_c, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
        nn.ReLU(inplace=True),
        nn.Conv2d(out_c, out_c, kernel_size=(ks, ks), padding=ks//2,stride=stride),
        nn.BatchNorm2d(out_c, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),
        nn.ReLU(inplace=True),
        )
        return conv_layer
    def forward(self, data_level0, data_level1):
        bs, num_frames,  num_channel, W0, H0 = data_level0.size()
        bs, num_frames, num_channel,  W1, H1 = data_level1.size()
        trans_data_level0 = data_level0.view(bs*num_frames, num_channel, W0, H0 )
        trans_data_level1 = data_level1.view(bs* num_frames, num_channel,  W1, H1)
        conv1 = self.conv_net[0](trans_data_level0)
        conv_out = torch.cat([conv1, trans_data_level1], dim=1) 
        for layer in self.conv_net[1:]:
            conv_out = self.dropout(layer(conv_out))
        avgpool = self.avgpool(conv_out)
        avgpool = avgpool.view(bs*num_frames, -1)
        out = self.fc(avgpool)
        if self.feature:
            return out
        else:
            out = self.classifier(out)
            return out
class Two_Stream_RNN(nn.Module):
    def __init__(self, mlp_hidden_units=[2048, 256, 256], dropout=0.5, label_name = 'arousal_valence', 
        num_phase=12):
        super(Two_Stream_RNN, self).__init__()
        '''    
        Definition of the MIMAMO Net, consisting of a spatial stream and a temperal stream, followed
         by a RNN.
        Parameters: 
            mlp_hidden_units: list of int, default is [2048, 256, 256]
                The MLP defined to transform the Resnet50 feature from 2048 dimensions to 256 dimensions
            label_name: string, one of {'arousal', 'valence', 'arousal_valence'}
                The label name decides the output layer dimensions
            num_phase: int, default 12
                number of phase difference images, input to the phase net.
        '''
        
        self.mlp = MLP(mlp_hidden_units)
        self.num_phase = num_phase
        self.phasenet = PhaseNet(48, 2*num_phase, hidden_units =[256, 256, 1], dropout=0.3, feature=True)
        self.transform = nn.Sequential(nn.Linear(512, 256),
                                      nn.ReLU(inplace=True),
                                      nn.BatchNorm1d(256),
                                      nn.Dropout(dropout))
        self.rnns = nn.GRU(256, 128, bidirectional=True, num_layers=2, dropout = 0.3)
        self.classifier = nn.Sequential(nn.Dropout(dropout),
                                        nn.Linear(256, len(label_name.split(""_""))),
                                        nn.BatchNorm1d(len(label_name.split(""_""))))
    def load_model_weights(self, model, model_path):
        ckp = torch.load(model_path)
        net_key = [key for key in ckp.keys() if (key !='epoch') and (key !='iter')][0]
        state_dict = ckp[net_key]
        model.load_state_dict(state_dict)
        return model
    def forward(self, phase_data, rgb_data):
        bs, num_frames = rgb_data.size(0), rgb_data.size(1)
        features_cnn = self.mlp(rgb_data)
        features_spatial = features_cnn.view(bs, num_frames, -1)
        phase_0, phase_1 = phase_data
        features_temporal = self.phasenet(phase_0, phase_1)
        features_temporal = features_temporal.view(bs, num_frames, -1)
        features = torch.cat([features_spatial, features_temporal], dim=-1)
        features = self.transform(features.view(bs*num_frames, -1))
        features = features.view(bs, num_frames, -1)
        outputs_rnns,  _ = self.rnns(features)
        outputs_rnns = outputs_rnns.view(bs* num_frames, -1)
        out = self.classifier(outputs_rnns)
        out = out.view(bs, num_frames, -1)
        return out
    
# if __name__ == ""__main__"":

#     from dataloader import Face_Dataset
#     import os
#     root_path = '/media/newssd/Aff-Wild_experiments/Aligned_Faces_train'
#     feature_path = '/media/newssd/Aff-Wild_experiments/Extracted_Features/Aff_wild_train/resnet50_ferplus_features_fps=30_pool5_7x7_s1'
#     annot_dir = '/media/newssd/Aff-Wild_experiments/annotations'
#     video_names = os.listdir(root_path)[:50]
#     train_dataset = Face_Dataset(root_path, feature_path, annot_dir, video_names, label_name='arousal', test_mode=False, num_phase=12, length=64, stride=32)
#     train_loader = torch.utils.data.DataLoader(
#         train_dataset, 
#         batch_size = 2, 
#         num_workers=0, pin_memory=False )
#     model = Two_Stream_RNN(mlp_hidden_units=[2048, 256, 256])
#     model.cuda()
#     model.train()
#     for phase_f, rgb_f, labels, ranges, videos in train_loader:
#         phase_0, phase_1 = phase_f
#         phase_0 = phase_0.type('torch.FloatTensor').cuda()
#         phase_1 = phase_1.type('torch.FloatTensor').cuda()
#         rgb_f = rgb_f.type('torch.FloatTensor').cuda()
#         labels = labels.type('torch.FloatTensor').cuda()
#         out = model([phase_0,phase_1], rgb_f)
        
        

"
api/models/download_models.sh,"wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Lb-Z9iJR7uCtIEa5CEtijyqevesO2tpq' -O model_weights.pth.tar

echo ""MIMAMO Net model weights have been downloaded.""
"
api/phase_difference_extractor.py,"from steerable.SCFpyr_PyTorch import SCFpyr_PyTorch
from steerable.utils import get_device, make_grid_coeff
from utils.phase_utils import *
from PIL import Image
import numpy as np
class Phase_Difference_Extractor(object):
    def __init__(self, height=5, nbands=4, scale_factor=2, 
        extract_level=1, visualize=False):
        '''Phase_Difference_Extractor: A class to do steerable pyramid computation, extract the phase and phase difference
        Usage: 
              build_pyramid(): build complex steerable pyramid coefficients
              extract(): extract phase differences
        Parameters:
            height: int, default 5
                The coefficients levels including low-pass and high-pass
            nbands: int, default 4
                The number of orientations of the bandpass filters
            scale_factor: int, default 2
                Spatial resolution reduction scale scale_factor
            extract_level: int, or list of int numbers, default 1
                If extract_level is an int number, build_pyramid() will only return the coefficients in one level;
                If extract_level is a list, build_pyramid() will only return the coefficients of multiple levels.
            visualize: bool, default False
               If true, the build_pyramid() and extract() will show the processed results.
        '''
    
        self.pyramid = SCFpyr_PyTorch(
            height=height, 
            nbands=nbands,
            scale_factor=scale_factor, 
            device=get_device()
        )
        self.height = height
        self.nbands = nbands
        self.scale_factor = scale_factor
        self.extract_level = extract_level
        self.visualize = visualize
    def build_pyramid(self, im_batch, symmetry = True):
        """"""
        input image batch has 4 dimensions: batch size, number of phase images, W, H
        """"""
        bs, num_phase_frames, W, H =im_batch.size()
        trans_im_batch = im_batch.view(bs*num_phase_frames, 1, W, H) # the second dim is 1, indicating it's grayscale image
        if symmetry:
            trans_im_batch = symmetric_extension_batch(trans_im_batch)
        #tic= time()
        coeff_batch = self.pyramid.build(trans_im_batch)
        #print(""process {} images for {}"".format(bs*num_phase_frames, time()-tic))
        if not isinstance(coeff_batch, list):
            raise ValueError('Batch of coefficients must be a list')

        if self.visualize :
            example_id = 10 # the 10th image from number of phase images
            example_coeff = extract_from_batch(coeff_batch , example_id, symmetry)
            example_coeff = make_grid_coeff(example_coeff)
            example_coeff = Image.fromarray(example_coeff)
            example_img = trans_im_batch[example_id,0,...].cpu().numpy()
            example_img = Image.fromarray(255*example_img/example_img.max())
            example_img.show()
            example_img_remove_symm = trans_im_batch[example_id,0,...].cpu().numpy()
            example_img_remove_symm = 255*example_img_remove_symm/example_img_remove_symm.max()
            if symmetry:
                W, H = example_img_remove_symm.shape
                example_img_remove_symm = example_img_remove_symm[:W//2, :H//2]
                example_img_remove_symm = Image.fromarray(example_img_remove_symm)
                example_img_remove_symm.show()
            example_coeff.show()
        if isinstance(self.extract_level, int):
            extr_level_coeff_batch = self.extract_coeff_level(self.extract_level, coeff_batch)
            W, H, _  = extr_level_coeff_batch.size()[-3:]
            nbands = extr_level_coeff_batch.size()[0]
            extr_level_coeff_batch = extr_level_coeff_batch.view(nbands, bs, num_phase_frames,  W, H, 2)
            extr_level_coeff_batch = extr_level_coeff_batch.permute(1, 0, 2, 3, 4, 5 ).contiguous()
            if symmetry:
                extr_level_coeff_batch = extr_level_coeff_batch[..., :W//2, :H//2, :]
        elif isinstance(self.extract_level, list):
            extr_level_coeff_batch = []
            for level in self.extract_level:
                level_coeff_batch = self.extract_coeff_level(level, coeff_batch)
                W, H, _  = level_coeff_batch.size()[-3:]
                nbands = level_coeff_batch.size()[0]
                level_coeff_batch = level_coeff_batch.view(nbands, bs, num_phase_frames,  W, H, 2)
                level_coeff_batch = level_coeff_batch.permute(1, 0, 2, 3, 4, 5 ).contiguous()
                if symmetry:
                    level_coeff_batch = level_coeff_batch[..., :W//2, :H//2, :]
                extr_level_coeff_batch.append(level_coeff_batch)
        return extr_level_coeff_batch
    def extract_coeff_level(self, level, coeff_batch):
        extr_level_coeff_batch = coeff_batch[level]
        assert isinstance(extr_level_coeff_batch, list)
        extr_level_coeff_batch = torch.stack(extr_level_coeff_batch, 0)
        return extr_level_coeff_batch
    def extract(self, coeff_batch):
        """"""
        coeff batch has dimension: batch size, nbands, number phase frames (17), W, H, 2   (2 is for real part and imaginary part) 
        """"""
        bs, n_bands, n_phase_frames, W,H,_ = coeff_batch.size()
        trans_coeff_batch = coeff_batch.view(bs*  n_bands* n_phase_frames, W, H, -1)
        real_coeff_batch, imag_coeff_batch = torch.unbind(trans_coeff_batch, -1)
        phase_batch = torch.atan2(imag_coeff_batch, real_coeff_batch)
        mag_batch = torch.sqrt(torch.pow(imag_coeff_batch, 2)+torch.pow(real_coeff_batch, 2))
        phase_batch = phase_batch.view(bs*n_bands, n_phase_frames, W, H)
        EPS = 1e-10
        mag_batch = mag_batch.view(bs*n_bands, n_phase_frames, W, H) +EPS # TO avoid mag==0
        assert (mag_batch<=0.0).nonzero().size(0)==0
        
        # phase unwrap over time
        phase_batch = torch_unwrap(phase_batch, discont = math.pi, dim=-3)
        # phase denoising (amplitude-based gaussian blur)
        g_kernel = torch.from_numpy(gaussian_kernel(std=2, tap=11))
        #denoised_phase_batch = amplitude_based_gaussian_blur_numpy(mag_batch, phase_batch, g_kernel)
        denoised_phase_batch = amplitude_based_gaussian_blur(mag_batch, phase_batch, g_kernel)
        denoised_phase_batch = denoised_phase_batch.view(bs, n_bands, n_phase_frames, W, H)
        # phase difference 
        phase_difference_batch = torch_diff(denoised_phase_batch, dim=2)
        phase_difference_batch =  phase_difference_batch.view(bs, n_bands, n_phase_frames-1, W, H)
        if self.visualize:
            phase_example = phase_batch.view(bs, n_bands, n_phase_frames, W, H)[0,...]
            mag_example = mag_batch.view(bs, n_bands, n_phase_frames, W, H)[0,...]
            denoised_phase_example = denoised_phase_batch.view(bs, n_bands, n_phase_frames, W, H)[0,...]
            phase_diff_example = phase_difference_batch.view(bs, n_bands, n_phase_frames-1, W, H)[0,...]
            self.show_3D_subplots(phase_example, title=""phase example"", first_k_frames=2)
            self.show_3D_subplots(mag_example, title=""magnitude example"", first_k_frames=2)
            self.show_3D_subplots(denoised_phase_example, title=""denoised phase example"", first_k_frames=2)
            self.show_3D_subplots(phase_diff_example, title=""phase difference example"", first_k_frames=2)
        # denoised phase centered
        mean = denoised_phase_batch.mean(-1).mean(-1)
        mean = mean.unsqueeze(-1).unsqueeze(-1)
        denoised_phase_batch = denoised_phase_batch - mean 
        mean = phase_difference_batch.mean(-1).mean(-1)
        mean = mean.unsqueeze(-1).unsqueeze(-1)
        phase_difference_batch = phase_difference_batch - mean
        phase_difference_batch = torch.clamp(phase_difference_batch, -5*math.pi, 5*math.pi)
        return phase_difference_batch

    def show_3D_subplots(self, data, title, first_k_frames = None):
        """"""
        data has dimensions: nbands, n_phase_frames, W, H
        """"""
        nbands, n_phase_frames, W, H = data.size()
        m = nbands
        n = first_k_frames if first_k_frames is not None else n_phase_frames
        
        X, Y = range(1, W+1), range(1, H+1)
        Xm, Ym = np.meshgrid(X, Y)
        for i in range(m):
            fig, ax = plt.subplots(nrows=1, ncols=n, subplot_kw={'projection':""3d""})
            for j in range(n):
                img = data[i, j , ...].cpu().numpy()
                surf = ax[j].plot_surface(Xm, Ym,img , rstride=1, cstride=1, cmap=cm.coolwarm,linewidth=0, antialiased=False)
            fig.colorbar(surf, shrink=0.5, aspect=10)
            fig.suptitle(title+"": orientation {}"".format(i))
        plt.show()"
api/readme.md,"# MIMAMO-Net api

This repository contain all scripts that needed for running MIMAMO Net for video files. MIMAMO Net is a model designed for temperoal emotion recognition, i.e., valence and arousal, where valence describes how positive or negative the person is, and arousal describes how active or calm the person is. Using this MIMAMO-api, you are able to get the valence and arousal predictions on each frame of an input video where the human faces are available.

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.

### Prerequisites

- python3
My recommendation is to use Anaconda to create an virtual environment. For example:
```
conda create --name myenv --python=3.6
```
And then activate the virtual environment by:
```
conda activate myenv
```
- matplotlib3.0.3
```
pip install matplotlib==3.0.3
```
- tqdm
```
pip install tqdm
```
- pandas
```
pip install pandas
```
- ffmpeg
```
sudo apt-get install ffmpeg 
```
- OpenFace

To install OpenFace in the root directory of this project:
```
git clone https://github.com/TadasBaltrusaitis/OpenFace.git
cd OpenFace
```
Then download the needed models by:
```
bash download_models.sh
```
It's better to install some dependencies required by OpenCV before you run 'install.sh':
```
sudo apt-get update
sudo apt-get upgrade
sudo apt-get install build-essential cmake pkg-config # Install developer tools used to compile OpenCV
sudo apt-get install libjpeg8-dev libtiff5-dev libjasper-dev libpng12-dev #  Install libraries and packages used to read various image formats from disk
sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev
sudo apt-get install libxvidcore-dev libx264-dev # Install a few libraries used to read video formats from disk
```
And then install OpenFace by:
```
bash install.sh
```
- pytorch-benchmarks
Install pytorch-benchmarks in the root directory of your project:
```
git clone https://github.com/albanie/pytorch-benchmarks.git
```
Create a directory in pytorch-benchmarks to store the resnet50 model and weights:
```
mkdir pytorch-benchmarks/ferplus/
```
Then download the resnet50 model and weights by
```
wget -P pytorch-benchmarks/ferplus/ http://www.robots.ox.ac.uk/~albanie/models/pytorch-mcn/resnet50_ferplus_dag.py

wget -P pytorch-benchmarks/ferplus/ http://www.robots.ox.ac.uk/~albanie/models/pytorch-mcn/resnet50_ferplus_dag.pth 
```
- torch, torchvision, numpy
My recommendation is to use Anaconda to install torch and torchvision with cudatoolkit:
```
conda install pytorch torchvision cudatoolkit=10.1 -c pytorch
```
The installed torch version should be 1.4.0 and the installed torchvision version should be 0.5.0. This line will also install numpy-1.18.1.

### Usage

A step by step tutorial on how to use this api:

First, you should download the MIMAMO net pretrained model weights:

```
cd models
bash download_models.sh
```

And run

```
python run_example.py
```
The print out results should be
```
Prediction takes 7.4361 seconds for 309 frames, average 0.0241 seconds for one frame.
utterance_1 predictions
      valence   arousal
0    0.573943  0.623364
1    0.563206  0.647939
2    0.539191  0.648681
3    0.524443  0.691737
4    0.380667  0.585094
..        ...       ...
304 -0.128373  0.573663
305 -0.200220  0.520263
306 -0.083073  0.392294
307 -0.211973  0.374694
308 -0.290508  0.416637
```


## Authors

* **Didan Deng** - *Initial work* 
## License

## Acknowledgments



"
api/resnet50_extractor.py,"import os
import sys
#os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
#os.environ['CUDA_VISIBLE_DEVICES'] = str(0)
from sampler.image_sampler import Image_Sampler
import torch
from torch.nn import functional as F
from tqdm import tqdm
from utils.model_utils import load_model, compose_transforms
import numpy as np
from steerable.utils import get_device
device = get_device()
class Resnet50_Extractor(object):
    def __init__(self, benchmark_dir = 'pytorch-benchmarks',model_name= 'resnet50_ferplus_dag',
                 feature_layer = 'pool5_7x7_s1'):
        ''' Resnet50_Extractor: A feature extractor to extractor the final convolutional layer's 
         feature vector (2048 dimensional) and save those feature vectors to npy file in an 
         output directory.

        Parameters: 
            benchmark_dir: string, default 'pytorch-benchmarks'
                The pytorch-benchmarks is installed in benchmark_dir.
            model_name: string, default 'resnet50_ferplus_dag'
                The model name for resnet50 model.
            feature_layer: string, default is 'pool5_7x7_s1'
                The output feature layer for resnet50 model is the final convolutional layer named
                'pool5_7x7_s1'.
        '''
        self.benchmark_dir = os.path.abspath(benchmark_dir)
        self.model_name = model_name
        self.feature_layer = feature_layer

        assert os.path.exists(self.benchmark_dir), 'benchmark_dir must exits'
        # load resnet50 model
        model_dir = os.path.abspath(os.path.join(self.benchmark_dir, 'ferplus'))
        self.model = load_model(self.model_name, model_dir)
        self.model = self.model.to(device)
        self.model.eval()
        # load transformation function
        meta = self.model.meta
        self.transform = compose_transforms(meta, center_crop=True)
    def run(self, input_dir, output_dir, batch_size=64, video_name=''):
        '''        
        input_dir: string, 
            The input_dir should have one subdir containing all cropped and aligned face images for 
            a video (extracted by OpenFace). The input_dir should be named after the video name.
        output_dir: string
            All extracted feature vectors will be stored in output directory.
        '''
        assert os.path.exists(input_dir), 'input dir must exsit!'
        assert len(os.listdir(input_dir)) != 0, 'input dir must not be empty!'
        assert len(video_name)!=0, 'input video name cannot be empty!'
        dataset = Image_Sampler(video_name, input_dir, test_mode = True, transform=self.transform)
        data_loader = torch.utils.data.DataLoader(
            dataset, 
            batch_size = batch_size, 
            shuffle=False, drop_last=False,
            num_workers=8, pin_memory=False )
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        else:
            if (len(os.listdir(output_dir)) != 0):
                if ('.npy' in os.listdir(output_dir)[0]):
                    print(""output_dir {} already exists, feature extraction skipped."".format(output_dir))
                    return
        with torch.no_grad():
            for ims, target, img_path, video_name in tqdm(data_loader):
                ims = ims.to(device)
                output = self.get_vec(ims)
                for feature, path, video_n in zip(output, img_path, video_name):
                    des_path = os.path.join(output_dir, ""%05d.npy""%self.get_frame_index(path))
                    np.save(des_path, feature)
        return 
    def get_vec( self,  image):
        bs = image.size(0)
        layer = self.model._modules.get(self.feature_layer)
        my_embedding= torch.zeros([bs, 2048, 1, 1])
        def copy_data(m, i, o):
            my_embedding.copy_(o.data)
        h = layer.register_forward_hook(copy_data)
        h_x = self.model(image)
        h.remove()
        return F.relu(my_embedding.squeeze())

    def get_frame_index(self, frame_path):
        frame_name = frame_path.split('/')[-1]
        frame_num  = int(frame_name.split('.')[0].split('_')[-1])
        return frame_num
"
api/run_example.py,"import os
from tester import Tester
import time
example_video = 'examples/utterance_1.mp4'
model_weight_path = 'models/model_weights.pth.tar'

tester = Tester(model_weight_path, batch_size=64, workers=8, quiet=True)
tic = time.time()
results = tester.test(example_video)
took = time.time()-tic
n_frames = results[list(results.keys())[0]].shape[0]
print(""Prediction takes {:.4f} seconds for {} frames, average {:.4f} seconds for one frame."".format(
	took, n_frames, took/n_frames))

for video in results.keys():
	print(""{} predictions"".format(video))
	print(results[video])
"
api/sampler/image_sampler.py,"from PIL import Image, ImageOps
import torch.utils.data as data
import numpy as np
import torchvision.transforms as transforms
import glob
import os
class VideoRecord(object):
    def __init__(self, video, face_dir, annot_dir, label_name, test_mode = False):
        '''        
        A VideoRecord that records all information of this video (and frames)
        '''
        self.video = video
        self.face_dir = face_dir
        self.annot_dir = annot_dir
        self.label_name = label_name 
        if self.label_name is not None:
            self.label_name = [self.label_name] if '_' not in self.label_name else self.label_name.split(""_"")
        self.test_mode = test_mode
        self.path_label = self.get_path_label()
     
    def get_path_label(self):
        '''        
        return all the frames and labels of this video
        '''
        
        frames = glob.glob(os.path.join(self.face_dir, self.video+""_aligned"", '*.bmp'))
        frames = sorted(frames, key  = lambda x: os.path.basename(x).split(""."")[0].split(""_"")[-1])
        if len(frames)==0:
            raise ValueError(""number of frames of video {} should not be zero."".format(self.video))
        if not self.test_mode:
            assert self.label_name is not None, 'label name should not be None when test mode is False'
            assert self.annot_dir is not None, 'annot_dir should not be None when test mode is False'
            total_labels = []
            for label_name in self.label_name:
                annot_file = os.path.join(self.annot_dir, label_name+"".txt"")
                if (not os.path.exists(annot_file)):
                    raise ValueError(""Annotation file not found."")
                else:
                    f = open(annot_file, ""r"")
                    corr_frames, labels = [], []
                    for i, x in enumerate(f):
                        label = float(x)
                        corr_frame = os.path.join(self.face_dir, self.video+""_aligned"", 'frame_det_00_{0:06d}.bmp'.format(i+1))
                        if os.path.exists(corr_frame):
                            corr_frames.append(corr_frame)
                            labels.append(label)
                        else:
                            # skip those frames and labels
                            continue
                    f.close()
                    assert len(corr_frames) == len(labels)
                    total_labels.append(np.array(labels))
                total_labels = np.array(total_labels)
                assert len(total_labels) == len(corr_frames)
                return [corr_frames, total_labels]
        else:
            N = 1
            if self.label_name is not None:
                N = len(self.label_name)
             
            return [frames, np.array([[-100]*N]*len(frames))]

class Image_Sampler(data.Dataset):
    def __init__(self, video_name, root_path, test_mode =False,  
        annot_dir=None, label_name=None, transform = None, verbose=False,
        size = 224):
        ''' Image sampler for processed video (cropped & aligned faces), which generates an iterator, 
        each time returning a tuple containing (an image (torch.tensor), label, frame path, video name)

    Parameters: 
        video_name: string
            The processed video name
        root_path: string
            The directory path where cropped and aligned faces are stored (optionally other feature files).
        test_mode: bool, default False
            If True, it means the image sampler will only sample images, not annotations. Then annot_dir,
            label_name will not be used. And the labels will be dummy outputs.
            If False, the image sampler will sample both images and annotations. Then annot_dir, label_name
            need to be specified.
        annot_dir: string, default None
            If test_mode is False, the annot_dir needs to be a directory containing arousal.txt or valence.txt
            or both.
        label_name: string, default None
            If test_mode is False, the label_name needs to be one of {'arousal', 'valence', 'arousal_valence'}
        transform: torchvision.transforms.Compose object, default is None
            Transformation functions for images.
        verbose: bool, default False
            Whether to print out video information.
        size: int, default 112
            sampled image size.
        '''
        self.video_name = video_name
        self.root_path = root_path
        self.annot_dir = annot_dir
        self.label_name = label_name
        self.test_mode = test_mode
        self.transform = transform
        self.size = size
        self.verbose = verbose
        self.parse_video()
        if self.transform is None:
            self._create_transform()
        assert self.transform is not None
        
    def parse_video(self):
        self.video_record = VideoRecord(self.video_name, self.root_path, 
            self.annot_dir, self.label_name, self.test_mode)
        frames, labels = self.video_record.path_label
        if self.verbose:
            print(""video {} has {} frames"".format(self.video_name, len(frames)))
        self.frame_ids = np.arange(len(frames))
    def __len__(self):
        return len(self.frame_ids)
    def __getitem__(self, index):
        f_id = self.frame_ids[index]
        frames, labels = self.video_record.path_label
        frame, label = frames[f_id], labels[f_id]
        img = Image.open(frame)
        img = self.transform(img)
        return img, label, frame, self.video_record.video

    def _create_transform(self):
        if not self.test_mode:
            img_size = self.size
            resize = int(img_size * 1.2)
            transform_list = [transforms.Resize(resize),
                              transforms.RandomCrop(img_size),
                              transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),
                              transforms.RandomHorizontalFlip(),
                              transforms.ToTensor(),
                              transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                   std=[0.229, 0.224, 0.225]),
                            ]
        else:
            img_size = self.size
            transform_list = [transforms.Resize(img_size),
                            transforms.ToTensor(),
                            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                 std=[0.229, 0.224, 0.225]),
                            ]
        self.transform = transforms.Compose(transform_list)"
api/sampler/snippet_sampler.py,"import torch.utils.data as data
from PIL import Image
import os
import os.path
import numpy as np
import glob
from torch import nn as nn
import torch
import torchvision
from utils.data_utils import GroupRandomHorizontalFlip,GroupRandomCrop,GroupScale,Stack,ToTorchFormatTensor

class VideoRecord(object):
    def __init__(self, video, feature_dir, annot_dir, label_name, test_mode = False):
        self.video = video
        self.feature_dir = feature_dir
        self.annot_dir = annot_dir
        self.label_name = label_name
        if self.label_name is not None:
            self.label_name = [self.label_name] if '_' not in self.label_name else self.label_name.split(""_"")
        self.test_mode = test_mode
        self.path_label = self.get_path_label()
     
    def get_path_label(self):
        frames = glob.glob(os.path.join(self.feature_dir, '*.npy'))
        frames = sorted(frames, key  = lambda x: os.path.basename(x).split(""."")[0])
        if len(frames)==0:
            raise ValueError(""number of frames of video {} should not be zero."".format(self.video))
        if not self.test_mode:
            assert self.label_name is not None, 'label name should not be None when test mode is False'
            assert self.annot_dir is not None, 'annot_dir should not be None when test mode is False'
            if (any([not os.path.exists(file) for file in annot_file])):
                raise ValueError(""Annotation file not found when test mode is False"")
            annot_file = [os.path.join(self.annot_dir, ln+"".txt"") for ln in self.label_name]
            total_labels = []
            for file in annot_file:
                f = open(file, ""r"")
                corr_frames, labels = [], []
                for i, x in enumerate(f):
                    label = float(x)
                    corr_frame = os.path.join(self.feature_dir, '{0:05d}.npy'.format(i+1))
                    if os.path.exists(corr_frame):
                        corr_frames.append(corr_frame)
                        labels.append(label)
                    else:
                        # skip those frames and labels
                        continue
                f.close()
                total_labels.append(labels)
            assert len(corr_frames) == len(labels)
            total_labels = np.asarray(total_labels)
            total_labels = total_labels.transpose(1, 0)
            return [corr_frames, total_labels]
        else:
            N = 1
            if self.label_name is not None:
                N = len(self.label_name)
            return [frames, np.array([[-100] * N]*len(frames))]

class Snippet_Sampler(data.Dataset):
    def __init__(self, video_name, root_path, feature_path,
                 annot_dir=None, label_name=None, test_mode =True, 
                 num_phase=12, phase_size = 48, 
                 length=64, stride=64,
                 verbose=False):
        '''Snippet_Sampler: An image sampler to sample snippets (a snippet consists of one RGB image and N
        greyscale images). The main inputs to it are two directories, one is the OpenFace processed faces directory,
        another is the directory containing renset50 feature vectors. 
        Parameters:
            video_name: string
                The processed video name
            root_path: string
                The directory path where cropped and aligned faces are stored (optionally other feature files).
            feature_path, string,
                The directory containing all renset50 feature vectors. 
            test_mode: bool, default False
                If True, it means the image sampler will only sample images, not annotations. Then annot_dir,
                label_name will not be used. And the labels will be dummy outputs.
                If False, the image sampler will sample both images and annotations. Then annot_dir, label_name
                need to be specified.
            label_name: string, default None
                If test_mode is False, the label_name needs to be one of {'arousal', 'valence', 'arousal_valence'}
            num_phase: int, default 12
                number of phase difference images, input to the phase net of the mimamo net.
            phase_size: int, default 48
                phase image size, default is 48x48
            length: int, default 64
                The length of snippets returned.
            stride: int, default 32
                The stride taken when sampling sequence of snippets. If stride<length, it means 
                adjacent sequence will overlap with each other.

        '''
        self.video_name = video_name
        self.root_path = root_path
        self.feature_path = feature_path
        self.annot_dir = annot_dir
        
        self.label_name = label_name
        self.test_mode = test_mode
        self.length = length 
        self.stride = stride 
        self.num_phase = num_phase
        self.phase_size = phase_size
        self.verbose = verbose
        self.parse_video()

    def parse_video(self):
        
        self.video_record = VideoRecord(self.video_name, self.feature_path, 
            self.annot_dir, self.label_name, self.test_mode)
        frames, labels = self.video_record.path_label
        if len(frames) < self.length: 
            print(""The length exceeds the number of exsisting frames, the sampling length has been changed to {}"".format(len(frames)))
            self.length = len(frames)
            self.stride = len(frames)
        self.seq_ranges = list()
        start, end = 0, self.length
        while end <= len(frames) and (start<len(frames)):
            self.seq_ranges.append([start, end]) 
            start +=self.stride
            end = start+self.length
        assert len(self.seq_ranges)!=0, ""No snippet is sampled.""
        if self.seq_ranges[-1][1] < len(frames):
            start = len(frames) - self.length
            end = len(frames)
            self.seq_ranges.append([start, end])  
        if self.verbose:        
            print(""videos {}, number of seqs:{}"".format(self.video_name, len(self.seq_ranges)))

    def __len__(self):
        return len(self.seq_ranges)
    def __getitem__(self, index):
        
        seq_ranges = self.seq_ranges[index]
        start, end = seq_ranges
        frames, labels = self.video_record.path_label
        seq_frames, seq_labels = frames[start:end], labels[start:end]
        # sample rgb images (features)
        imgs = []
        for f in seq_frames:
            imgs.append(np.load(f))
        # sample phase images 
        # figure ids
        sample_f_ids = []
        for f_id in range(start, end):
            phase_ids = []
            for i in range(self.num_phase+1):
                step = i-self.num_phase//2
                id_0 = max(0,f_id + step)
                id_0 = min(id_0, len(frames)-1) 
                phase_ids.append(id_0)
            sample_f_ids.append(phase_ids)
        sample_frames = [[frames[id] for id in ids] for ids in sample_f_ids]
        # load greyscale images
        phase_images= []
        for frames in sample_frames:
            phase_img_list = []
            for frame in frames:
                f_index = int(os.path.basename(frame).split(""."")[0])
                img_frame = os.path.join(self.root_path, self.video_record.video+""_aligned"", 
                    'frame_det_00_{:06d}.bmp'.format(f_index))
                try:
                   img = Image.open(img_frame).convert('L')
                except:
                    raise ValueError(""incorrect face path"")    
                phase_img_list.append(img)
            phase_images.append(phase_img_list)
        if not self.test_mode:
            random_seed = np.random.randint(250)
            phase_transform = torchvision.transforms.Compose(
                                   [GroupRandomHorizontalFlip(seed=random_seed),
                                   GroupRandomCrop(size=int(self.phase_size*0.85), seed=random_seed),
                                   GroupScale(size=self.phase_size),
                                   Stack(),
                                   ToTorchFormatTensor()])
        else:
            phase_transform = torchvision.transforms.Compose([
                                   GroupScale(size=self.phase_size),
                                   Stack(),
                                   ToTorchFormatTensor()]) 
        flat_phase_images = []
        for sublist in phase_images:
            flat_phase_images.extend(sublist)
        flat_phase_images = phase_transform(flat_phase_images)
        phase_images = flat_phase_images.view(len(phase_images), self.num_phase+1, self.phase_size, self.phase_size)
        return phase_images, np.array(imgs), np.array(seq_labels), np.array([start, end]), self.video_record.video
    

if __name__ == '__main__':
    root_path = '/media/newssd/Aff-Wild_experiments/Aligned_Faces_train'
    feature_path = '/media/newssd/Aff-Wild_experiments/Extracted_Features/Aff_wild_train/resnet50_ferplus_features_fps=30_pool5_7x7_s1'
    annot_dir = '/media/newssd/Aff-Wild_experiments/annotations'
    video_names = os.listdir(feature_path)[:25]

    train_dataset = Face_Dataset(root_path, feature_path, annot_dir, video_names, label_name='arousal_valence',  num_phase=12 , phase_size=48, test_mode=True)
    train_loader = torch.utils.data.DataLoader(
        train_dataset, 
        batch_size = 4, 
        num_workers=0, pin_memory=False )
    for phase_f, rgb_f, label, seq_range, video_names in train_loader:
        phase_0, phase_1 = phase_f
        
"
api/steerable/SCFpyr_NumPy.py,"# MIT License
#
# Copyright (c) 2020 Didan Deng
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to conditions.
#
# Author: Didan Deng
# Date Created: 2020-03-31

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np

import steerable.math_utils as math_utils
pointOp = math_utils.pointOp
factorial = math_utils.factorial
        
################################################################################

class SCFpyr_NumPy():
    '''
    This is a modified version of buildSFpyr, that constructs a
    complex-valued steerable pyramid  using Hilbert-transform pairs
    of filters. Note that the imaginary parts will *not* be steerable.

    Description of this transform appears in: Portilla & Simoncelli,
    International Journal of Computer Vision, 40(1):49-71, Oct 2000.
    Further information: http://www.cns.nyu.edu/~eero/STEERPYR/

    Modified code from the perceptual repository:
      https://github.com/andreydung/Steerable-filter

    This code looks very similar to the original Matlab code:
      https://github.com/LabForComputationalVision/matlabPyrTools/blob/master/buildSCFpyr.m

    Also looks very similar to the original Python code presented here:
      https://github.com/LabForComputationalVision/pyPyrTools/blob/master/pyPyrTools/SCFpyr.py

    '''

    def __init__(self, height=5, nbands=4, scale_factor=2, precision = 32):
        self.nbands  = nbands  # number of orientation bands
        self.height  = height  # including low-pass and high-pass
        self.scale_factor = scale_factor
        self.precision = precision
        assert self.precision in [32, 64]
        self.dtype = eval('np.float{}'.format(self.precision))        
        # Cache constants
        self.lutsize = 1024
        self.Xcosn = np.pi * np.array(range(-(2*self.lutsize+1), (self.lutsize+2)))/self.lutsize
        self.alpha = (self.Xcosn + np.pi) % (2*np.pi) - np.pi


    ################################################################################
    # Construction of Steerable Pyramid

    def build(self, im):
        ''' Decomposes an image into it's complex steerable pyramid. 
        The pyramid typically has ~4 levels and 4-8 orientations. 
        
        Args:
            im_batch (np.ndarray): single image [H,W]
        
        Returns:
            pyramid: list containing np.ndarray objects storing the pyramid
        '''

        assert len(im.shape) == 2, 'Input im must be grayscale'
        assert im.dtype == self.dtype, 'Input muse be {}'.format(self.dtype)
        height, width = im.shape

        # Check whether image size is sufficient for number of levels
        if self.height > int(np.floor(np.log2(min(width, height))) - 2):
            raise RuntimeError('Cannot build {} levels, image too small.'.format(self.height))
        
        # Prepare a grid
        log_rad, angle = math_utils.prepare_grid(height, width)

        # Radial transition function (a raised cosine in log-frequency):
        Xrcos, Yrcos = math_utils.rcosFn(1, -0.5)
        Yrcos = np.sqrt(Yrcos)

        YIrcos = np.sqrt(1 - Yrcos**2)
        lo0mask = pointOp(log_rad, YIrcos, Xrcos)
        hi0mask = pointOp(log_rad, Yrcos, Xrcos)

        # Shift the zero-frequency component to the center of the spectrum.
        imdft = np.fft.fftshift(np.fft.fft2(im))

        # Low-pass
        lo0dft = imdft * lo0mask

        # Recursive build the steerable pyramid
        coeff = self._build_levels(lo0dft, log_rad, angle, Xrcos, Yrcos, self.height-1)

        # High-pass
        hi0dft = imdft * hi0mask
        hi0 = np.fft.ifft2(np.fft.ifftshift(hi0dft))
        coeff.insert(0, hi0.real)
        return coeff


    def _build_levels(self, lodft, log_rad, angle, Xrcos, Yrcos, height):

        if height <= 1:

            # Low-pass
            lo0 = np.fft.ifftshift(lodft)
            lo0 = np.fft.ifft2(lo0)
            coeff = [lo0.real]

        else:
            
            Xrcos = Xrcos - np.log2(self.scale_factor)

            ####################################################################
            ####################### Orientation bandpass #######################
            ####################################################################

            himask = pointOp(log_rad, Yrcos, Xrcos)

            order = self.nbands - 1
            const = np.power(2, 2*order) * np.square(factorial(order)) / (self.nbands * factorial(2*order))
            Ycosn = 2*np.sqrt(const) * np.power(np.cos(self.Xcosn), order) * (np.abs(self.alpha) < np.pi/2)

            # Loop through all orientation bands
            orientations = []
            for b in range(self.nbands):
                anglemask = pointOp(angle, Ycosn, self.Xcosn + np.pi*b/self.nbands)
                banddft = np.power(np.complex(0, -1), self.nbands - 1) * lodft * anglemask * himask
                band = np.fft.ifft2(np.fft.ifftshift(banddft))
                orientations.append(band)

            ####################################################################
            ######################## Subsample lowpass #########################
            ####################################################################

            dims = np.array(lodft.shape)

            # Both are tuples of size 2
            low_ind_start = (np.ceil((dims+0.5)/2) - np.ceil((np.ceil((dims-0.5)/2)+0.5)/2)).astype(int)
            low_ind_end = (low_ind_start + np.ceil((dims-0.5)/2)).astype(int)
          
            # Selection
            log_rad = log_rad[low_ind_start[0]:low_ind_end[0], low_ind_start[1]:low_ind_end[1]]
            angle   = angle[low_ind_start[0]:low_ind_end[0], low_ind_start[1]:low_ind_end[1]]
            lodft   = lodft[low_ind_start[0]:low_ind_end[0], low_ind_start[1]:low_ind_end[1]]

            # Subsampling in frequency domain
            YIrcos = np.abs(np.sqrt(1 - Yrcos**2))
            lomask = pointOp(log_rad, YIrcos, Xrcos)
            lodft = lomask * lodft

            ####################################################################
            ####################### Recursion next level #######################
            ####################################################################

            coeff = self._build_levels(lodft, log_rad, angle, Xrcos, Yrcos, height-1)
            coeff.insert(0, orientations)

        return coeff

    ############################################################################
    ########################### RECONSTRUCTION #################################
    ############################################################################

    def reconstruct(self, coeff):

        if self.nbands != len(coeff[1]):
            raise Exception(""Unmatched number of orientations"")

        height, width = coeff[0].shape
        log_rad, angle = math_utils.prepare_grid(height, width)

        Xrcos, Yrcos = math_utils.rcosFn(1, -0.5)
        Yrcos  = np.sqrt(Yrcos)
        YIrcos = np.sqrt(np.abs(1 - Yrcos**2))

        lo0mask = pointOp(log_rad, YIrcos, Xrcos)
        hi0mask = pointOp(log_rad, Yrcos, Xrcos)

        tempdft = self._reconstruct_levels(coeff[1:], log_rad, Xrcos, Yrcos, angle)

        hidft = np.fft.fftshift(np.fft.fft2(coeff[0]))
        outdft = tempdft * lo0mask + hidft * hi0mask

        reconstruction = np.fft.ifftshift(outdft)
        reconstruction = np.fft.ifft2(reconstruction)
        reconstruction = reconstruction.real

        return reconstruction

    def _reconstruct_levels(self, coeff, log_rad, Xrcos, Yrcos, angle):

        if len(coeff) == 1:
            dft = np.fft.fft2(coeff[0])
            dft = np.fft.fftshift(dft)
            return dft

        Xrcos = Xrcos - np.log2(self.scale_factor)

        ####################################################################
        ####################### Orientation Residue ########################
        ####################################################################

        himask = pointOp(log_rad, Yrcos, Xrcos)

        lutsize = 1024
        Xcosn = np.pi * np.array(range(-(2*lutsize+1), (lutsize+2)))/lutsize
        order = self.nbands - 1
        const = np.power(2, 2*order) * np.square(factorial(order)) / (self.nbands * factorial(2*order))
        Ycosn = np.sqrt(const) * np.power(np.cos(Xcosn), order)

        orientdft = np.zeros(coeff[0][0].shape)

        for b in range(self.nbands):
            anglemask = pointOp(angle, Ycosn, Xcosn + np.pi * b/self.nbands)
            banddft = np.fft.fft2(coeff[0][b])
            banddft = np.fft.fftshift(banddft)
            orientdft = orientdft + np.power(np.complex(0, 1), order) * banddft * anglemask * himask

        ####################################################################
        ########## Lowpass component are upsampled and convoluted ##########
        ####################################################################

        dims = np.array(coeff[0][0].shape)

        lostart = (np.ceil((dims+0.5)/2) - np.ceil((np.ceil((dims-0.5)/2)+0.5)/2)).astype(np.int32)
        loend = lostart + np.ceil((dims-0.5)/2).astype(np.int32)

        nlog_rad = log_rad[lostart[0]:loend[0], lostart[1]:loend[1]]
        nangle = angle[lostart[0]:loend[0], lostart[1]:loend[1]]
        YIrcos = np.sqrt(np.abs(1 - Yrcos**2))
        lomask = pointOp(nlog_rad, YIrcos, Xrcos)

        ################################################################################

        # Recursive call for image reconstruction
        nresdft = self._reconstruct_levels(coeff[1:], nlog_rad, Xrcos, Yrcos, nangle)

        resdft = np.zeros(dims, 'complex')
        resdft[lostart[0]:loend[0], lostart[1]:loend[1]] = nresdft * lomask

        return resdft + orientdft
"
api/steerable/SCFpyr_PyTorch.py,"# MIT License
#
# Copyright (c) 2020 Didan Deng
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to conditions.
#
# Author: Didan Deng
# Date Created: 2020-03-31

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import torch

import steerable.math_utils as math_utils
pointOp = math_utils.pointOp
factorial = math_utils.factorial

################################################################################
################################################################################


class SCFpyr_PyTorch(object):
    '''
    This is a modified version of buildSFpyr, that constructs a
    complex-valued steerable pyramid  using Hilbert-transform pairs
    of filters. Note that the imaginary parts will *not* be steerable.

    Description of this transform appears in: Portilla & Simoncelli,
    International Journal of Computer Vision, 40(1):49-71, Oct 2000.
    Further information: http://www.cns.nyu.edu/~eero/STEERPYR/

    Modified code from the perceptual repository:
      https://github.com/andreydung/Steerable-filter

    This code looks very similar to the original Matlab code:
      https://github.com/LabForComputationalVision/matlabPyrTools/blob/master/buildSCFpyr.m

    Also looks very similar to the original Python code presented here:
      https://github.com/LabForComputationalVision/pyPyrTools/blob/master/pyPyrTools/SCFpyr.py

    '''

    def __init__(self, height=5, nbands=4, scale_factor=2, device=None, precision=32):
        self.height = height  # including low-pass and high-pass
        self.nbands = nbands  # number of orientation bands
        self.scale_factor = scale_factor
        self.device = torch.device('cpu') if device is None else device
        self.precision = precision
        assert self.precision in [32, 64]
        self.dtype = eval('torch.float{}'.format(self.precision))
        torch.set_default_dtype(self.dtype)
        # Cache constants
        self.lutsize = 1024
        self.Xcosn = np.pi * np.array(range(-(2*self.lutsize+1), (self.lutsize+2)))/self.lutsize
        self.alpha = (self.Xcosn + np.pi) % (2*np.pi) - np.pi
        self.complex_fact_construct   = np.power(np.complex(0, -1), self.nbands-1)
        self.complex_fact_reconstruct = np.power(np.complex(0, 1), self.nbands-1)
        
    ################################################################################
    # Construction of Steerable Pyramid

    def build(self, im_batch):
        ''' Decomposes a batch of images into a complex steerable pyramid. 
        The pyramid typically has ~4 levels and 4-8 orientations. 
        
        Args:
            im_batch (torch.Tensor): Batch of images of shape [N,C,H,W]
        
        Returns:
            pyramid: list containing torch.Tensor objects storing the pyramid
        '''
        
        assert im_batch.device == self.device, 'Devices invalid (pyr = {}, batch = {})'.format(self.device, im_batch.device)
        assert im_batch.dtype == self.dtype, 'Image batch must be torch.float{}'.format(self.precision)
        assert im_batch.dim() == 4, 'Image batch must be of shape [N,C,H,W]'
        assert im_batch.shape[1] == 1, 'Second dimension must be 1 encoding grayscale image'

        im_batch = im_batch.squeeze(1)  # flatten channels dim
        height, width = im_batch.shape[2], im_batch.shape[1] 
        
        # Check whether image size is sufficient for number of levels
        if self.height > int(np.floor(np.log2(min(width, height))) - 2):
            raise RuntimeError('Cannot build {} levels, image too small.'.format(self.height))
        
        # Prepare a grid
        log_rad, angle = math_utils.prepare_grid(height, width)

        # Radial transition function (a raised cosine in log-frequency):
        Xrcos, Yrcos = math_utils.rcosFn(1, -0.5)
        Yrcos = np.sqrt(Yrcos)

        YIrcos = np.sqrt(1 - Yrcos**2)

        lo0mask = pointOp(log_rad, YIrcos, Xrcos)
        hi0mask = pointOp(log_rad, Yrcos, Xrcos)

        # Note that we expand dims to support broadcasting later
        lo0mask = torch.from_numpy(lo0mask)[None,:,:,None].type(self.dtype).to(self.device)
        hi0mask = torch.from_numpy(hi0mask)[None,:,:,None].type(self.dtype).to(self.device)

        # Fourier transform (2D) and shifting
        batch_dft = torch.rfft(im_batch, signal_ndim=2, onesided=False)
        batch_dft = math_utils.batch_fftshift2d(batch_dft)

        # Low-pass
        lo0dft = batch_dft * lo0mask

        # Start recursively building the pyramids
        coeff = self._build_levels(lo0dft, log_rad, angle, Xrcos, Yrcos, self.height-1)

        # High-pass
        hi0dft = batch_dft * hi0mask
        hi0 = math_utils.batch_ifftshift2d(hi0dft)
        hi0 = torch.ifft(hi0, signal_ndim=2)
        hi0_real = torch.unbind(hi0, -1)[0]
        coeff.insert(0, hi0_real)
        return coeff

    def _build_levels(self, lodft, log_rad, angle, Xrcos, Yrcos, height):
        
        if height <= 1:

            # Low-pass
            lo0 = math_utils.batch_ifftshift2d(lodft)
            lo0 = torch.ifft(lo0, signal_ndim=2)
            lo0_real = torch.unbind(lo0, -1)[0]
            coeff = [lo0_real]

        else:
            
            Xrcos = Xrcos - np.log2(self.scale_factor)

            ####################################################################
            ####################### Orientation bandpass #######################
            ####################################################################

            himask = pointOp(log_rad, Yrcos, Xrcos)
            himask = torch.from_numpy(himask[None,:,:,None]).type(self.dtype).to(self.device)

            order = self.nbands - 1
            const = np.power(2, 2*order) * np.square(factorial(order)) / (self.nbands * factorial(2*order))
            Ycosn = 2*np.sqrt(const) * np.power(np.cos(self.Xcosn), order) * (np.abs(self.alpha) < np.pi/2) # [n,]

            # Loop through all orientation bands
            orientations = []
            for b in range(self.nbands):

                anglemask = pointOp(angle, Ycosn, self.Xcosn + np.pi*b/self.nbands)
                anglemask = anglemask[None,:,:,None]  # for broadcasting
                anglemask = torch.from_numpy(anglemask).type(self.dtype).to(self.device)

                # Bandpass filtering                
                banddft = lodft * anglemask * himask

                # Now multiply with complex number
                # (x+yi)(u+vi) = (xu-yv) + (xv+yu)i
                banddft = torch.unbind(banddft, -1)
                banddft_real = self.complex_fact_construct.real*banddft[0] - self.complex_fact_construct.imag*banddft[1]
                banddft_imag = self.complex_fact_construct.real*banddft[1] + self.complex_fact_construct.imag*banddft[0]
                banddft = torch.stack((banddft_real, banddft_imag), -1)

                band = math_utils.batch_ifftshift2d(banddft)
                band = torch.ifft(band, signal_ndim=2)
                orientations.append(band)

            ####################################################################
            ######################## Subsample lowpass #########################
            ####################################################################

            # Don't consider batch_size and imag/real dim
            dims = np.array(lodft.shape[1:3])  

            # Both are tuples of size 2
            low_ind_start = (np.ceil((dims+0.5)/2) - np.ceil((np.ceil((dims-0.5)/2)+0.5)/2)).astype(int)
            low_ind_end   = (low_ind_start + np.ceil((dims-0.5)/2)).astype(int)

            # Subsampling indices
            log_rad = log_rad[low_ind_start[0]:low_ind_end[0],low_ind_start[1]:low_ind_end[1]]
            angle = angle[low_ind_start[0]:low_ind_end[0],low_ind_start[1]:low_ind_end[1]]

            # Actual subsampling
            lodft = lodft[:,low_ind_start[0]:low_ind_end[0],low_ind_start[1]:low_ind_end[1],:]

            # Filtering
            YIrcos = np.abs(np.sqrt(1 - Yrcos**2))
            lomask = pointOp(log_rad, YIrcos, Xrcos)
            lomask = torch.from_numpy(lomask[None,:,:,None]).type(self.dtype)
            lomask = lomask.to(self.device)

            # Convolution in spatial domain
            lodft = lomask * lodft

            ####################################################################
            ####################### Recursion next level #######################
            ####################################################################

            coeff = self._build_levels(lodft, log_rad, angle, Xrcos, Yrcos, height-1)
            coeff.insert(0, orientations)

        return coeff

    ############################################################################
    ########################### RECONSTRUCTION #################################
    ############################################################################

    def reconstruct(self, coeff):

        if self.nbands != len(coeff[1]):
            raise Exception(""Unmatched number of orientations"")

        height, width = coeff[0].shape[2], coeff[0].shape[1] 
        log_rad, angle = math_utils.prepare_grid(height, width)

        Xrcos, Yrcos = math_utils.rcosFn(1, -0.5)
        Yrcos  = np.sqrt(Yrcos)
        YIrcos = np.sqrt(np.abs(1 - Yrcos**2))

        lo0mask = pointOp(log_rad, YIrcos, Xrcos)
        hi0mask = pointOp(log_rad, Yrcos, Xrcos)

        # Note that we expand dims to support broadcasting later
        lo0mask = torch.from_numpy(lo0mask)[None,:,:,None].type(self.dtype).to(self.device)
        hi0mask = torch.from_numpy(hi0mask)[None,:,:,None].type(self.dtype).to(self.device)

        # Start recursive reconstruction
        tempdft = self._reconstruct_levels(coeff[1:], log_rad, Xrcos, Yrcos, angle)

        hidft = torch.rfft(coeff[0], signal_ndim=2, onesided=False)
        hidft = math_utils.batch_fftshift2d(hidft)

        outdft = tempdft * lo0mask + hidft * hi0mask

        reconstruction = math_utils.batch_ifftshift2d(outdft)
        reconstruction = torch.ifft(reconstruction, signal_ndim=2)
        reconstruction = torch.unbind(reconstruction, -1)[0]  # real

        return reconstruction

    def _reconstruct_levels(self, coeff, log_rad, Xrcos, Yrcos, angle):

        if len(coeff) == 1:
            dft = torch.rfft(coeff[0], signal_ndim=2, onesided=False)
            dft = math_utils.batch_fftshift2d(dft)
            return dft

        Xrcos = Xrcos - np.log2(self.scale_factor)

        ####################################################################
        ####################### Orientation Residue ########################
        ####################################################################

        himask = pointOp(log_rad, Yrcos, Xrcos)
        himask = torch.from_numpy(himask[None,:,:,None]).type(self.dtype).to(self.device)

        lutsize = 1024
        Xcosn = np.pi * np.array(range(-(2*lutsize+1), (lutsize+2)))/lutsize
        order = self.nbands - 1
        const = np.power(2, 2*order) * np.square(factorial(order)) / (self.nbands * factorial(2*order))
        Ycosn = np.sqrt(const) * np.power(np.cos(Xcosn), order)

        orientdft = torch.zeros_like(coeff[0][0])
        for b in range(self.nbands):

            anglemask = pointOp(angle, Ycosn, Xcosn + np.pi * b/self.nbands)
            anglemask = anglemask[None,:,:,None]  # for broadcasting
            anglemask = torch.from_numpy(anglemask).type(self.dtype).to(self.device)

            banddft = torch.fft(coeff[0][b], signal_ndim=2)
            banddft = math_utils.batch_fftshift2d(banddft)

            banddft = banddft * anglemask * himask
            banddft = torch.unbind(banddft, -1)
            banddft_real = self.complex_fact_reconstruct.real*banddft[0] - self.complex_fact_reconstruct.imag*banddft[1]
            banddft_imag = self.complex_fact_reconstruct.real*banddft[1] + self.complex_fact_reconstruct.imag*banddft[0]
            banddft = torch.stack((banddft_real, banddft_imag), -1)

            orientdft = orientdft + banddft

        ####################################################################
        ########## Lowpass component are upsampled and convoluted ##########
        ####################################################################
        
        dims = np.array(coeff[0][0].shape[1:3])
        
        lostart = (np.ceil((dims+0.5)/2) - np.ceil((np.ceil((dims-0.5)/2)+0.5)/2)).astype(np.int32)
        loend = lostart + np.ceil((dims-0.5)/2).astype(np.int32)

        nlog_rad = log_rad[lostart[0]:loend[0], lostart[1]:loend[1]]
        nangle = angle[lostart[0]:loend[0], lostart[1]:loend[1]]
        YIrcos = np.sqrt(np.abs(1 - Yrcos**2))
        lomask = pointOp(nlog_rad, YIrcos, Xrcos)

        # Filtering
        lomask = pointOp(nlog_rad, YIrcos, Xrcos)
        lomask = torch.from_numpy(lomask[None,:,:,None]).type(self.dtype)
        lomask = lomask.to(self.device)

        ################################################################################

        # Recursive call for image reconstruction        
        nresdft = self._reconstruct_levels(coeff[1:], nlog_rad, Xrcos, Yrcos, nangle)

        resdft = torch.zeros_like(coeff[0][0]).to(self.device)
        resdft[:,lostart[0]:loend[0], lostart[1]:loend[1],:] = nresdft * lomask

        return resdft + orientdft
"
api/steerable/math_utils.py,"# MIT License
#
# Copyright (c) 2020 Didan Deng
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to conditions.
#
# Author: Didan Deng
# Date Created: 2020-03-31

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import torch

################################################################################
################################################################################

def roll_n(X, axis, n):
    f_idx = tuple(slice(None, None, None) if i != axis else slice(0, n, None) for i in range(X.dim()))
    b_idx = tuple(slice(None, None, None) if i != axis else slice(n, None, None) for i in range(X.dim()))
    front = X[f_idx]
    back = X[b_idx]
    return torch.cat([back, front], axis)

def batch_fftshift2d(x):
    real, imag = torch.unbind(x, -1)
    for dim in range(1, len(real.size())):
        n_shift = real.size(dim)//2
        if real.size(dim) % 2 != 0:
            n_shift += 1  # for odd-sized images
        real = roll_n(real, axis=dim, n=n_shift)
        imag = roll_n(imag, axis=dim, n=n_shift)
    return torch.stack((real, imag), -1)  # last dim=2 (real&imag)

def batch_ifftshift2d(x):
    real, imag = torch.unbind(x, -1)
    for dim in range(len(real.size()) - 1, 0, -1):
        real = roll_n(real, axis=dim, n=real.size(dim)//2)
        imag = roll_n(imag, axis=dim, n=imag.size(dim)//2)
    return torch.stack((real, imag), -1)  # last dim=2 (real&imag)

################################################################################
################################################################################

def prepare_grid(m, n):
    x = np.linspace(-(m // 2)/(m / 2), (m // 2)/(m / 2) - (1 - m % 2)*2/m, num=m)
    y = np.linspace(-(n // 2)/(n / 2), (n // 2)/(n / 2) - (1 - n % 2)*2/n, num=n)
    xv, yv = np.meshgrid(y, x)
    angle = np.arctan2(yv, xv)
    rad = np.sqrt(xv**2 + yv**2)
    rad[m//2][n//2] = rad[m//2][n//2 - 1]
    log_rad = np.log2(rad)
    return log_rad, angle

def rcosFn(width, position):
    N = 256  # abritrary
    X = np.pi * np.array(range(-N-1, 2))/2/N
    Y = np.cos(X)**2
    Y[0] = Y[1]
    Y[N+2] = Y[N+1]
    X = position + 2*width/np.pi*(X + np.pi/4)
    return X, Y

def pointOp(im, Y, X):
    out = np.interp(im.flatten(), X, Y)
    return np.reshape(out, im.shape)

def getlist(coeff):
    straight = [bands for scale in coeff[1:-1] for bands in scale]
    straight = [coeff[0]] + straight + [coeff[-1]]
    return straight
def factorial(n):
    assert type(n)==int 
    if n==1:
        return n
    else:
        return n*factorial(n-1)
################################################################################
# NumPy reference implementation (fftshift and ifftshift)

# def fftshift(x, axes=None):
#     """"""
#     Shift the zero-frequency component to the center of the spectrum.
#     This function swaps half-spaces for all axes listed (defaults to all).
#     Note that ``y[0]`` is the Nyquist component only if ``len(x)`` is even.
#     Parameters
#     """"""
#     x = np.asarray(x)
#     if axes is None:
#         axes = tuple(range(x.ndim))
#         shift = [dim // 2 for dim in x.shape]
#     shift = [x.shape[ax] // 2 for ax in axes]
#     return np.roll(x, shift, axes)
#
# def ifftshift(x, axes=None):
#     """"""
#     The inverse of `fftshift`. Although identical for even-length `x`, the
#     functions differ by one sample for odd-length `x`.
#     """"""
#     x = np.asarray(x)
#     if axes is None:
#         axes = tuple(range(x.ndim))
#         shift = [-(dim // 2) for dim in x.shape]
#     shift = [-(x.shape[ax] // 2) for ax in axes]
#     return np.roll(x, shift, axes)
"
api/steerable/utils.py,"# MIT License
#
# Copyright (c) 2020 Didan Deng
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to conditions.
#
# Author: Didan Deng
# Date Created: 2020-03-31

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

import torch
import torchvision


################################################################################

ToPIL = torchvision.transforms.ToPILImage()
Grayscale = torchvision.transforms.Grayscale()
RandomCrop = torchvision.transforms.RandomCrop

def get_device(device='cuda:0'):
    assert isinstance(device, str)
    num_cuda = torch.cuda.device_count()

    if 'cuda' in device:
        if num_cuda > 0:
            # Found CUDA device, use the GPU
            return torch.device(device)
        # Fallback to CPU
        print('No CUDA devices found, falling back to CPU')
        device = 'cpu'

    if not torch.backends.mkl.is_available():
        raise NotImplementedError(
            'torch.fft on the CPU requires MKL back-end. ' +
            'Please recompile your PyTorch distribution.')
    return torch.device('cpu')

def load_image_batch(image_file, batch_size, image_size=200):
    if not os.path.isfile(image_file):
        raise FileNotFoundError('Image file not found on disk: {}'.format(image_file))
    im = ToPIL(Image.open(image_file))
    im = Grayscale(im)
    im_batch = np.zeros((batch_size, image_size, image_size), np.float32)
    for i in range(batch_size):
        im_batch[i] = RandomCrop(image_size)(im)
    # insert channels dim and rescale
    return im_batch[:,None,:,:]/225.

def show_image_batch(im_batch):
    assert isinstance(im_batch, torch.Tensor)
    im_batch = torchvision.utils.make_grid(im_batch).numpy()
    im_batch = np.transpose(im_batch.squeeze(1), (1,2,0))
    plt.imshow(im_batch)
    plt.axis('off')
    plt.tight_layout()
    plt.show()
    return im_batch

def extract_from_batch(coeff_batch, example_idx=0):
    '''
    Given the batched Complex Steerable Pyramid, extract the coefficients
    for a single example from the batch. Additionally, it converts all
    torch.Tensor's to np.ndarrays' and changes creates proper np.complex
    objects for all the orientation bands. 

    Args:
        coeff_batch (list): list containing low-pass, high-pass and pyr levels
        example_idx (int, optional): Defaults to 0. index in batch to extract
    
    Returns:
        list: list containing low-pass, high-pass and pyr levels as np.ndarray
    '''
    if not isinstance(coeff_batch, list):
        raise ValueError('Batch of coefficients must be a list')
    coeff = []  # coefficient for single example
    for coeff_level in coeff_batch:
        if isinstance(coeff_level, torch.Tensor):
            # Low- or High-Pass
            coeff_level_numpy = coeff_level[example_idx].cpu().numpy()
            coeff.append(coeff_level_numpy)
        elif isinstance(coeff_level, list):
            coeff_orientations_numpy = []
            for coeff_orientation in coeff_level:
                if isinstance(coeff_orientation, torch.Tensor):
                    coeff_orientation_numpy = coeff_orientation[example_idx].cpu().numpy()
                    coeff_orientation_numpy = coeff_orientation_numpy[:,:,0] + 1j*coeff_orientation_numpy[:,:,1]
                else:
                    raise ValueError('coeff orientation must be of type (list, torch.Tensor)')
                coeff_orientations_numpy.append(coeff_orientation_numpy)
            coeff.append(coeff_orientations_numpy)
        else:
            raise ValueError('coeff level must be of type (list, torch.Tensor)')
    return coeff

################################################################################

def make_grid_coeff(coeff, normalize=True):
    '''
    Visualization function for building a large image that contains the
    low-pass, high-pass and all intermediate levels in the steerable pyramid. 
    For the complex intermediate bands, the real part is visualized.
    
    Args:
        coeff (list): complex pyramid stored as list containing all levels
        normalize (bool, optional): Defaults to True. Whether to normalize each band
    
    Returns:
        np.ndarray: large image that contains grid of all bands and orientations
    '''
    M, N = coeff[1][0].shape
    Norients = len(coeff[1])
    out = np.zeros((M * 2 - coeff[-1].shape[0], Norients * N))
    currentx, currenty = 0, 0

    for i in range(1, len(coeff[:-1])):
        for j in range(len(coeff[1])):
            tmp = coeff[i][j].real
            m, n = tmp.shape
            if normalize:
                tmp = 255 * tmp/tmp.max()
            tmp[m-1,:] = 255
            tmp[:,n-1] = 255
            out[currentx:currentx+m,currenty:currenty+n] = tmp
            currenty += n
        currentx += coeff[i][0].shape[0]
        currenty = 0

    m, n = coeff[-1].shape
    out[currentx: currentx+m, currenty: currenty+n] = 255 * coeff[-1]/coeff[-1].max()
    out[0,:] = 255
    out[:,0] = 255
    return out.astype(np.uint8)
"
api/tester.py,"from sampler.snippet_sampler import Snippet_Sampler
from video_processor import Video_Processor
from resnet50_extractor import Resnet50_Extractor
from phase_difference_extractor import Phase_Difference_Extractor
from mimamo_net import Two_Stream_RNN
from torch.autograd import Variable as Variable
import os
import torch
import tqdm
import numpy as np
import pandas as pd
from steerable.utils import get_device
device = get_device()
class Tester(object):
    def __init__(self,
             # parameters for testing
                 model_path , 
                 batch_size,
                 workers = 0,
                 # parameters for Video_Processor
                 save_size=112, nomask=True, grey=False, quiet=True,
                 tracked_vid=False, noface_save=False,
                 OpenFace_exe = 'OpenFace/build/bin/FeatureExtraction',
                 # parameters for Resnet50_Extractor
                 benchmark_dir = 'pytorch-benchmarks',model_name= 'resnet50_ferplus_dag',
                 feature_layer = 'pool5_7x7_s1',
                 # parameters for Snippet_Sampler
                 num_phase=12, phase_size = 48, 
                 length=64, stride=64,
                 # parameters for Phase_Difference_Extractor
                 height=4, nbands=2, scale_factor=2, 
                 extract_level = [1,2]
                 ):
        self.batch_size = batch_size
        self.workers = workers
        self.num_phase = num_phase
        self.phase_size = phase_size
        self.length = length
        self.stride = stride
        self.video_processor = Video_Processor(save_size, nomask, grey, quiet,
                              tracked_vid, noface_save, OpenFace_exe)
        self.resnet50_extractor =  Resnet50_Extractor(benchmark_dir, model_name, feature_layer)
        self.phase_difference_extractor = Phase_Difference_Extractor(height, nbands, scale_factor, 
                                          extract_level, not quiet)
        self.model = Two_Stream_RNN()
        assert os.path.exists(model_path)
        checkpoint = torch.load(model_path, map_location=device)
        self.model.load_state_dict(checkpoint['state_dict'])
        start_epoch = checkpoint['epoch']
        print(""load checkpoint from {}, epoch:{}"".format(model_path, start_epoch))
        self.model.to(device)
        self.label_name = ['valence', 'arousal'] # the pretrained model output format
    def test(self, input_video):
        video_name = os.path.basename(input_video).split('.')[0]
        # first input video is processed using OpenFace
        opface_output_dir = os.path.join(os.path.dirname(input_video), 
                video_name+""_opface"")
        self.video_processor.process(input_video, opface_output_dir)
        # then the cropped and aligned faces are fed to resnet50_extractor
        feature_dir = os.path.join(os.path.dirname(input_video), 
                video_name+""_pool5"")
        self.resnet50_extractor.run(opface_output_dir, feature_dir, video_name = video_name)
        
        # sampling images
        dataset = Snippet_Sampler(video_name, opface_output_dir, feature_dir,
        	annot_dir = None, label_name = 'valence_arousal',
            test_mode = True, num_phase=self.num_phase, phase_size = self.phase_size, 
            length=self.length, stride=self.stride)
        data_loader = torch.utils.data.DataLoader(
                       dataset,
                       batch_size=self.batch_size,
                       num_workers=self.workers, pin_memory=False)
        results = self.test_on_dataloader(data_loader, self.model)
        return results

    def test_on_dataloader(self, dataloader, model, train_mean=None, train_std=None):
        model.eval()
        sample_names = []
        sample_preds = []
        sample_ranges = []
        for i, data_batch in enumerate(dataloader):
            phase_f, rgb_f, label, ranges, names = data_batch
            with torch.no_grad():
                phase_f = phase_f.type('torch.FloatTensor').to(device)
                phase_0, phase_1 = self.phase_diff_output(phase_f, self.phase_difference_extractor)
                rgb_f = Variable(rgb_f.type('torch.FloatTensor').to(device))
                phase_0 = Variable(phase_0.type('torch.FloatTensor').to(device))
                phase_1 = Variable(phase_1.type('torch.FloatTensor').to(device))
            
            output = model([phase_0,phase_1], rgb_f)
            sample_names.append(names)
            sample_ranges.append(ranges)
            sample_preds.append(output.cpu().data.numpy())
        sample_names = np.concatenate([arr for arr in sample_names], axis=0)
        sample_preds = np.concatenate([arr for arr in sample_preds], axis=0)
        n_sample, n_length, n_labels = sample_preds.shape
        if train_mean is not None and train_std is not None:
            # scale 
            trans_sample_preds = sample_preds.reshape(-1, n_labels)
            trans_sample_preds = np.array([correct(trans_sample_preds[:, i], train_mean[i], train_std[i]) for i  in range(n_labels)])
            sample_preds = trans_sample_preds.reshape(n_sample, n_length, n_labels)
        sample_ranges = np.concatenate([arr for arr in sample_ranges], axis=0)
        video_dict = {}
        for video in sample_names:
            mask = sample_names==video
            video_ranges = sample_ranges[mask]
            if video not in video_dict.keys():
                max_len = max([ranges[-1] for ranges in video_ranges])
                video_dict[video] = np.zeros((max_len, n_labels))
            video_preds = sample_preds[mask]
            # make sure the dataset returns full range of video frames
            min_f, max_f = 0, 0
            for rg, pred in zip(video_ranges, video_preds):
                start, end = rg
                video_dict[video][start:end, :] = pred
                min_f = min(min_f, start)
                max_f = max(max_f, end)
            assert (min_f==0) and (max_f == max_len)
        for video in video_dict.keys():
            video_dict[video] = pd.DataFrame(data=video_dict[video], columns=self.label_name)
        return video_dict  
    def phase_diff_output(self, phase_batch, steerable_pyramid):
        """"""
        extract the first level and the second level phase difference images 
        """"""
        sp = steerable_pyramid
        bs, num_frames, num_phases, W, H = phase_batch.size()
        
        coeff_batch = sp.build_pyramid(phase_batch.view(bs*num_frames, num_phases, W, H))
        assert isinstance(coeff_batch, list)
        phase_batch_0 = sp.extract(coeff_batch[0])
        N, n_ch, n_ph, W, H= phase_batch_0.size()
        phase_batch_0 = phase_batch_0.view(N, -1, W, H)
        phase_batch_0 = phase_batch_0.view(bs, num_frames, -1, W, H)
        phase_batch_1 = sp.extract(coeff_batch[1])
        N, n_ch, n_ph, W, H= phase_batch_1.size()
        phase_batch_1 = phase_batch_1.view(N, -1, W, H)
        phase_batch_1 = phase_batch_1.view(bs, num_frames, -1, W, H)
        return phase_batch_0, phase_batch_1
"
api/utils/data_utils.py,"import torchvision.transforms as transforms
import numbers
import torchvision
import random
from PIL import Image, ImageOps
import numpy as np
import numbers
import math
import torch

class GroupRandomCrop(object):
    def __init__(self, size, seed=None):
        if isinstance(size, numbers.Number):
            self.size = (int(size), int(size))
        else:
            self.size = size
        self.seed = seed
    def __call__(self, img_group):

        w, h = img_group[0].size
        th, tw = self.size

        out_images = list()
        if self.seed is not None:
            random.seed(self.seed)
        x1 = random.randint(0, w - tw)
        y1 = random.randint(0, h - th)

        for img in img_group:
            assert(img.size[0] == w and img.size[1] == h)
            if w == tw and h == th:
                out_images.append(img)
            else:
                out_images.append(img.crop((x1, y1, x1 + tw, y1 + th)))

        return out_images

class GroupRandomHorizontalFlip(object):
    """"""Randomly horizontally flips the given PIL.Image with a probability of 0.5
    """"""
    def __init__(self, seed=None):
        self.seed = seed

    def __call__(self, img_group, is_flow=False):
        if self.seed is not None:
            random.seed(self.seed)
        v = random.random()
        if v < 0.5:
            ret = [img.transpose(Image.FLIP_LEFT_RIGHT) for img in img_group]
            return ret
        else:
            return img_group


class GroupNormalize(object):
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def __call__(self, tensor):
        rep_mean = self.mean * (tensor.size()[0]//len(self.mean))
        rep_std = self.std * (tensor.size()[0]//len(self.std))

        # TODO: make efficient
        for t, m, s in zip(tensor, rep_mean, rep_std):
            t.sub_(m).div_(s)

        return tensor


class GroupScale(object):
    """""" Rescales the input PIL.Image to the given 'size'.
    'size' will be the size of the smaller edge.
    For example, if height > width, then image will be
    rescaled to (size * height / width, size)
    size: size of the smaller edge
    interpolation: Default: PIL.Image.BILINEAR
    """"""

    def __init__(self, size, interpolation=Image.LANCZOS):
        self.worker = torchvision.transforms.Resize(size, interpolation)

    def __call__(self, img_group):
        return [self.worker(img) for img in img_group]



class Stack(object):

    def __init__(self, roll=False):
        self.roll = roll

    def __call__(self, img_group):
        if img_group[0].mode == 'L':
            return np.stack(img_group, axis=2)
        elif img_group[0].mode == 'RGB':
            if self.roll:
                return np.stack([np.array(x)[:, :, ::-1] for x in img_group], axis=3)
            else:
                return np.stack(img_group, axis=3)


class ToTorchFormatTensor(object):
    """""" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]
    to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] """"""
    def __init__(self, div=True):
        self.div = div

    def __call__(self, pic):
        if isinstance(pic, np.ndarray):
            # handle numpy array
            if len(pic.shape)==3:
                W, H, n_frames =pic.shape
                img = torch.from_numpy(pic).permute(2, 0, 1).contiguous() #frames, W, H
            elif len(pic.shape)==4:
                W, H, channel, n_frames = pic.shape
                img = torch.from_numpy(pic).permute(3, 2, 0, 1).contiguous() #frames, channels, W, H
        else:
            raise ValueError(""input image has to be ndarray!"")
        return img.float().div(255) if self.div else img.float()


class IdentityTransform(object):

    def __call__(self, data):
        return data

class RandomCrop(object):
    def __init__(self, size, v):
        if isinstance(size, numbers.Number):
            self.size = (int(size), int(size))
        else:
            self.size = size
        self.v = v
    def __call__(self, img):

        w, h = img.size
        th, tw = self.size
        x1 = int(( w - tw)*self.v)
        y1 = int(( h - th)*self.v)
        #print(""print x, y:"", x1, y1)
        assert(img.size[0] == w and img.size[1] == h)
        if w == tw and h == th:
            out_image = img
        else:
            out_image = img.crop((x1, y1, x1 + tw, y1 + th)) #same cropping method for all images in the same group
        return out_image

class RandomHorizontalFlip(object):
    """"""Randomly horizontally flips the given PIL.Image with a probability of 0.5
    """"""
    def __init__(self, v):
        self.v = v
        return
    def __call__(self, img):
        if self.v < 0.5:
            img = img.transpose(Image.FLIP_LEFT_RIGHT) 
        #print (""horiontal flip: "",self.v)
        return img

def augment_transforms(meta, resize=256, random_crop=True, 
                      override_meta_imsize=False):
    normalize = transforms.Normalize(mean=meta['mean'], std=meta['std'])
    im_size = meta['imageSize']
    assert im_size[0] == im_size[1], 'expected square image size'
    if random_crop:
        v = random.random()
        transform_list = [transforms.Resize(resize),
                          RandomCrop(im_size[0], v),
                          RandomHorizontalFlip(v)]
    else:
        if override_meta_imsize:
            im_size = (resize, resize)
        transform_list = [transforms.Resize(size=(im_size[0], im_size[1]))]
    transform_list += [transforms.ToTensor()]
    if meta['std'] == [1,1,1]: # common amongst mcn models
        transform_list += [lambda x: x * 255.0]
    transform_list.append(normalize)
    return transforms.Compose(transform_list)
"
api/utils/model_utils.py,"import os
import argparse
from os.path import join as pjoin
import torchvision.transforms as transforms
import six
def compose_transforms(meta, resize=256, center_crop=True,
                       override_meta_imsize=False):
    """"""Compose preprocessing transforms for model

    The imported models use a range of different preprocessing options,
    depending on how they were originally trained. Models trained in MatConvNet
    typically require input images that have been scaled to [0,255], rather
    than the [0,1] range favoured by PyTorch.

    Args:
        meta (dict): model preprocessing requirements
        resize (int) [256]: resize the input image to this size
        center_crop (bool) [True]: whether to center crop the image
        override_meta_imsize (bool) [False]: if true, use the value of `resize`
           to select the image input size, rather than the properties contained
           in meta (this option only applies when center cropping is not used.

    Return:
        (transforms.Compose): Composition of preprocessing transforms
    """"""
    normalize = transforms.Normalize(mean=meta['mean'], std=meta['std'])
    im_size = meta['imageSize']
    assert im_size[0] == im_size[1], 'expected square image size'
    if center_crop:
        transform_list = [transforms.Resize(resize),
                          transforms.CenterCrop(size=(im_size[0], im_size[1]))]
    else:
        if override_meta_imsize:
            im_size = (resize, resize)
        transform_list = [transforms.Resize(size=(im_size[0], im_size[1]))]
    transform_list += [transforms.ToTensor()]
    if meta['std'] == [1, 1, 1]:  # common amongst mcn models
        transform_list += [lambda x: x * 255.0]
    transform_list.append(normalize)
    return transforms.Compose(transform_list)


def load_module_2or3(model_name, model_def_path):
    """"""Load model definition module in a manner that is compatible with
    both Python2 and Python3

    Args:
        model_name: The name of the model to be loaded
        model_def_path: The filepath of the module containing the definition

    Return:
        The loaded python module.""""""
    if six.PY3:
        import importlib.util
        spec = importlib.util.spec_from_file_location(model_name, model_def_path)
        mod = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(mod)
    else:
        import importlib
        dirname = os.path.dirname(model_def_path)
        sys.path.insert(0, dirname)
        module_name = os.path.splitext(os.path.basename(model_def_path))[0]
        mod = importlib.import_module(module_name)
    return mod
def load_model(model_name, MODEL_DIR):
    """"""Load imoprted PyTorch model by name

    Args:
        model_name (str): the name of the model to be loaded

    Return:
        nn.Module: the loaded network
    """"""
    model_def_path = pjoin(MODEL_DIR, model_name + '.py')
    weights_path = pjoin(MODEL_DIR, model_name + '.pth')
    mod = load_module_2or3(model_name, model_def_path)
    func = getattr(mod, model_name)
    net = func(weights_path=weights_path)
    return net

"
api/utils/phase_utils.py,"import torch
import math
from torch.nn import functional as F
import numpy as np
def torch_unwrap(tensor, discont=math.pi, dim=-1):
    nd = len(tensor.size())
    dd = torch_diff(tensor, dim=dim)
    slice1 = [slice(None, None)]*nd     # full slices
    slice1[dim] = slice(1, None)
    slice1 = tuple(slice1)
    PI = math.pi
    ddmod = torch.fmod(dd + PI, 2*PI) - PI
    id1 = (ddmod == -PI) & (dd > 0)
    ddmod[id1] = PI
    ph_correct = ddmod - dd
    id2 = torch.abs(dd) < discont
    ph_correct[id2] = 0
    up = tensor.clone().detach()
    up[slice1] = tensor[slice1] + ph_correct.cumsum(dim=dim)
    return up
def torch_diff(tensor,n=1, dim=-1):
    """"""
    tensor : Input Tensor
    n : int, optional
        The number of times values are differenced. If zero, the input
        is returned as-is.
    axis : int, optional
        The axis along which the difference is taken, default is the
        last axis.
    """"""
    nd = len(tensor.size())
    slice1 = [slice(None)] * nd
    slice2 = [slice(None)] * nd
    slice1[dim] = slice(1, None)
    slice2[dim] = slice(None, -1)
    slice1 = tuple(slice1)
    slice2 = tuple(slice2)
    for _ in range(n):
        tensor = tensor[slice1] - tensor[slice2]
    return tensor
def extract_from_batch(coeff_batch, example_idx=0, symmetry = True):
    '''
    Given the batched Complex Steerable Pyramid, extract the coefficients
    for a single example from the batch. Additionally, it converts all
    torch.Tensor's to np.ndarrays' and changes creates proper np.complex
    objects for all the orientation bands. 
    Args:
        coeff_batch (list): list containing low-pass, high-pass and pyr levels
        example_idx (int, optional): Defaults to 0. index in batch to extract
    
    Returns:
        list: list containing low-pass, high-pass and pyr levels as np.ndarray
    '''
    if not isinstance(coeff_batch, list):
        raise ValueError('Batch of coefficients must be a list')
    coeff = []  # coefficient for single example
    for coeff_level in coeff_batch:
        if isinstance(coeff_level, torch.Tensor):
            # Low- or High-Pass
            coeff_level_numpy = coeff_level[example_idx].cpu().numpy()
            if symmetry:
                W, H = coeff_level_numpy.shape
                coeff_level_numpy = coeff_level_numpy[:W//2, :H//2]
            coeff.append(coeff_level_numpy)
        elif isinstance(coeff_level, list):
            coeff_orientations_numpy = []
            for coeff_orientation in coeff_level:
                coeff_orientation_numpy = coeff_orientation[example_idx].cpu().numpy()
                coeff_orientation_numpy = coeff_orientation_numpy[:,:,0] + 1j*coeff_orientation_numpy[:,:,1]
                if symmetry:
                    W, H = coeff_orientation_numpy.shape
                    coeff_orientation_numpy= coeff_orientation_numpy[:W//2, :H//2]
                coeff_orientations_numpy.append(coeff_orientation_numpy)
            coeff.append(coeff_orientations_numpy)
        else:
            raise ValueError('coeff leve must be of type (list, torch.Tensor)')
    return coeff
def amplitude_based_gaussian_blur(mag, phase, g_kernel):
    bs, n_frames, W, H = phase.size()
    mag_phase = torch.mul(mag, phase)
    in_channel = n_frames
    out_channel = n_frames
    m, n = g_kernel.size()
    filters = torch.stack([g_kernel]*out_channel, dim=0)
    filters = torch.unsqueeze(filters, 1) # (output_channel, input_channel/groups, W, H)
    filters = filters.type(phase.type())
    mag_phase_blurred = F.conv2d(mag_phase, filters, groups = in_channel, padding=m//2)
    mag_blurred = F.conv2d(mag, filters, groups = in_channel, padding=m//2)
    result = torch.div(mag_phase_blurred, mag_blurred)
    return result
def amplitude_based_gaussian_blurcoeff_batch_numpy(mag, phase, g_kernel):
    bs, n_frames, W, H = phase.size()
    phase = phase.cpu().numpy()
    mag = mag.cpu().numpy()
    g_kernel = g_kernel.cpu().numpy()
    from scipy.signal import convolve2d
    new_phase = []
    for b in range(bs):
        new_phase_b = []
        for f in range(n_frames):
            p = phase[b,f,...]
            m = mag[b, f,...]
            denoised_phase =  convolve2d(np.multiply(p, m), g_kernel, mode='same')/convolve2d(m, g_kernel, mode='same')
            new_phase_b.append(denoised_phase)
        new_phase.append(new_phase_b)
    new_phase = np.asarray(new_phase)
    return torch.Tensor(new_phase).type('torch.FloatTensor').cuda(async=True)
def gaussian_kernel(std, tap = 11):
    kernel = np.zeros((tap, tap))
    for x in range(tap):
        for y in range(tap):
            x0 = x - tap//2
            y0 = y - tap//2
            kernel[x, y] = np.exp(- (x0**2+y0**2)/(2*std**2))
    return kernel
def symmetric_extension_batch(img_batch):
    #  img_batch, None, W, H (the last two aixes are two dimensional image)
    img_batch_inverse_col = img_batch.clone().detach()
    inv_idx_col = torch.arange(img_batch.size(-1)-1, -1, -1).long()
    img_batch_inverse_col = img_batch_inverse_col[..., :, inv_idx_col]
    img_batch_inverse_row = img_batch.clone().detach()
    inv_idx_row = torch.arange(img_batch.size(-2)-1, -1, -1).long()
    img_batch_inverse_row = img_batch_inverse_row[..., inv_idx_row, :]
    img_batch_inverse_row_col = img_batch_inverse_col.clone().detach()
    img_batch_inverse_row_col = img_batch_inverse_row_col[:,:, inv_idx_row, :]
    img_batch_0 = torch.cat([img_batch, img_batch_inverse_col], dim=-1)
    img_batch_1 = torch.cat([img_batch_inverse_row, img_batch_inverse_row_col], dim=-1)
    new_img_batch = torch.cat([img_batch_0, img_batch_1], dim=-2)
    return new_img_batch
"
api/video_processor.py,"import os
class Video_Processor(object):
    def __init__(self, size=112, nomask=True, grey=False, quiet=True,
                 tracked_vid=False, noface_save=False,
                 OpenFace_exe = 'OpenFace/build/bin/FeatureExtraction'):
        ''' Video Processor using OpenFace to do face detection and face alignment
        Given an input video, this processor will create a directory where all cropped and aligned 
        faces are saved.

        Parameters: 
            size: int, default 112
                The output faces will be saved as images where the width and height are size pixels.
            nomask: bool, default True
                If True, the output face image will not be masked (mask the region except the face).
                Otherwise, the output face image will be masked, not containing background.
            grey: bool, default False
                If True, the output face image will be saved as greyscale images instead of RGB images.
            quiet: bool, default False
                If False, will print out the processing steps live.
            tracked_vid: bool, default False
                If True, will save the tracked video, which is an output video with detected landmarks.
            noface_save: bool, default False
                If True, those frames where face detection is failed will be saved (blank image); 
                else those failed frames will not saved.
            OpenFace_exe: String, default is 'OpenFace/build/bin/FeatureExtraction'
                By default, the OpenFace library is installed in the same directory as Video_Processor.
                It can be changed to the current OpenFace executable file.
        '''
        self.size = size
        self.nomask = nomask
        self.grey = grey
        self.quiet = quiet
        self.tracked_vid = tracked_vid
        self.noface_save = noface_save
        self.OpenFace_exe = OpenFace_exe 
        if not isinstance(self.OpenFace_exe, str) or not os.path.exists(self.OpenFace_exe):
            raise ValueError(""OpenFace_exe has to be string object and needs to exist."")
        self.OpenFace_exe = os.path.abspath(self.OpenFace_exe)
    def process(self, input_video, output_dir=None):
        '''        
        Arguments:
            input_video: String
               The input video path, or the input sequence directory, where each image representing a frame, e.g. 001.jpg, 002.jpg, 003.jpg ... 200.jpg
            output_dir: String, default None
               The output faces will be saved in output_dir. By default the output_dir will be in 
               the same parent directory as the input video is.
        '''

        if not isinstance(input_video, str) or not os.path.exists(input_video):
            raise ValueError(""input video has to be string object and needs to exist."")
        if os.path.isdir(input_video):
            assert len(os.listdir(input_video))>0, ""Input sequence directory {} cannot be empty"".format(input_video)
            arg_input = '-fdir'
        else:
            arg_input = '-f'

        input_video = os.path.abspath(input_video)
        if output_dir is None:
            output_dir = os.path.join(os.path.dirname(input_video), 
                os.path.basename(input_video).split('.')[0])
        if isinstance(output_dir, str):
            if not os.path.exists(output_dir):
                os.makedirs(output_dir)
            else:
            	print(""output dir exists: {}. Video processing skipped."".format(output_dir))
            	return
        else:
            raise ValueError(""output_dir should be string object."")
        opface_option = "" {} "".format(arg_input)+input_video + "" -out_dir ""+ output_dir +"" -simsize ""+ str(self.size)
        opface_option += "" -2Dfp -3Dfp -pdmparams -pose -aus -gaze -simalign ""

        if not self.noface_save:
            opface_option +="" -nobadaligned ""
        if self.tracked_vid:
            opface_option +="" -tracked ""
        if self.nomask:
            opface_option+= "" -nomask""
        if self.grey:
            opface_option += "" -g""
        if self.quiet:
            opface_option += "" -q""
        # execution
        call = self.OpenFace_exe + opface_option
        os.system(call)



"
docs/walkthrough.md,"# MIMAMO-Net Python Library

Python implementation of client side functions for interacting with the MIMAMO-Net.

## Usage
With inputs of video snippets of a face emotion, process the video with MIMAMO-Net and gives a result of an array (what is the name/term of that result?)
1. Parse the video file with OpenFace
2. Parse the cropped and aligned face images with Resnet 50 feature extraction
3. Use the Image sampler to create face snippets
4. more explanations
5. etc


## Methods
### Component - Image Sampler
`Aff-wild-exps/dataloader.py`
#### *class* `Face_Dataset`
  **Arguments**:
  - root_path
    - type: `str`
    - the path to root dir (just an example, explain more on the purpose of this dir)
  - feature_path
  - annot_dir
  - video_name_list
  - label_name
  - py_level=4
  - py_nbands=2
  - test_mode=False
    - type: `bool`
    - explains why the `False` flag is default and what happens when it's changed to `True`
  - num_phase=12
  - phase_size = 48
  - length=64
  - stride=32
  - return_phase=False
  
  **Functionality**:
  - Parses videos from a directory and return a list (?) (I don't fully know what is the return type of the class `Face_Dataset`)

  **expected returned object**: `iterable` object of lists
  ```
  [[phase_batch_0,phase_batch_1], np.array(imgs), np.array(seq_labels), np.array([start, end]), video_record.video]
  ```
  - [phase_batch_0,phase_batch_1]
    - explanation
  - np.array(imgs)
    - explanation
    - imgs
      - explanation
  - np.array(seq_labels)
  - np.array([start, end])
    - start
    - end
  - video_record.video

  **example usage**: (I don't think this is sufficient explanation of what the code does, need help on this part)
  ```
  # instantiate a Face_Dataset object
  train_dataset = Face_Dataset(root_path, feature_path, annot_dir, video_names, label_name='arousal_valence',  num_phase=12 , phase_size=48, test_mode=True)
  # explanation here
  train_loader = torch.utils.data.DataLoader(
    train_dataset, 
    batch_size = 4, 
    num_workers=0, pin_memory=False )
  ```

**example returned object**:
  ```
[
    [
        value1,
        value2
    ],
    object,
    object,
    object
]
  ```  

### Component - Phase difference extraction
`/Aff-wild-exps/utils.py`
#### *class* `Steerable_Pyramid_Phase`
  **Arguments**:
  - example
    - type: `int`
    - (just an example)
  
  **Functionality**:
  - Example (needs input)

  **expected returned object**: `iterable` object of lists (just example)
  ```
  [[phase_batch_0,phase_batch_1], np.array(imgs), np.array(seq_labels), np.array([start, end]), video_record.video]
  ```
  - [phase_batch_0,phase_batch_1]
    - explanation
  - np.array(imgs)
    - explanation
    - imgs
      - explanation
  - np.array(seq_labels)
  - np.array([start, end])
    - start
    - end
  - video_record.video

  **example usage**: (I don't think this is sufficient explanation of what the code does, need help on this part)
  ```
  # objective of this code snipper
  print(""hello world"")
  ```

**example returned object**:
  ```
[
    [
        value1,
        value2
    ],
    object,
    object,
    object
]
  ```  
"
model.png,
scripts/CNN_feature_extraction.py,"
from __future__ import division

import os
import time
import six
import sys
from tqdm import tqdm
import argparse
import pickle
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import torch.utils.data
from os.path import join as pjoin
import torch.backends.cudnn as cudnn
import torchvision.transforms as transforms
import glob
import numbers
from PIL import Image, ImageOps
import random
import copy
# for torch lower version
import torch._utils
from torch.nn import functional as F
try:
    torch._utils._rebuild_tensor_v2
except AttributeError:
    def _rebuild_tensor_v2(storage, storage_offset, size, stride, requires_grad, backward_hooks):
        tensor = torch._utils._rebuild_tensor(storage, storage_offset, size, stride)
        tensor.requires_grad = requires_grad
        tensor._backward_hooks = backward_hooks
        return tensor
    torch._utils._rebuild_tensor_v2 = _rebuild_tensor_v2
global parsed
import torch.utils.data as data
class VideoRecord(object):
    def __init__(self, video, face_dir, annot_dir, label_name, test_mode = False):
        self.video = video
        self.face_dir = face_dir
        self.annot_dir = annot_dir
        self.label_name = label_name
        self.test_mode = test_mode
        self.path_label = self.get_path_label()
     
    def get_path_label(self):
        frames = glob.glob(os.path.join(self.face_dir, self.video, self.video+""_aligned"", '*.bmp'))
        frames = sorted(frames, key  = lambda x: os.path.basename(x).split(""."")[0].split(""_"")[-1])
        if len(frames)==0:
            raise ValueError(""number of frames of video {} should not be zero."".format(self.video))
        annot_file = os.path.join(self.annot_dir, 'train',self.label_name, self.video+"".txt"")
        if (not self.test_mode) and (not os.path.exists(annot_file)):
            raise ValueError(""Annotation file not found: the training mode should always has annotation file!"")
        if self.test_mode:
            return [frames, [-100]*len(frames)]
        else:
            f = open(annot_file, ""r"")
            corr_frames, labels = [], []
            for i, x in enumerate(f):
                label = float(x)
                corr_frame = os.path.join(self.face_dir, self.video, self.video+""_aligned"", 'frame_det_00_{0:06d}.bmp'.format(i+1))
                if os.path.exists(corr_frame):
                    corr_frames.append(corr_frame)
                    labels.append(label)
                else:
                    # skip those frames and labels
                    continue
            f.close()
            assert len(corr_frames) == len(labels)
            return [corr_frames, labels]
    def __str__(self):
        string = ''
        for key, record in self.utterance_dict.items():
            string += str(record)+'\n'
        return string
 
class Face_Dataset(data.Dataset):
    def __init__(self, root_path, annot_dir, video_name_list,  label_name, test_mode =False, transform = None):
        self.root_path = root_path
        self.annot_dir = annot_dir
        self.video_name_list = video_name_list
        self.label_name = label_name
        self.test_mode = test_mode
        self.transform = transform
        self.parse_videos()

    def parse_videos(self):
        videos = self.video_name_list
        self.video_list = list()
        self.frame_ids = []
        self.video_ids = []
        i = 0
        for vid in tqdm(videos):
            v_record = VideoRecord(vid, self.root_path, self.annot_dir, self.label_name, self.test_mode)
            frames, labels = v_record.path_label
            if len(frames) !=0 and (len(frames)==len(labels)):
                self.video_list.append(v_record)
            self.video_ids.extend([i]*len(frames))
            self.frame_ids.extend(list(np.arange(len(frames))))
            i +=1
        print(""number of videos:{}, number of frames:{}"".format(len(self.video_list), len(self.frame_ids)))

    def __len__(self):
        return len(self.frame_ids)
    def __getitem__(self, index):
        f_id = self.frame_ids[index]
        video_record = self.video_list[self.video_ids[index]]
        frames, labels = video_record.path_label
        frame, label = frames[f_id], labels[f_id]
        img = Image.open(frame)
        if self.transform is not None:
            img = self.transform(img)
        return img, label, frame, video_record.video
def load_module_2or3(model_name, model_def_path):
    """"""Load model definition module in a manner that is compatible with
    both Python2 and Python3

    Args:
        model_name: The name of the model to be loaded
        model_def_path: The filepath of the module containing the definition

    Return:
        The loaded python module.""""""
    if six.PY3:
        import importlib.util
        spec = importlib.util.spec_from_file_location(model_name, model_def_path)
        mod = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(mod)
    else:
        import importlib
        dirname = os.path.dirname(model_def_path)
        sys.path.insert(0, dirname)
        module_name = os.path.splitext(os.path.basename(model_def_path))[0]
        mod = importlib.import_module(module_name)
    return mod
def load_model(model_name, MODEL_DIR):
    """"""Load imoprted PyTorch model by name

    Args:
        model_name (str): the name of the model to be loaded

    Return:
        nn.Module: the loaded network
    """"""
    model_def_path = pjoin(MODEL_DIR, model_name + '.py')
    weights_path = pjoin(MODEL_DIR, model_name + '.pth')
    mod = load_module_2or3(model_name, model_def_path)
    func = getattr(mod, model_name)
    net = func(weights_path=weights_path)
    return net

def compose_transforms(meta, resize=256, center_crop=True, 
                      override_meta_imsize=False):
    """"""Compose preprocessing transforms for model

    The imported models use a range of different preprocessing options,
    depending on how they were originally trained. Models trained in MatConvNet
    typically require input images that have been scaled to [0,255], rather
    than the [0,1] range favoured by PyTorch.

    Args:
        meta (dict): model preprocessing requirements
        resize (int) [256]: resize the input image to this size
        center_crop (bool) [True]: whether to center crop the image
        override_meta_imsize (bool) [False]: if true, use the value of `resize`
           to select the image input size, rather than the properties contained
           in meta (this option only applies when center cropping is not used.

    Return:
        (transforms.Compose): Composition of preprocessing transforms
    """"""
    normalize = transforms.Normalize(mean=meta['mean'], std=meta['std'])
    im_size = meta['imageSize']
    assert im_size[0] == im_size[1], 'expected square image size'
    if center_crop:
        transform_list = [transforms.Resize(resize),
                          transforms.CenterCrop(size=(im_size[0], im_size[1]))]
    else:
        if override_meta_imsize:
            im_size = (resize, resize)
        transform_list = [transforms.Resize(size=(im_size[0], im_size[1]))]
            
    transform_list += [transforms.ToTensor()]
    if meta['std'] == [1,1,1]: # common amongst mcn models
        transform_list += [lambda x: x * 255.0]
    transform_list.append(normalize)
    return transforms.Compose(transform_list)

def augment_transforms(meta, resize=256, random_crop=True, 
                      override_meta_imsize=False):
    normalize = transforms.Normalize(mean=meta['mean'], std=meta['std'])
    im_size = meta['imageSize']
    assert im_size[0] == im_size[1], 'expected square image size'
    if random_crop:
        v = random.random()
        transform_list = [transforms.Resize(resize),
                          RandomCrop(im_size[0], v),
                          RandomHorizontalFlip(v)]
    else:
        if override_meta_imsize:
            im_size = (resize, resize)
        transform_list = [transforms.Resize(size=(im_size[0], im_size[1]))]
    transform_list += [transforms.ToTensor()]
    if meta['std'] == [1,1,1]: # common amongst mcn models
        transform_list += [lambda x: x * 255.0]
    transform_list.append(normalize)
    return transforms.Compose(transform_list)
class RandomCrop(object):
    def __init__(self, size, v):
        if isinstance(size, numbers.Number):
            self.size = (int(size), int(size))
        else:
            self.size = size
        self.v = v
    def __call__(self, img):

        w, h = img.size
        th, tw = self.size
        x1 = int(( w - tw)*self.v)
        y1 = int(( h - th)*self.v)
        #print(""print x, y:"", x1, y1)
        assert(img.size[0] == w and img.size[1] == h)
        if w == tw and h == th:
            out_image = img
        else:
            out_image = img.crop((x1, y1, x1 + tw, y1 + th)) #same cropping method for all images in the same group
        return out_image

class RandomHorizontalFlip(object):
    """"""Randomly horizontally flips the given PIL.Image with a probability of 0.5
    """"""
    def __init__(self, v):
        self.v = v
        return
    def __call__(self, img):
        if self.v < 0.5:
            img = img.transpose(Image.FLIP_LEFT_RIGHT) 
        #print (""horiontal flip: "",self.v)
        return img
def get_vec( model, layer_name, image):
    bs = image.size(0)
    if parsed.layer_name == 'pool5_full':
        layer_name = 'pool5'
    layer = model._modules.get(layer_name)
    if parsed.layer_name == 'fc7':
        layer_output_size = 4096
        my_embedding = torch.zeros(bs, layer_output_size)
    elif parsed.layer_name == 'fc8':
        my_embedding = torch.zeros(bs, 7)
    elif parsed.layer_name == 'pool5' or parsed.layer_name == 'pool5_full':
        my_embedding = torch.zeros([bs, 512, 7, 7])
    elif parsed.layer_name == 'pool4':
        my_embedding = torch.zeros([bs, 512, 14, 14])
    elif parsed.layer_name == 'pool3':
        my_embedding = torch.zeros([bs, 256, 28, 28])
    elif parsed.layer_name =='pool5_7x7_s1':
         my_embedding= torch.zeros([bs, 2048, 1, 1])
    elif parsed.layer_name =='conv5_3_3x3_relu':
         my_embedding = torch.zeros([bs, 512, 7, 7])
    def copy_data(m, i, o):
        my_embedding.copy_(o.data)
    h = layer.register_forward_hook(copy_data)
    h_x = model(image)
    h.remove()
    if parsed.layer_name == 'pool5' or parsed.layer_name == 'conv5_3_3x3_relu':
        GAP_layer = nn.AvgPool2d(kernel_size = [7,7], stride=(1, 1))
        my_embedding = GAP_layer(my_embedding)
    return F.relu(my_embedding.squeeze())
def get_frame_index(frame_path):
    frame_name = frame_path.split('/')[-1]
    frame_num  = int(frame_name.split('.')[0].split('_')[-1])
    return frame_num
def predict(data_loader,layer_name, model, des_dir):
    with torch.no_grad():
        for ims, target, img_path, video_name in tqdm(data_loader):
            ims = ims.cuda(async=True)
            output = get_vec(model, layer_name, ims)
            for feature, path, video_n in zip(output, img_path, video_name):
                des_path = os.path.join(des_dir, video_n)
                if not os.path.exists(des_path):
                    os.makedirs(des_path)
                des_path = os.path.join(des_path, ""%05d.npy""%get_frame_index(path))
                np.save(des_path, feature)


def feature_extraction(model, loader, des_dir):
    model.eval()
    predict(loader, parsed.layer_name, model, des_dir)
    
def main():
    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
    os.environ['CUDA_VISIBLE_DEVICES'] = str(0)
    MODEL_DIR = '/media/newssd/pytorch-benchmarks/models/fer+'
    model_name = 'resnet50_ferplus_dag'
    model = load_model(model_name, MODEL_DIR)
    try:
        device = torch.device(""cuda"")
        model = model.to(device)
    except:
        torch.cuda.set_device(0)
        model = model.cuda()
    DATA_DIR = parsed.data_root
    annot_dir = '/media/newssd/Aff-Wild_experiments/annotations'
    video_names = os.listdir(DATA_DIR)
    meta = model.meta
    preproc_transforms = compose_transforms(meta, center_crop=False) if not parsed.augment else augment_transforms(meta, random_crop=True)

    dataset = Face_Dataset(DATA_DIR, annot_dir, video_names, label_name='arousal', test_mode=False if 'train' in DATA_DIR else True, transform=preproc_transforms)
    data_loader = torch.utils.data.DataLoader(
        dataset, 
        batch_size = 4, 
        num_workers=0, pin_memory=False )
    des_dir = pjoin(parsed.save_root, '_'.join(['{}_features'.format(model_name), 'fps='+str(parsed.fps), parsed.layer_name]))
    if not os.path.exists(des_dir):
        os.makedirs(des_dir)
    feature_extraction(model, data_loader, des_dir)
if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description='Run.')           
    parser.add_argument('--refresh', dest='refresh', action='store_true',
                        help='refresh feature cache') 
    parser.add_argument('--fps',type=int, default = 30, 
                        help='frames per second to extract') 
    parser.add_argument('--layer_name',type=str, default = 'pool5_7x7_s1', 
                        help='frames per second to extract') 
    parser.add_argument('--augment', action=""store_true"", 
                        help='whether to extract augmented features for train set only ') 

    parser.add_argument('--save_root', type=str, default='')
    parser.add_argument('--data_root', type=str, default='')
    parser.set_defaults(refresh=False)
    parsed = parser.parse_args()
    
    main()
"
scripts/readme,"
"
